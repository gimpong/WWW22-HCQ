Experiment directory: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet
Preparing the dataloaders ...
Loading dataset ActivityNet_val1_trainval in ram ...
Finish loading dataset ActivityNet_val1_trainval in ram, taking 387.1994411945343 s.
Loading dataset ActivityNet_val1_test in ram ...
Finish loading dataset ActivityNet_val1_test in ram, taking 219.55244636535645 s.
Loading dataset ActivityNet_val1_test in ram ...
Finish loading dataset ActivityNet_val1_test in ram, taking 202.65831422805786 s.
Training ...
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch0.pth ...
Done in 2.358s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch0.pth ...
Done in 4.459s
 epoch          : 0
 loss           : 0
 learning_rate  : 5e-05
 n_samples      : 0
 n_steps        : 0
 ActivityNet_val1_test/t2v_metrics/R1: 0.04067520846044336
 ActivityNet_val1_test/t2v_metrics/R5: 0.16270083384177345
 ActivityNet_val1_test/t2v_metrics/R10: 0.26438885499288184
 ActivityNet_val1_test/t2v_metrics/R50: 0.9965426072808623
 ActivityNet_val1_test/t2v_metrics/MedR: 2433.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 2456.781269066504
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 0.12050014514571365
 ActivityNet_val1_test/v2t_metrics/R1: 0.02033760423022168
 ActivityNet_val1_test/v2t_metrics/R5: 0.14236322961155176
 ActivityNet_val1_test/v2t_metrics/R10: 0.24405125076266015
 ActivityNet_val1_test/v2t_metrics/R50: 1.0575554199715274
 ActivityNet_val1_test/v2t_metrics/MedR: 2434.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 2452.2141549725443
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 0.08906892698572073
 mnt_best       : 0.12050014514571365
 not_improved_count: 0
Train Epoch: 1 [1/1000 32/32000 (0%)] Loss: 2.51016 (semantic_loss: 0.56245, quant_loss: 1.94727, bit_balance_loss: 0.00044) batch_time=23.49631 
Train Epoch: 1 [6/1000 192/32000 (1%)] Loss: 2.11048 (semantic_loss: 0.15980, quant_loss: 1.95020, bit_balance_loss: 0.00048) batch_time=0.19613 
Train Epoch: 1 [11/1000 352/32000 (1%)] Loss: 2.09199 (semantic_loss: 0.14321, quant_loss: 1.94824, bit_balance_loss: 0.00054) batch_time=0.19933 
Train Epoch: 1 [16/1000 512/32000 (2%)] Loss: 2.09021 (semantic_loss: 0.14044, quant_loss: 1.94922, bit_balance_loss: 0.00055) batch_time=0.22934 
Train Epoch: 1 [21/1000 672/32000 (2%)] Loss: 2.08738 (semantic_loss: 0.13955, quant_loss: 1.94727, bit_balance_loss: 0.00057) batch_time=0.38902 
Train Epoch: 1 [26/1000 832/32000 (3%)] Loss: 2.08806 (semantic_loss: 0.13925, quant_loss: 1.94824, bit_balance_loss: 0.00056) batch_time=0.24687 
Train Epoch: 1 [31/1000 992/32000 (3%)] Loss: 2.08709 (semantic_loss: 0.13926, quant_loss: 1.94727, bit_balance_loss: 0.00057) batch_time=0.24341 
Train Epoch: 1 [36/1000 1152/32000 (4%)] Loss: 2.08804 (semantic_loss: 0.13923, quant_loss: 1.94824, bit_balance_loss: 0.00057) batch_time=0.22368 
Train Epoch: 1 [41/1000 1312/32000 (4%)] Loss: 2.08817 (semantic_loss: 0.13936, quant_loss: 1.94824, bit_balance_loss: 0.00057) batch_time=0.21173 
Train Epoch: 1 [46/1000 1472/32000 (5%)] Loss: 2.08792 (semantic_loss: 0.13911, quant_loss: 1.94824, bit_balance_loss: 0.00057) batch_time=0.21316 
Train Epoch: 1 [51/1000 1632/32000 (5%)] Loss: 2.08798 (semantic_loss: 0.13917, quant_loss: 1.94824, bit_balance_loss: 0.00057) batch_time=0.23860 
Train Epoch: 1 [56/1000 1792/32000 (6%)] Loss: 2.08880 (semantic_loss: 0.13902, quant_loss: 1.94922, bit_balance_loss: 0.00056) batch_time=0.19897 
Train Epoch: 1 [61/1000 1952/32000 (6%)] Loss: 2.08673 (semantic_loss: 0.13890, quant_loss: 1.94727, bit_balance_loss: 0.00056) batch_time=0.20517 
Train Epoch: 1 [66/1000 2112/32000 (7%)] Loss: 2.08662 (semantic_loss: 0.13879, quant_loss: 1.94727, bit_balance_loss: 0.00056) batch_time=0.31912 
Train Epoch: 1 [71/1000 2272/32000 (7%)] Loss: 2.08666 (semantic_loss: 0.13883, quant_loss: 1.94727, bit_balance_loss: 0.00056) batch_time=0.19466 
Train Epoch: 1 [76/1000 2432/32000 (8%)] Loss: 2.08866 (semantic_loss: 0.13888, quant_loss: 1.94922, bit_balance_loss: 0.00055) batch_time=0.20918 
Train Epoch: 1 [81/1000 2592/32000 (8%)] Loss: 2.08843 (semantic_loss: 0.13865, quant_loss: 1.94922, bit_balance_loss: 0.00055) batch_time=0.19873 
Train Epoch: 1 [86/1000 2752/32000 (9%)] Loss: 2.08972 (semantic_loss: 0.13801, quant_loss: 1.95117, bit_balance_loss: 0.00054) batch_time=0.20697 
Train Epoch: 1 [91/1000 2912/32000 (9%)] Loss: 2.08779 (semantic_loss: 0.13803, quant_loss: 1.94922, bit_balance_loss: 0.00054) batch_time=0.21246 
Train Epoch: 1 [96/1000 3072/32000 (10%)] Loss: 2.08618 (semantic_loss: 0.13741, quant_loss: 1.94824, bit_balance_loss: 0.00053) batch_time=0.22608 
Train Epoch: 1 [101/1000 3232/32000 (10%)] Loss: 2.08689 (semantic_loss: 0.13716, quant_loss: 1.94922, bit_balance_loss: 0.00052) batch_time=0.19460 
Train Epoch: 1 [106/1000 3392/32000 (11%)] Loss: 2.08463 (semantic_loss: 0.13588, quant_loss: 1.94824, bit_balance_loss: 0.00051) batch_time=0.19343 
Train Epoch: 1 [111/1000 3552/32000 (11%)] Loss: 2.08138 (semantic_loss: 0.13264, quant_loss: 1.94824, bit_balance_loss: 0.00049) batch_time=0.22356 
Train Epoch: 1 [116/1000 3712/32000 (12%)] Loss: 2.08258 (semantic_loss: 0.13385, quant_loss: 1.94824, bit_balance_loss: 0.00048) batch_time=0.21634 
Train Epoch: 1 [121/1000 3872/32000 (12%)] Loss: 2.07981 (semantic_loss: 0.13014, quant_loss: 1.94922, bit_balance_loss: 0.00045) batch_time=0.19240 
Train Epoch: 1 [126/1000 4032/32000 (13%)] Loss: 2.07901 (semantic_loss: 0.12935, quant_loss: 1.94922, bit_balance_loss: 0.00044) batch_time=0.21615 
Train Epoch: 1 [131/1000 4192/32000 (13%)] Loss: 2.07975 (semantic_loss: 0.13204, quant_loss: 1.94727, bit_balance_loss: 0.00045) batch_time=0.19288 
Train Epoch: 1 [136/1000 4352/32000 (14%)] Loss: 2.07452 (semantic_loss: 0.12487, quant_loss: 1.94922, bit_balance_loss: 0.00043) batch_time=0.61972 
Train Epoch: 1 [141/1000 4512/32000 (14%)] Loss: 2.07119 (semantic_loss: 0.12252, quant_loss: 1.94824, bit_balance_loss: 0.00042) batch_time=0.22702 
Train Epoch: 1 [146/1000 4672/32000 (15%)] Loss: 2.07501 (semantic_loss: 0.12635, quant_loss: 1.94824, bit_balance_loss: 0.00042) batch_time=0.19410 
Train Epoch: 1 [151/1000 4832/32000 (15%)] Loss: 2.07415 (semantic_loss: 0.12355, quant_loss: 1.95020, bit_balance_loss: 0.00040) batch_time=0.19245 
Train Epoch: 1 [156/1000 4992/32000 (16%)] Loss: 2.06994 (semantic_loss: 0.12033, quant_loss: 1.94922, bit_balance_loss: 0.00039) batch_time=0.20601 
Train Epoch: 1 [161/1000 5152/32000 (16%)] Loss: 2.06872 (semantic_loss: 0.11814, quant_loss: 1.95020, bit_balance_loss: 0.00039) batch_time=0.41972 
Train Epoch: 1 [166/1000 5312/32000 (17%)] Loss: 2.06701 (semantic_loss: 0.11838, quant_loss: 1.94824, bit_balance_loss: 0.00038) batch_time=0.24084 
Train Epoch: 1 [171/1000 5472/32000 (17%)] Loss: 2.07268 (semantic_loss: 0.12406, quant_loss: 1.94824, bit_balance_loss: 0.00038) batch_time=0.21877 
Train Epoch: 1 [176/1000 5632/32000 (18%)] Loss: 2.06488 (semantic_loss: 0.11627, quant_loss: 1.94824, bit_balance_loss: 0.00037) batch_time=0.24329 
Train Epoch: 1 [181/1000 5792/32000 (18%)] Loss: 2.07262 (semantic_loss: 0.12303, quant_loss: 1.94922, bit_balance_loss: 0.00038) batch_time=0.23078 
Train Epoch: 1 [186/1000 5952/32000 (19%)] Loss: 2.06497 (semantic_loss: 0.11635, quant_loss: 1.94824, bit_balance_loss: 0.00038) batch_time=0.22166 
Train Epoch: 1 [191/1000 6112/32000 (19%)] Loss: 2.06152 (semantic_loss: 0.11289, quant_loss: 1.94824, bit_balance_loss: 0.00038) batch_time=0.22062 
Train Epoch: 1 [196/1000 6272/32000 (20%)] Loss: 2.06177 (semantic_loss: 0.11316, quant_loss: 1.94824, bit_balance_loss: 0.00037) batch_time=0.21511 
Train Epoch: 1 [201/1000 6432/32000 (20%)] Loss: 2.05773 (semantic_loss: 0.10815, quant_loss: 1.94922, bit_balance_loss: 0.00036) batch_time=0.20655 
Train Epoch: 1 [206/1000 6592/32000 (21%)] Loss: 2.05655 (semantic_loss: 0.10795, quant_loss: 1.94824, bit_balance_loss: 0.00035) batch_time=0.20158 
Train Epoch: 1 [211/1000 6752/32000 (21%)] Loss: 2.05613 (semantic_loss: 0.10753, quant_loss: 1.94824, bit_balance_loss: 0.00036) batch_time=0.20135 
Train Epoch: 1 [216/1000 6912/32000 (22%)] Loss: 2.05576 (semantic_loss: 0.10716, quant_loss: 1.94824, bit_balance_loss: 0.00036) batch_time=0.20172 
Train Epoch: 1 [221/1000 7072/32000 (22%)] Loss: 2.05739 (semantic_loss: 0.10782, quant_loss: 1.94922, bit_balance_loss: 0.00035) batch_time=0.19324 
Train Epoch: 1 [226/1000 7232/32000 (23%)] Loss: 2.05598 (semantic_loss: 0.10740, quant_loss: 1.94824, bit_balance_loss: 0.00034) batch_time=0.63769 
Train Epoch: 1 [231/1000 7392/32000 (23%)] Loss: 2.05294 (semantic_loss: 0.10339, quant_loss: 1.94922, bit_balance_loss: 0.00034) batch_time=0.22010 
Train Epoch: 1 [236/1000 7552/32000 (24%)] Loss: 2.05609 (semantic_loss: 0.10848, quant_loss: 1.94727, bit_balance_loss: 0.00034) batch_time=0.20476 
Train Epoch: 1 [241/1000 7712/32000 (24%)] Loss: 2.05070 (semantic_loss: 0.10211, quant_loss: 1.94824, bit_balance_loss: 0.00035) batch_time=0.20528 
Train Epoch: 1 [246/1000 7872/32000 (25%)] Loss: 2.04502 (semantic_loss: 0.09644, quant_loss: 1.94824, bit_balance_loss: 0.00033) batch_time=0.19984 
Train Epoch: 1 [251/1000 8032/32000 (25%)] Loss: 2.04146 (semantic_loss: 0.09290, quant_loss: 1.94824, bit_balance_loss: 0.00032) batch_time=0.19374 
Train Epoch: 1 [256/1000 8192/32000 (26%)] Loss: 2.04551 (semantic_loss: 0.09793, quant_loss: 1.94727, bit_balance_loss: 0.00032) batch_time=0.19100 
Train Epoch: 1 [261/1000 8352/32000 (26%)] Loss: 2.04813 (semantic_loss: 0.09859, quant_loss: 1.94922, bit_balance_loss: 0.00032) batch_time=0.19255 
Train Epoch: 1 [266/1000 8512/32000 (27%)] Loss: 2.04544 (semantic_loss: 0.09786, quant_loss: 1.94727, bit_balance_loss: 0.00031) batch_time=0.19746 
Train Epoch: 1 [271/1000 8672/32000 (27%)] Loss: 2.04087 (semantic_loss: 0.09329, quant_loss: 1.94727, bit_balance_loss: 0.00031) batch_time=0.19554 
Train Epoch: 1 [276/1000 8832/32000 (28%)] Loss: 2.04646 (semantic_loss: 0.09693, quant_loss: 1.94922, bit_balance_loss: 0.00032) batch_time=0.19374 
Train Epoch: 1 [281/1000 8992/32000 (28%)] Loss: 2.05073 (semantic_loss: 0.10218, quant_loss: 1.94824, bit_balance_loss: 0.00030) batch_time=0.22202 
Train Epoch: 1 [286/1000 9152/32000 (29%)] Loss: 2.04001 (semantic_loss: 0.09049, quant_loss: 1.94922, bit_balance_loss: 0.00030) batch_time=0.20045 
Train Epoch: 1 [291/1000 9312/32000 (29%)] Loss: 2.03960 (semantic_loss: 0.09105, quant_loss: 1.94824, bit_balance_loss: 0.00030) batch_time=0.20336 
Train Epoch: 1 [296/1000 9472/32000 (30%)] Loss: 2.04242 (semantic_loss: 0.09388, quant_loss: 1.94824, bit_balance_loss: 0.00030) batch_time=0.20128 
Train Epoch: 1 [301/1000 9632/32000 (30%)] Loss: 2.03794 (semantic_loss: 0.08939, quant_loss: 1.94824, bit_balance_loss: 0.00030) batch_time=0.18895 
Train Epoch: 1 [306/1000 9792/32000 (31%)] Loss: 2.03451 (semantic_loss: 0.08694, quant_loss: 1.94727, bit_balance_loss: 0.00030) batch_time=0.23127 
Train Epoch: 1 [311/1000 9952/32000 (31%)] Loss: 2.04748 (semantic_loss: 0.09894, quant_loss: 1.94824, bit_balance_loss: 0.00030) batch_time=0.20888 
Train Epoch: 1 [316/1000 10112/32000 (32%)] Loss: 2.04163 (semantic_loss: 0.09407, quant_loss: 1.94727, bit_balance_loss: 0.00030) batch_time=0.21066 
Train Epoch: 1 [321/1000 10272/32000 (32%)] Loss: 2.03229 (semantic_loss: 0.08279, quant_loss: 1.94922, bit_balance_loss: 0.00028) batch_time=0.25362 
Train Epoch: 1 [326/1000 10432/32000 (33%)] Loss: 2.03644 (semantic_loss: 0.08791, quant_loss: 1.94824, bit_balance_loss: 0.00028) batch_time=0.22702 
Train Epoch: 1 [331/1000 10592/32000 (33%)] Loss: 2.03278 (semantic_loss: 0.08425, quant_loss: 1.94824, bit_balance_loss: 0.00028) batch_time=0.26468 
Train Epoch: 1 [336/1000 10752/32000 (34%)] Loss: 2.03383 (semantic_loss: 0.08531, quant_loss: 1.94824, bit_balance_loss: 0.00028) batch_time=0.28227 
Train Epoch: 1 [341/1000 10912/32000 (34%)] Loss: 2.03356 (semantic_loss: 0.08406, quant_loss: 1.94922, bit_balance_loss: 0.00028) batch_time=0.38060 
Train Epoch: 1 [346/1000 11072/32000 (35%)] Loss: 2.02677 (semantic_loss: 0.07923, quant_loss: 1.94727, bit_balance_loss: 0.00028) batch_time=0.22305 
Train Epoch: 1 [351/1000 11232/32000 (35%)] Loss: 2.03305 (semantic_loss: 0.08453, quant_loss: 1.94824, bit_balance_loss: 0.00028) batch_time=0.20379 
Train Epoch: 1 [356/1000 11392/32000 (36%)] Loss: 2.02437 (semantic_loss: 0.07683, quant_loss: 1.94727, bit_balance_loss: 0.00027) batch_time=0.20176 
Train Epoch: 1 [361/1000 11552/32000 (36%)] Loss: 2.02505 (semantic_loss: 0.07655, quant_loss: 1.94824, bit_balance_loss: 0.00026) batch_time=0.19426 
Train Epoch: 1 [366/1000 11712/32000 (37%)] Loss: 2.03177 (semantic_loss: 0.08424, quant_loss: 1.94727, bit_balance_loss: 0.00027) batch_time=0.19621 
Train Epoch: 1 [371/1000 11872/32000 (37%)] Loss: 2.03023 (semantic_loss: 0.08270, quant_loss: 1.94727, bit_balance_loss: 0.00027) batch_time=0.19620 
Train Epoch: 1 [376/1000 12032/32000 (38%)] Loss: 2.01684 (semantic_loss: 0.06931, quant_loss: 1.94727, bit_balance_loss: 0.00026) batch_time=0.19396 
Train Epoch: 1 [381/1000 12192/32000 (38%)] Loss: 2.02107 (semantic_loss: 0.07354, quant_loss: 1.94727, bit_balance_loss: 0.00026) batch_time=0.19650 
Train Epoch: 1 [386/1000 12352/32000 (39%)] Loss: 2.02911 (semantic_loss: 0.08157, quant_loss: 1.94727, bit_balance_loss: 0.00027) batch_time=0.32539 
Train Epoch: 1 [391/1000 12512/32000 (39%)] Loss: 2.02472 (semantic_loss: 0.07524, quant_loss: 1.94922, bit_balance_loss: 0.00027) batch_time=0.19970 
Train Epoch: 1 [396/1000 12672/32000 (40%)] Loss: 2.02404 (semantic_loss: 0.07750, quant_loss: 1.94629, bit_balance_loss: 0.00026) batch_time=0.19116 
Train Epoch: 1 [401/1000 12832/32000 (40%)] Loss: 2.02314 (semantic_loss: 0.07563, quant_loss: 1.94727, bit_balance_loss: 0.00025) batch_time=0.20855 
Train Epoch: 1 [406/1000 12992/32000 (41%)] Loss: 2.02619 (semantic_loss: 0.07770, quant_loss: 1.94824, bit_balance_loss: 0.00025) batch_time=0.20205 
Train Epoch: 1 [411/1000 13152/32000 (41%)] Loss: 2.02288 (semantic_loss: 0.07536, quant_loss: 1.94727, bit_balance_loss: 0.00025) batch_time=0.19416 
Train Epoch: 1 [416/1000 13312/32000 (42%)] Loss: 2.02406 (semantic_loss: 0.07655, quant_loss: 1.94727, bit_balance_loss: 0.00024) batch_time=0.20641 
Train Epoch: 1 [421/1000 13472/32000 (42%)] Loss: 2.01009 (semantic_loss: 0.06258, quant_loss: 1.94727, bit_balance_loss: 0.00025) batch_time=0.19425 
Train Epoch: 1 [426/1000 13632/32000 (43%)] Loss: 2.01386 (semantic_loss: 0.06537, quant_loss: 1.94824, bit_balance_loss: 0.00024) batch_time=0.19376 
Train Epoch: 1 [431/1000 13792/32000 (43%)] Loss: 2.01797 (semantic_loss: 0.07045, quant_loss: 1.94727, bit_balance_loss: 0.00025) batch_time=0.22836 
Train Epoch: 1 [436/1000 13952/32000 (44%)] Loss: 2.01490 (semantic_loss: 0.06740, quant_loss: 1.94727, bit_balance_loss: 0.00024) batch_time=0.19343 
Train Epoch: 1 [441/1000 14112/32000 (44%)] Loss: 2.02392 (semantic_loss: 0.07642, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.19258 
Train Epoch: 1 [446/1000 14272/32000 (45%)] Loss: 2.01830 (semantic_loss: 0.07078, quant_loss: 1.94727, bit_balance_loss: 0.00025) batch_time=0.23886 
Train Epoch: 1 [451/1000 14432/32000 (45%)] Loss: 2.01766 (semantic_loss: 0.06820, quant_loss: 1.94922, bit_balance_loss: 0.00024) batch_time=0.19999 
Train Epoch: 1 [456/1000 14592/32000 (46%)] Loss: 2.01866 (semantic_loss: 0.06921, quant_loss: 1.94922, bit_balance_loss: 0.00023) batch_time=0.68335 
Train Epoch: 1 [461/1000 14752/32000 (46%)] Loss: 2.01459 (semantic_loss: 0.06613, quant_loss: 1.94824, bit_balance_loss: 0.00023) batch_time=0.26339 
Train Epoch: 1 [466/1000 14912/32000 (47%)] Loss: 2.00743 (semantic_loss: 0.05895, quant_loss: 1.94824, bit_balance_loss: 0.00023) batch_time=0.22162 
Train Epoch: 1 [471/1000 15072/32000 (47%)] Loss: 2.02628 (semantic_loss: 0.07585, quant_loss: 1.95020, bit_balance_loss: 0.00023) batch_time=0.28601 
Train Epoch: 1 [476/1000 15232/32000 (48%)] Loss: 2.01665 (semantic_loss: 0.06915, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.22820 
Train Epoch: 1 [481/1000 15392/32000 (48%)] Loss: 2.01946 (semantic_loss: 0.07098, quant_loss: 1.94824, bit_balance_loss: 0.00023) batch_time=0.51663 
Train Epoch: 1 [486/1000 15552/32000 (49%)] Loss: 2.01514 (semantic_loss: 0.06765, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21351 
Train Epoch: 1 [491/1000 15712/32000 (49%)] Loss: 2.01580 (semantic_loss: 0.06733, quant_loss: 1.94824, bit_balance_loss: 0.00023) batch_time=0.20024 
Train Epoch: 1 [496/1000 15872/32000 (50%)] Loss: 2.02172 (semantic_loss: 0.07227, quant_loss: 1.94922, bit_balance_loss: 0.00023) batch_time=0.19553 
Train Epoch: 1 [501/1000 16032/32000 (50%)] Loss: 2.01260 (semantic_loss: 0.06413, quant_loss: 1.94824, bit_balance_loss: 0.00023) batch_time=0.19264 
Train Epoch: 1 [506/1000 16192/32000 (51%)] Loss: 2.01603 (semantic_loss: 0.06855, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20086 
Train Epoch: 1 [511/1000 16352/32000 (51%)] Loss: 2.01528 (semantic_loss: 0.06779, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21486 
Train Epoch: 1 [516/1000 16512/32000 (52%)] Loss: 2.01038 (semantic_loss: 0.06190, quant_loss: 1.94824, bit_balance_loss: 0.00023) batch_time=0.19316 
Train Epoch: 1 [521/1000 16672/32000 (52%)] Loss: 2.01242 (semantic_loss: 0.06298, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.20226 
Train Epoch: 1 [526/1000 16832/32000 (53%)] Loss: 2.01624 (semantic_loss: 0.06681, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.21656 
Train Epoch: 1 [531/1000 16992/32000 (53%)] Loss: 2.01199 (semantic_loss: 0.06351, quant_loss: 1.94824, bit_balance_loss: 0.00023) batch_time=0.21527 
Train Epoch: 1 [536/1000 17152/32000 (54%)] Loss: 2.00851 (semantic_loss: 0.06101, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.21073 
Train Epoch: 1 [541/1000 17312/32000 (54%)] Loss: 2.01061 (semantic_loss: 0.06117, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.20166 
Train Epoch: 1 [546/1000 17472/32000 (55%)] Loss: 2.00505 (semantic_loss: 0.05659, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.62547 
Train Epoch: 1 [551/1000 17632/32000 (55%)] Loss: 2.00606 (semantic_loss: 0.05858, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19261 
Train Epoch: 1 [556/1000 17792/32000 (56%)] Loss: 2.01276 (semantic_loss: 0.06332, quant_loss: 1.94922, bit_balance_loss: 0.00023) batch_time=0.19512 
Train Epoch: 1 [561/1000 17952/32000 (56%)] Loss: 2.00847 (semantic_loss: 0.05903, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.19318 
Train Epoch: 1 [566/1000 18112/32000 (57%)] Loss: 2.00556 (semantic_loss: 0.05711, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20449 
Train Epoch: 1 [571/1000 18272/32000 (57%)] Loss: 2.00794 (semantic_loss: 0.05851, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.19338 
Train Epoch: 1 [576/1000 18432/32000 (58%)] Loss: 2.01066 (semantic_loss: 0.06317, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.19303 
Train Epoch: 1 [581/1000 18592/32000 (58%)] Loss: 2.01324 (semantic_loss: 0.06380, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.19301 
Train Epoch: 1 [586/1000 18752/32000 (59%)] Loss: 2.00496 (semantic_loss: 0.05748, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21686 
Train Epoch: 1 [591/1000 18912/32000 (59%)] Loss: 2.01459 (semantic_loss: 0.06516, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.19703 
Train Epoch: 1 [596/1000 19072/32000 (60%)] Loss: 2.01220 (semantic_loss: 0.06277, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.20707 
Train Epoch: 1 [601/1000 19232/32000 (60%)] Loss: 2.01176 (semantic_loss: 0.06427, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.24311 
Train Epoch: 1 [606/1000 19392/32000 (61%)] Loss: 2.00734 (semantic_loss: 0.05986, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.23375 
Train Epoch: 1 [611/1000 19552/32000 (61%)] Loss: 2.01022 (semantic_loss: 0.06175, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.24941 
Train Epoch: 1 [616/1000 19712/32000 (62%)] Loss: 2.00647 (semantic_loss: 0.05801, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.24868 
Train Epoch: 1 [621/1000 19872/32000 (62%)] Loss: 2.01029 (semantic_loss: 0.06280, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.25030 
Train Epoch: 1 [626/1000 20032/32000 (63%)] Loss: 2.01320 (semantic_loss: 0.06474, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.26220 
Train Epoch: 1 [631/1000 20192/32000 (63%)] Loss: 2.00714 (semantic_loss: 0.05966, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20316 
Train Epoch: 1 [636/1000 20352/32000 (64%)] Loss: 2.00674 (semantic_loss: 0.05927, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21917 
Train Epoch: 1 [641/1000 20512/32000 (64%)] Loss: 2.01273 (semantic_loss: 0.06330, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.20127 
Train Epoch: 1 [646/1000 20672/32000 (65%)] Loss: 2.00044 (semantic_loss: 0.05296, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19346 
Train Epoch: 1 [651/1000 20832/32000 (65%)] Loss: 2.00125 (semantic_loss: 0.05377, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19385 
Train Epoch: 1 [656/1000 20992/32000 (66%)] Loss: 2.01647 (semantic_loss: 0.06800, quant_loss: 1.94824, bit_balance_loss: 0.00023) batch_time=0.19593 
Train Epoch: 1 [661/1000 21152/32000 (66%)] Loss: 2.01103 (semantic_loss: 0.06257, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20834 
Train Epoch: 1 [666/1000 21312/32000 (67%)] Loss: 2.00592 (semantic_loss: 0.05845, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20632 
Train Epoch: 1 [671/1000 21472/32000 (67%)] Loss: 1.99670 (semantic_loss: 0.04923, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20653 
Train Epoch: 1 [676/1000 21632/32000 (68%)] Loss: 2.00281 (semantic_loss: 0.05533, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20530 
Train Epoch: 1 [681/1000 21792/32000 (68%)] Loss: 1.99966 (semantic_loss: 0.05218, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20114 
Train Epoch: 1 [686/1000 21952/32000 (69%)] Loss: 2.01096 (semantic_loss: 0.06249, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19567 
Train Epoch: 1 [691/1000 22112/32000 (69%)] Loss: 2.00171 (semantic_loss: 0.05521, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20002 
Train Epoch: 1 [696/1000 22272/32000 (70%)] Loss: 2.00323 (semantic_loss: 0.05380, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.21947 
Train Epoch: 1 [701/1000 22432/32000 (70%)] Loss: 2.01041 (semantic_loss: 0.06195, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19293 
Train Epoch: 1 [706/1000 22592/32000 (71%)] Loss: 2.01315 (semantic_loss: 0.06469, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.31966 
Train Epoch: 1 [711/1000 22752/32000 (71%)] Loss: 2.00381 (semantic_loss: 0.05438, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.47598 
Train Epoch: 1 [716/1000 22912/32000 (72%)] Loss: 2.00662 (semantic_loss: 0.05914, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19281 
Train Epoch: 1 [721/1000 23072/32000 (72%)] Loss: 2.00397 (semantic_loss: 0.05649, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19435 
Train Epoch: 1 [726/1000 23232/32000 (73%)] Loss: 2.00387 (semantic_loss: 0.05835, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19935 
Train Epoch: 1 [731/1000 23392/32000 (73%)] Loss: 2.00413 (semantic_loss: 0.05372, quant_loss: 1.95020, bit_balance_loss: 0.00021) batch_time=0.20457 
Train Epoch: 1 [736/1000 23552/32000 (74%)] Loss: 2.00424 (semantic_loss: 0.05676, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20011 
Train Epoch: 1 [741/1000 23712/32000 (74%)] Loss: 2.00134 (semantic_loss: 0.05287, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.23954 
Train Epoch: 1 [746/1000 23872/32000 (75%)] Loss: 1.99451 (semantic_loss: 0.04704, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22339 
Train Epoch: 1 [751/1000 24032/32000 (75%)] Loss: 1.99586 (semantic_loss: 0.04741, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22898 
Train Epoch: 1 [756/1000 24192/32000 (76%)] Loss: 1.99806 (semantic_loss: 0.04764, quant_loss: 1.95020, bit_balance_loss: 0.00022) batch_time=0.20575 
Train Epoch: 1 [761/1000 24352/32000 (76%)] Loss: 2.00180 (semantic_loss: 0.05237, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.21464 
Train Epoch: 1 [766/1000 24512/32000 (77%)] Loss: 2.00333 (semantic_loss: 0.05585, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22663 
Train Epoch: 1 [771/1000 24672/32000 (77%)] Loss: 2.00216 (semantic_loss: 0.05467, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=1.64782 
Train Epoch: 1 [776/1000 24832/32000 (78%)] Loss: 1.99812 (semantic_loss: 0.05064, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19457 
Train Epoch: 1 [781/1000 24992/32000 (78%)] Loss: 2.00828 (semantic_loss: 0.05982, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19292 
Train Epoch: 1 [786/1000 25152/32000 (79%)] Loss: 2.00971 (semantic_loss: 0.06222, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.19987 
Train Epoch: 1 [791/1000 25312/32000 (79%)] Loss: 1.99451 (semantic_loss: 0.04703, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21212 
Train Epoch: 1 [796/1000 25472/32000 (80%)] Loss: 2.00380 (semantic_loss: 0.05436, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.19457 
Train Epoch: 1 [801/1000 25632/32000 (80%)] Loss: 1.99461 (semantic_loss: 0.04518, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.20780 
Train Epoch: 1 [806/1000 25792/32000 (81%)] Loss: 2.00224 (semantic_loss: 0.05378, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19826 
Train Epoch: 1 [811/1000 25952/32000 (81%)] Loss: 2.00386 (semantic_loss: 0.05540, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20212 
Train Epoch: 1 [816/1000 26112/32000 (82%)] Loss: 2.00222 (semantic_loss: 0.05377, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22684 
Train Epoch: 1 [821/1000 26272/32000 (82%)] Loss: 1.99974 (semantic_loss: 0.05226, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21134 
Train Epoch: 1 [826/1000 26432/32000 (83%)] Loss: 1.99741 (semantic_loss: 0.04993, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19946 
Train Epoch: 1 [831/1000 26592/32000 (83%)] Loss: 2.00087 (semantic_loss: 0.05339, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19990 
Train Epoch: 1 [836/1000 26752/32000 (84%)] Loss: 1.99822 (semantic_loss: 0.05074, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19074 
Train Epoch: 1 [841/1000 26912/32000 (84%)] Loss: 1.99809 (semantic_loss: 0.04963, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19489 
Train Epoch: 1 [846/1000 27072/32000 (85%)] Loss: 2.00115 (semantic_loss: 0.05172, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.19088 
Train Epoch: 1 [851/1000 27232/32000 (85%)] Loss: 1.99470 (semantic_loss: 0.04624, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19026 
Train Epoch: 1 [856/1000 27392/32000 (86%)] Loss: 2.00662 (semantic_loss: 0.05719, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.19626 
Train Epoch: 1 [861/1000 27552/32000 (86%)] Loss: 2.01087 (semantic_loss: 0.06241, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19664 
Train Epoch: 1 [866/1000 27712/32000 (87%)] Loss: 2.00580 (semantic_loss: 0.05734, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20205 
Train Epoch: 1 [871/1000 27872/32000 (87%)] Loss: 1.99492 (semantic_loss: 0.04744, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19767 
Train Epoch: 1 [876/1000 28032/32000 (88%)] Loss: 1.99955 (semantic_loss: 0.05013, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.19419 
Train Epoch: 1 [881/1000 28192/32000 (88%)] Loss: 1.99731 (semantic_loss: 0.04982, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21765 
Train Epoch: 1 [886/1000 28352/32000 (89%)] Loss: 2.00415 (semantic_loss: 0.05569, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20266 
Train Epoch: 1 [891/1000 28512/32000 (89%)] Loss: 2.00092 (semantic_loss: 0.05247, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21297 
Train Epoch: 1 [896/1000 28672/32000 (90%)] Loss: 2.00227 (semantic_loss: 0.05381, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.22472 
Train Epoch: 1 [901/1000 28832/32000 (90%)] Loss: 2.00135 (semantic_loss: 0.05191, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.20821 
Train Epoch: 1 [906/1000 28992/32000 (91%)] Loss: 1.99337 (semantic_loss: 0.04589, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21029 
Train Epoch: 1 [911/1000 29152/32000 (91%)] Loss: 1.99719 (semantic_loss: 0.04972, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23076 
Train Epoch: 1 [916/1000 29312/32000 (92%)] Loss: 2.00092 (semantic_loss: 0.05246, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.23734 
Train Epoch: 1 [921/1000 29472/32000 (92%)] Loss: 1.99250 (semantic_loss: 0.04600, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19722 
Train Epoch: 1 [926/1000 29632/32000 (93%)] Loss: 2.00018 (semantic_loss: 0.05172, quant_loss: 1.94824, bit_balance_loss: 0.00023) batch_time=0.20688 
Train Epoch: 1 [931/1000 29792/32000 (93%)] Loss: 1.99665 (semantic_loss: 0.05014, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19488 
Train Epoch: 1 [936/1000 29952/32000 (94%)] Loss: 1.99852 (semantic_loss: 0.05103, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.19408 
Train Epoch: 1 [941/1000 30112/32000 (94%)] Loss: 1.99884 (semantic_loss: 0.05038, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19684 
Train Epoch: 1 [946/1000 30272/32000 (95%)] Loss: 1.99915 (semantic_loss: 0.05167, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20292 
Train Epoch: 1 [951/1000 30432/32000 (95%)] Loss: 1.99914 (semantic_loss: 0.04971, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.19815 
Train Epoch: 1 [956/1000 30592/32000 (96%)] Loss: 2.00084 (semantic_loss: 0.05238, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20010 
Train Epoch: 1 [961/1000 30752/32000 (96%)] Loss: 1.99685 (semantic_loss: 0.04839, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.26591 
Train Epoch: 1 [966/1000 30912/32000 (97%)] Loss: 1.99900 (semantic_loss: 0.05153, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19384 
Train Epoch: 1 [971/1000 31072/32000 (97%)] Loss: 1.99521 (semantic_loss: 0.04675, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19412 
Train Epoch: 1 [976/1000 31232/32000 (98%)] Loss: 2.00122 (semantic_loss: 0.05374, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19116 
Train Epoch: 1 [981/1000 31392/32000 (98%)] Loss: 1.99680 (semantic_loss: 0.04933, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19165 
Train Epoch: 1 [986/1000 31552/32000 (99%)] Loss: 1.99959 (semantic_loss: 0.05211, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19339 
Train Epoch: 1 [991/1000 31712/32000 (99%)] Loss: 1.99635 (semantic_loss: 0.04691, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.20242 
Train Epoch: 1 [996/1000 31872/32000 (100%)] Loss: 1.99737 (semantic_loss: 0.04989, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20141 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch1.pth ...
Done in 5.226s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch1.pth ...
Done in 10.038s
 epoch          : 1
 loss           : 2.0288533198833467
 learning_rate  : 5e-05
 n_samples      : 32000
 n_steps        : 1000
 ActivityNet_val1_test/t2v_metrics/R1: 3.1930038641448037
 ActivityNet_val1_test/t2v_metrics/R5: 14.256660565385397
 ActivityNet_val1_test/t2v_metrics/R10: 24.283099450884684
 ActivityNet_val1_test/t2v_metrics/R50: 64.18547895057962
 ActivityNet_val1_test/t2v_metrics/MedR: 30.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 79.25991458206224
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 10.339680618956063
 ActivityNet_val1_test/v2t_metrics/R1: 3.8031319910514543
 ActivityNet_val1_test/v2t_metrics/R5: 15.456579214968476
 ActivityNet_val1_test/v2t_metrics/R10: 25.360992475086434
 ActivityNet_val1_test/v2t_metrics/R50: 63.92109009558674
 ActivityNet_val1_test/v2t_metrics/MedR: 29.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 83.93675005084401
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 11.423705777723619
 mnt_best       : 10.339680618956063
 not_improved_count: 0
Train Epoch: 2 [1/1000 32/32000 (0%)] Loss: 1.99507 (semantic_loss: 0.04564, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=26.57726 
Train Epoch: 2 [6/1000 192/32000 (1%)] Loss: 2.00267 (semantic_loss: 0.05519, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20733 
Train Epoch: 2 [11/1000 352/32000 (1%)] Loss: 1.99780 (semantic_loss: 0.04933, quant_loss: 1.94824, bit_balance_loss: 0.00023) batch_time=0.20224 
Train Epoch: 2 [16/1000 512/32000 (2%)] Loss: 1.99723 (semantic_loss: 0.04877, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.46534 
Train Epoch: 2 [21/1000 672/32000 (2%)] Loss: 1.99008 (semantic_loss: 0.04163, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19353 
Train Epoch: 2 [26/1000 832/32000 (3%)] Loss: 1.99645 (semantic_loss: 0.04897, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19954 
Train Epoch: 2 [31/1000 992/32000 (3%)] Loss: 1.99260 (semantic_loss: 0.04414, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20104 
Train Epoch: 2 [36/1000 1152/32000 (4%)] Loss: 1.99240 (semantic_loss: 0.04395, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19773 
Train Epoch: 2 [41/1000 1312/32000 (4%)] Loss: 1.99131 (semantic_loss: 0.04481, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19642 
Train Epoch: 2 [46/1000 1472/32000 (5%)] Loss: 2.00215 (semantic_loss: 0.05272, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.22024 
Train Epoch: 2 [51/1000 1632/32000 (5%)] Loss: 1.99250 (semantic_loss: 0.04405, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19540 
Train Epoch: 2 [56/1000 1792/32000 (6%)] Loss: 1.99548 (semantic_loss: 0.04703, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19232 
Train Epoch: 2 [61/1000 1952/32000 (6%)] Loss: 1.99558 (semantic_loss: 0.04907, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19347 
Train Epoch: 2 [66/1000 2112/32000 (7%)] Loss: 1.99506 (semantic_loss: 0.04661, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19684 
Train Epoch: 2 [71/1000 2272/32000 (7%)] Loss: 1.99051 (semantic_loss: 0.04401, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23215 
Train Epoch: 2 [76/1000 2432/32000 (8%)] Loss: 1.99280 (semantic_loss: 0.04532, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22931 
Train Epoch: 2 [81/1000 2592/32000 (8%)] Loss: 1.99549 (semantic_loss: 0.04704, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.40386 
Train Epoch: 2 [86/1000 2752/32000 (9%)] Loss: 2.00141 (semantic_loss: 0.05392, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.24964 
Train Epoch: 2 [91/1000 2912/32000 (9%)] Loss: 1.99212 (semantic_loss: 0.04464, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21686 
Train Epoch: 2 [96/1000 3072/32000 (10%)] Loss: 2.00442 (semantic_loss: 0.05693, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21251 
Train Epoch: 2 [101/1000 3232/32000 (10%)] Loss: 1.99754 (semantic_loss: 0.04908, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20612 
Train Epoch: 2 [106/1000 3392/32000 (11%)] Loss: 1.99666 (semantic_loss: 0.04723, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.20818 
Train Epoch: 2 [111/1000 3552/32000 (11%)] Loss: 1.99386 (semantic_loss: 0.04443, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.28440 
Train Epoch: 2 [116/1000 3712/32000 (12%)] Loss: 1.99387 (semantic_loss: 0.04541, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19163 
Train Epoch: 2 [121/1000 3872/32000 (12%)] Loss: 1.98912 (semantic_loss: 0.03969, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.19645 
Train Epoch: 2 [126/1000 4032/32000 (13%)] Loss: 2.00426 (semantic_loss: 0.05580, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19232 
Train Epoch: 2 [131/1000 4192/32000 (13%)] Loss: 1.99399 (semantic_loss: 0.04651, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19417 
Train Epoch: 2 [136/1000 4352/32000 (14%)] Loss: 1.99695 (semantic_loss: 0.04850, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19958 
Train Epoch: 2 [141/1000 4512/32000 (14%)] Loss: 1.99098 (semantic_loss: 0.04351, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21658 
Train Epoch: 2 [146/1000 4672/32000 (15%)] Loss: 1.99375 (semantic_loss: 0.04530, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20528 
Train Epoch: 2 [151/1000 4832/32000 (15%)] Loss: 1.99993 (semantic_loss: 0.05148, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21966 
Train Epoch: 2 [156/1000 4992/32000 (16%)] Loss: 1.99101 (semantic_loss: 0.04353, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20128 
Train Epoch: 2 [161/1000 5152/32000 (16%)] Loss: 1.99420 (semantic_loss: 0.04477, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.19261 
Train Epoch: 2 [166/1000 5312/32000 (17%)] Loss: 1.99581 (semantic_loss: 0.04833, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.64328 
Train Epoch: 2 [171/1000 5472/32000 (17%)] Loss: 1.99559 (semantic_loss: 0.04811, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25575 
Train Epoch: 2 [176/1000 5632/32000 (18%)] Loss: 1.98709 (semantic_loss: 0.04059, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20163 
Train Epoch: 2 [181/1000 5792/32000 (18%)] Loss: 1.99454 (semantic_loss: 0.04706, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20165 
Train Epoch: 2 [186/1000 5952/32000 (19%)] Loss: 1.99825 (semantic_loss: 0.04978, quant_loss: 1.94824, bit_balance_loss: 0.00023) batch_time=0.20239 
Train Epoch: 2 [191/1000 6112/32000 (19%)] Loss: 1.99340 (semantic_loss: 0.04592, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19329 
Train Epoch: 2 [196/1000 6272/32000 (20%)] Loss: 1.99617 (semantic_loss: 0.04869, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21251 
Train Epoch: 2 [201/1000 6432/32000 (20%)] Loss: 1.99485 (semantic_loss: 0.04737, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21267 
Train Epoch: 2 [206/1000 6592/32000 (21%)] Loss: 1.99469 (semantic_loss: 0.04721, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22008 
Train Epoch: 2 [211/1000 6752/32000 (21%)] Loss: 1.99602 (semantic_loss: 0.04853, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.23487 
Train Epoch: 2 [216/1000 6912/32000 (22%)] Loss: 1.99932 (semantic_loss: 0.05086, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.27079 
Train Epoch: 2 [221/1000 7072/32000 (22%)] Loss: 1.99851 (semantic_loss: 0.05102, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22388 
Train Epoch: 2 [226/1000 7232/32000 (23%)] Loss: 2.00237 (semantic_loss: 0.05293, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.26137 
Train Epoch: 2 [231/1000 7392/32000 (23%)] Loss: 1.98911 (semantic_loss: 0.04261, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22894 
Train Epoch: 2 [236/1000 7552/32000 (24%)] Loss: 1.98925 (semantic_loss: 0.04080, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21712 
Train Epoch: 2 [241/1000 7712/32000 (24%)] Loss: 1.99135 (semantic_loss: 0.04290, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22865 
Train Epoch: 2 [246/1000 7872/32000 (25%)] Loss: 1.98736 (semantic_loss: 0.03988, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19709 
Train Epoch: 2 [251/1000 8032/32000 (25%)] Loss: 1.99591 (semantic_loss: 0.04842, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20934 
Train Epoch: 2 [256/1000 8192/32000 (26%)] Loss: 1.98956 (semantic_loss: 0.04207, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20868 
Train Epoch: 2 [261/1000 8352/32000 (26%)] Loss: 1.98782 (semantic_loss: 0.03937, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20368 
Train Epoch: 2 [266/1000 8512/32000 (27%)] Loss: 1.99285 (semantic_loss: 0.04440, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22139 
Train Epoch: 2 [271/1000 8672/32000 (27%)] Loss: 1.99108 (semantic_loss: 0.04262, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21608 
Train Epoch: 2 [276/1000 8832/32000 (28%)] Loss: 2.00249 (semantic_loss: 0.05501, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21901 
Train Epoch: 2 [281/1000 8992/32000 (28%)] Loss: 1.98960 (semantic_loss: 0.04114, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20624 
Train Epoch: 2 [286/1000 9152/32000 (29%)] Loss: 1.99261 (semantic_loss: 0.04513, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26564 
Train Epoch: 2 [291/1000 9312/32000 (29%)] Loss: 1.98873 (semantic_loss: 0.04125, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.47574 
Train Epoch: 2 [296/1000 9472/32000 (30%)] Loss: 1.99029 (semantic_loss: 0.04282, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19668 
Train Epoch: 2 [301/1000 9632/32000 (30%)] Loss: 1.98691 (semantic_loss: 0.03944, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19300 
Train Epoch: 2 [306/1000 9792/32000 (31%)] Loss: 1.99411 (semantic_loss: 0.04664, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19615 
Train Epoch: 2 [311/1000 9952/32000 (31%)] Loss: 1.99442 (semantic_loss: 0.04499, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.19755 
Train Epoch: 2 [316/1000 10112/32000 (32%)] Loss: 1.98711 (semantic_loss: 0.03964, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20816 
Train Epoch: 2 [321/1000 10272/32000 (32%)] Loss: 1.99428 (semantic_loss: 0.04680, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20796 
Train Epoch: 2 [326/1000 10432/32000 (33%)] Loss: 1.99354 (semantic_loss: 0.04606, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22245 
Train Epoch: 2 [331/1000 10592/32000 (33%)] Loss: 1.98603 (semantic_loss: 0.03855, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19270 
Train Epoch: 2 [336/1000 10752/32000 (34%)] Loss: 1.99406 (semantic_loss: 0.04657, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.41978 
Train Epoch: 2 [341/1000 10912/32000 (34%)] Loss: 1.99035 (semantic_loss: 0.04093, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.19412 
Train Epoch: 2 [346/1000 11072/32000 (35%)] Loss: 1.98934 (semantic_loss: 0.04089, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19319 
Train Epoch: 2 [351/1000 11232/32000 (35%)] Loss: 1.98963 (semantic_loss: 0.04215, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20947 
Train Epoch: 2 [356/1000 11392/32000 (36%)] Loss: 1.99474 (semantic_loss: 0.04726, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20884 
Train Epoch: 2 [361/1000 11552/32000 (36%)] Loss: 1.98654 (semantic_loss: 0.03809, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.26860 
Train Epoch: 2 [366/1000 11712/32000 (37%)] Loss: 1.98729 (semantic_loss: 0.03786, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.20871 
Train Epoch: 2 [371/1000 11872/32000 (37%)] Loss: 1.98943 (semantic_loss: 0.04195, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20903 
Train Epoch: 2 [376/1000 12032/32000 (38%)] Loss: 1.99410 (semantic_loss: 0.04661, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22650 
Train Epoch: 2 [381/1000 12192/32000 (38%)] Loss: 1.98997 (semantic_loss: 0.04250, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22043 
Train Epoch: 2 [386/1000 12352/32000 (39%)] Loss: 1.99346 (semantic_loss: 0.04500, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.26758 
Train Epoch: 2 [391/1000 12512/32000 (39%)] Loss: 1.98907 (semantic_loss: 0.04061, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20311 
Train Epoch: 2 [396/1000 12672/32000 (40%)] Loss: 1.99214 (semantic_loss: 0.04467, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21138 
Train Epoch: 2 [401/1000 12832/32000 (40%)] Loss: 1.98738 (semantic_loss: 0.03989, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.33370 
Train Epoch: 2 [406/1000 12992/32000 (41%)] Loss: 1.98737 (semantic_loss: 0.03891, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19388 
Train Epoch: 2 [411/1000 13152/32000 (41%)] Loss: 1.98899 (semantic_loss: 0.04054, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21574 
Train Epoch: 2 [416/1000 13312/32000 (42%)] Loss: 1.99048 (semantic_loss: 0.04203, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20791 
Train Epoch: 2 [421/1000 13472/32000 (42%)] Loss: 1.99307 (semantic_loss: 0.04559, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19418 
Train Epoch: 2 [426/1000 13632/32000 (43%)] Loss: 1.98682 (semantic_loss: 0.03934, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20989 
Train Epoch: 2 [431/1000 13792/32000 (43%)] Loss: 1.99015 (semantic_loss: 0.04267, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.27982 
Train Epoch: 2 [436/1000 13952/32000 (44%)] Loss: 1.99291 (semantic_loss: 0.04640, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19292 
Train Epoch: 2 [441/1000 14112/32000 (44%)] Loss: 1.98494 (semantic_loss: 0.03650, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19153 
Train Epoch: 2 [446/1000 14272/32000 (45%)] Loss: 1.98548 (semantic_loss: 0.03704, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20930 
Train Epoch: 2 [451/1000 14432/32000 (45%)] Loss: 1.99296 (semantic_loss: 0.04450, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19096 
Train Epoch: 2 [456/1000 14592/32000 (46%)] Loss: 1.98782 (semantic_loss: 0.03936, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19326 
Train Epoch: 2 [461/1000 14752/32000 (46%)] Loss: 1.98660 (semantic_loss: 0.04010, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20695 
Train Epoch: 2 [466/1000 14912/32000 (47%)] Loss: 1.99444 (semantic_loss: 0.04697, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19466 
Train Epoch: 2 [471/1000 15072/32000 (47%)] Loss: 1.98819 (semantic_loss: 0.03974, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21393 
Train Epoch: 2 [476/1000 15232/32000 (48%)] Loss: 1.99358 (semantic_loss: 0.04611, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19509 
Train Epoch: 2 [481/1000 15392/32000 (48%)] Loss: 1.99602 (semantic_loss: 0.04756, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19446 
Train Epoch: 2 [486/1000 15552/32000 (49%)] Loss: 1.99327 (semantic_loss: 0.04579, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.58816 
Train Epoch: 2 [491/1000 15712/32000 (49%)] Loss: 1.98678 (semantic_loss: 0.03833, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.24830 
Train Epoch: 2 [496/1000 15872/32000 (50%)] Loss: 1.98961 (semantic_loss: 0.04116, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.23020 
Train Epoch: 2 [501/1000 16032/32000 (50%)] Loss: 1.98907 (semantic_loss: 0.04159, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22974 
Train Epoch: 2 [506/1000 16192/32000 (51%)] Loss: 1.98831 (semantic_loss: 0.04083, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24541 
Train Epoch: 2 [511/1000 16352/32000 (51%)] Loss: 1.99004 (semantic_loss: 0.04255, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21890 
Train Epoch: 2 [516/1000 16512/32000 (52%)] Loss: 1.98525 (semantic_loss: 0.03778, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23029 
Train Epoch: 2 [521/1000 16672/32000 (52%)] Loss: 1.99291 (semantic_loss: 0.04543, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.23502 
Train Epoch: 2 [526/1000 16832/32000 (53%)] Loss: 1.98935 (semantic_loss: 0.04186, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20441 
Train Epoch: 2 [531/1000 16992/32000 (53%)] Loss: 1.99321 (semantic_loss: 0.04476, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20240 
Train Epoch: 2 [536/1000 17152/32000 (54%)] Loss: 1.98784 (semantic_loss: 0.04037, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20234 
Train Epoch: 2 [541/1000 17312/32000 (54%)] Loss: 1.98926 (semantic_loss: 0.03983, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.19488 
Train Epoch: 2 [546/1000 17472/32000 (55%)] Loss: 1.99127 (semantic_loss: 0.04379, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19770 
Train Epoch: 2 [551/1000 17632/32000 (55%)] Loss: 1.98454 (semantic_loss: 0.03707, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19878 
Train Epoch: 2 [556/1000 17792/32000 (56%)] Loss: 1.99056 (semantic_loss: 0.04209, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20868 
Train Epoch: 2 [561/1000 17952/32000 (56%)] Loss: 1.99644 (semantic_loss: 0.04799, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20537 
Train Epoch: 2 [566/1000 18112/32000 (57%)] Loss: 1.99212 (semantic_loss: 0.04365, quant_loss: 1.94824, bit_balance_loss: 0.00023) batch_time=0.20189 
Train Epoch: 2 [571/1000 18272/32000 (57%)] Loss: 1.99513 (semantic_loss: 0.04667, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20499 
Train Epoch: 2 [576/1000 18432/32000 (58%)] Loss: 1.99096 (semantic_loss: 0.04349, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20277 
Train Epoch: 2 [581/1000 18592/32000 (58%)] Loss: 1.99280 (semantic_loss: 0.04239, quant_loss: 1.95020, bit_balance_loss: 0.00021) batch_time=0.19356 
Train Epoch: 2 [586/1000 18752/32000 (59%)] Loss: 1.98628 (semantic_loss: 0.03684, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.19201 
Train Epoch: 2 [591/1000 18912/32000 (59%)] Loss: 1.98836 (semantic_loss: 0.04089, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19395 
Train Epoch: 2 [596/1000 19072/32000 (60%)] Loss: 1.98489 (semantic_loss: 0.03644, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20935 
Train Epoch: 2 [601/1000 19232/32000 (60%)] Loss: 1.98326 (semantic_loss: 0.03676, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19867 
Train Epoch: 2 [606/1000 19392/32000 (61%)] Loss: 1.99191 (semantic_loss: 0.04443, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19813 
Train Epoch: 2 [611/1000 19552/32000 (61%)] Loss: 1.99050 (semantic_loss: 0.04302, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19449 
Train Epoch: 2 [616/1000 19712/32000 (62%)] Loss: 1.98754 (semantic_loss: 0.04104, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19000 
Train Epoch: 2 [621/1000 19872/32000 (62%)] Loss: 1.99150 (semantic_loss: 0.04304, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19030 
Train Epoch: 2 [626/1000 20032/32000 (63%)] Loss: 1.98900 (semantic_loss: 0.04055, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19823 
Train Epoch: 2 [631/1000 20192/32000 (63%)] Loss: 1.98790 (semantic_loss: 0.04043, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20124 
Train Epoch: 2 [636/1000 20352/32000 (64%)] Loss: 1.98957 (semantic_loss: 0.04111, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19341 
Train Epoch: 2 [641/1000 20512/32000 (64%)] Loss: 1.98248 (semantic_loss: 0.03501, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20961 
Train Epoch: 2 [646/1000 20672/32000 (65%)] Loss: 1.98428 (semantic_loss: 0.03680, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22681 
Train Epoch: 2 [651/1000 20832/32000 (65%)] Loss: 1.99094 (semantic_loss: 0.04444, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22740 
Train Epoch: 2 [656/1000 20992/32000 (66%)] Loss: 1.99173 (semantic_loss: 0.04425, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26209 
Train Epoch: 2 [661/1000 21152/32000 (66%)] Loss: 1.98346 (semantic_loss: 0.03696, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.29487 
Train Epoch: 2 [666/1000 21312/32000 (67%)] Loss: 1.98282 (semantic_loss: 0.03535, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21372 
Train Epoch: 2 [671/1000 21472/32000 (67%)] Loss: 1.98686 (semantic_loss: 0.03841, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21114 
Train Epoch: 2 [676/1000 21632/32000 (68%)] Loss: 1.98615 (semantic_loss: 0.03770, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21361 
Train Epoch: 2 [681/1000 21792/32000 (68%)] Loss: 1.99036 (semantic_loss: 0.04190, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20111 
Train Epoch: 2 [686/1000 21952/32000 (69%)] Loss: 1.98465 (semantic_loss: 0.03717, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20204 
Train Epoch: 2 [691/1000 22112/32000 (69%)] Loss: 1.98713 (semantic_loss: 0.03868, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20543 
Train Epoch: 2 [696/1000 22272/32000 (70%)] Loss: 1.99318 (semantic_loss: 0.04570, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20151 
Train Epoch: 2 [701/1000 22432/32000 (70%)] Loss: 1.98708 (semantic_loss: 0.03959, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19802 
Train Epoch: 2 [706/1000 22592/32000 (71%)] Loss: 1.98765 (semantic_loss: 0.04115, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20628 
Train Epoch: 2 [711/1000 22752/32000 (71%)] Loss: 1.98648 (semantic_loss: 0.03705, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.19531 
Train Epoch: 2 [716/1000 22912/32000 (72%)] Loss: 1.98188 (semantic_loss: 0.03441, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.75757 
Train Epoch: 2 [721/1000 23072/32000 (72%)] Loss: 1.98426 (semantic_loss: 0.03580, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20997 
Train Epoch: 2 [726/1000 23232/32000 (73%)] Loss: 1.98721 (semantic_loss: 0.03876, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20082 
Train Epoch: 2 [731/1000 23392/32000 (73%)] Loss: 1.98692 (semantic_loss: 0.03944, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19503 
Train Epoch: 2 [736/1000 23552/32000 (74%)] Loss: 1.98854 (semantic_loss: 0.04009, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19340 
Train Epoch: 2 [741/1000 23712/32000 (74%)] Loss: 1.98164 (semantic_loss: 0.03416, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20280 
Train Epoch: 2 [746/1000 23872/32000 (75%)] Loss: 1.99297 (semantic_loss: 0.04549, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21242 
Train Epoch: 2 [751/1000 24032/32000 (75%)] Loss: 1.98743 (semantic_loss: 0.03898, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22329 
Train Epoch: 2 [756/1000 24192/32000 (76%)] Loss: 1.99020 (semantic_loss: 0.04076, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.20051 
Train Epoch: 2 [761/1000 24352/32000 (76%)] Loss: 1.98577 (semantic_loss: 0.03830, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19682 
Train Epoch: 2 [766/1000 24512/32000 (77%)] Loss: 1.98519 (semantic_loss: 0.03869, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20898 
Train Epoch: 2 [771/1000 24672/32000 (77%)] Loss: 1.98578 (semantic_loss: 0.03733, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19679 
Train Epoch: 2 [776/1000 24832/32000 (78%)] Loss: 1.99323 (semantic_loss: 0.04477, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19721 
Train Epoch: 2 [781/1000 24992/32000 (78%)] Loss: 1.99303 (semantic_loss: 0.04555, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21406 
Train Epoch: 2 [786/1000 25152/32000 (79%)] Loss: 1.98472 (semantic_loss: 0.03725, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21785 
Train Epoch: 2 [791/1000 25312/32000 (79%)] Loss: 1.98926 (semantic_loss: 0.04079, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.21089 
Train Epoch: 2 [796/1000 25472/32000 (80%)] Loss: 1.98644 (semantic_loss: 0.03897, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20760 
Train Epoch: 2 [801/1000 25632/32000 (80%)] Loss: 1.99514 (semantic_loss: 0.04766, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22602 
Train Epoch: 2 [806/1000 25792/32000 (81%)] Loss: 1.98769 (semantic_loss: 0.04021, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21142 
Train Epoch: 2 [811/1000 25952/32000 (81%)] Loss: 1.99118 (semantic_loss: 0.04369, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21486 
Train Epoch: 2 [816/1000 26112/32000 (82%)] Loss: 1.98951 (semantic_loss: 0.04008, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.27239 
Train Epoch: 2 [821/1000 26272/32000 (82%)] Loss: 1.99194 (semantic_loss: 0.04349, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22251 
Train Epoch: 2 [826/1000 26432/32000 (83%)] Loss: 1.98373 (semantic_loss: 0.03430, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.19687 
Train Epoch: 2 [831/1000 26592/32000 (83%)] Loss: 1.98633 (semantic_loss: 0.03690, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.19313 
Train Epoch: 2 [836/1000 26752/32000 (84%)] Loss: 1.98139 (semantic_loss: 0.03391, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19171 
Train Epoch: 2 [841/1000 26912/32000 (84%)] Loss: 1.98872 (semantic_loss: 0.04026, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19279 
Train Epoch: 2 [846/1000 27072/32000 (85%)] Loss: 1.98829 (semantic_loss: 0.03983, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19826 
Train Epoch: 2 [851/1000 27232/32000 (85%)] Loss: 1.98189 (semantic_loss: 0.03442, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19296 
Train Epoch: 2 [856/1000 27392/32000 (86%)] Loss: 1.98356 (semantic_loss: 0.03511, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20220 
Train Epoch: 2 [861/1000 27552/32000 (86%)] Loss: 1.98765 (semantic_loss: 0.03920, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20067 
Train Epoch: 2 [866/1000 27712/32000 (87%)] Loss: 1.98490 (semantic_loss: 0.03547, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.20115 
Train Epoch: 2 [871/1000 27872/32000 (87%)] Loss: 1.98634 (semantic_loss: 0.03886, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19884 
Train Epoch: 2 [876/1000 28032/32000 (88%)] Loss: 1.98380 (semantic_loss: 0.03535, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19882 
Train Epoch: 2 [881/1000 28192/32000 (88%)] Loss: 1.98592 (semantic_loss: 0.03748, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22038 
Train Epoch: 2 [886/1000 28352/32000 (89%)] Loss: 1.98323 (semantic_loss: 0.03478, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20086 
Train Epoch: 2 [891/1000 28512/32000 (89%)] Loss: 1.98993 (semantic_loss: 0.04245, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19602 
Train Epoch: 2 [896/1000 28672/32000 (90%)] Loss: 1.98679 (semantic_loss: 0.03931, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20974 
Train Epoch: 2 [901/1000 28832/32000 (90%)] Loss: 1.98193 (semantic_loss: 0.03348, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19164 
Train Epoch: 2 [906/1000 28992/32000 (91%)] Loss: 1.98260 (semantic_loss: 0.03415, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19582 
Train Epoch: 2 [911/1000 29152/32000 (91%)] Loss: 1.99137 (semantic_loss: 0.04390, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20086 
Train Epoch: 2 [916/1000 29312/32000 (92%)] Loss: 1.98057 (semantic_loss: 0.03407, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19176 
Train Epoch: 2 [921/1000 29472/32000 (92%)] Loss: 1.98284 (semantic_loss: 0.03536, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19366 
Train Epoch: 2 [926/1000 29632/32000 (93%)] Loss: 1.98876 (semantic_loss: 0.04128, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19317 
Train Epoch: 2 [931/1000 29792/32000 (93%)] Loss: 1.98777 (semantic_loss: 0.04029, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19466 
Train Epoch: 2 [936/1000 29952/32000 (94%)] Loss: 1.98680 (semantic_loss: 0.03932, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18900 
Train Epoch: 2 [941/1000 30112/32000 (94%)] Loss: 1.98393 (semantic_loss: 0.03449, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.82062 
Train Epoch: 2 [946/1000 30272/32000 (95%)] Loss: 1.98385 (semantic_loss: 0.03638, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22046 
Train Epoch: 2 [951/1000 30432/32000 (95%)] Loss: 1.98237 (semantic_loss: 0.03391, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.25531 
Train Epoch: 2 [956/1000 30592/32000 (96%)] Loss: 1.98314 (semantic_loss: 0.03663, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21363 
Train Epoch: 2 [961/1000 30752/32000 (96%)] Loss: 1.98692 (semantic_loss: 0.03944, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21494 
Train Epoch: 2 [966/1000 30912/32000 (97%)] Loss: 1.98123 (semantic_loss: 0.03376, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20847 
Train Epoch: 2 [971/1000 31072/32000 (97%)] Loss: 1.99448 (semantic_loss: 0.04504, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.21645 
Train Epoch: 2 [976/1000 31232/32000 (98%)] Loss: 1.98262 (semantic_loss: 0.03514, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20178 
Train Epoch: 2 [981/1000 31392/32000 (98%)] Loss: 1.98596 (semantic_loss: 0.03751, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.27593 
Train Epoch: 2 [986/1000 31552/32000 (99%)] Loss: 1.99448 (semantic_loss: 0.04699, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20647 
Train Epoch: 2 [991/1000 31712/32000 (99%)] Loss: 1.97775 (semantic_loss: 0.03028, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19764 
Train Epoch: 2 [996/1000 31872/32000 (100%)] Loss: 1.98722 (semantic_loss: 0.03974, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20375 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch2.pth ...
Done in 6.587s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch2.pth ...
Done in 12.198s
removing stale ckpt [epoch 1] [took 0.00s]
removing stale ckpt [epoch 0] [took 0.00s]
 epoch          : 2
 loss           : 1.9898631527423858
 learning_rate  : 4.5e-05
 n_samples      : 64000
 n_steps        : 2000
 ActivityNet_val1_test/t2v_metrics/R1: 5.511490746390075
 ActivityNet_val1_test/t2v_metrics/R5: 22.12731340248119
 ActivityNet_val1_test/t2v_metrics/R10: 35.692495424039045
 ActivityNet_val1_test/t2v_metrics/R50: 77.16087044946106
 ActivityNet_val1_test/t2v_metrics/MedR: 18.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 54.25757575757576
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 16.327695558577492
 ActivityNet_val1_test/v2t_metrics/R1: 5.592841163310962
 ActivityNet_val1_test/v2t_metrics/R5: 21.700223713646533
 ActivityNet_val1_test/v2t_metrics/R10: 35.38743136058572
 ActivityNet_val1_test/v2t_metrics/R50: 76.16432784218019
 ActivityNet_val1_test/v2t_metrics/MedR: 18.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 60.571181614805774
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 16.254810552534522
 mnt_best       : 16.327695558577492
 not_improved_count: 0
Train Epoch: 3 [1/1000 32/32000 (0%)] Loss: 1.98984 (semantic_loss: 0.04041, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=22.76339 
Train Epoch: 3 [6/1000 192/32000 (1%)] Loss: 1.98647 (semantic_loss: 0.03802, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20906 
Train Epoch: 3 [11/1000 352/32000 (1%)] Loss: 1.98288 (semantic_loss: 0.03442, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22174 
Train Epoch: 3 [16/1000 512/32000 (2%)] Loss: 1.98811 (semantic_loss: 0.04063, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=1.09442 
Train Epoch: 3 [21/1000 672/32000 (2%)] Loss: 1.98756 (semantic_loss: 0.03911, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22195 
Train Epoch: 3 [26/1000 832/32000 (3%)] Loss: 1.98492 (semantic_loss: 0.03745, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22932 
Train Epoch: 3 [31/1000 992/32000 (3%)] Loss: 1.98336 (semantic_loss: 0.03588, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22964 
Train Epoch: 3 [36/1000 1152/32000 (4%)] Loss: 1.97743 (semantic_loss: 0.02898, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19487 
Train Epoch: 3 [41/1000 1312/32000 (4%)] Loss: 1.98132 (semantic_loss: 0.03482, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24581 
Train Epoch: 3 [46/1000 1472/32000 (5%)] Loss: 1.98286 (semantic_loss: 0.03442, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18989 
Train Epoch: 3 [51/1000 1632/32000 (5%)] Loss: 1.98549 (semantic_loss: 0.03899, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22054 
Train Epoch: 3 [56/1000 1792/32000 (6%)] Loss: 1.98732 (semantic_loss: 0.03887, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20469 
Train Epoch: 3 [61/1000 1952/32000 (6%)] Loss: 1.98240 (semantic_loss: 0.03395, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20290 
Train Epoch: 3 [66/1000 2112/32000 (7%)] Loss: 1.98225 (semantic_loss: 0.03478, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22555 
Train Epoch: 3 [71/1000 2272/32000 (7%)] Loss: 1.98048 (semantic_loss: 0.03301, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21526 
Train Epoch: 3 [76/1000 2432/32000 (8%)] Loss: 1.98025 (semantic_loss: 0.03277, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20866 
Train Epoch: 3 [81/1000 2592/32000 (8%)] Loss: 1.98333 (semantic_loss: 0.03585, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18830 
Train Epoch: 3 [86/1000 2752/32000 (9%)] Loss: 1.98851 (semantic_loss: 0.04104, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18656 
Train Epoch: 3 [91/1000 2912/32000 (9%)] Loss: 1.98350 (semantic_loss: 0.03603, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19562 
Train Epoch: 3 [96/1000 3072/32000 (10%)] Loss: 1.97727 (semantic_loss: 0.02980, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21470 
Train Epoch: 3 [101/1000 3232/32000 (10%)] Loss: 1.98424 (semantic_loss: 0.03677, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19006 
Train Epoch: 3 [106/1000 3392/32000 (11%)] Loss: 1.99438 (semantic_loss: 0.04592, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19660 
Train Epoch: 3 [111/1000 3552/32000 (11%)] Loss: 1.98148 (semantic_loss: 0.03303, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19432 
Train Epoch: 3 [116/1000 3712/32000 (12%)] Loss: 1.98306 (semantic_loss: 0.03558, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20361 
Train Epoch: 3 [121/1000 3872/32000 (12%)] Loss: 1.98280 (semantic_loss: 0.03434, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20369 
Train Epoch: 3 [126/1000 4032/32000 (13%)] Loss: 1.98079 (semantic_loss: 0.03233, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.23992 
Train Epoch: 3 [131/1000 4192/32000 (13%)] Loss: 1.98862 (semantic_loss: 0.03919, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.26021 
Train Epoch: 3 [136/1000 4352/32000 (14%)] Loss: 1.97851 (semantic_loss: 0.03103, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25733 
Train Epoch: 3 [141/1000 4512/32000 (14%)] Loss: 1.97651 (semantic_loss: 0.02903, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25255 
Train Epoch: 3 [146/1000 4672/32000 (15%)] Loss: 1.98190 (semantic_loss: 0.03541, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21440 
Train Epoch: 3 [151/1000 4832/32000 (15%)] Loss: 1.98461 (semantic_loss: 0.03713, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20964 
Train Epoch: 3 [156/1000 4992/32000 (16%)] Loss: 1.97965 (semantic_loss: 0.03217, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23148 
Train Epoch: 3 [161/1000 5152/32000 (16%)] Loss: 1.98411 (semantic_loss: 0.03761, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24427 
Train Epoch: 3 [166/1000 5312/32000 (17%)] Loss: 1.97618 (semantic_loss: 0.03067, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19824 
Train Epoch: 3 [171/1000 5472/32000 (17%)] Loss: 1.98404 (semantic_loss: 0.03657, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22218 
Train Epoch: 3 [176/1000 5632/32000 (18%)] Loss: 1.98141 (semantic_loss: 0.03393, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19598 
Train Epoch: 3 [181/1000 5792/32000 (18%)] Loss: 1.98326 (semantic_loss: 0.03479, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20316 
Train Epoch: 3 [186/1000 5952/32000 (19%)] Loss: 1.98423 (semantic_loss: 0.03675, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20864 
Train Epoch: 3 [191/1000 6112/32000 (19%)] Loss: 1.98180 (semantic_loss: 0.03433, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20765 
Train Epoch: 3 [196/1000 6272/32000 (20%)] Loss: 1.98219 (semantic_loss: 0.03374, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19328 
Train Epoch: 3 [201/1000 6432/32000 (20%)] Loss: 1.98945 (semantic_loss: 0.04100, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21321 
Train Epoch: 3 [206/1000 6592/32000 (21%)] Loss: 1.98588 (semantic_loss: 0.03840, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20007 
Train Epoch: 3 [211/1000 6752/32000 (21%)] Loss: 1.97842 (semantic_loss: 0.03094, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18874 
Train Epoch: 3 [216/1000 6912/32000 (22%)] Loss: 1.98234 (semantic_loss: 0.03389, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19272 
Train Epoch: 3 [221/1000 7072/32000 (22%)] Loss: 1.98366 (semantic_loss: 0.03619, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19656 
Train Epoch: 3 [226/1000 7232/32000 (23%)] Loss: 1.97544 (semantic_loss: 0.02894, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19631 
Train Epoch: 3 [231/1000 7392/32000 (23%)] Loss: 1.98280 (semantic_loss: 0.03533, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19537 
Train Epoch: 3 [236/1000 7552/32000 (24%)] Loss: 1.97609 (semantic_loss: 0.02764, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20711 
Train Epoch: 3 [241/1000 7712/32000 (24%)] Loss: 1.99065 (semantic_loss: 0.04120, quant_loss: 1.94922, bit_balance_loss: 0.00023) batch_time=0.20825 
Train Epoch: 3 [246/1000 7872/32000 (25%)] Loss: 1.97560 (semantic_loss: 0.02813, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21157 
Train Epoch: 3 [251/1000 8032/32000 (25%)] Loss: 1.98163 (semantic_loss: 0.03416, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19413 
Train Epoch: 3 [256/1000 8192/32000 (26%)] Loss: 1.98263 (semantic_loss: 0.03515, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19854 
Train Epoch: 3 [261/1000 8352/32000 (26%)] Loss: 1.97828 (semantic_loss: 0.03080, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25203 
Train Epoch: 3 [266/1000 8512/32000 (27%)] Loss: 1.99164 (semantic_loss: 0.04318, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20954 
Train Epoch: 3 [271/1000 8672/32000 (27%)] Loss: 1.98469 (semantic_loss: 0.03722, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21381 
Train Epoch: 3 [276/1000 8832/32000 (28%)] Loss: 1.97832 (semantic_loss: 0.03085, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22328 
Train Epoch: 3 [281/1000 8992/32000 (28%)] Loss: 1.98344 (semantic_loss: 0.03401, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.23065 
Train Epoch: 3 [286/1000 9152/32000 (29%)] Loss: 1.98799 (semantic_loss: 0.03952, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20920 
Train Epoch: 3 [291/1000 9312/32000 (29%)] Loss: 1.97929 (semantic_loss: 0.03182, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24180 
Train Epoch: 3 [296/1000 9472/32000 (30%)] Loss: 1.97920 (semantic_loss: 0.03075, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.66783 
Train Epoch: 3 [301/1000 9632/32000 (30%)] Loss: 1.98167 (semantic_loss: 0.03420, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21694 
Train Epoch: 3 [306/1000 9792/32000 (31%)] Loss: 1.98406 (semantic_loss: 0.03658, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23818 
Train Epoch: 3 [311/1000 9952/32000 (31%)] Loss: 1.98575 (semantic_loss: 0.03827, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.42283 
Train Epoch: 3 [316/1000 10112/32000 (32%)] Loss: 1.98566 (semantic_loss: 0.03721, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19768 
Train Epoch: 3 [321/1000 10272/32000 (32%)] Loss: 1.98730 (semantic_loss: 0.03884, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21959 
Train Epoch: 3 [326/1000 10432/32000 (33%)] Loss: 1.98447 (semantic_loss: 0.03601, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19743 
Train Epoch: 3 [331/1000 10592/32000 (33%)] Loss: 1.98205 (semantic_loss: 0.03360, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21512 
Train Epoch: 3 [336/1000 10752/32000 (34%)] Loss: 1.98612 (semantic_loss: 0.03864, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.95114 
Train Epoch: 3 [341/1000 10912/32000 (34%)] Loss: 1.97860 (semantic_loss: 0.03112, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20134 
Train Epoch: 3 [346/1000 11072/32000 (35%)] Loss: 1.98002 (semantic_loss: 0.03255, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19444 
Train Epoch: 3 [351/1000 11232/32000 (35%)] Loss: 1.98158 (semantic_loss: 0.03411, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21209 
Train Epoch: 3 [356/1000 11392/32000 (36%)] Loss: 1.98105 (semantic_loss: 0.03260, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19601 
Train Epoch: 3 [361/1000 11552/32000 (36%)] Loss: 1.97880 (semantic_loss: 0.03132, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19328 
Train Epoch: 3 [366/1000 11712/32000 (37%)] Loss: 1.97959 (semantic_loss: 0.03114, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19235 
Train Epoch: 3 [371/1000 11872/32000 (37%)] Loss: 1.98678 (semantic_loss: 0.04028, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18916 
Train Epoch: 3 [376/1000 12032/32000 (38%)] Loss: 1.98322 (semantic_loss: 0.03672, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20052 
Train Epoch: 3 [381/1000 12192/32000 (38%)] Loss: 1.98220 (semantic_loss: 0.03473, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18898 
Train Epoch: 3 [386/1000 12352/32000 (39%)] Loss: 1.98121 (semantic_loss: 0.03373, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19727 
Train Epoch: 3 [391/1000 12512/32000 (39%)] Loss: 1.98368 (semantic_loss: 0.03620, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19757 
Train Epoch: 3 [396/1000 12672/32000 (40%)] Loss: 1.98534 (semantic_loss: 0.03786, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23001 
Train Epoch: 3 [401/1000 12832/32000 (40%)] Loss: 1.98266 (semantic_loss: 0.03420, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19978 
Train Epoch: 3 [406/1000 12992/32000 (41%)] Loss: 1.98584 (semantic_loss: 0.03837, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19155 
Train Epoch: 3 [411/1000 13152/32000 (41%)] Loss: 1.97980 (semantic_loss: 0.03232, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19660 
Train Epoch: 3 [416/1000 13312/32000 (42%)] Loss: 1.98589 (semantic_loss: 0.03841, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22786 
Train Epoch: 3 [421/1000 13472/32000 (42%)] Loss: 1.98195 (semantic_loss: 0.03446, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22196 
Train Epoch: 3 [426/1000 13632/32000 (43%)] Loss: 1.98039 (semantic_loss: 0.03389, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21947 
Train Epoch: 3 [431/1000 13792/32000 (43%)] Loss: 1.98440 (semantic_loss: 0.03691, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20569 
Train Epoch: 3 [436/1000 13952/32000 (44%)] Loss: 1.98397 (semantic_loss: 0.03455, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.21042 
Train Epoch: 3 [441/1000 14112/32000 (44%)] Loss: 1.98055 (semantic_loss: 0.03210, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.25607 
Train Epoch: 3 [446/1000 14272/32000 (45%)] Loss: 1.98332 (semantic_loss: 0.03487, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20293 
Train Epoch: 3 [451/1000 14432/32000 (45%)] Loss: 1.97978 (semantic_loss: 0.03230, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25723 
Train Epoch: 3 [456/1000 14592/32000 (46%)] Loss: 1.98106 (semantic_loss: 0.03261, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19789 
Train Epoch: 3 [461/1000 14752/32000 (46%)] Loss: 1.97963 (semantic_loss: 0.03118, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20664 
Train Epoch: 3 [466/1000 14912/32000 (47%)] Loss: 1.98054 (semantic_loss: 0.03307, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19833 
Train Epoch: 3 [471/1000 15072/32000 (47%)] Loss: 1.98365 (semantic_loss: 0.03518, quant_loss: 1.94824, bit_balance_loss: 0.00023) batch_time=0.20178 
Train Epoch: 3 [476/1000 15232/32000 (48%)] Loss: 1.98384 (semantic_loss: 0.03441, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.20988 
Train Epoch: 3 [481/1000 15392/32000 (48%)] Loss: 1.98439 (semantic_loss: 0.03497, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.19367 
Train Epoch: 3 [486/1000 15552/32000 (49%)] Loss: 1.98009 (semantic_loss: 0.03261, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19796 
Train Epoch: 3 [491/1000 15712/32000 (49%)] Loss: 1.98279 (semantic_loss: 0.03434, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22069 
Train Epoch: 3 [496/1000 15872/32000 (50%)] Loss: 1.98165 (semantic_loss: 0.03319, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20519 
Train Epoch: 3 [501/1000 16032/32000 (50%)] Loss: 1.97814 (semantic_loss: 0.03164, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21424 
Train Epoch: 3 [506/1000 16192/32000 (51%)] Loss: 1.98246 (semantic_loss: 0.03595, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20574 
Train Epoch: 3 [511/1000 16352/32000 (51%)] Loss: 1.97866 (semantic_loss: 0.03021, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19091 
Train Epoch: 3 [516/1000 16512/32000 (52%)] Loss: 1.98400 (semantic_loss: 0.03653, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19500 
Train Epoch: 3 [521/1000 16672/32000 (52%)] Loss: 1.98372 (semantic_loss: 0.03527, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19734 
Train Epoch: 3 [526/1000 16832/32000 (53%)] Loss: 1.97885 (semantic_loss: 0.03040, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21839 
Train Epoch: 3 [531/1000 16992/32000 (53%)] Loss: 1.98530 (semantic_loss: 0.03587, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.19979 
Train Epoch: 3 [536/1000 17152/32000 (54%)] Loss: 1.98406 (semantic_loss: 0.03560, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.22969 
Train Epoch: 3 [541/1000 17312/32000 (54%)] Loss: 1.97769 (semantic_loss: 0.03020, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22308 
Train Epoch: 3 [546/1000 17472/32000 (55%)] Loss: 1.98172 (semantic_loss: 0.03424, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20064 
Train Epoch: 3 [551/1000 17632/32000 (55%)] Loss: 1.98274 (semantic_loss: 0.03330, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.19036 
Train Epoch: 3 [556/1000 17792/32000 (56%)] Loss: 1.98396 (semantic_loss: 0.03647, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19720 
Train Epoch: 3 [561/1000 17952/32000 (56%)] Loss: 1.98350 (semantic_loss: 0.03505, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20483 
Train Epoch: 3 [566/1000 18112/32000 (57%)] Loss: 1.97480 (semantic_loss: 0.02733, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21805 
Train Epoch: 3 [571/1000 18272/32000 (57%)] Loss: 1.98139 (semantic_loss: 0.03489, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23961 
Train Epoch: 3 [576/1000 18432/32000 (58%)] Loss: 1.98067 (semantic_loss: 0.03124, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.21599 
Train Epoch: 3 [581/1000 18592/32000 (58%)] Loss: 1.98631 (semantic_loss: 0.03883, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.28493 
Train Epoch: 3 [586/1000 18752/32000 (59%)] Loss: 1.97985 (semantic_loss: 0.03140, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22435 
Train Epoch: 3 [591/1000 18912/32000 (59%)] Loss: 1.98132 (semantic_loss: 0.03383, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.26093 
Train Epoch: 3 [596/1000 19072/32000 (60%)] Loss: 1.98112 (semantic_loss: 0.03267, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.21036 
Train Epoch: 3 [601/1000 19232/32000 (60%)] Loss: 1.97646 (semantic_loss: 0.02899, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21064 
Train Epoch: 3 [606/1000 19392/32000 (61%)] Loss: 1.98394 (semantic_loss: 0.03549, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19235 
Train Epoch: 3 [611/1000 19552/32000 (61%)] Loss: 1.97973 (semantic_loss: 0.03127, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18942 
Train Epoch: 3 [616/1000 19712/32000 (62%)] Loss: 1.97805 (semantic_loss: 0.03058, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.48640 
Train Epoch: 3 [621/1000 19872/32000 (62%)] Loss: 1.98090 (semantic_loss: 0.03440, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18611 
Train Epoch: 3 [626/1000 20032/32000 (63%)] Loss: 1.98222 (semantic_loss: 0.03377, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18817 
Train Epoch: 3 [631/1000 20192/32000 (63%)] Loss: 1.98200 (semantic_loss: 0.03452, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.35306 
Train Epoch: 3 [636/1000 20352/32000 (64%)] Loss: 1.98122 (semantic_loss: 0.03374, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18978 
Train Epoch: 3 [641/1000 20512/32000 (64%)] Loss: 1.97915 (semantic_loss: 0.03168, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22537 
Train Epoch: 3 [646/1000 20672/32000 (65%)] Loss: 1.98843 (semantic_loss: 0.04095, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20321 
Train Epoch: 3 [651/1000 20832/32000 (65%)] Loss: 1.97882 (semantic_loss: 0.03134, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18867 
Train Epoch: 3 [656/1000 20992/32000 (66%)] Loss: 1.98126 (semantic_loss: 0.03377, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.86127 
Train Epoch: 3 [661/1000 21152/32000 (66%)] Loss: 1.98713 (semantic_loss: 0.03964, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19006 
Train Epoch: 3 [666/1000 21312/32000 (67%)] Loss: 1.98186 (semantic_loss: 0.03438, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19138 
Train Epoch: 3 [671/1000 21472/32000 (67%)] Loss: 1.97876 (semantic_loss: 0.03127, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21366 
Train Epoch: 3 [676/1000 21632/32000 (68%)] Loss: 1.98189 (semantic_loss: 0.03344, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18987 
Train Epoch: 3 [681/1000 21792/32000 (68%)] Loss: 1.98157 (semantic_loss: 0.03311, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18831 
Train Epoch: 3 [686/1000 21952/32000 (69%)] Loss: 1.98290 (semantic_loss: 0.03542, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20318 
Train Epoch: 3 [691/1000 22112/32000 (69%)] Loss: 1.98560 (semantic_loss: 0.03714, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18958 
Train Epoch: 3 [696/1000 22272/32000 (70%)] Loss: 1.98113 (semantic_loss: 0.03365, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18795 
Train Epoch: 3 [701/1000 22432/32000 (70%)] Loss: 1.97845 (semantic_loss: 0.03194, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19044 
Train Epoch: 3 [706/1000 22592/32000 (71%)] Loss: 1.98045 (semantic_loss: 0.03297, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18958 
Train Epoch: 3 [711/1000 22752/32000 (71%)] Loss: 1.98577 (semantic_loss: 0.03829, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21472 
Train Epoch: 3 [716/1000 22912/32000 (72%)] Loss: 1.98269 (semantic_loss: 0.03423, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.26920 
Train Epoch: 3 [721/1000 23072/32000 (72%)] Loss: 1.98396 (semantic_loss: 0.03648, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21986 
Train Epoch: 3 [726/1000 23232/32000 (73%)] Loss: 1.98243 (semantic_loss: 0.03592, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19793 
Train Epoch: 3 [731/1000 23392/32000 (73%)] Loss: 1.98142 (semantic_loss: 0.03297, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20139 
Train Epoch: 3 [736/1000 23552/32000 (74%)] Loss: 1.98267 (semantic_loss: 0.03422, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19421 
Train Epoch: 3 [741/1000 23712/32000 (74%)] Loss: 1.97734 (semantic_loss: 0.02985, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20729 
Train Epoch: 3 [746/1000 23872/32000 (75%)] Loss: 1.98149 (semantic_loss: 0.03304, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19066 
Train Epoch: 3 [751/1000 24032/32000 (75%)] Loss: 1.97891 (semantic_loss: 0.03144, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18851 
Train Epoch: 3 [756/1000 24192/32000 (76%)] Loss: 1.97975 (semantic_loss: 0.03227, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18852 
Train Epoch: 3 [761/1000 24352/32000 (76%)] Loss: 1.98109 (semantic_loss: 0.03263, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19166 
Train Epoch: 3 [766/1000 24512/32000 (77%)] Loss: 1.97812 (semantic_loss: 0.03161, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19219 
Train Epoch: 3 [771/1000 24672/32000 (77%)] Loss: 1.98175 (semantic_loss: 0.03525, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21374 
Train Epoch: 3 [776/1000 24832/32000 (78%)] Loss: 1.98048 (semantic_loss: 0.03202, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19228 
Train Epoch: 3 [781/1000 24992/32000 (78%)] Loss: 1.97924 (semantic_loss: 0.03275, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19698 
Train Epoch: 3 [786/1000 25152/32000 (79%)] Loss: 1.97780 (semantic_loss: 0.03129, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19760 
Train Epoch: 3 [791/1000 25312/32000 (79%)] Loss: 1.98047 (semantic_loss: 0.03202, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19495 
Train Epoch: 3 [796/1000 25472/32000 (80%)] Loss: 1.97894 (semantic_loss: 0.03147, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18380 
Train Epoch: 3 [801/1000 25632/32000 (80%)] Loss: 1.98029 (semantic_loss: 0.03184, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18762 
Train Epoch: 3 [806/1000 25792/32000 (81%)] Loss: 1.98064 (semantic_loss: 0.03315, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18844 
Train Epoch: 3 [811/1000 25952/32000 (81%)] Loss: 1.98026 (semantic_loss: 0.03376, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20956 
Train Epoch: 3 [816/1000 26112/32000 (82%)] Loss: 1.97927 (semantic_loss: 0.03277, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19141 
Train Epoch: 3 [821/1000 26272/32000 (82%)] Loss: 1.97675 (semantic_loss: 0.02927, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19168 
Train Epoch: 3 [826/1000 26432/32000 (83%)] Loss: 1.97819 (semantic_loss: 0.03070, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19199 
Train Epoch: 3 [831/1000 26592/32000 (83%)] Loss: 1.98076 (semantic_loss: 0.03328, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19209 
Train Epoch: 3 [836/1000 26752/32000 (84%)] Loss: 1.98952 (semantic_loss: 0.04106, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19494 
Train Epoch: 3 [841/1000 26912/32000 (84%)] Loss: 1.98520 (semantic_loss: 0.03674, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18870 
Train Epoch: 3 [846/1000 27072/32000 (85%)] Loss: 1.98678 (semantic_loss: 0.03929, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19527 
Train Epoch: 3 [851/1000 27232/32000 (85%)] Loss: 1.97593 (semantic_loss: 0.02845, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19551 
Train Epoch: 3 [856/1000 27392/32000 (86%)] Loss: 1.98270 (semantic_loss: 0.03522, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19372 
Train Epoch: 3 [861/1000 27552/32000 (86%)] Loss: 1.98236 (semantic_loss: 0.03389, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20157 
Train Epoch: 3 [866/1000 27712/32000 (87%)] Loss: 1.98327 (semantic_loss: 0.03482, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20733 
Train Epoch: 3 [871/1000 27872/32000 (87%)] Loss: 1.98136 (semantic_loss: 0.03291, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20857 
Train Epoch: 3 [876/1000 28032/32000 (88%)] Loss: 1.97627 (semantic_loss: 0.02880, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20018 
Train Epoch: 3 [881/1000 28192/32000 (88%)] Loss: 1.98271 (semantic_loss: 0.03523, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20188 
Train Epoch: 3 [886/1000 28352/32000 (89%)] Loss: 1.98040 (semantic_loss: 0.03195, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20016 
Train Epoch: 3 [891/1000 28512/32000 (89%)] Loss: 1.97927 (semantic_loss: 0.03083, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19248 
Train Epoch: 3 [896/1000 28672/32000 (90%)] Loss: 1.97643 (semantic_loss: 0.02895, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20066 
Train Epoch: 3 [901/1000 28832/32000 (90%)] Loss: 1.98034 (semantic_loss: 0.03189, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.24730 
Train Epoch: 3 [906/1000 28992/32000 (91%)] Loss: 1.97975 (semantic_loss: 0.03130, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19040 
Train Epoch: 3 [911/1000 29152/32000 (91%)] Loss: 1.98025 (semantic_loss: 0.03180, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18691 
Train Epoch: 3 [916/1000 29312/32000 (92%)] Loss: 1.97931 (semantic_loss: 0.03085, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18647 
Train Epoch: 3 [921/1000 29472/32000 (92%)] Loss: 1.97771 (semantic_loss: 0.03023, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18795 
Train Epoch: 3 [926/1000 29632/32000 (93%)] Loss: 1.97947 (semantic_loss: 0.03199, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19234 
Train Epoch: 3 [931/1000 29792/32000 (93%)] Loss: 1.97306 (semantic_loss: 0.02460, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19490 
Train Epoch: 3 [936/1000 29952/32000 (94%)] Loss: 1.98046 (semantic_loss: 0.03396, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.77906 
Train Epoch: 3 [941/1000 30112/32000 (94%)] Loss: 1.98298 (semantic_loss: 0.03648, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.31150 
Train Epoch: 3 [946/1000 30272/32000 (95%)] Loss: 1.98036 (semantic_loss: 0.03191, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.31835 
Train Epoch: 3 [951/1000 30432/32000 (95%)] Loss: 1.98090 (semantic_loss: 0.03342, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.48882 
Train Epoch: 3 [956/1000 30592/32000 (96%)] Loss: 1.98249 (semantic_loss: 0.03403, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20071 
Train Epoch: 3 [961/1000 30752/32000 (96%)] Loss: 1.98183 (semantic_loss: 0.03435, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23106 
Train Epoch: 3 [966/1000 30912/32000 (97%)] Loss: 1.97447 (semantic_loss: 0.02700, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21612 
Train Epoch: 3 [971/1000 31072/32000 (97%)] Loss: 1.97857 (semantic_loss: 0.03012, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21799 
Train Epoch: 3 [976/1000 31232/32000 (98%)] Loss: 1.97997 (semantic_loss: 0.03152, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19704 
Train Epoch: 3 [981/1000 31392/32000 (98%)] Loss: 1.98231 (semantic_loss: 0.03482, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.24214 
Train Epoch: 3 [986/1000 31552/32000 (99%)] Loss: 1.97560 (semantic_loss: 0.02714, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19904 
Train Epoch: 3 [991/1000 31712/32000 (99%)] Loss: 1.98120 (semantic_loss: 0.03372, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20800 
Train Epoch: 3 [996/1000 31872/32000 (100%)] Loss: 1.97957 (semantic_loss: 0.03208, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19321 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch3.pth ...
Done in 4.926s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch3.pth ...
Done in 9.698s
removing stale ckpt [epoch 2] [took 0.00s]
 epoch          : 3
 loss           : 1.981938192009926
 learning_rate  : 4.05e-05
 n_samples      : 96000
 n_steps        : 3000
 ActivityNet_val1_test/t2v_metrics/R1: 6.487695749440716
 ActivityNet_val1_test/t2v_metrics/R5: 26.316859873906854
 ActivityNet_val1_test/t2v_metrics/R10: 42.05816554809844
 ActivityNet_val1_test/t2v_metrics/R50: 82.26560911124669
 ActivityNet_val1_test/t2v_metrics/MedR: 14.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 48.574435631482615
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 19.29263833513959
 ActivityNet_val1_test/v2t_metrics/R1: 7.5045759609518
 ActivityNet_val1_test/v2t_metrics/R5: 27.96420581655481
 ActivityNet_val1_test/v2t_metrics/R10: 43.82753711612772
 ActivityNet_val1_test/v2t_metrics/R50: 82.42830994508847
 ActivityNet_val1_test/v2t_metrics/MedR: 13.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 50.626296522269676
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 20.95198806288963
 mnt_best       : 19.29263833513959
 not_improved_count: 0
Train Epoch: 4 [1/1000 32/32000 (0%)] Loss: 1.97754 (semantic_loss: 0.03006, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=25.64406 
Train Epoch: 4 [6/1000 192/32000 (1%)] Loss: 1.98141 (semantic_loss: 0.03393, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21296 
Train Epoch: 4 [11/1000 352/32000 (1%)] Loss: 1.97741 (semantic_loss: 0.02993, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19603 
Train Epoch: 4 [16/1000 512/32000 (2%)] Loss: 1.98003 (semantic_loss: 0.03157, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.35129 
Train Epoch: 4 [21/1000 672/32000 (2%)] Loss: 1.98091 (semantic_loss: 0.03441, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18866 
Train Epoch: 4 [26/1000 832/32000 (3%)] Loss: 1.97563 (semantic_loss: 0.02718, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19711 
Train Epoch: 4 [31/1000 992/32000 (3%)] Loss: 1.97669 (semantic_loss: 0.02921, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20361 
Train Epoch: 4 [36/1000 1152/32000 (4%)] Loss: 1.98072 (semantic_loss: 0.03226, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20435 
Train Epoch: 4 [41/1000 1312/32000 (4%)] Loss: 1.98618 (semantic_loss: 0.03773, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21997 
Train Epoch: 4 [46/1000 1472/32000 (5%)] Loss: 1.97514 (semantic_loss: 0.02766, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19396 
Train Epoch: 4 [51/1000 1632/32000 (5%)] Loss: 1.97888 (semantic_loss: 0.03140, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21659 
Train Epoch: 4 [56/1000 1792/32000 (6%)] Loss: 1.97708 (semantic_loss: 0.02961, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20002 
Train Epoch: 4 [61/1000 1952/32000 (6%)] Loss: 1.97538 (semantic_loss: 0.02790, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22545 
Train Epoch: 4 [66/1000 2112/32000 (7%)] Loss: 1.97637 (semantic_loss: 0.02889, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20130 
Train Epoch: 4 [71/1000 2272/32000 (7%)] Loss: 1.97447 (semantic_loss: 0.02602, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19283 
Train Epoch: 4 [76/1000 2432/32000 (8%)] Loss: 1.98640 (semantic_loss: 0.03893, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20804 
Train Epoch: 4 [81/1000 2592/32000 (8%)] Loss: 1.97818 (semantic_loss: 0.02972, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20464 
Train Epoch: 4 [86/1000 2752/32000 (9%)] Loss: 1.97816 (semantic_loss: 0.03068, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22537 
Train Epoch: 4 [91/1000 2912/32000 (9%)] Loss: 1.98581 (semantic_loss: 0.03736, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.25915 
Train Epoch: 4 [96/1000 3072/32000 (10%)] Loss: 1.97486 (semantic_loss: 0.02737, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.48405 
Train Epoch: 4 [101/1000 3232/32000 (10%)] Loss: 1.98021 (semantic_loss: 0.03273, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=1.26668 
Train Epoch: 4 [106/1000 3392/32000 (11%)] Loss: 1.98186 (semantic_loss: 0.03243, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.22506 
Train Epoch: 4 [111/1000 3552/32000 (11%)] Loss: 1.97734 (semantic_loss: 0.02889, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21522 
Train Epoch: 4 [116/1000 3712/32000 (12%)] Loss: 1.98269 (semantic_loss: 0.03424, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20308 
Train Epoch: 4 [121/1000 3872/32000 (12%)] Loss: 1.98121 (semantic_loss: 0.03373, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21063 
Train Epoch: 4 [126/1000 4032/32000 (13%)] Loss: 1.97950 (semantic_loss: 0.03104, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20410 
Train Epoch: 4 [131/1000 4192/32000 (13%)] Loss: 1.97371 (semantic_loss: 0.02721, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19508 
Train Epoch: 4 [136/1000 4352/32000 (14%)] Loss: 1.97908 (semantic_loss: 0.03062, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19436 
Train Epoch: 4 [141/1000 4512/32000 (14%)] Loss: 1.98237 (semantic_loss: 0.03489, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22717 
Train Epoch: 4 [146/1000 4672/32000 (15%)] Loss: 1.98711 (semantic_loss: 0.03963, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20382 
Train Epoch: 4 [151/1000 4832/32000 (15%)] Loss: 1.97964 (semantic_loss: 0.03313, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19523 
Train Epoch: 4 [156/1000 4992/32000 (16%)] Loss: 1.97811 (semantic_loss: 0.03064, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20012 
Train Epoch: 4 [161/1000 5152/32000 (16%)] Loss: 1.97309 (semantic_loss: 0.02464, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.55219 
Train Epoch: 4 [166/1000 5312/32000 (17%)] Loss: 1.97964 (semantic_loss: 0.03118, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20872 
Train Epoch: 4 [171/1000 5472/32000 (17%)] Loss: 1.97799 (semantic_loss: 0.03148, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19468 
Train Epoch: 4 [176/1000 5632/32000 (18%)] Loss: 1.97734 (semantic_loss: 0.02986, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19184 
Train Epoch: 4 [181/1000 5792/32000 (18%)] Loss: 1.97715 (semantic_loss: 0.02966, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19689 
Train Epoch: 4 [186/1000 5952/32000 (19%)] Loss: 1.97861 (semantic_loss: 0.03209, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.19101 
Train Epoch: 4 [191/1000 6112/32000 (19%)] Loss: 1.98024 (semantic_loss: 0.03179, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19664 
Train Epoch: 4 [196/1000 6272/32000 (20%)] Loss: 1.97505 (semantic_loss: 0.02758, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19022 
Train Epoch: 4 [201/1000 6432/32000 (20%)] Loss: 1.97452 (semantic_loss: 0.02801, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20176 
Train Epoch: 4 [206/1000 6592/32000 (21%)] Loss: 1.97497 (semantic_loss: 0.02847, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20515 
Train Epoch: 4 [211/1000 6752/32000 (21%)] Loss: 1.98030 (semantic_loss: 0.03184, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19413 
Train Epoch: 4 [216/1000 6912/32000 (22%)] Loss: 1.97739 (semantic_loss: 0.02992, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19361 
Train Epoch: 4 [221/1000 7072/32000 (22%)] Loss: 1.98144 (semantic_loss: 0.03298, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.22179 
Train Epoch: 4 [226/1000 7232/32000 (23%)] Loss: 1.98753 (semantic_loss: 0.04006, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19300 
Train Epoch: 4 [231/1000 7392/32000 (23%)] Loss: 1.97676 (semantic_loss: 0.03024, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.21580 
Train Epoch: 4 [236/1000 7552/32000 (24%)] Loss: 1.97334 (semantic_loss: 0.02586, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24397 
Train Epoch: 4 [241/1000 7712/32000 (24%)] Loss: 1.97695 (semantic_loss: 0.03045, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.25196 
Train Epoch: 4 [246/1000 7872/32000 (25%)] Loss: 1.97909 (semantic_loss: 0.03162, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25119 
Train Epoch: 4 [251/1000 8032/32000 (25%)] Loss: 1.98265 (semantic_loss: 0.03516, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22118 
Train Epoch: 4 [256/1000 8192/32000 (26%)] Loss: 1.98287 (semantic_loss: 0.03441, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20305 
Train Epoch: 4 [261/1000 8352/32000 (26%)] Loss: 1.97302 (semantic_loss: 0.02650, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21160 
Train Epoch: 4 [266/1000 8512/32000 (27%)] Loss: 1.98286 (semantic_loss: 0.03441, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20620 
Train Epoch: 4 [271/1000 8672/32000 (27%)] Loss: 1.97623 (semantic_loss: 0.02875, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19878 
Train Epoch: 4 [276/1000 8832/32000 (28%)] Loss: 1.98094 (semantic_loss: 0.03248, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.44664 
Train Epoch: 4 [281/1000 8992/32000 (28%)] Loss: 1.98096 (semantic_loss: 0.03347, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19349 
Train Epoch: 4 [286/1000 9152/32000 (29%)] Loss: 1.97841 (semantic_loss: 0.02996, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21601 
Train Epoch: 4 [291/1000 9312/32000 (29%)] Loss: 1.97883 (semantic_loss: 0.03135, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19694 
Train Epoch: 4 [296/1000 9472/32000 (30%)] Loss: 1.98403 (semantic_loss: 0.03557, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20368 
Train Epoch: 4 [301/1000 9632/32000 (30%)] Loss: 1.97870 (semantic_loss: 0.03122, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19816 
Train Epoch: 4 [306/1000 9792/32000 (31%)] Loss: 1.98198 (semantic_loss: 0.03451, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22354 
Train Epoch: 4 [311/1000 9952/32000 (31%)] Loss: 1.97726 (semantic_loss: 0.02881, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20564 
Train Epoch: 4 [316/1000 10112/32000 (32%)] Loss: 1.98453 (semantic_loss: 0.03705, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19384 
Train Epoch: 4 [321/1000 10272/32000 (32%)] Loss: 1.97941 (semantic_loss: 0.03193, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19326 
Train Epoch: 4 [326/1000 10432/32000 (33%)] Loss: 1.98046 (semantic_loss: 0.03395, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21965 
Train Epoch: 4 [331/1000 10592/32000 (33%)] Loss: 1.97814 (semantic_loss: 0.03066, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19152 
Train Epoch: 4 [336/1000 10752/32000 (34%)] Loss: 1.98014 (semantic_loss: 0.03169, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19169 
Train Epoch: 4 [341/1000 10912/32000 (34%)] Loss: 1.97897 (semantic_loss: 0.03150, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19503 
Train Epoch: 4 [346/1000 11072/32000 (35%)] Loss: 1.97669 (semantic_loss: 0.02921, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19243 
Train Epoch: 4 [351/1000 11232/32000 (35%)] Loss: 1.97676 (semantic_loss: 0.02928, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19526 
Train Epoch: 4 [356/1000 11392/32000 (36%)] Loss: 1.97709 (semantic_loss: 0.02864, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20867 
Train Epoch: 4 [361/1000 11552/32000 (36%)] Loss: 1.97427 (semantic_loss: 0.02679, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19608 
Train Epoch: 4 [366/1000 11712/32000 (37%)] Loss: 1.97755 (semantic_loss: 0.03006, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19615 
Train Epoch: 4 [371/1000 11872/32000 (37%)] Loss: 1.97571 (semantic_loss: 0.02726, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19796 
Train Epoch: 4 [376/1000 12032/32000 (38%)] Loss: 1.97581 (semantic_loss: 0.02833, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22824 
Train Epoch: 4 [381/1000 12192/32000 (38%)] Loss: 1.97494 (semantic_loss: 0.02746, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24549 
Train Epoch: 4 [386/1000 12352/32000 (39%)] Loss: 1.97618 (semantic_loss: 0.02772, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22149 
Train Epoch: 4 [391/1000 12512/32000 (39%)] Loss: 1.97430 (semantic_loss: 0.02780, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21200 
Train Epoch: 4 [396/1000 12672/32000 (40%)] Loss: 1.97516 (semantic_loss: 0.02768, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19123 
Train Epoch: 4 [401/1000 12832/32000 (40%)] Loss: 1.97843 (semantic_loss: 0.03096, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21033 
Train Epoch: 4 [406/1000 12992/32000 (41%)] Loss: 1.97930 (semantic_loss: 0.03280, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21309 
Train Epoch: 4 [411/1000 13152/32000 (41%)] Loss: 1.97548 (semantic_loss: 0.02800, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19578 
Train Epoch: 4 [416/1000 13312/32000 (42%)] Loss: 1.97429 (semantic_loss: 0.02681, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.35473 
Train Epoch: 4 [421/1000 13472/32000 (42%)] Loss: 1.97800 (semantic_loss: 0.02955, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18797 
Train Epoch: 4 [426/1000 13632/32000 (43%)] Loss: 1.97597 (semantic_loss: 0.02947, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18891 
Train Epoch: 4 [431/1000 13792/32000 (43%)] Loss: 1.98346 (semantic_loss: 0.03500, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18678 
Train Epoch: 4 [436/1000 13952/32000 (44%)] Loss: 1.97348 (semantic_loss: 0.02600, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20803 
Train Epoch: 4 [441/1000 14112/32000 (44%)] Loss: 1.97815 (semantic_loss: 0.02971, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18460 
Train Epoch: 4 [446/1000 14272/32000 (45%)] Loss: 1.97770 (semantic_loss: 0.03119, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19931 
Train Epoch: 4 [451/1000 14432/32000 (45%)] Loss: 1.98224 (semantic_loss: 0.03378, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19246 
Train Epoch: 4 [456/1000 14592/32000 (46%)] Loss: 1.97708 (semantic_loss: 0.02862, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20639 
Train Epoch: 4 [461/1000 14752/32000 (46%)] Loss: 1.97423 (semantic_loss: 0.02675, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18578 
Train Epoch: 4 [466/1000 14912/32000 (47%)] Loss: 1.97786 (semantic_loss: 0.02941, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18438 
Train Epoch: 4 [471/1000 15072/32000 (47%)] Loss: 1.98467 (semantic_loss: 0.03817, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18949 
Train Epoch: 4 [476/1000 15232/32000 (48%)] Loss: 1.98151 (semantic_loss: 0.03305, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19635 
Train Epoch: 4 [481/1000 15392/32000 (48%)] Loss: 1.98163 (semantic_loss: 0.03512, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.50432 
Train Epoch: 4 [486/1000 15552/32000 (49%)] Loss: 1.97978 (semantic_loss: 0.03327, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18807 
Train Epoch: 4 [491/1000 15712/32000 (49%)] Loss: 1.97717 (semantic_loss: 0.02872, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19674 
Train Epoch: 4 [496/1000 15872/32000 (50%)] Loss: 1.97867 (semantic_loss: 0.03022, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18316 
Train Epoch: 4 [501/1000 16032/32000 (50%)] Loss: 1.97145 (semantic_loss: 0.02398, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18496 
Train Epoch: 4 [506/1000 16192/32000 (51%)] Loss: 1.98084 (semantic_loss: 0.03336, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18821 
Train Epoch: 4 [511/1000 16352/32000 (51%)] Loss: 1.97335 (semantic_loss: 0.02489, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19051 
Train Epoch: 4 [516/1000 16512/32000 (52%)] Loss: 1.97429 (semantic_loss: 0.02779, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18843 
Train Epoch: 4 [521/1000 16672/32000 (52%)] Loss: 1.97768 (semantic_loss: 0.03019, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18831 
Train Epoch: 4 [526/1000 16832/32000 (53%)] Loss: 1.97686 (semantic_loss: 0.03037, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21221 
Train Epoch: 4 [531/1000 16992/32000 (53%)] Loss: 1.97843 (semantic_loss: 0.03095, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20239 
Train Epoch: 4 [536/1000 17152/32000 (54%)] Loss: 1.97960 (semantic_loss: 0.03212, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21469 
Train Epoch: 4 [541/1000 17312/32000 (54%)] Loss: 1.97749 (semantic_loss: 0.02903, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.22333 
Train Epoch: 4 [546/1000 17472/32000 (55%)] Loss: 1.97977 (semantic_loss: 0.03131, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20398 
Train Epoch: 4 [551/1000 17632/32000 (55%)] Loss: 1.97836 (semantic_loss: 0.03088, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.74010 
Train Epoch: 4 [556/1000 17792/32000 (56%)] Loss: 1.97712 (semantic_loss: 0.02963, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20491 
Train Epoch: 4 [561/1000 17952/32000 (56%)] Loss: 1.97507 (semantic_loss: 0.02856, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19538 
Train Epoch: 4 [566/1000 18112/32000 (57%)] Loss: 1.97262 (semantic_loss: 0.02613, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18919 
Train Epoch: 4 [571/1000 18272/32000 (57%)] Loss: 1.97296 (semantic_loss: 0.02549, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19116 
Train Epoch: 4 [576/1000 18432/32000 (58%)] Loss: 1.98010 (semantic_loss: 0.03164, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18600 
Train Epoch: 4 [581/1000 18592/32000 (58%)] Loss: 1.97358 (semantic_loss: 0.02610, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20120 
Train Epoch: 4 [586/1000 18752/32000 (59%)] Loss: 1.97684 (semantic_loss: 0.03034, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19102 
Train Epoch: 4 [591/1000 18912/32000 (59%)] Loss: 1.98198 (semantic_loss: 0.03353, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19194 
Train Epoch: 4 [596/1000 19072/32000 (60%)] Loss: 1.97714 (semantic_loss: 0.02965, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.39689 
Train Epoch: 4 [601/1000 19232/32000 (60%)] Loss: 1.97595 (semantic_loss: 0.02750, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19285 
Train Epoch: 4 [606/1000 19392/32000 (61%)] Loss: 1.97634 (semantic_loss: 0.02886, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20921 
Train Epoch: 4 [611/1000 19552/32000 (61%)] Loss: 1.98003 (semantic_loss: 0.03353, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18577 
Train Epoch: 4 [616/1000 19712/32000 (62%)] Loss: 1.97694 (semantic_loss: 0.02946, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19921 
Train Epoch: 4 [621/1000 19872/32000 (62%)] Loss: 1.97525 (semantic_loss: 0.02875, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18755 
Train Epoch: 4 [626/1000 20032/32000 (63%)] Loss: 1.97613 (semantic_loss: 0.02865, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19067 
Train Epoch: 4 [631/1000 20192/32000 (63%)] Loss: 1.97963 (semantic_loss: 0.03215, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19306 
Train Epoch: 4 [636/1000 20352/32000 (64%)] Loss: 1.97565 (semantic_loss: 0.02719, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18993 
Train Epoch: 4 [641/1000 20512/32000 (64%)] Loss: 1.98021 (semantic_loss: 0.03078, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.18747 
Train Epoch: 4 [646/1000 20672/32000 (65%)] Loss: 1.97582 (semantic_loss: 0.02834, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20358 
Train Epoch: 4 [651/1000 20832/32000 (65%)] Loss: 1.97378 (semantic_loss: 0.02436, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.19735 
Train Epoch: 4 [656/1000 20992/32000 (66%)] Loss: 1.97660 (semantic_loss: 0.02911, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19036 
Train Epoch: 4 [661/1000 21152/32000 (66%)] Loss: 1.97984 (semantic_loss: 0.03041, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.20256 
Train Epoch: 4 [666/1000 21312/32000 (67%)] Loss: 1.97820 (semantic_loss: 0.03169, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18924 
Train Epoch: 4 [671/1000 21472/32000 (67%)] Loss: 1.97563 (semantic_loss: 0.02815, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18735 
Train Epoch: 4 [676/1000 21632/32000 (68%)] Loss: 1.97858 (semantic_loss: 0.02915, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.24553 
Train Epoch: 4 [681/1000 21792/32000 (68%)] Loss: 1.97981 (semantic_loss: 0.03135, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.22660 
Train Epoch: 4 [686/1000 21952/32000 (69%)] Loss: 1.97661 (semantic_loss: 0.02913, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20769 
Train Epoch: 4 [691/1000 22112/32000 (69%)] Loss: 1.97655 (semantic_loss: 0.02907, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20431 
Train Epoch: 4 [696/1000 22272/32000 (70%)] Loss: 1.97751 (semantic_loss: 0.02905, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20849 
Train Epoch: 4 [701/1000 22432/32000 (70%)] Loss: 1.97692 (semantic_loss: 0.03042, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20174 
Train Epoch: 4 [706/1000 22592/32000 (71%)] Loss: 1.97624 (semantic_loss: 0.02973, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21056 
Train Epoch: 4 [711/1000 22752/32000 (71%)] Loss: 1.97715 (semantic_loss: 0.02871, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20320 
Train Epoch: 4 [716/1000 22912/32000 (72%)] Loss: 1.97563 (semantic_loss: 0.02717, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20715 
Train Epoch: 4 [721/1000 23072/32000 (72%)] Loss: 1.97502 (semantic_loss: 0.02657, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20557 
Train Epoch: 4 [726/1000 23232/32000 (73%)] Loss: 1.97536 (semantic_loss: 0.02691, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19258 
Train Epoch: 4 [731/1000 23392/32000 (73%)] Loss: 1.97591 (semantic_loss: 0.02843, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18879 
Train Epoch: 4 [736/1000 23552/32000 (74%)] Loss: 1.97948 (semantic_loss: 0.03004, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.32230 
Train Epoch: 4 [741/1000 23712/32000 (74%)] Loss: 1.97319 (semantic_loss: 0.02572, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18537 
Train Epoch: 4 [746/1000 23872/32000 (75%)] Loss: 1.97545 (semantic_loss: 0.02798, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19028 
Train Epoch: 4 [751/1000 24032/32000 (75%)] Loss: 1.97544 (semantic_loss: 0.02894, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19227 
Train Epoch: 4 [756/1000 24192/32000 (76%)] Loss: 1.97669 (semantic_loss: 0.02920, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19320 
Train Epoch: 4 [761/1000 24352/32000 (76%)] Loss: 1.98055 (semantic_loss: 0.03307, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18556 
Train Epoch: 4 [766/1000 24512/32000 (77%)] Loss: 1.97800 (semantic_loss: 0.03150, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18671 
Train Epoch: 4 [771/1000 24672/32000 (77%)] Loss: 1.97591 (semantic_loss: 0.02941, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18690 
Train Epoch: 4 [776/1000 24832/32000 (78%)] Loss: 1.97569 (semantic_loss: 0.02821, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19020 
Train Epoch: 4 [781/1000 24992/32000 (78%)] Loss: 1.97584 (semantic_loss: 0.02836, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18822 
Train Epoch: 4 [786/1000 25152/32000 (79%)] Loss: 1.98137 (semantic_loss: 0.03389, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19397 
Train Epoch: 4 [791/1000 25312/32000 (79%)] Loss: 1.97479 (semantic_loss: 0.02732, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18640 
Train Epoch: 4 [796/1000 25472/32000 (80%)] Loss: 1.97456 (semantic_loss: 0.02806, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18527 
Train Epoch: 4 [801/1000 25632/32000 (80%)] Loss: 1.98007 (semantic_loss: 0.03161, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.47704 
Train Epoch: 4 [806/1000 25792/32000 (81%)] Loss: 1.97994 (semantic_loss: 0.03148, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19780 
Train Epoch: 4 [811/1000 25952/32000 (81%)] Loss: 1.97364 (semantic_loss: 0.02713, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21457 
Train Epoch: 4 [816/1000 26112/32000 (82%)] Loss: 1.97437 (semantic_loss: 0.02787, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18794 
Train Epoch: 4 [821/1000 26272/32000 (82%)] Loss: 1.97442 (semantic_loss: 0.02596, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18739 
Train Epoch: 4 [826/1000 26432/32000 (83%)] Loss: 1.97340 (semantic_loss: 0.02592, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18677 
Train Epoch: 4 [831/1000 26592/32000 (83%)] Loss: 1.97153 (semantic_loss: 0.02406, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20265 
Train Epoch: 4 [836/1000 26752/32000 (84%)] Loss: 1.97040 (semantic_loss: 0.02293, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22283 
Train Epoch: 4 [841/1000 26912/32000 (84%)] Loss: 1.97577 (semantic_loss: 0.02732, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.23282 
Train Epoch: 4 [846/1000 27072/32000 (85%)] Loss: 1.98054 (semantic_loss: 0.03209, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21139 
Train Epoch: 4 [851/1000 27232/32000 (85%)] Loss: 1.97750 (semantic_loss: 0.03002, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20049 
Train Epoch: 4 [856/1000 27392/32000 (86%)] Loss: 1.97889 (semantic_loss: 0.03141, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20411 
Train Epoch: 4 [861/1000 27552/32000 (86%)] Loss: 1.97665 (semantic_loss: 0.02917, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20515 
Train Epoch: 4 [866/1000 27712/32000 (87%)] Loss: 1.97947 (semantic_loss: 0.03199, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20555 
Train Epoch: 4 [871/1000 27872/32000 (87%)] Loss: 1.97557 (semantic_loss: 0.02907, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.78442 
Train Epoch: 4 [876/1000 28032/32000 (88%)] Loss: 1.97236 (semantic_loss: 0.02586, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19255 
Train Epoch: 4 [881/1000 28192/32000 (88%)] Loss: 1.97515 (semantic_loss: 0.02670, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18622 
Train Epoch: 4 [886/1000 28352/32000 (89%)] Loss: 1.97783 (semantic_loss: 0.02937, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18862 
Train Epoch: 4 [891/1000 28512/32000 (89%)] Loss: 1.97534 (semantic_loss: 0.02689, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18727 
Train Epoch: 4 [896/1000 28672/32000 (90%)] Loss: 1.97432 (semantic_loss: 0.02782, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18750 
Train Epoch: 4 [901/1000 28832/32000 (90%)] Loss: 1.97325 (semantic_loss: 0.02577, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19513 
Train Epoch: 4 [906/1000 28992/32000 (91%)] Loss: 1.97781 (semantic_loss: 0.03033, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21225 
Train Epoch: 4 [911/1000 29152/32000 (91%)] Loss: 1.97345 (semantic_loss: 0.02500, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19330 
Train Epoch: 4 [916/1000 29312/32000 (92%)] Loss: 1.97994 (semantic_loss: 0.03149, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.38237 
Train Epoch: 4 [921/1000 29472/32000 (92%)] Loss: 1.97612 (semantic_loss: 0.02962, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19373 
Train Epoch: 4 [926/1000 29632/32000 (93%)] Loss: 1.97463 (semantic_loss: 0.02715, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19790 
Train Epoch: 4 [931/1000 29792/32000 (93%)] Loss: 1.97677 (semantic_loss: 0.02929, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19353 
Train Epoch: 4 [936/1000 29952/32000 (94%)] Loss: 1.97924 (semantic_loss: 0.03079, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18031 
Train Epoch: 4 [941/1000 30112/32000 (94%)] Loss: 1.97908 (semantic_loss: 0.03161, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19150 
Train Epoch: 4 [946/1000 30272/32000 (95%)] Loss: 1.97780 (semantic_loss: 0.03032, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18626 
Train Epoch: 4 [951/1000 30432/32000 (95%)] Loss: 1.98303 (semantic_loss: 0.03457, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18857 
Train Epoch: 4 [956/1000 30592/32000 (96%)] Loss: 1.97850 (semantic_loss: 0.03102, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18837 
Train Epoch: 4 [961/1000 30752/32000 (96%)] Loss: 1.97982 (semantic_loss: 0.03232, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.18532 
Train Epoch: 4 [966/1000 30912/32000 (97%)] Loss: 1.97671 (semantic_loss: 0.02924, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18499 
Train Epoch: 4 [971/1000 31072/32000 (97%)] Loss: 1.97636 (semantic_loss: 0.02985, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18858 
Train Epoch: 4 [976/1000 31232/32000 (98%)] Loss: 1.97714 (semantic_loss: 0.02966, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19214 
Train Epoch: 4 [981/1000 31392/32000 (98%)] Loss: 1.97648 (semantic_loss: 0.02901, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18470 
Train Epoch: 4 [986/1000 31552/32000 (99%)] Loss: 1.97377 (semantic_loss: 0.02629, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18988 
Train Epoch: 4 [991/1000 31712/32000 (99%)] Loss: 1.98238 (semantic_loss: 0.03392, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18821 
Train Epoch: 4 [996/1000 31872/32000 (100%)] Loss: 1.97902 (semantic_loss: 0.03153, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21646 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch4.pth ...
Done in 4.304s
removing stale ckpt [epoch 3] [took 0.00s]
 epoch          : 4
 loss           : 1.9779358742237092
 learning_rate  : 3.6450000000000005e-05
 n_samples      : 128000
 n_steps        : 4000
 ActivityNet_val1_test/t2v_metrics/R1: 6.467358145210494
 ActivityNet_val1_test/t2v_metrics/R5: 25.991458206223307
 ActivityNet_val1_test/t2v_metrics/R10: 41.16331096196868
 ActivityNet_val1_test/t2v_metrics/R50: 81.79784421395159
 ActivityNet_val1_test/t2v_metrics/MedR: 14.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 49.226357535082364
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 19.055599612331324
 ActivityNet_val1_test/v2t_metrics/R1: 7.097823876347366
 ActivityNet_val1_test/v2t_metrics/R5: 28.330282692698802
 ActivityNet_val1_test/v2t_metrics/R10: 44.76306691071792
 ActivityNet_val1_test/v2t_metrics/R50: 81.9808826520236
 ActivityNet_val1_test/v2t_metrics/MedR: 13.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 49.54209884075656
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 20.801691615716837
 mnt_best       : 19.29263833513959
 not_improved_count: 1
Train Epoch: 5 [1/1000 32/32000 (0%)] Loss: 1.97925 (semantic_loss: 0.03177, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=23.87288 
Train Epoch: 5 [6/1000 192/32000 (1%)] Loss: 1.97834 (semantic_loss: 0.02989, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19174 
Train Epoch: 5 [11/1000 352/32000 (1%)] Loss: 1.97546 (semantic_loss: 0.02798, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18693 
Train Epoch: 5 [16/1000 512/32000 (2%)] Loss: 1.97421 (semantic_loss: 0.02770, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18453 
Train Epoch: 5 [21/1000 672/32000 (2%)] Loss: 1.97993 (semantic_loss: 0.03147, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18719 
Train Epoch: 5 [26/1000 832/32000 (3%)] Loss: 1.97949 (semantic_loss: 0.03201, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18840 
Train Epoch: 5 [31/1000 992/32000 (3%)] Loss: 1.97764 (semantic_loss: 0.02918, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19655 
Train Epoch: 5 [36/1000 1152/32000 (4%)] Loss: 1.97448 (semantic_loss: 0.02700, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19593 
Train Epoch: 5 [41/1000 1312/32000 (4%)] Loss: 1.97589 (semantic_loss: 0.02744, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19184 
Train Epoch: 5 [46/1000 1472/32000 (5%)] Loss: 1.97599 (semantic_loss: 0.02754, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18966 
Train Epoch: 5 [51/1000 1632/32000 (5%)] Loss: 1.98240 (semantic_loss: 0.03395, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18394 
Train Epoch: 5 [56/1000 1792/32000 (6%)] Loss: 1.97492 (semantic_loss: 0.02841, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19060 
Train Epoch: 5 [61/1000 1952/32000 (6%)] Loss: 1.97262 (semantic_loss: 0.02514, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18673 
Train Epoch: 5 [66/1000 2112/32000 (7%)] Loss: 1.97528 (semantic_loss: 0.02878, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.37808 
Train Epoch: 5 [71/1000 2272/32000 (7%)] Loss: 1.97742 (semantic_loss: 0.02994, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20028 
Train Epoch: 5 [76/1000 2432/32000 (8%)] Loss: 1.97576 (semantic_loss: 0.02730, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=1.29675 
Train Epoch: 5 [81/1000 2592/32000 (8%)] Loss: 1.97884 (semantic_loss: 0.03039, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18606 
Train Epoch: 5 [86/1000 2752/32000 (9%)] Loss: 1.97795 (semantic_loss: 0.02949, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18460 
Train Epoch: 5 [91/1000 2912/32000 (9%)] Loss: 1.97339 (semantic_loss: 0.02591, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18699 
Train Epoch: 5 [96/1000 3072/32000 (10%)] Loss: 1.97410 (semantic_loss: 0.02564, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18581 
Train Epoch: 5 [101/1000 3232/32000 (10%)] Loss: 1.97077 (semantic_loss: 0.02427, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18786 
Train Epoch: 5 [106/1000 3392/32000 (11%)] Loss: 1.97899 (semantic_loss: 0.03053, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18818 
Train Epoch: 5 [111/1000 3552/32000 (11%)] Loss: 1.97451 (semantic_loss: 0.02606, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19567 
Train Epoch: 5 [116/1000 3712/32000 (12%)] Loss: 1.97416 (semantic_loss: 0.02668, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18737 
Train Epoch: 5 [121/1000 3872/32000 (12%)] Loss: 1.97393 (semantic_loss: 0.02645, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20776 
Train Epoch: 5 [126/1000 4032/32000 (13%)] Loss: 1.97334 (semantic_loss: 0.02684, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20593 
Train Epoch: 5 [131/1000 4192/32000 (13%)] Loss: 1.97126 (semantic_loss: 0.02378, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22927 
Train Epoch: 5 [136/1000 4352/32000 (14%)] Loss: 1.97688 (semantic_loss: 0.02842, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22114 
Train Epoch: 5 [141/1000 4512/32000 (14%)] Loss: 1.97486 (semantic_loss: 0.02835, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22286 
Train Epoch: 5 [146/1000 4672/32000 (15%)] Loss: 1.97518 (semantic_loss: 0.02673, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19775 
Train Epoch: 5 [151/1000 4832/32000 (15%)] Loss: 1.97383 (semantic_loss: 0.02635, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19886 
Train Epoch: 5 [156/1000 4992/32000 (16%)] Loss: 1.97702 (semantic_loss: 0.03052, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19105 
Train Epoch: 5 [161/1000 5152/32000 (16%)] Loss: 1.97284 (semantic_loss: 0.02536, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18545 
Train Epoch: 5 [166/1000 5312/32000 (17%)] Loss: 1.97442 (semantic_loss: 0.02791, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18949 
Train Epoch: 5 [171/1000 5472/32000 (17%)] Loss: 1.97232 (semantic_loss: 0.02485, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18507 
Train Epoch: 5 [176/1000 5632/32000 (18%)] Loss: 1.96999 (semantic_loss: 0.02252, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18565 
Train Epoch: 5 [181/1000 5792/32000 (18%)] Loss: 1.97615 (semantic_loss: 0.02866, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18695 
Train Epoch: 5 [186/1000 5952/32000 (19%)] Loss: 1.97515 (semantic_loss: 0.02767, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19245 
Train Epoch: 5 [191/1000 6112/32000 (19%)] Loss: 1.97282 (semantic_loss: 0.02535, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18984 
Train Epoch: 5 [196/1000 6272/32000 (20%)] Loss: 1.97677 (semantic_loss: 0.02831, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19067 
Train Epoch: 5 [201/1000 6432/32000 (20%)] Loss: 1.97495 (semantic_loss: 0.02747, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18990 
Train Epoch: 5 [206/1000 6592/32000 (21%)] Loss: 1.97249 (semantic_loss: 0.02502, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18910 
Train Epoch: 5 [211/1000 6752/32000 (21%)] Loss: 1.96990 (semantic_loss: 0.02341, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18367 
Train Epoch: 5 [216/1000 6912/32000 (22%)] Loss: 1.97461 (semantic_loss: 0.02615, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18852 
Train Epoch: 5 [221/1000 7072/32000 (22%)] Loss: 1.97256 (semantic_loss: 0.02509, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18567 
Train Epoch: 5 [226/1000 7232/32000 (23%)] Loss: 1.97009 (semantic_loss: 0.02261, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18467 
Train Epoch: 5 [231/1000 7392/32000 (23%)] Loss: 1.97226 (semantic_loss: 0.02576, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18328 
Train Epoch: 5 [236/1000 7552/32000 (24%)] Loss: 1.97674 (semantic_loss: 0.02926, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18598 
Train Epoch: 5 [241/1000 7712/32000 (24%)] Loss: 1.97808 (semantic_loss: 0.03060, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18793 
Train Epoch: 5 [246/1000 7872/32000 (25%)] Loss: 1.97203 (semantic_loss: 0.02553, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18700 
Train Epoch: 5 [251/1000 8032/32000 (25%)] Loss: 1.97529 (semantic_loss: 0.02780, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18791 
Train Epoch: 5 [256/1000 8192/32000 (26%)] Loss: 1.98365 (semantic_loss: 0.03715, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18520 
Train Epoch: 5 [261/1000 8352/32000 (26%)] Loss: 1.97217 (semantic_loss: 0.02371, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.23290 
Train Epoch: 5 [266/1000 8512/32000 (27%)] Loss: 1.97330 (semantic_loss: 0.02387, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.18904 
Train Epoch: 5 [271/1000 8672/32000 (27%)] Loss: 1.98080 (semantic_loss: 0.03332, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18646 
Train Epoch: 5 [276/1000 8832/32000 (28%)] Loss: 1.97284 (semantic_loss: 0.02536, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18846 
Train Epoch: 5 [281/1000 8992/32000 (28%)] Loss: 1.97088 (semantic_loss: 0.02437, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21074 
Train Epoch: 5 [286/1000 9152/32000 (29%)] Loss: 1.97244 (semantic_loss: 0.02497, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21022 
Train Epoch: 5 [291/1000 9312/32000 (29%)] Loss: 1.97737 (semantic_loss: 0.02893, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21424 
Train Epoch: 5 [296/1000 9472/32000 (30%)] Loss: 1.97371 (semantic_loss: 0.02720, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19936 
Train Epoch: 5 [301/1000 9632/32000 (30%)] Loss: 1.97460 (semantic_loss: 0.02712, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20218 
Train Epoch: 5 [306/1000 9792/32000 (31%)] Loss: 1.98075 (semantic_loss: 0.03328, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20290 
Train Epoch: 5 [311/1000 9952/32000 (31%)] Loss: 1.97747 (semantic_loss: 0.02901, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20221 
Train Epoch: 5 [316/1000 10112/32000 (32%)] Loss: 1.97499 (semantic_loss: 0.02654, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19037 
Train Epoch: 5 [321/1000 10272/32000 (32%)] Loss: 1.97850 (semantic_loss: 0.03103, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19574 
Train Epoch: 5 [326/1000 10432/32000 (33%)] Loss: 1.97483 (semantic_loss: 0.02736, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19491 
Train Epoch: 5 [331/1000 10592/32000 (33%)] Loss: 1.97209 (semantic_loss: 0.02558, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20956 
Train Epoch: 5 [336/1000 10752/32000 (34%)] Loss: 1.97958 (semantic_loss: 0.03112, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18714 
Train Epoch: 5 [341/1000 10912/32000 (34%)] Loss: 1.97635 (semantic_loss: 0.02886, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18772 
Train Epoch: 5 [346/1000 11072/32000 (35%)] Loss: 1.97981 (semantic_loss: 0.03234, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19161 
Train Epoch: 5 [351/1000 11232/32000 (35%)] Loss: 1.97471 (semantic_loss: 0.02822, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20237 
Train Epoch: 5 [356/1000 11392/32000 (36%)] Loss: 1.97011 (semantic_loss: 0.02166, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19072 
Train Epoch: 5 [361/1000 11552/32000 (36%)] Loss: 1.97787 (semantic_loss: 0.02940, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19035 
Train Epoch: 5 [366/1000 11712/32000 (37%)] Loss: 1.97741 (semantic_loss: 0.02896, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18455 
Train Epoch: 5 [371/1000 11872/32000 (37%)] Loss: 1.97536 (semantic_loss: 0.02690, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19883 
Train Epoch: 5 [376/1000 12032/32000 (38%)] Loss: 1.96942 (semantic_loss: 0.02195, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18590 
Train Epoch: 5 [381/1000 12192/32000 (38%)] Loss: 1.96997 (semantic_loss: 0.02250, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18820 
Train Epoch: 5 [386/1000 12352/32000 (39%)] Loss: 1.97899 (semantic_loss: 0.03152, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.39041 
Train Epoch: 5 [391/1000 12512/32000 (39%)] Loss: 1.97467 (semantic_loss: 0.02720, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18308 
Train Epoch: 5 [396/1000 12672/32000 (40%)] Loss: 1.97821 (semantic_loss: 0.03073, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=1.26513 
Train Epoch: 5 [401/1000 12832/32000 (40%)] Loss: 1.98082 (semantic_loss: 0.03334, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18824 
Train Epoch: 5 [406/1000 12992/32000 (41%)] Loss: 1.97315 (semantic_loss: 0.02469, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19037 
Train Epoch: 5 [411/1000 13152/32000 (41%)] Loss: 1.97629 (semantic_loss: 0.02881, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21448 
Train Epoch: 5 [416/1000 13312/32000 (42%)] Loss: 1.97438 (semantic_loss: 0.02690, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19030 
Train Epoch: 5 [421/1000 13472/32000 (42%)] Loss: 1.97510 (semantic_loss: 0.02762, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21253 
Train Epoch: 5 [426/1000 13632/32000 (43%)] Loss: 1.97199 (semantic_loss: 0.02549, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18790 
Train Epoch: 5 [431/1000 13792/32000 (43%)] Loss: 1.97434 (semantic_loss: 0.02686, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18811 
Train Epoch: 5 [436/1000 13952/32000 (44%)] Loss: 1.97901 (semantic_loss: 0.03055, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19027 
Train Epoch: 5 [441/1000 14112/32000 (44%)] Loss: 1.97019 (semantic_loss: 0.02272, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19973 
Train Epoch: 5 [446/1000 14272/32000 (45%)] Loss: 1.97616 (semantic_loss: 0.02869, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20380 
Train Epoch: 5 [451/1000 14432/32000 (45%)] Loss: 1.97684 (semantic_loss: 0.02935, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20767 
Train Epoch: 5 [456/1000 14592/32000 (46%)] Loss: 1.97721 (semantic_loss: 0.02973, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20617 
Train Epoch: 5 [461/1000 14752/32000 (46%)] Loss: 1.98001 (semantic_loss: 0.03155, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20136 
Train Epoch: 5 [466/1000 14912/32000 (47%)] Loss: 1.97528 (semantic_loss: 0.02683, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20601 
Train Epoch: 5 [471/1000 15072/32000 (47%)] Loss: 1.97721 (semantic_loss: 0.02875, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19887 
Train Epoch: 5 [476/1000 15232/32000 (48%)] Loss: 1.97145 (semantic_loss: 0.02398, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19437 
Train Epoch: 5 [481/1000 15392/32000 (48%)] Loss: 1.97871 (semantic_loss: 0.02928, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.18611 
Train Epoch: 5 [486/1000 15552/32000 (49%)] Loss: 1.97419 (semantic_loss: 0.02671, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19195 
Train Epoch: 5 [491/1000 15712/32000 (49%)] Loss: 1.97285 (semantic_loss: 0.02635, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19341 
Train Epoch: 5 [496/1000 15872/32000 (50%)] Loss: 1.97375 (semantic_loss: 0.02626, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18662 
Train Epoch: 5 [501/1000 16032/32000 (50%)] Loss: 1.97751 (semantic_loss: 0.02906, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18690 
Train Epoch: 5 [506/1000 16192/32000 (51%)] Loss: 1.97063 (semantic_loss: 0.02315, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18940 
Train Epoch: 5 [511/1000 16352/32000 (51%)] Loss: 1.97311 (semantic_loss: 0.02466, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18973 
Train Epoch: 5 [516/1000 16512/32000 (52%)] Loss: 1.97526 (semantic_loss: 0.02876, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19425 
Train Epoch: 5 [521/1000 16672/32000 (52%)] Loss: 1.97749 (semantic_loss: 0.03000, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19053 
Train Epoch: 5 [526/1000 16832/32000 (53%)] Loss: 1.97917 (semantic_loss: 0.03267, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18962 
Train Epoch: 5 [531/1000 16992/32000 (53%)] Loss: 1.97693 (semantic_loss: 0.02847, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18748 
Train Epoch: 5 [536/1000 17152/32000 (54%)] Loss: 1.97459 (semantic_loss: 0.02809, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18930 
Train Epoch: 5 [541/1000 17312/32000 (54%)] Loss: 1.97141 (semantic_loss: 0.02393, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19931 
Train Epoch: 5 [546/1000 17472/32000 (55%)] Loss: 1.97410 (semantic_loss: 0.02759, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18778 
Train Epoch: 5 [551/1000 17632/32000 (55%)] Loss: 1.97226 (semantic_loss: 0.02576, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18700 
Train Epoch: 5 [556/1000 17792/32000 (56%)] Loss: 1.97483 (semantic_loss: 0.02735, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18772 
Train Epoch: 5 [561/1000 17952/32000 (56%)] Loss: 1.97705 (semantic_loss: 0.02957, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18500 
Train Epoch: 5 [566/1000 18112/32000 (57%)] Loss: 1.97327 (semantic_loss: 0.02579, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19230 
Train Epoch: 5 [571/1000 18272/32000 (57%)] Loss: 1.97393 (semantic_loss: 0.02841, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18515 
Train Epoch: 5 [576/1000 18432/32000 (58%)] Loss: 1.97832 (semantic_loss: 0.03085, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18810 
Train Epoch: 5 [581/1000 18592/32000 (58%)] Loss: 1.97834 (semantic_loss: 0.03086, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22830 
Train Epoch: 5 [586/1000 18752/32000 (59%)] Loss: 1.97756 (semantic_loss: 0.02911, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19138 
Train Epoch: 5 [591/1000 18912/32000 (59%)] Loss: 1.97458 (semantic_loss: 0.02710, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20331 
Train Epoch: 5 [596/1000 19072/32000 (60%)] Loss: 1.97439 (semantic_loss: 0.02690, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22521 
Train Epoch: 5 [601/1000 19232/32000 (60%)] Loss: 1.97795 (semantic_loss: 0.02950, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21305 
Train Epoch: 5 [606/1000 19392/32000 (61%)] Loss: 1.97646 (semantic_loss: 0.02801, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19667 
Train Epoch: 5 [611/1000 19552/32000 (61%)] Loss: 1.97604 (semantic_loss: 0.02856, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.25103 
Train Epoch: 5 [616/1000 19712/32000 (62%)] Loss: 1.97789 (semantic_loss: 0.02943, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20497 
Train Epoch: 5 [621/1000 19872/32000 (62%)] Loss: 1.96961 (semantic_loss: 0.02214, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19138 
Train Epoch: 5 [626/1000 20032/32000 (63%)] Loss: 1.97770 (semantic_loss: 0.03022, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18913 
Train Epoch: 5 [631/1000 20192/32000 (63%)] Loss: 1.97666 (semantic_loss: 0.03113, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18703 
Train Epoch: 5 [636/1000 20352/32000 (64%)] Loss: 1.97651 (semantic_loss: 0.02804, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18531 
Train Epoch: 5 [641/1000 20512/32000 (64%)] Loss: 1.97269 (semantic_loss: 0.02521, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18761 
Train Epoch: 5 [646/1000 20672/32000 (65%)] Loss: 1.97550 (semantic_loss: 0.02900, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18582 
Train Epoch: 5 [651/1000 20832/32000 (65%)] Loss: 1.97418 (semantic_loss: 0.02670, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19130 
Train Epoch: 5 [656/1000 20992/32000 (66%)] Loss: 1.97106 (semantic_loss: 0.02358, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19300 
Train Epoch: 5 [661/1000 21152/32000 (66%)] Loss: 1.97238 (semantic_loss: 0.02392, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19220 
Train Epoch: 5 [666/1000 21312/32000 (67%)] Loss: 1.97203 (semantic_loss: 0.02455, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18757 
Train Epoch: 5 [671/1000 21472/32000 (67%)] Loss: 1.97191 (semantic_loss: 0.02443, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18666 
Train Epoch: 5 [676/1000 21632/32000 (68%)] Loss: 1.97280 (semantic_loss: 0.02532, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18792 
Train Epoch: 5 [681/1000 21792/32000 (68%)] Loss: 1.97799 (semantic_loss: 0.03051, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18978 
Train Epoch: 5 [686/1000 21952/32000 (69%)] Loss: 1.97034 (semantic_loss: 0.02383, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19009 
Train Epoch: 5 [691/1000 22112/32000 (69%)] Loss: 1.97082 (semantic_loss: 0.02432, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18847 
Train Epoch: 5 [696/1000 22272/32000 (70%)] Loss: 1.97589 (semantic_loss: 0.02743, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20473 
Train Epoch: 5 [701/1000 22432/32000 (70%)] Loss: 1.97364 (semantic_loss: 0.02714, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18801 
Train Epoch: 5 [706/1000 22592/32000 (71%)] Loss: 1.97201 (semantic_loss: 0.02453, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.40497 
Train Epoch: 5 [711/1000 22752/32000 (71%)] Loss: 1.97657 (semantic_loss: 0.03007, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20056 
Train Epoch: 5 [716/1000 22912/32000 (72%)] Loss: 1.97770 (semantic_loss: 0.02925, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=1.37746 
Train Epoch: 5 [721/1000 23072/32000 (72%)] Loss: 1.97635 (semantic_loss: 0.02692, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.18569 
Train Epoch: 5 [726/1000 23232/32000 (73%)] Loss: 1.97250 (semantic_loss: 0.02600, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18922 
Train Epoch: 5 [731/1000 23392/32000 (73%)] Loss: 1.97209 (semantic_loss: 0.02461, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19387 
Train Epoch: 5 [736/1000 23552/32000 (74%)] Loss: 1.97145 (semantic_loss: 0.02397, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18516 
Train Epoch: 5 [741/1000 23712/32000 (74%)] Loss: 1.97227 (semantic_loss: 0.02479, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18847 
Train Epoch: 5 [746/1000 23872/32000 (75%)] Loss: 1.97950 (semantic_loss: 0.03202, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18763 
Train Epoch: 5 [751/1000 24032/32000 (75%)] Loss: 1.97367 (semantic_loss: 0.02717, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20780 
Train Epoch: 5 [756/1000 24192/32000 (76%)] Loss: 1.97543 (semantic_loss: 0.02698, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21770 
Train Epoch: 5 [761/1000 24352/32000 (76%)] Loss: 1.97544 (semantic_loss: 0.02699, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21335 
Train Epoch: 5 [766/1000 24512/32000 (77%)] Loss: 1.97440 (semantic_loss: 0.02790, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22417 
Train Epoch: 5 [771/1000 24672/32000 (77%)] Loss: 1.97069 (semantic_loss: 0.02321, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20602 
Train Epoch: 5 [776/1000 24832/32000 (78%)] Loss: 1.97361 (semantic_loss: 0.02710, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19815 
Train Epoch: 5 [781/1000 24992/32000 (78%)] Loss: 1.97009 (semantic_loss: 0.02457, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19247 
Train Epoch: 5 [786/1000 25152/32000 (79%)] Loss: 1.97755 (semantic_loss: 0.03007, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19339 
Train Epoch: 5 [791/1000 25312/32000 (79%)] Loss: 1.97388 (semantic_loss: 0.02542, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18863 
Train Epoch: 5 [796/1000 25472/32000 (80%)] Loss: 1.97001 (semantic_loss: 0.02253, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18783 
Train Epoch: 5 [801/1000 25632/32000 (80%)] Loss: 1.96914 (semantic_loss: 0.02264, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19195 
Train Epoch: 5 [806/1000 25792/32000 (81%)] Loss: 1.97141 (semantic_loss: 0.02394, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18913 
Train Epoch: 5 [811/1000 25952/32000 (81%)] Loss: 1.97883 (semantic_loss: 0.03037, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18868 
Train Epoch: 5 [816/1000 26112/32000 (82%)] Loss: 1.97270 (semantic_loss: 0.02620, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19302 
Train Epoch: 5 [821/1000 26272/32000 (82%)] Loss: 1.97320 (semantic_loss: 0.02572, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19466 
Train Epoch: 5 [826/1000 26432/32000 (83%)] Loss: 1.97511 (semantic_loss: 0.02763, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19082 
Train Epoch: 5 [831/1000 26592/32000 (83%)] Loss: 1.97229 (semantic_loss: 0.02481, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18718 
Train Epoch: 5 [836/1000 26752/32000 (84%)] Loss: 1.97630 (semantic_loss: 0.02881, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18318 
Train Epoch: 5 [841/1000 26912/32000 (84%)] Loss: 1.97484 (semantic_loss: 0.02736, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18860 
Train Epoch: 5 [846/1000 27072/32000 (85%)] Loss: 1.97473 (semantic_loss: 0.02724, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18976 
Train Epoch: 5 [851/1000 27232/32000 (85%)] Loss: 1.97751 (semantic_loss: 0.02905, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19025 
Train Epoch: 5 [856/1000 27392/32000 (86%)] Loss: 1.97310 (semantic_loss: 0.02562, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18887 
Train Epoch: 5 [861/1000 27552/32000 (86%)] Loss: 1.97291 (semantic_loss: 0.02640, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18929 
Train Epoch: 5 [866/1000 27712/32000 (87%)] Loss: 1.97176 (semantic_loss: 0.02427, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18622 
Train Epoch: 5 [871/1000 27872/32000 (87%)] Loss: 1.97485 (semantic_loss: 0.02835, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18501 
Train Epoch: 5 [876/1000 28032/32000 (88%)] Loss: 1.97332 (semantic_loss: 0.02584, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18811 
Train Epoch: 5 [881/1000 28192/32000 (88%)] Loss: 1.97412 (semantic_loss: 0.02664, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20473 
Train Epoch: 5 [886/1000 28352/32000 (89%)] Loss: 1.97003 (semantic_loss: 0.02255, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18839 
Train Epoch: 5 [891/1000 28512/32000 (89%)] Loss: 1.97009 (semantic_loss: 0.02261, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19026 
Train Epoch: 5 [896/1000 28672/32000 (90%)] Loss: 1.97521 (semantic_loss: 0.02774, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18814 
Train Epoch: 5 [901/1000 28832/32000 (90%)] Loss: 1.97771 (semantic_loss: 0.03120, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24298 
Train Epoch: 5 [906/1000 28992/32000 (91%)] Loss: 1.96982 (semantic_loss: 0.02430, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.21641 
Train Epoch: 5 [911/1000 29152/32000 (91%)] Loss: 1.97895 (semantic_loss: 0.03343, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20876 
Train Epoch: 5 [916/1000 29312/32000 (92%)] Loss: 1.97361 (semantic_loss: 0.02612, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19394 
Train Epoch: 5 [921/1000 29472/32000 (92%)] Loss: 1.97708 (semantic_loss: 0.02863, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20434 
Train Epoch: 5 [926/1000 29632/32000 (93%)] Loss: 1.97355 (semantic_loss: 0.02509, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20770 
Train Epoch: 5 [931/1000 29792/32000 (93%)] Loss: 1.97762 (semantic_loss: 0.02917, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20027 
Train Epoch: 5 [936/1000 29952/32000 (94%)] Loss: 1.97193 (semantic_loss: 0.02445, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20585 
Train Epoch: 5 [941/1000 30112/32000 (94%)] Loss: 1.97143 (semantic_loss: 0.02394, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21289 
Train Epoch: 5 [946/1000 30272/32000 (95%)] Loss: 1.97176 (semantic_loss: 0.02428, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18414 
Train Epoch: 5 [951/1000 30432/32000 (95%)] Loss: 1.97290 (semantic_loss: 0.02640, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19408 
Train Epoch: 5 [956/1000 30592/32000 (96%)] Loss: 1.97476 (semantic_loss: 0.02728, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18935 
Train Epoch: 5 [961/1000 30752/32000 (96%)] Loss: 1.97087 (semantic_loss: 0.02339, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18742 
Train Epoch: 5 [966/1000 30912/32000 (97%)] Loss: 1.97422 (semantic_loss: 0.02675, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21916 
Train Epoch: 5 [971/1000 31072/32000 (97%)] Loss: 1.97373 (semantic_loss: 0.02527, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19091 
Train Epoch: 5 [976/1000 31232/32000 (98%)] Loss: 1.97255 (semantic_loss: 0.02507, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20112 
Train Epoch: 5 [981/1000 31392/32000 (98%)] Loss: 1.97175 (semantic_loss: 0.02427, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18636 
Train Epoch: 5 [986/1000 31552/32000 (99%)] Loss: 1.97968 (semantic_loss: 0.03123, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18554 
Train Epoch: 5 [991/1000 31712/32000 (99%)] Loss: 1.97712 (semantic_loss: 0.02867, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18572 
Train Epoch: 5 [996/1000 31872/32000 (100%)] Loss: 1.97551 (semantic_loss: 0.02802, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.18992 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch5.pth ...
Done in 5.431s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch5.pth ...
Done in 10.457s
removing stale ckpt [epoch 4] [took 0.05s]
 epoch          : 5
 loss           : 1.9748551261425018
 learning_rate  : 3.280500000000001e-05
 n_samples      : 160000
 n_steps        : 5000
 ActivityNet_val1_test/t2v_metrics/R1: 7.99267846247712
 ActivityNet_val1_test/t2v_metrics/R5: 28.08623144193614
 ActivityNet_val1_test/t2v_metrics/R10: 43.78686190766728
 ActivityNet_val1_test/t2v_metrics/R50: 83.68924140736222
 ActivityNet_val1_test/t2v_metrics/MedR: 13.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 47.628635346756155
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 21.421171158467622
 ActivityNet_val1_test/v2t_metrics/R1: 8.643481797844213
 ActivityNet_val1_test/v2t_metrics/R5: 30.648769574944073
 ActivityNet_val1_test/v2t_metrics/R10: 46.95952816758186
 ActivityNet_val1_test/v2t_metrics/R50: 84.09599349196665
 ActivityNet_val1_test/v2t_metrics/MedR: 12.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 48.66209070571487
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 23.17084276461473
 mnt_best       : 21.421171158467622
 not_improved_count: 0
Train Epoch: 6 [1/1000 32/32000 (0%)] Loss: 1.97793 (semantic_loss: 0.03045, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=20.15692 
Train Epoch: 6 [6/1000 192/32000 (1%)] Loss: 1.97195 (semantic_loss: 0.02447, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22929 
Train Epoch: 6 [11/1000 352/32000 (1%)] Loss: 1.97112 (semantic_loss: 0.02461, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19924 
Train Epoch: 6 [16/1000 512/32000 (2%)] Loss: 1.97585 (semantic_loss: 0.02837, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.88897 
Train Epoch: 6 [21/1000 672/32000 (2%)] Loss: 1.97366 (semantic_loss: 0.02521, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22340 
Train Epoch: 6 [26/1000 832/32000 (3%)] Loss: 1.96903 (semantic_loss: 0.02156, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.49813 
Train Epoch: 6 [31/1000 992/32000 (3%)] Loss: 1.97838 (semantic_loss: 0.03090, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19354 
Train Epoch: 6 [36/1000 1152/32000 (4%)] Loss: 1.97130 (semantic_loss: 0.02382, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21558 
Train Epoch: 6 [41/1000 1312/32000 (4%)] Loss: 1.98030 (semantic_loss: 0.03184, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20021 
Train Epoch: 6 [46/1000 1472/32000 (5%)] Loss: 1.97905 (semantic_loss: 0.03157, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20639 
Train Epoch: 6 [51/1000 1632/32000 (5%)] Loss: 1.96821 (semantic_loss: 0.02171, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21612 
Train Epoch: 6 [56/1000 1792/32000 (6%)] Loss: 1.97456 (semantic_loss: 0.02708, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19260 
Train Epoch: 6 [61/1000 1952/32000 (6%)] Loss: 1.97249 (semantic_loss: 0.02404, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18397 
Train Epoch: 6 [66/1000 2112/32000 (7%)] Loss: 1.97386 (semantic_loss: 0.02541, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18785 
Train Epoch: 6 [71/1000 2272/32000 (7%)] Loss: 1.96701 (semantic_loss: 0.02051, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18850 
Train Epoch: 6 [76/1000 2432/32000 (8%)] Loss: 1.97657 (semantic_loss: 0.02812, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18781 
Train Epoch: 6 [81/1000 2592/32000 (8%)] Loss: 1.98091 (semantic_loss: 0.03440, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19404 
Train Epoch: 6 [86/1000 2752/32000 (9%)] Loss: 1.97179 (semantic_loss: 0.02528, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18802 
Train Epoch: 6 [91/1000 2912/32000 (9%)] Loss: 1.97485 (semantic_loss: 0.02834, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18840 
Train Epoch: 6 [96/1000 3072/32000 (10%)] Loss: 1.97997 (semantic_loss: 0.03347, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.30346 
Train Epoch: 6 [101/1000 3232/32000 (10%)] Loss: 1.97488 (semantic_loss: 0.02642, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18582 
Train Epoch: 6 [106/1000 3392/32000 (11%)] Loss: 1.97387 (semantic_loss: 0.02639, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18575 
Train Epoch: 6 [111/1000 3552/32000 (11%)] Loss: 1.97687 (semantic_loss: 0.02744, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.21879 
Train Epoch: 6 [116/1000 3712/32000 (12%)] Loss: 1.97809 (semantic_loss: 0.03159, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18621 
Train Epoch: 6 [121/1000 3872/32000 (12%)] Loss: 1.97817 (semantic_loss: 0.03166, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18724 
Train Epoch: 6 [126/1000 4032/32000 (13%)] Loss: 1.97251 (semantic_loss: 0.02503, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19385 
Train Epoch: 6 [131/1000 4192/32000 (13%)] Loss: 1.97296 (semantic_loss: 0.02549, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.38331 
Train Epoch: 6 [136/1000 4352/32000 (14%)] Loss: 1.97427 (semantic_loss: 0.02581, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18404 
Train Epoch: 6 [141/1000 4512/32000 (14%)] Loss: 1.97443 (semantic_loss: 0.02695, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20391 
Train Epoch: 6 [146/1000 4672/32000 (15%)] Loss: 1.97551 (semantic_loss: 0.02802, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.30233 
Train Epoch: 6 [151/1000 4832/32000 (15%)] Loss: 1.97119 (semantic_loss: 0.02272, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18845 
Train Epoch: 6 [156/1000 4992/32000 (16%)] Loss: 1.97005 (semantic_loss: 0.02257, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18361 
Train Epoch: 6 [161/1000 5152/32000 (16%)] Loss: 1.97261 (semantic_loss: 0.02513, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19464 
Train Epoch: 6 [166/1000 5312/32000 (17%)] Loss: 1.97212 (semantic_loss: 0.02464, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21744 
Train Epoch: 6 [171/1000 5472/32000 (17%)] Loss: 1.97005 (semantic_loss: 0.02257, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21888 
Train Epoch: 6 [176/1000 5632/32000 (18%)] Loss: 1.97442 (semantic_loss: 0.02791, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.23781 
Train Epoch: 6 [181/1000 5792/32000 (18%)] Loss: 1.97546 (semantic_loss: 0.02799, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22942 
Train Epoch: 6 [186/1000 5952/32000 (19%)] Loss: 1.97474 (semantic_loss: 0.02628, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20871 
Train Epoch: 6 [191/1000 6112/32000 (19%)] Loss: 1.97646 (semantic_loss: 0.02897, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18977 
Train Epoch: 6 [196/1000 6272/32000 (20%)] Loss: 1.97476 (semantic_loss: 0.02728, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20794 
Train Epoch: 6 [201/1000 6432/32000 (20%)] Loss: 1.97312 (semantic_loss: 0.02661, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19228 
Train Epoch: 6 [206/1000 6592/32000 (21%)] Loss: 1.97444 (semantic_loss: 0.02696, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18624 
Train Epoch: 6 [211/1000 6752/32000 (21%)] Loss: 1.97203 (semantic_loss: 0.02553, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19239 
Train Epoch: 6 [216/1000 6912/32000 (22%)] Loss: 1.97510 (semantic_loss: 0.02665, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18851 
Train Epoch: 6 [221/1000 7072/32000 (22%)] Loss: 1.97397 (semantic_loss: 0.02746, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18735 
Train Epoch: 6 [226/1000 7232/32000 (23%)] Loss: 1.96852 (semantic_loss: 0.02105, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19444 
Train Epoch: 6 [231/1000 7392/32000 (23%)] Loss: 1.96913 (semantic_loss: 0.02165, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19015 
Train Epoch: 6 [236/1000 7552/32000 (24%)] Loss: 1.97353 (semantic_loss: 0.02606, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19047 
Train Epoch: 6 [241/1000 7712/32000 (24%)] Loss: 1.97770 (semantic_loss: 0.02924, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18707 
Train Epoch: 6 [246/1000 7872/32000 (25%)] Loss: 1.97222 (semantic_loss: 0.02668, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.19602 
Train Epoch: 6 [251/1000 8032/32000 (25%)] Loss: 1.97102 (semantic_loss: 0.02354, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18547 
Train Epoch: 6 [256/1000 8192/32000 (26%)] Loss: 1.96853 (semantic_loss: 0.02202, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20669 
Train Epoch: 6 [261/1000 8352/32000 (26%)] Loss: 1.97342 (semantic_loss: 0.02594, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18504 
Train Epoch: 6 [266/1000 8512/32000 (27%)] Loss: 1.97049 (semantic_loss: 0.02301, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20072 
Train Epoch: 6 [271/1000 8672/32000 (27%)] Loss: 1.96963 (semantic_loss: 0.02215, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19322 
Train Epoch: 6 [276/1000 8832/32000 (28%)] Loss: 1.97574 (semantic_loss: 0.02825, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20442 
Train Epoch: 6 [281/1000 8992/32000 (28%)] Loss: 1.97093 (semantic_loss: 0.02345, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18981 
Train Epoch: 6 [286/1000 9152/32000 (29%)] Loss: 1.97099 (semantic_loss: 0.02449, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18826 
Train Epoch: 6 [291/1000 9312/32000 (29%)] Loss: 1.97418 (semantic_loss: 0.02768, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18537 
Train Epoch: 6 [296/1000 9472/32000 (30%)] Loss: 1.96992 (semantic_loss: 0.02244, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18812 
Train Epoch: 6 [301/1000 9632/32000 (30%)] Loss: 1.97095 (semantic_loss: 0.02347, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18782 
Train Epoch: 6 [306/1000 9792/32000 (31%)] Loss: 1.97102 (semantic_loss: 0.02548, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18577 
Train Epoch: 6 [311/1000 9952/32000 (31%)] Loss: 1.97296 (semantic_loss: 0.02549, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18936 
Train Epoch: 6 [316/1000 10112/32000 (32%)] Loss: 1.97417 (semantic_loss: 0.02670, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18820 
Train Epoch: 6 [321/1000 10272/32000 (32%)] Loss: 1.97094 (semantic_loss: 0.02443, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18684 
Train Epoch: 6 [326/1000 10432/32000 (33%)] Loss: 1.97310 (semantic_loss: 0.02465, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.24630 
Train Epoch: 6 [331/1000 10592/32000 (33%)] Loss: 1.96956 (semantic_loss: 0.02306, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22437 
Train Epoch: 6 [336/1000 10752/32000 (34%)] Loss: 1.97232 (semantic_loss: 0.02483, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.98541 
Train Epoch: 6 [341/1000 10912/32000 (34%)] Loss: 1.96734 (semantic_loss: 0.01986, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19579 
Train Epoch: 6 [346/1000 11072/32000 (35%)] Loss: 1.97594 (semantic_loss: 0.02748, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.44310 
Train Epoch: 6 [351/1000 11232/32000 (35%)] Loss: 1.96930 (semantic_loss: 0.02183, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21047 
Train Epoch: 6 [356/1000 11392/32000 (36%)] Loss: 1.97516 (semantic_loss: 0.02768, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.49903 
Train Epoch: 6 [361/1000 11552/32000 (36%)] Loss: 1.97293 (semantic_loss: 0.02545, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18816 
Train Epoch: 6 [366/1000 11712/32000 (37%)] Loss: 1.97276 (semantic_loss: 0.02528, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18568 
Train Epoch: 6 [371/1000 11872/32000 (37%)] Loss: 1.97029 (semantic_loss: 0.02378, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18577 
Train Epoch: 6 [376/1000 12032/32000 (38%)] Loss: 1.97071 (semantic_loss: 0.02421, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18855 
Train Epoch: 6 [381/1000 12192/32000 (38%)] Loss: 1.96825 (semantic_loss: 0.02077, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18612 
Train Epoch: 6 [386/1000 12352/32000 (39%)] Loss: 1.97547 (semantic_loss: 0.02897, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18688 
Train Epoch: 6 [391/1000 12512/32000 (39%)] Loss: 1.97354 (semantic_loss: 0.02509, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20060 
Train Epoch: 6 [396/1000 12672/32000 (40%)] Loss: 1.97338 (semantic_loss: 0.02591, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19350 
Train Epoch: 6 [401/1000 12832/32000 (40%)] Loss: 1.96924 (semantic_loss: 0.02176, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19789 
Train Epoch: 6 [406/1000 12992/32000 (41%)] Loss: 1.96991 (semantic_loss: 0.02341, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18747 
Train Epoch: 6 [411/1000 13152/32000 (41%)] Loss: 1.96974 (semantic_loss: 0.02226, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18953 
Train Epoch: 6 [416/1000 13312/32000 (42%)] Loss: 1.97191 (semantic_loss: 0.02442, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.30611 
Train Epoch: 6 [421/1000 13472/32000 (42%)] Loss: 1.97758 (semantic_loss: 0.03107, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18920 
Train Epoch: 6 [426/1000 13632/32000 (43%)] Loss: 1.97427 (semantic_loss: 0.02776, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19518 
Train Epoch: 6 [431/1000 13792/32000 (43%)] Loss: 1.97075 (semantic_loss: 0.02327, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22086 
Train Epoch: 6 [436/1000 13952/32000 (44%)] Loss: 1.97554 (semantic_loss: 0.02806, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18652 
Train Epoch: 6 [441/1000 14112/32000 (44%)] Loss: 1.97574 (semantic_loss: 0.02825, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18885 
Train Epoch: 6 [446/1000 14272/32000 (45%)] Loss: 1.97249 (semantic_loss: 0.02501, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18727 
Train Epoch: 6 [451/1000 14432/32000 (45%)] Loss: 1.97215 (semantic_loss: 0.02467, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.37055 
Train Epoch: 6 [456/1000 14592/32000 (46%)] Loss: 1.97070 (semantic_loss: 0.02323, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20404 
Train Epoch: 6 [461/1000 14752/32000 (46%)] Loss: 1.97256 (semantic_loss: 0.02508, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20381 
Train Epoch: 6 [466/1000 14912/32000 (47%)] Loss: 1.96757 (semantic_loss: 0.02107, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.30964 
Train Epoch: 6 [471/1000 15072/32000 (47%)] Loss: 1.97240 (semantic_loss: 0.02394, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18780 
Train Epoch: 6 [476/1000 15232/32000 (48%)] Loss: 1.97485 (semantic_loss: 0.02736, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21155 
Train Epoch: 6 [481/1000 15392/32000 (48%)] Loss: 1.97409 (semantic_loss: 0.02661, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22731 
Train Epoch: 6 [486/1000 15552/32000 (49%)] Loss: 1.97445 (semantic_loss: 0.02697, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21351 
Train Epoch: 6 [491/1000 15712/32000 (49%)] Loss: 1.96937 (semantic_loss: 0.02286, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22052 
Train Epoch: 6 [496/1000 15872/32000 (50%)] Loss: 1.97327 (semantic_loss: 0.02579, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19569 
Train Epoch: 6 [501/1000 16032/32000 (50%)] Loss: 1.97173 (semantic_loss: 0.02425, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19740 
Train Epoch: 6 [506/1000 16192/32000 (51%)] Loss: 1.97011 (semantic_loss: 0.02263, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19978 
Train Epoch: 6 [511/1000 16352/32000 (51%)] Loss: 1.97048 (semantic_loss: 0.02397, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19646 
Train Epoch: 6 [516/1000 16512/32000 (52%)] Loss: 1.96987 (semantic_loss: 0.02239, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18922 
Train Epoch: 6 [521/1000 16672/32000 (52%)] Loss: 1.97015 (semantic_loss: 0.02073, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.21563 
Train Epoch: 6 [526/1000 16832/32000 (53%)] Loss: 1.97942 (semantic_loss: 0.03096, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18506 
Train Epoch: 6 [531/1000 16992/32000 (53%)] Loss: 1.96931 (semantic_loss: 0.02183, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18562 
Train Epoch: 6 [536/1000 17152/32000 (54%)] Loss: 1.97263 (semantic_loss: 0.02613, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18657 
Train Epoch: 6 [541/1000 17312/32000 (54%)] Loss: 1.97352 (semantic_loss: 0.02604, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20296 
Train Epoch: 6 [546/1000 17472/32000 (55%)] Loss: 1.97385 (semantic_loss: 0.02539, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20961 
Train Epoch: 6 [551/1000 17632/32000 (55%)] Loss: 1.97597 (semantic_loss: 0.02848, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19555 
Train Epoch: 6 [556/1000 17792/32000 (56%)] Loss: 1.96731 (semantic_loss: 0.02081, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18753 
Train Epoch: 6 [561/1000 17952/32000 (56%)] Loss: 1.97209 (semantic_loss: 0.02559, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20964 
Train Epoch: 6 [566/1000 18112/32000 (57%)] Loss: 1.97497 (semantic_loss: 0.02652, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22138 
Train Epoch: 6 [571/1000 18272/32000 (57%)] Loss: 1.97457 (semantic_loss: 0.02611, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19691 
Train Epoch: 6 [576/1000 18432/32000 (58%)] Loss: 1.97048 (semantic_loss: 0.02300, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20742 
Train Epoch: 6 [581/1000 18592/32000 (58%)] Loss: 1.97222 (semantic_loss: 0.02376, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19273 
Train Epoch: 6 [586/1000 18752/32000 (59%)] Loss: 1.97179 (semantic_loss: 0.02530, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20387 
Train Epoch: 6 [591/1000 18912/32000 (59%)] Loss: 1.97352 (semantic_loss: 0.02604, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19176 
Train Epoch: 6 [596/1000 19072/32000 (60%)] Loss: 1.97237 (semantic_loss: 0.02489, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19512 
Train Epoch: 6 [601/1000 19232/32000 (60%)] Loss: 1.97572 (semantic_loss: 0.02726, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18690 
Train Epoch: 6 [606/1000 19392/32000 (61%)] Loss: 1.96892 (semantic_loss: 0.02144, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19468 
Train Epoch: 6 [611/1000 19552/32000 (61%)] Loss: 1.97319 (semantic_loss: 0.02571, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18893 
Train Epoch: 6 [616/1000 19712/32000 (62%)] Loss: 1.97313 (semantic_loss: 0.02663, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18779 
Train Epoch: 6 [621/1000 19872/32000 (62%)] Loss: 1.97448 (semantic_loss: 0.02602, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18695 
Train Epoch: 6 [626/1000 20032/32000 (63%)] Loss: 1.97153 (semantic_loss: 0.02405, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20631 
Train Epoch: 6 [631/1000 20192/32000 (63%)] Loss: 1.97131 (semantic_loss: 0.02383, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20630 
Train Epoch: 6 [636/1000 20352/32000 (64%)] Loss: 1.97374 (semantic_loss: 0.02626, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23247 
Train Epoch: 6 [641/1000 20512/32000 (64%)] Loss: 1.96939 (semantic_loss: 0.02192, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21287 
Train Epoch: 6 [646/1000 20672/32000 (65%)] Loss: 1.97575 (semantic_loss: 0.02729, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.23370 
Train Epoch: 6 [651/1000 20832/32000 (65%)] Loss: 1.97382 (semantic_loss: 0.02634, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21433 
Train Epoch: 6 [656/1000 20992/32000 (66%)] Loss: 1.97375 (semantic_loss: 0.02627, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.82896 
Train Epoch: 6 [661/1000 21152/32000 (66%)] Loss: 1.97329 (semantic_loss: 0.02581, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19975 
Train Epoch: 6 [666/1000 21312/32000 (67%)] Loss: 1.97084 (semantic_loss: 0.02239, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.37294 
Train Epoch: 6 [671/1000 21472/32000 (67%)] Loss: 1.97590 (semantic_loss: 0.02745, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18879 
Train Epoch: 6 [676/1000 21632/32000 (68%)] Loss: 1.97384 (semantic_loss: 0.02733, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19047 
Train Epoch: 6 [681/1000 21792/32000 (68%)] Loss: 1.96925 (semantic_loss: 0.02275, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18762 
Train Epoch: 6 [686/1000 21952/32000 (69%)] Loss: 1.97281 (semantic_loss: 0.02533, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18589 
Train Epoch: 6 [691/1000 22112/32000 (69%)] Loss: 1.96880 (semantic_loss: 0.02229, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19429 
Train Epoch: 6 [696/1000 22272/32000 (70%)] Loss: 1.97376 (semantic_loss: 0.02530, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19152 
Train Epoch: 6 [701/1000 22432/32000 (70%)] Loss: 1.97244 (semantic_loss: 0.02496, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19743 
Train Epoch: 6 [706/1000 22592/32000 (71%)] Loss: 1.97070 (semantic_loss: 0.02322, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18397 
Train Epoch: 6 [711/1000 22752/32000 (71%)] Loss: 1.97772 (semantic_loss: 0.03024, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18579 
Train Epoch: 6 [716/1000 22912/32000 (72%)] Loss: 1.97227 (semantic_loss: 0.02577, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19099 
Train Epoch: 6 [721/1000 23072/32000 (72%)] Loss: 1.97180 (semantic_loss: 0.02529, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18642 
Train Epoch: 6 [726/1000 23232/32000 (73%)] Loss: 1.96775 (semantic_loss: 0.02125, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18816 
Train Epoch: 6 [731/1000 23392/32000 (73%)] Loss: 1.97484 (semantic_loss: 0.02541, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.18761 
Train Epoch: 6 [736/1000 23552/32000 (74%)] Loss: 1.97230 (semantic_loss: 0.02384, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.30958 
Train Epoch: 6 [741/1000 23712/32000 (74%)] Loss: 1.97129 (semantic_loss: 0.02381, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19037 
Train Epoch: 6 [746/1000 23872/32000 (75%)] Loss: 1.96794 (semantic_loss: 0.02143, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19182 
Train Epoch: 6 [751/1000 24032/32000 (75%)] Loss: 1.97290 (semantic_loss: 0.02543, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22518 
Train Epoch: 6 [756/1000 24192/32000 (76%)] Loss: 1.96886 (semantic_loss: 0.02235, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18476 
Train Epoch: 6 [761/1000 24352/32000 (76%)] Loss: 1.97210 (semantic_loss: 0.02560, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18546 
Train Epoch: 6 [766/1000 24512/32000 (77%)] Loss: 1.97535 (semantic_loss: 0.02787, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18739 
Train Epoch: 6 [771/1000 24672/32000 (77%)] Loss: 1.97901 (semantic_loss: 0.03055, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.36764 
Train Epoch: 6 [776/1000 24832/32000 (78%)] Loss: 1.97181 (semantic_loss: 0.02238, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.18391 
Train Epoch: 6 [781/1000 24992/32000 (78%)] Loss: 1.97091 (semantic_loss: 0.02246, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20426 
Train Epoch: 6 [786/1000 25152/32000 (79%)] Loss: 1.97147 (semantic_loss: 0.02302, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.36247 
Train Epoch: 6 [791/1000 25312/32000 (79%)] Loss: 1.97071 (semantic_loss: 0.02420, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22400 
Train Epoch: 6 [796/1000 25472/32000 (80%)] Loss: 1.97333 (semantic_loss: 0.02584, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19689 
Train Epoch: 6 [801/1000 25632/32000 (80%)] Loss: 1.96879 (semantic_loss: 0.02131, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20144 
Train Epoch: 6 [806/1000 25792/32000 (81%)] Loss: 1.96970 (semantic_loss: 0.02319, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20944 
Train Epoch: 6 [811/1000 25952/32000 (81%)] Loss: 1.97005 (semantic_loss: 0.02159, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20375 
Train Epoch: 6 [816/1000 26112/32000 (82%)] Loss: 1.97320 (semantic_loss: 0.02572, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20830 
Train Epoch: 6 [821/1000 26272/32000 (82%)] Loss: 1.96855 (semantic_loss: 0.02107, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20328 
Train Epoch: 6 [826/1000 26432/32000 (83%)] Loss: 1.97012 (semantic_loss: 0.02362, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18836 
Train Epoch: 6 [831/1000 26592/32000 (83%)] Loss: 1.97186 (semantic_loss: 0.02339, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18865 
Train Epoch: 6 [836/1000 26752/32000 (84%)] Loss: 1.96961 (semantic_loss: 0.02311, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18681 
Train Epoch: 6 [841/1000 26912/32000 (84%)] Loss: 1.97209 (semantic_loss: 0.02461, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18650 
Train Epoch: 6 [846/1000 27072/32000 (85%)] Loss: 1.98354 (semantic_loss: 0.03604, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.18896 
Train Epoch: 6 [851/1000 27232/32000 (85%)] Loss: 1.97684 (semantic_loss: 0.02838, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19230 
Train Epoch: 6 [856/1000 27392/32000 (86%)] Loss: 1.97157 (semantic_loss: 0.02409, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19197 
Train Epoch: 6 [861/1000 27552/32000 (86%)] Loss: 1.96872 (semantic_loss: 0.02027, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19004 
Train Epoch: 6 [866/1000 27712/32000 (87%)] Loss: 1.97000 (semantic_loss: 0.02155, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19012 
Train Epoch: 6 [871/1000 27872/32000 (87%)] Loss: 1.97075 (semantic_loss: 0.02327, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18790 
Train Epoch: 6 [876/1000 28032/32000 (88%)] Loss: 1.96887 (semantic_loss: 0.02334, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18319 
Train Epoch: 6 [881/1000 28192/32000 (88%)] Loss: 1.97074 (semantic_loss: 0.02326, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18617 
Train Epoch: 6 [886/1000 28352/32000 (89%)] Loss: 1.96959 (semantic_loss: 0.02113, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19632 
Train Epoch: 6 [891/1000 28512/32000 (89%)] Loss: 1.97552 (semantic_loss: 0.02609, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.18615 
Train Epoch: 6 [896/1000 28672/32000 (90%)] Loss: 1.96992 (semantic_loss: 0.02244, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20741 
Train Epoch: 6 [901/1000 28832/32000 (90%)] Loss: 1.97405 (semantic_loss: 0.02559, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18611 
Train Epoch: 6 [906/1000 28992/32000 (91%)] Loss: 1.97157 (semantic_loss: 0.02409, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20420 
Train Epoch: 6 [911/1000 29152/32000 (91%)] Loss: 1.96807 (semantic_loss: 0.02059, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19201 
Train Epoch: 6 [916/1000 29312/32000 (92%)] Loss: 1.97296 (semantic_loss: 0.02450, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18929 
Train Epoch: 6 [921/1000 29472/32000 (92%)] Loss: 1.97123 (semantic_loss: 0.02472, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18794 
Train Epoch: 6 [926/1000 29632/32000 (93%)] Loss: 1.97297 (semantic_loss: 0.02549, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18936 
Train Epoch: 6 [931/1000 29792/32000 (93%)] Loss: 1.97912 (semantic_loss: 0.03164, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18866 
Train Epoch: 6 [936/1000 29952/32000 (94%)] Loss: 1.97129 (semantic_loss: 0.02381, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21802 
Train Epoch: 6 [941/1000 30112/32000 (94%)] Loss: 1.97043 (semantic_loss: 0.02295, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23446 
Train Epoch: 6 [946/1000 30272/32000 (95%)] Loss: 1.97291 (semantic_loss: 0.02544, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23055 
Train Epoch: 6 [951/1000 30432/32000 (95%)] Loss: 1.97420 (semantic_loss: 0.02672, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20557 
Train Epoch: 6 [956/1000 30592/32000 (96%)] Loss: 1.97339 (semantic_loss: 0.02591, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20802 
Train Epoch: 6 [961/1000 30752/32000 (96%)] Loss: 1.97170 (semantic_loss: 0.02422, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19715 
Train Epoch: 6 [966/1000 30912/32000 (97%)] Loss: 1.97041 (semantic_loss: 0.02294, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19136 
Train Epoch: 6 [971/1000 31072/32000 (97%)] Loss: 1.97262 (semantic_loss: 0.02612, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19698 
Train Epoch: 6 [976/1000 31232/32000 (98%)] Loss: 1.96939 (semantic_loss: 0.02191, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19296 
Train Epoch: 6 [981/1000 31392/32000 (98%)] Loss: 1.97098 (semantic_loss: 0.02350, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18835 
Train Epoch: 6 [986/1000 31552/32000 (99%)] Loss: 1.97169 (semantic_loss: 0.02421, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.37819 
Train Epoch: 6 [991/1000 31712/32000 (99%)] Loss: 1.97172 (semantic_loss: 0.02522, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18768 
Train Epoch: 6 [996/1000 31872/32000 (100%)] Loss: 1.97452 (semantic_loss: 0.02802, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18849 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch6.pth ...
Done in 4.504s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch6.pth ...
Done in 8.804s
removing stale ckpt [epoch 5] [took 0.00s]
 epoch          : 6
 loss           : 1.9727462356090546
 learning_rate  : 2.952450000000001e-05
 n_samples      : 192000
 n_steps        : 6000
 ActivityNet_val1_test/t2v_metrics/R1: 8.541793776693105
 ActivityNet_val1_test/t2v_metrics/R5: 30.730119991864957
 ActivityNet_val1_test/t2v_metrics/R10: 47.75269473256051
 ActivityNet_val1_test/t2v_metrics/R50: 84.72645922310352
 ActivityNet_val1_test/t2v_metrics/MedR: 11.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 44.06518202155786
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 23.229350884503987
 ActivityNet_val1_test/v2t_metrics/R1: 9.233272320520642
 ActivityNet_val1_test/v2t_metrics/R5: 32.37746593451291
 ActivityNet_val1_test/v2t_metrics/R10: 48.5865365059996
 ActivityNet_val1_test/v2t_metrics/R50: 85.15354891193817
 ActivityNet_val1_test/v2t_metrics/MedR: 11.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 45.35051860890787
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 24.39896928933716
 mnt_best       : 23.229350884503987
 not_improved_count: 0
Train Epoch: 7 [1/1000 32/32000 (0%)] Loss: 1.97106 (semantic_loss: 0.02261, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=22.91649 
Train Epoch: 7 [6/1000 192/32000 (1%)] Loss: 1.97088 (semantic_loss: 0.02145, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.18394 
Train Epoch: 7 [11/1000 352/32000 (1%)] Loss: 1.97254 (semantic_loss: 0.02604, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18401 
Train Epoch: 7 [16/1000 512/32000 (2%)] Loss: 1.97520 (semantic_loss: 0.02772, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.43211 
Train Epoch: 7 [21/1000 672/32000 (2%)] Loss: 1.97197 (semantic_loss: 0.02546, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18760 
Train Epoch: 7 [26/1000 832/32000 (3%)] Loss: 1.97572 (semantic_loss: 0.02824, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18603 
Train Epoch: 7 [31/1000 992/32000 (3%)] Loss: 1.96893 (semantic_loss: 0.02144, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18661 
Train Epoch: 7 [36/1000 1152/32000 (4%)] Loss: 1.97039 (semantic_loss: 0.02193, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18494 
Train Epoch: 7 [41/1000 1312/32000 (4%)] Loss: 1.97312 (semantic_loss: 0.02564, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18692 
Train Epoch: 7 [46/1000 1472/32000 (5%)] Loss: 1.97194 (semantic_loss: 0.02447, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18572 
Train Epoch: 7 [51/1000 1632/32000 (5%)] Loss: 1.97191 (semantic_loss: 0.02346, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18585 
Train Epoch: 7 [56/1000 1792/32000 (6%)] Loss: 1.97046 (semantic_loss: 0.02395, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18625 
Train Epoch: 7 [61/1000 1952/32000 (6%)] Loss: 1.96808 (semantic_loss: 0.02061, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18498 
Train Epoch: 7 [66/1000 2112/32000 (7%)] Loss: 1.97107 (semantic_loss: 0.02456, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.29046 
Train Epoch: 7 [71/1000 2272/32000 (7%)] Loss: 1.97350 (semantic_loss: 0.02700, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24638 
Train Epoch: 7 [76/1000 2432/32000 (8%)] Loss: 1.97314 (semantic_loss: 0.02566, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21554 
Train Epoch: 7 [81/1000 2592/32000 (8%)] Loss: 1.96705 (semantic_loss: 0.02054, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21247 
Train Epoch: 7 [86/1000 2752/32000 (9%)] Loss: 1.97138 (semantic_loss: 0.02293, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19828 
Train Epoch: 7 [91/1000 2912/32000 (9%)] Loss: 1.97748 (semantic_loss: 0.02902, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20051 
Train Epoch: 7 [96/1000 3072/32000 (10%)] Loss: 1.96998 (semantic_loss: 0.02152, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19287 
Train Epoch: 7 [101/1000 3232/32000 (10%)] Loss: 1.96876 (semantic_loss: 0.02128, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19886 
Train Epoch: 7 [106/1000 3392/32000 (11%)] Loss: 1.96853 (semantic_loss: 0.02203, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18887 
Train Epoch: 7 [111/1000 3552/32000 (11%)] Loss: 1.97276 (semantic_loss: 0.02626, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21879 
Train Epoch: 7 [116/1000 3712/32000 (12%)] Loss: 1.97136 (semantic_loss: 0.02389, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18637 
Train Epoch: 7 [121/1000 3872/32000 (12%)] Loss: 1.97447 (semantic_loss: 0.02698, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.20360 
Train Epoch: 7 [126/1000 4032/32000 (13%)] Loss: 1.96823 (semantic_loss: 0.02075, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18934 
Train Epoch: 7 [131/1000 4192/32000 (13%)] Loss: 1.97160 (semantic_loss: 0.02412, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25590 
Train Epoch: 7 [136/1000 4352/32000 (14%)] Loss: 1.97594 (semantic_loss: 0.02846, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20899 
Train Epoch: 7 [141/1000 4512/32000 (14%)] Loss: 1.96673 (semantic_loss: 0.01925, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19586 
Train Epoch: 7 [146/1000 4672/32000 (15%)] Loss: 1.97161 (semantic_loss: 0.02413, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18430 
Train Epoch: 7 [151/1000 4832/32000 (15%)] Loss: 1.96914 (semantic_loss: 0.02166, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18537 
Train Epoch: 7 [156/1000 4992/32000 (16%)] Loss: 1.96667 (semantic_loss: 0.02017, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18559 
Train Epoch: 7 [161/1000 5152/32000 (16%)] Loss: 1.97738 (semantic_loss: 0.03087, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.18804 
Train Epoch: 7 [166/1000 5312/32000 (17%)] Loss: 1.96869 (semantic_loss: 0.02121, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18511 
Train Epoch: 7 [171/1000 5472/32000 (17%)] Loss: 1.97243 (semantic_loss: 0.02592, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18444 
Train Epoch: 7 [176/1000 5632/32000 (18%)] Loss: 1.97589 (semantic_loss: 0.02842, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.34821 
Train Epoch: 7 [181/1000 5792/32000 (18%)] Loss: 1.96533 (semantic_loss: 0.01785, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18418 
Train Epoch: 7 [186/1000 5952/32000 (19%)] Loss: 1.96930 (semantic_loss: 0.02182, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.27839 
Train Epoch: 7 [191/1000 6112/32000 (19%)] Loss: 1.97388 (semantic_loss: 0.02736, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.18493 
Train Epoch: 7 [196/1000 6272/32000 (20%)] Loss: 1.97025 (semantic_loss: 0.02179, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18896 
Train Epoch: 7 [201/1000 6432/32000 (20%)] Loss: 1.97257 (semantic_loss: 0.02510, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18981 
Train Epoch: 7 [206/1000 6592/32000 (21%)] Loss: 1.96825 (semantic_loss: 0.02077, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18695 
Train Epoch: 7 [211/1000 6752/32000 (21%)] Loss: 1.97348 (semantic_loss: 0.02600, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.30203 
Train Epoch: 7 [216/1000 6912/32000 (22%)] Loss: 1.97351 (semantic_loss: 0.02602, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19494 
Train Epoch: 7 [221/1000 7072/32000 (22%)] Loss: 1.97030 (semantic_loss: 0.02477, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18816 
Train Epoch: 7 [226/1000 7232/32000 (23%)] Loss: 1.97121 (semantic_loss: 0.02275, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18782 
Train Epoch: 7 [231/1000 7392/32000 (23%)] Loss: 1.97283 (semantic_loss: 0.02535, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20793 
Train Epoch: 7 [236/1000 7552/32000 (24%)] Loss: 1.97107 (semantic_loss: 0.02262, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21631 
Train Epoch: 7 [241/1000 7712/32000 (24%)] Loss: 1.97143 (semantic_loss: 0.02298, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21954 
Train Epoch: 7 [246/1000 7872/32000 (25%)] Loss: 1.96937 (semantic_loss: 0.02190, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20051 
Train Epoch: 7 [251/1000 8032/32000 (25%)] Loss: 1.97194 (semantic_loss: 0.02642, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19397 
Train Epoch: 7 [256/1000 8192/32000 (26%)] Loss: 1.97049 (semantic_loss: 0.02203, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19819 
Train Epoch: 7 [261/1000 8352/32000 (26%)] Loss: 1.97560 (semantic_loss: 0.02714, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20512 
Train Epoch: 7 [266/1000 8512/32000 (27%)] Loss: 1.97019 (semantic_loss: 0.02271, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18910 
Train Epoch: 7 [271/1000 8672/32000 (27%)] Loss: 1.97004 (semantic_loss: 0.02256, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20783 
Train Epoch: 7 [276/1000 8832/32000 (28%)] Loss: 1.97124 (semantic_loss: 0.02375, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.49582 
Train Epoch: 7 [281/1000 8992/32000 (28%)] Loss: 1.97098 (semantic_loss: 0.02447, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18470 
Train Epoch: 7 [286/1000 9152/32000 (29%)] Loss: 1.96767 (semantic_loss: 0.02117, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18570 
Train Epoch: 7 [291/1000 9312/32000 (29%)] Loss: 1.96909 (semantic_loss: 0.02161, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21068 
Train Epoch: 7 [296/1000 9472/32000 (30%)] Loss: 1.97164 (semantic_loss: 0.02417, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20347 
Train Epoch: 7 [301/1000 9632/32000 (30%)] Loss: 1.97493 (semantic_loss: 0.02842, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.23492 
Train Epoch: 7 [306/1000 9792/32000 (31%)] Loss: 1.97401 (semantic_loss: 0.02652, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18944 
Train Epoch: 7 [311/1000 9952/32000 (31%)] Loss: 1.97454 (semantic_loss: 0.02706, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18461 
Train Epoch: 7 [316/1000 10112/32000 (32%)] Loss: 1.96745 (semantic_loss: 0.02095, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18777 
Train Epoch: 7 [321/1000 10272/32000 (32%)] Loss: 1.97022 (semantic_loss: 0.02176, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18937 
Train Epoch: 7 [326/1000 10432/32000 (33%)] Loss: 1.96808 (semantic_loss: 0.02158, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18509 
Train Epoch: 7 [331/1000 10592/32000 (33%)] Loss: 1.97577 (semantic_loss: 0.02928, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18849 
Train Epoch: 7 [336/1000 10752/32000 (34%)] Loss: 1.97012 (semantic_loss: 0.02166, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.45603 
Train Epoch: 7 [341/1000 10912/32000 (34%)] Loss: 1.97428 (semantic_loss: 0.02777, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18738 
Train Epoch: 7 [346/1000 11072/32000 (35%)] Loss: 1.96910 (semantic_loss: 0.02162, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18433 
Train Epoch: 7 [351/1000 11232/32000 (35%)] Loss: 1.96993 (semantic_loss: 0.02245, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18434 
Train Epoch: 7 [356/1000 11392/32000 (36%)] Loss: 1.97163 (semantic_loss: 0.02415, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18541 
Train Epoch: 7 [361/1000 11552/32000 (36%)] Loss: 1.97125 (semantic_loss: 0.02377, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18626 
Train Epoch: 7 [366/1000 11712/32000 (37%)] Loss: 1.97016 (semantic_loss: 0.02171, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18637 
Train Epoch: 7 [371/1000 11872/32000 (37%)] Loss: 1.96707 (semantic_loss: 0.01960, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19296 
Train Epoch: 7 [376/1000 12032/32000 (38%)] Loss: 1.97525 (semantic_loss: 0.02680, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18613 
Train Epoch: 7 [381/1000 12192/32000 (38%)] Loss: 1.97005 (semantic_loss: 0.02160, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18992 
Train Epoch: 7 [386/1000 12352/32000 (39%)] Loss: 1.97227 (semantic_loss: 0.02479, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.25171 
Train Epoch: 7 [391/1000 12512/32000 (39%)] Loss: 1.97201 (semantic_loss: 0.02551, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22624 
Train Epoch: 7 [396/1000 12672/32000 (40%)] Loss: 1.97974 (semantic_loss: 0.03128, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20154 
Train Epoch: 7 [401/1000 12832/32000 (40%)] Loss: 1.96921 (semantic_loss: 0.02270, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20914 
Train Epoch: 7 [406/1000 12992/32000 (41%)] Loss: 1.96788 (semantic_loss: 0.02235, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19810 
Train Epoch: 7 [411/1000 13152/32000 (41%)] Loss: 1.97364 (semantic_loss: 0.02714, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20000 
Train Epoch: 7 [416/1000 13312/32000 (42%)] Loss: 1.96949 (semantic_loss: 0.02201, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20697 
Train Epoch: 7 [421/1000 13472/32000 (42%)] Loss: 1.97084 (semantic_loss: 0.02238, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19790 
Train Epoch: 7 [426/1000 13632/32000 (43%)] Loss: 1.97084 (semantic_loss: 0.02336, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19068 
Train Epoch: 7 [431/1000 13792/32000 (43%)] Loss: 1.97386 (semantic_loss: 0.02638, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22267 
Train Epoch: 7 [436/1000 13952/32000 (44%)] Loss: 1.97233 (semantic_loss: 0.02583, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21045 
Train Epoch: 7 [441/1000 14112/32000 (44%)] Loss: 1.97021 (semantic_loss: 0.02274, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18597 
Train Epoch: 7 [446/1000 14272/32000 (45%)] Loss: 1.96959 (semantic_loss: 0.02309, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19073 
Train Epoch: 7 [451/1000 14432/32000 (45%)] Loss: 1.96901 (semantic_loss: 0.02250, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.26159 
Train Epoch: 7 [456/1000 14592/32000 (46%)] Loss: 1.97008 (semantic_loss: 0.02357, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22367 
Train Epoch: 7 [461/1000 14752/32000 (46%)] Loss: 1.97201 (semantic_loss: 0.02452, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19755 
Train Epoch: 7 [466/1000 14912/32000 (47%)] Loss: 1.96757 (semantic_loss: 0.02106, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18869 
Train Epoch: 7 [471/1000 15072/32000 (47%)] Loss: 1.96832 (semantic_loss: 0.01987, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18841 
Train Epoch: 7 [476/1000 15232/32000 (48%)] Loss: 1.96922 (semantic_loss: 0.02272, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18839 
Train Epoch: 7 [481/1000 15392/32000 (48%)] Loss: 1.96514 (semantic_loss: 0.01863, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19237 
Train Epoch: 7 [486/1000 15552/32000 (49%)] Loss: 1.96682 (semantic_loss: 0.01934, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19278 
Train Epoch: 7 [491/1000 15712/32000 (49%)] Loss: 1.96797 (semantic_loss: 0.02147, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19650 
Train Epoch: 7 [496/1000 15872/32000 (50%)] Loss: 1.97294 (semantic_loss: 0.02643, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.34907 
Train Epoch: 7 [501/1000 16032/32000 (50%)] Loss: 1.97031 (semantic_loss: 0.02283, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18642 
Train Epoch: 7 [506/1000 16192/32000 (51%)] Loss: 1.96975 (semantic_loss: 0.02324, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.28903 
Train Epoch: 7 [511/1000 16352/32000 (51%)] Loss: 1.97502 (semantic_loss: 0.02754, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19069 
Train Epoch: 7 [516/1000 16512/32000 (52%)] Loss: 1.96766 (semantic_loss: 0.02116, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20768 
Train Epoch: 7 [521/1000 16672/32000 (52%)] Loss: 1.97348 (semantic_loss: 0.02600, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18414 
Train Epoch: 7 [526/1000 16832/32000 (53%)] Loss: 1.97173 (semantic_loss: 0.02424, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.18517 
Train Epoch: 7 [531/1000 16992/32000 (53%)] Loss: 1.97190 (semantic_loss: 0.02442, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.29774 
Train Epoch: 7 [536/1000 17152/32000 (54%)] Loss: 1.97181 (semantic_loss: 0.02434, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18542 
Train Epoch: 7 [541/1000 17312/32000 (54%)] Loss: 1.96861 (semantic_loss: 0.02113, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18862 
Train Epoch: 7 [546/1000 17472/32000 (55%)] Loss: 1.96949 (semantic_loss: 0.02299, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18862 
Train Epoch: 7 [551/1000 17632/32000 (55%)] Loss: 1.97535 (semantic_loss: 0.02786, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20575 
Train Epoch: 7 [556/1000 17792/32000 (56%)] Loss: 1.96825 (semantic_loss: 0.02175, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21460 
Train Epoch: 7 [561/1000 17952/32000 (56%)] Loss: 1.96979 (semantic_loss: 0.02231, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22019 
Train Epoch: 7 [566/1000 18112/32000 (57%)] Loss: 1.97215 (semantic_loss: 0.02466, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20733 
Train Epoch: 7 [571/1000 18272/32000 (57%)] Loss: 1.96737 (semantic_loss: 0.02086, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21080 
Train Epoch: 7 [576/1000 18432/32000 (58%)] Loss: 1.97036 (semantic_loss: 0.02287, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20914 
Train Epoch: 7 [581/1000 18592/32000 (58%)] Loss: 1.97340 (semantic_loss: 0.02494, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19428 
Train Epoch: 7 [586/1000 18752/32000 (59%)] Loss: 1.97242 (semantic_loss: 0.02494, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20207 
Train Epoch: 7 [591/1000 18912/32000 (59%)] Loss: 1.97019 (semantic_loss: 0.02368, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21353 
Train Epoch: 7 [596/1000 19072/32000 (60%)] Loss: 1.97045 (semantic_loss: 0.02394, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.51500 
Train Epoch: 7 [601/1000 19232/32000 (60%)] Loss: 1.97208 (semantic_loss: 0.02460, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18937 
Train Epoch: 7 [606/1000 19392/32000 (61%)] Loss: 1.96932 (semantic_loss: 0.02184, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19510 
Train Epoch: 7 [611/1000 19552/32000 (61%)] Loss: 1.97037 (semantic_loss: 0.02386, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18456 
Train Epoch: 7 [616/1000 19712/32000 (62%)] Loss: 1.96727 (semantic_loss: 0.01979, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19247 
Train Epoch: 7 [621/1000 19872/32000 (62%)] Loss: 1.97141 (semantic_loss: 0.02295, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22773 
Train Epoch: 7 [626/1000 20032/32000 (63%)] Loss: 1.96832 (semantic_loss: 0.02083, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19300 
Train Epoch: 7 [631/1000 20192/32000 (63%)] Loss: 1.96898 (semantic_loss: 0.02150, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19279 
Train Epoch: 7 [636/1000 20352/32000 (64%)] Loss: 1.97298 (semantic_loss: 0.02647, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18988 
Train Epoch: 7 [641/1000 20512/32000 (64%)] Loss: 1.96813 (semantic_loss: 0.02065, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19674 
Train Epoch: 7 [646/1000 20672/32000 (65%)] Loss: 1.97079 (semantic_loss: 0.02234, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19016 
Train Epoch: 7 [651/1000 20832/32000 (65%)] Loss: 1.97221 (semantic_loss: 0.02375, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19123 
Train Epoch: 7 [656/1000 20992/32000 (66%)] Loss: 1.96854 (semantic_loss: 0.02203, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.44620 
Train Epoch: 7 [661/1000 21152/32000 (66%)] Loss: 1.96706 (semantic_loss: 0.01958, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18839 
Train Epoch: 7 [666/1000 21312/32000 (67%)] Loss: 1.97419 (semantic_loss: 0.02574, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18867 
Train Epoch: 7 [671/1000 21472/32000 (67%)] Loss: 1.96572 (semantic_loss: 0.01921, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18899 
Train Epoch: 7 [676/1000 21632/32000 (68%)] Loss: 1.96528 (semantic_loss: 0.01780, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18743 
Train Epoch: 7 [681/1000 21792/32000 (68%)] Loss: 1.96638 (semantic_loss: 0.01890, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18640 
Train Epoch: 7 [686/1000 21952/32000 (69%)] Loss: 1.96962 (semantic_loss: 0.02117, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18445 
Train Epoch: 7 [691/1000 22112/32000 (69%)] Loss: 1.96749 (semantic_loss: 0.02001, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18642 
Train Epoch: 7 [696/1000 22272/32000 (70%)] Loss: 1.96879 (semantic_loss: 0.02227, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.18573 
Train Epoch: 7 [701/1000 22432/32000 (70%)] Loss: 1.96695 (semantic_loss: 0.02045, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19639 
Train Epoch: 7 [706/1000 22592/32000 (71%)] Loss: 1.97470 (semantic_loss: 0.02722, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26306 
Train Epoch: 7 [711/1000 22752/32000 (71%)] Loss: 1.97306 (semantic_loss: 0.02460, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21872 
Train Epoch: 7 [716/1000 22912/32000 (72%)] Loss: 1.97030 (semantic_loss: 0.02380, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20750 
Train Epoch: 7 [721/1000 23072/32000 (72%)] Loss: 1.96940 (semantic_loss: 0.02192, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22149 
Train Epoch: 7 [726/1000 23232/32000 (73%)] Loss: 1.96598 (semantic_loss: 0.01948, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20076 
Train Epoch: 7 [731/1000 23392/32000 (73%)] Loss: 1.97010 (semantic_loss: 0.02164, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20059 
Train Epoch: 7 [736/1000 23552/32000 (74%)] Loss: 1.96845 (semantic_loss: 0.01999, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19906 
Train Epoch: 7 [741/1000 23712/32000 (74%)] Loss: 1.96611 (semantic_loss: 0.01960, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19137 
Train Epoch: 7 [746/1000 23872/32000 (75%)] Loss: 1.97179 (semantic_loss: 0.02334, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19856 
Train Epoch: 7 [751/1000 24032/32000 (75%)] Loss: 1.97126 (semantic_loss: 0.02281, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.22253 
Train Epoch: 7 [756/1000 24192/32000 (76%)] Loss: 1.96886 (semantic_loss: 0.02139, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19062 
Train Epoch: 7 [761/1000 24352/32000 (76%)] Loss: 1.96901 (semantic_loss: 0.02152, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19435 
Train Epoch: 7 [766/1000 24512/32000 (77%)] Loss: 1.97202 (semantic_loss: 0.02454, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18773 
Train Epoch: 7 [771/1000 24672/32000 (77%)] Loss: 1.97084 (semantic_loss: 0.02433, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.25724 
Train Epoch: 7 [776/1000 24832/32000 (78%)] Loss: 1.96754 (semantic_loss: 0.02005, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20737 
Train Epoch: 7 [781/1000 24992/32000 (78%)] Loss: 1.97048 (semantic_loss: 0.02300, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19921 
Train Epoch: 7 [786/1000 25152/32000 (79%)] Loss: 1.97032 (semantic_loss: 0.02284, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19594 
Train Epoch: 7 [791/1000 25312/32000 (79%)] Loss: 1.96660 (semantic_loss: 0.01912, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18608 
Train Epoch: 7 [796/1000 25472/32000 (80%)] Loss: 1.96989 (semantic_loss: 0.02241, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18803 
Train Epoch: 7 [801/1000 25632/32000 (80%)] Loss: 1.97169 (semantic_loss: 0.02421, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18981 
Train Epoch: 7 [806/1000 25792/32000 (81%)] Loss: 1.96941 (semantic_loss: 0.02387, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.19367 
Train Epoch: 7 [811/1000 25952/32000 (81%)] Loss: 1.96872 (semantic_loss: 0.02222, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18258 
Train Epoch: 7 [816/1000 26112/32000 (82%)] Loss: 1.97012 (semantic_loss: 0.02167, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.34969 
Train Epoch: 7 [821/1000 26272/32000 (82%)] Loss: 1.96920 (semantic_loss: 0.02270, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19369 
Train Epoch: 7 [826/1000 26432/32000 (83%)] Loss: 1.97041 (semantic_loss: 0.02390, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.28575 
Train Epoch: 7 [831/1000 26592/32000 (83%)] Loss: 1.97366 (semantic_loss: 0.02618, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19348 
Train Epoch: 7 [836/1000 26752/32000 (84%)] Loss: 1.97133 (semantic_loss: 0.02385, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20883 
Train Epoch: 7 [841/1000 26912/32000 (84%)] Loss: 1.97347 (semantic_loss: 0.02502, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19598 
Train Epoch: 7 [846/1000 27072/32000 (85%)] Loss: 1.96566 (semantic_loss: 0.01818, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18686 
Train Epoch: 7 [851/1000 27232/32000 (85%)] Loss: 1.97154 (semantic_loss: 0.02504, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.29847 
Train Epoch: 7 [856/1000 27392/32000 (86%)] Loss: 1.97138 (semantic_loss: 0.02488, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20027 
Train Epoch: 7 [861/1000 27552/32000 (86%)] Loss: 1.96662 (semantic_loss: 0.02012, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21287 
Train Epoch: 7 [866/1000 27712/32000 (87%)] Loss: 1.97265 (semantic_loss: 0.02516, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21963 
Train Epoch: 7 [871/1000 27872/32000 (87%)] Loss: 1.96668 (semantic_loss: 0.01920, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20526 
Train Epoch: 7 [876/1000 28032/32000 (88%)] Loss: 1.96803 (semantic_loss: 0.02153, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20527 
Train Epoch: 7 [881/1000 28192/32000 (88%)] Loss: 1.97126 (semantic_loss: 0.02378, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19148 
Train Epoch: 7 [886/1000 28352/32000 (89%)] Loss: 1.97632 (semantic_loss: 0.02786, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20362 
Train Epoch: 7 [891/1000 28512/32000 (89%)] Loss: 1.97028 (semantic_loss: 0.02280, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20097 
Train Epoch: 7 [896/1000 28672/32000 (90%)] Loss: 1.97083 (semantic_loss: 0.02432, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19226 
Train Epoch: 7 [901/1000 28832/32000 (90%)] Loss: 1.96765 (semantic_loss: 0.02018, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19073 
Train Epoch: 7 [906/1000 28992/32000 (91%)] Loss: 1.96967 (semantic_loss: 0.02219, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18806 
Train Epoch: 7 [911/1000 29152/32000 (91%)] Loss: 1.96452 (semantic_loss: 0.01898, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.20820 
Train Epoch: 7 [916/1000 29312/32000 (92%)] Loss: 1.96904 (semantic_loss: 0.02156, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.52330 
Train Epoch: 7 [921/1000 29472/32000 (92%)] Loss: 1.96912 (semantic_loss: 0.02066, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18823 
Train Epoch: 7 [926/1000 29632/32000 (93%)] Loss: 1.97146 (semantic_loss: 0.02495, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19135 
Train Epoch: 7 [931/1000 29792/32000 (93%)] Loss: 1.97196 (semantic_loss: 0.02350, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20114 
Train Epoch: 7 [936/1000 29952/32000 (94%)] Loss: 1.96808 (semantic_loss: 0.02060, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19560 
Train Epoch: 7 [941/1000 30112/32000 (94%)] Loss: 1.96941 (semantic_loss: 0.02095, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.22070 
Train Epoch: 7 [946/1000 30272/32000 (95%)] Loss: 1.97248 (semantic_loss: 0.02402, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18634 
Train Epoch: 7 [951/1000 30432/32000 (95%)] Loss: 1.97496 (semantic_loss: 0.02845, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18665 
Train Epoch: 7 [956/1000 30592/32000 (96%)] Loss: 1.97151 (semantic_loss: 0.02499, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.18916 
Train Epoch: 7 [961/1000 30752/32000 (96%)] Loss: 1.97023 (semantic_loss: 0.02276, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19418 
Train Epoch: 7 [966/1000 30912/32000 (97%)] Loss: 1.97083 (semantic_loss: 0.02335, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18761 
Train Epoch: 7 [971/1000 31072/32000 (97%)] Loss: 1.96853 (semantic_loss: 0.02104, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19515 
Train Epoch: 7 [976/1000 31232/32000 (98%)] Loss: 1.97376 (semantic_loss: 0.02531, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.44609 
Train Epoch: 7 [981/1000 31392/32000 (98%)] Loss: 1.97003 (semantic_loss: 0.02254, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.18403 
Train Epoch: 7 [986/1000 31552/32000 (99%)] Loss: 1.97528 (semantic_loss: 0.02682, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18521 
Train Epoch: 7 [991/1000 31712/32000 (99%)] Loss: 1.97048 (semantic_loss: 0.02397, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18928 
Train Epoch: 7 [996/1000 31872/32000 (100%)] Loss: 1.97903 (semantic_loss: 0.03057, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18431 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch7.pth ...
Done in 4.317s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch7.pth ...
Done in 8.602s
removing stale ckpt [epoch 6] [took 0.00s]
 epoch          : 7
 loss           : 1.970827488899231
 learning_rate  : 2.657205000000001e-05
 n_samples      : 224000
 n_steps        : 7000
 ActivityNet_val1_test/t2v_metrics/R1: 9.131584299369534
 ActivityNet_val1_test/t2v_metrics/R5: 31.97071384990848
 ActivityNet_val1_test/t2v_metrics/R10: 47.915395566402275
 ActivityNet_val1_test/t2v_metrics/R50: 84.97051047386618
 ActivityNet_val1_test/t2v_metrics/MedR: 11.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 45.93298759406142
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 24.094865959521997
 ActivityNet_val1_test/v2t_metrics/R1: 8.989221069757983
 ActivityNet_val1_test/v2t_metrics/R5: 33.15029489526134
 ActivityNet_val1_test/v2t_metrics/R10: 50.33557046979866
 ActivityNet_val1_test/v2t_metrics/R50: 85.98739068537726
 ActivityNet_val1_test/v2t_metrics/MedR: 10.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 45.732560504372586
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 24.661991896600327
 mnt_best       : 24.094865959521997
 not_improved_count: 0
Train Epoch: 8 [1/1000 32/32000 (0%)] Loss: 1.96948 (semantic_loss: 0.02200, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=23.55906 
Train Epoch: 8 [6/1000 192/32000 (1%)] Loss: 1.96652 (semantic_loss: 0.01807, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.32193 
Train Epoch: 8 [11/1000 352/32000 (1%)] Loss: 1.96892 (semantic_loss: 0.02243, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.64578 
Train Epoch: 8 [16/1000 512/32000 (2%)] Loss: 1.97076 (semantic_loss: 0.02426, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18946 
Train Epoch: 8 [21/1000 672/32000 (2%)] Loss: 1.96896 (semantic_loss: 0.02246, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18829 
Train Epoch: 8 [26/1000 832/32000 (3%)] Loss: 1.96942 (semantic_loss: 0.02195, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18559 
Train Epoch: 8 [31/1000 992/32000 (3%)] Loss: 1.96781 (semantic_loss: 0.01936, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18612 
Train Epoch: 8 [36/1000 1152/32000 (4%)] Loss: 1.97082 (semantic_loss: 0.02333, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18606 
Train Epoch: 8 [41/1000 1312/32000 (4%)] Loss: 1.96702 (semantic_loss: 0.01954, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19335 
Train Epoch: 8 [46/1000 1472/32000 (5%)] Loss: 1.96997 (semantic_loss: 0.02346, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20032 
Train Epoch: 8 [51/1000 1632/32000 (5%)] Loss: 1.96666 (semantic_loss: 0.01918, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19768 
Train Epoch: 8 [56/1000 1792/32000 (6%)] Loss: 1.96730 (semantic_loss: 0.01982, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19297 
Train Epoch: 8 [61/1000 1952/32000 (6%)] Loss: 1.97053 (semantic_loss: 0.02305, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18907 
Train Epoch: 8 [66/1000 2112/32000 (7%)] Loss: 1.97273 (semantic_loss: 0.02524, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20486 
Train Epoch: 8 [71/1000 2272/32000 (7%)] Loss: 1.97057 (semantic_loss: 0.02212, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18576 
Train Epoch: 8 [76/1000 2432/32000 (8%)] Loss: 1.97444 (semantic_loss: 0.02695, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18532 
Train Epoch: 8 [81/1000 2592/32000 (8%)] Loss: 1.96907 (semantic_loss: 0.02159, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18569 
Train Epoch: 8 [86/1000 2752/32000 (9%)] Loss: 1.97514 (semantic_loss: 0.02864, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18746 
Train Epoch: 8 [91/1000 2912/32000 (9%)] Loss: 1.97203 (semantic_loss: 0.02650, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18918 
Train Epoch: 8 [96/1000 3072/32000 (10%)] Loss: 1.97476 (semantic_loss: 0.02728, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18656 
Train Epoch: 8 [101/1000 3232/32000 (10%)] Loss: 1.97121 (semantic_loss: 0.02470, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18647 
Train Epoch: 8 [106/1000 3392/32000 (11%)] Loss: 1.96729 (semantic_loss: 0.02079, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18928 
Train Epoch: 8 [111/1000 3552/32000 (11%)] Loss: 1.97452 (semantic_loss: 0.02703, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19003 
Train Epoch: 8 [116/1000 3712/32000 (12%)] Loss: 1.97001 (semantic_loss: 0.02252, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19454 
Train Epoch: 8 [121/1000 3872/32000 (12%)] Loss: 1.96720 (semantic_loss: 0.01972, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21443 
Train Epoch: 8 [126/1000 4032/32000 (13%)] Loss: 1.97210 (semantic_loss: 0.02363, quant_loss: 1.94824, bit_balance_loss: 0.00023) batch_time=0.19328 
Train Epoch: 8 [131/1000 4192/32000 (13%)] Loss: 1.96514 (semantic_loss: 0.01766, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18839 
Train Epoch: 8 [136/1000 4352/32000 (14%)] Loss: 1.97374 (semantic_loss: 0.02529, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18700 
Train Epoch: 8 [141/1000 4512/32000 (14%)] Loss: 1.96980 (semantic_loss: 0.02231, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21423 
Train Epoch: 8 [146/1000 4672/32000 (15%)] Loss: 1.97047 (semantic_loss: 0.02397, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.88364 
Train Epoch: 8 [151/1000 4832/32000 (15%)] Loss: 1.97392 (semantic_loss: 0.02741, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.40687 
Train Epoch: 8 [156/1000 4992/32000 (16%)] Loss: 1.96863 (semantic_loss: 0.02212, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.26066 
Train Epoch: 8 [161/1000 5152/32000 (16%)] Loss: 1.96656 (semantic_loss: 0.01909, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21098 
Train Epoch: 8 [166/1000 5312/32000 (17%)] Loss: 1.96572 (semantic_loss: 0.02019, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20728 
Train Epoch: 8 [171/1000 5472/32000 (17%)] Loss: 1.96710 (semantic_loss: 0.02059, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20542 
Train Epoch: 8 [176/1000 5632/32000 (18%)] Loss: 1.96958 (semantic_loss: 0.02307, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19402 
Train Epoch: 8 [181/1000 5792/32000 (18%)] Loss: 1.97506 (semantic_loss: 0.02660, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18950 
Train Epoch: 8 [186/1000 5952/32000 (19%)] Loss: 1.96823 (semantic_loss: 0.02172, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18724 
Train Epoch: 8 [191/1000 6112/32000 (19%)] Loss: 1.97318 (semantic_loss: 0.02570, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21155 
Train Epoch: 8 [196/1000 6272/32000 (20%)] Loss: 1.97075 (semantic_loss: 0.02326, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.18695 
Train Epoch: 8 [201/1000 6432/32000 (20%)] Loss: 1.96622 (semantic_loss: 0.01874, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18805 
Train Epoch: 8 [206/1000 6592/32000 (21%)] Loss: 1.96868 (semantic_loss: 0.02120, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19347 
Train Epoch: 8 [211/1000 6752/32000 (21%)] Loss: 1.97247 (semantic_loss: 0.02499, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20609 
Train Epoch: 8 [216/1000 6912/32000 (22%)] Loss: 1.97119 (semantic_loss: 0.02371, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20228 
Train Epoch: 8 [221/1000 7072/32000 (22%)] Loss: 1.97321 (semantic_loss: 0.02572, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18833 
Train Epoch: 8 [226/1000 7232/32000 (23%)] Loss: 1.97020 (semantic_loss: 0.02273, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18822 
Train Epoch: 8 [231/1000 7392/32000 (23%)] Loss: 1.96787 (semantic_loss: 0.02137, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20925 
Train Epoch: 8 [236/1000 7552/32000 (24%)] Loss: 1.96772 (semantic_loss: 0.02024, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20519 
Train Epoch: 8 [241/1000 7712/32000 (24%)] Loss: 1.96991 (semantic_loss: 0.02341, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20674 
Train Epoch: 8 [246/1000 7872/32000 (25%)] Loss: 1.96784 (semantic_loss: 0.02036, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20612 
Train Epoch: 8 [251/1000 8032/32000 (25%)] Loss: 1.96735 (semantic_loss: 0.02085, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20821 
Train Epoch: 8 [256/1000 8192/32000 (26%)] Loss: 1.96956 (semantic_loss: 0.02208, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20832 
Train Epoch: 8 [261/1000 8352/32000 (26%)] Loss: 1.97523 (semantic_loss: 0.02872, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18727 
Train Epoch: 8 [266/1000 8512/32000 (27%)] Loss: 1.96859 (semantic_loss: 0.02111, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21307 
Train Epoch: 8 [271/1000 8672/32000 (27%)] Loss: 1.97095 (semantic_loss: 0.02444, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18810 
Train Epoch: 8 [276/1000 8832/32000 (28%)] Loss: 1.97117 (semantic_loss: 0.02369, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19363 
Train Epoch: 8 [281/1000 8992/32000 (28%)] Loss: 1.96832 (semantic_loss: 0.01987, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20301 
Train Epoch: 8 [286/1000 9152/32000 (29%)] Loss: 1.97132 (semantic_loss: 0.02385, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18598 
Train Epoch: 8 [291/1000 9312/32000 (29%)] Loss: 1.96871 (semantic_loss: 0.02122, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20576 
Train Epoch: 8 [296/1000 9472/32000 (30%)] Loss: 1.96744 (semantic_loss: 0.02094, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20838 
Train Epoch: 8 [301/1000 9632/32000 (30%)] Loss: 1.96847 (semantic_loss: 0.02098, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21722 
Train Epoch: 8 [306/1000 9792/32000 (31%)] Loss: 1.97418 (semantic_loss: 0.02766, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.21663 
Train Epoch: 8 [311/1000 9952/32000 (31%)] Loss: 1.96980 (semantic_loss: 0.02232, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20188 
Train Epoch: 8 [316/1000 10112/32000 (32%)] Loss: 1.97470 (semantic_loss: 0.02721, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20195 
Train Epoch: 8 [321/1000 10272/32000 (32%)] Loss: 1.96968 (semantic_loss: 0.02220, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.33110 
Train Epoch: 8 [326/1000 10432/32000 (33%)] Loss: 1.97407 (semantic_loss: 0.02561, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19998 
Train Epoch: 8 [331/1000 10592/32000 (33%)] Loss: 1.96739 (semantic_loss: 0.02187, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.54476 
Train Epoch: 8 [336/1000 10752/32000 (34%)] Loss: 1.96973 (semantic_loss: 0.02226, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19167 
Train Epoch: 8 [341/1000 10912/32000 (34%)] Loss: 1.96839 (semantic_loss: 0.02090, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18589 
Train Epoch: 8 [346/1000 11072/32000 (35%)] Loss: 1.97083 (semantic_loss: 0.02335, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18651 
Train Epoch: 8 [351/1000 11232/32000 (35%)] Loss: 1.97225 (semantic_loss: 0.02476, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.18392 
Train Epoch: 8 [356/1000 11392/32000 (36%)] Loss: 1.96770 (semantic_loss: 0.02119, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19165 
Train Epoch: 8 [361/1000 11552/32000 (36%)] Loss: 1.96790 (semantic_loss: 0.02140, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19275 
Train Epoch: 8 [366/1000 11712/32000 (37%)] Loss: 1.96595 (semantic_loss: 0.01847, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19190 
Train Epoch: 8 [371/1000 11872/32000 (37%)] Loss: 1.96857 (semantic_loss: 0.02110, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20147 
Train Epoch: 8 [376/1000 12032/32000 (38%)] Loss: 1.97418 (semantic_loss: 0.02572, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18925 
Train Epoch: 8 [381/1000 12192/32000 (38%)] Loss: 1.97338 (semantic_loss: 0.02394, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.19160 
Train Epoch: 8 [386/1000 12352/32000 (39%)] Loss: 1.97111 (semantic_loss: 0.02362, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20551 
Train Epoch: 8 [391/1000 12512/32000 (39%)] Loss: 1.96472 (semantic_loss: 0.01724, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18893 
Train Epoch: 8 [396/1000 12672/32000 (40%)] Loss: 1.96527 (semantic_loss: 0.01877, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18683 
Train Epoch: 8 [401/1000 12832/32000 (40%)] Loss: 1.96677 (semantic_loss: 0.01831, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18733 
Train Epoch: 8 [406/1000 12992/32000 (41%)] Loss: 1.96892 (semantic_loss: 0.02047, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18558 
Train Epoch: 8 [411/1000 13152/32000 (41%)] Loss: 1.96975 (semantic_loss: 0.02130, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18433 
Train Epoch: 8 [416/1000 13312/32000 (42%)] Loss: 1.97470 (semantic_loss: 0.02722, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18696 
Train Epoch: 8 [421/1000 13472/32000 (42%)] Loss: 1.97199 (semantic_loss: 0.02549, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18529 
Train Epoch: 8 [426/1000 13632/32000 (43%)] Loss: 1.96548 (semantic_loss: 0.01897, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18631 
Train Epoch: 8 [431/1000 13792/32000 (43%)] Loss: 1.96498 (semantic_loss: 0.01750, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18568 
Train Epoch: 8 [436/1000 13952/32000 (44%)] Loss: 1.96954 (semantic_loss: 0.02206, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18607 
Train Epoch: 8 [441/1000 14112/32000 (44%)] Loss: 1.96824 (semantic_loss: 0.02076, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18802 
Train Epoch: 8 [446/1000 14272/32000 (45%)] Loss: 1.97492 (semantic_loss: 0.02743, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20530 
Train Epoch: 8 [451/1000 14432/32000 (45%)] Loss: 1.96886 (semantic_loss: 0.02235, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18815 
Train Epoch: 8 [456/1000 14592/32000 (46%)] Loss: 1.97189 (semantic_loss: 0.02441, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18580 
Train Epoch: 8 [461/1000 14752/32000 (46%)] Loss: 1.96634 (semantic_loss: 0.01984, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19220 
Train Epoch: 8 [466/1000 14912/32000 (47%)] Loss: 1.96500 (semantic_loss: 0.01850, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.67763 
Train Epoch: 8 [471/1000 15072/32000 (47%)] Loss: 1.97164 (semantic_loss: 0.02416, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.44848 
Train Epoch: 8 [476/1000 15232/32000 (48%)] Loss: 1.96960 (semantic_loss: 0.02115, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.27477 
Train Epoch: 8 [481/1000 15392/32000 (48%)] Loss: 1.97150 (semantic_loss: 0.02304, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20170 
Train Epoch: 8 [486/1000 15552/32000 (49%)] Loss: 1.96807 (semantic_loss: 0.01961, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20181 
Train Epoch: 8 [491/1000 15712/32000 (49%)] Loss: 1.97446 (semantic_loss: 0.02697, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22025 
Train Epoch: 8 [496/1000 15872/32000 (50%)] Loss: 1.96717 (semantic_loss: 0.02066, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21363 
Train Epoch: 8 [501/1000 16032/32000 (50%)] Loss: 1.96880 (semantic_loss: 0.02229, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20080 
Train Epoch: 8 [506/1000 16192/32000 (51%)] Loss: 1.97000 (semantic_loss: 0.02252, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18832 
Train Epoch: 8 [511/1000 16352/32000 (51%)] Loss: 1.96644 (semantic_loss: 0.01799, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18661 
Train Epoch: 8 [516/1000 16512/32000 (52%)] Loss: 1.97382 (semantic_loss: 0.02732, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19635 
Train Epoch: 8 [521/1000 16672/32000 (52%)] Loss: 1.96841 (semantic_loss: 0.02093, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18594 
Train Epoch: 8 [526/1000 16832/32000 (53%)] Loss: 1.97012 (semantic_loss: 0.02166, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.21001 
Train Epoch: 8 [531/1000 16992/32000 (53%)] Loss: 1.96749 (semantic_loss: 0.02001, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19600 
Train Epoch: 8 [536/1000 17152/32000 (54%)] Loss: 1.96912 (semantic_loss: 0.02164, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19269 
Train Epoch: 8 [541/1000 17312/32000 (54%)] Loss: 1.97242 (semantic_loss: 0.02299, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.18851 
Train Epoch: 8 [546/1000 17472/32000 (55%)] Loss: 1.97113 (semantic_loss: 0.02463, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18628 
Train Epoch: 8 [551/1000 17632/32000 (55%)] Loss: 1.97207 (semantic_loss: 0.02557, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18726 
Train Epoch: 8 [556/1000 17792/32000 (56%)] Loss: 1.96929 (semantic_loss: 0.02181, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19146 
Train Epoch: 8 [561/1000 17952/32000 (56%)] Loss: 1.96852 (semantic_loss: 0.02104, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19158 
Train Epoch: 8 [566/1000 18112/32000 (57%)] Loss: 1.96991 (semantic_loss: 0.02438, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18746 
Train Epoch: 8 [571/1000 18272/32000 (57%)] Loss: 1.96721 (semantic_loss: 0.01876, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18632 
Train Epoch: 8 [576/1000 18432/32000 (58%)] Loss: 1.97147 (semantic_loss: 0.02399, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18850 
Train Epoch: 8 [581/1000 18592/32000 (58%)] Loss: 1.97034 (semantic_loss: 0.02286, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18540 
Train Epoch: 8 [586/1000 18752/32000 (59%)] Loss: 1.96622 (semantic_loss: 0.01971, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18349 
Train Epoch: 8 [591/1000 18912/32000 (59%)] Loss: 1.96754 (semantic_loss: 0.02006, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18461 
Train Epoch: 8 [596/1000 19072/32000 (60%)] Loss: 1.97461 (semantic_loss: 0.02713, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19229 
Train Epoch: 8 [601/1000 19232/32000 (60%)] Loss: 1.96892 (semantic_loss: 0.02144, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20110 
Train Epoch: 8 [606/1000 19392/32000 (61%)] Loss: 1.96806 (semantic_loss: 0.02057, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20069 
Train Epoch: 8 [611/1000 19552/32000 (61%)] Loss: 1.96366 (semantic_loss: 0.01813, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18933 
Train Epoch: 8 [616/1000 19712/32000 (62%)] Loss: 1.97026 (semantic_loss: 0.02277, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18926 
Train Epoch: 8 [621/1000 19872/32000 (62%)] Loss: 1.96909 (semantic_loss: 0.02161, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18810 
Train Epoch: 8 [626/1000 20032/32000 (63%)] Loss: 1.97106 (semantic_loss: 0.02358, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19515 
Train Epoch: 8 [631/1000 20192/32000 (63%)] Loss: 1.96877 (semantic_loss: 0.02226, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20612 
Train Epoch: 8 [636/1000 20352/32000 (64%)] Loss: 1.96885 (semantic_loss: 0.02137, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21228 
Train Epoch: 8 [641/1000 20512/32000 (64%)] Loss: 1.96753 (semantic_loss: 0.02103, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.37493 
Train Epoch: 8 [646/1000 20672/32000 (65%)] Loss: 1.97431 (semantic_loss: 0.02683, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20371 
Train Epoch: 8 [651/1000 20832/32000 (65%)] Loss: 1.96818 (semantic_loss: 0.02167, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.58091 
Train Epoch: 8 [656/1000 20992/32000 (66%)] Loss: 1.96697 (semantic_loss: 0.01948, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19901 
Train Epoch: 8 [661/1000 21152/32000 (66%)] Loss: 1.96794 (semantic_loss: 0.02046, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19962 
Train Epoch: 8 [666/1000 21312/32000 (67%)] Loss: 1.96825 (semantic_loss: 0.02175, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19065 
Train Epoch: 8 [671/1000 21472/32000 (67%)] Loss: 1.97102 (semantic_loss: 0.02354, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18858 
Train Epoch: 8 [676/1000 21632/32000 (68%)] Loss: 1.96803 (semantic_loss: 0.02153, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18814 
Train Epoch: 8 [681/1000 21792/32000 (68%)] Loss: 1.96866 (semantic_loss: 0.02215, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18937 
Train Epoch: 8 [686/1000 21952/32000 (69%)] Loss: 1.97073 (semantic_loss: 0.02325, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18782 
Train Epoch: 8 [691/1000 22112/32000 (69%)] Loss: 1.97075 (semantic_loss: 0.02229, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18983 
Train Epoch: 8 [696/1000 22272/32000 (70%)] Loss: 1.96704 (semantic_loss: 0.01956, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20172 
Train Epoch: 8 [701/1000 22432/32000 (70%)] Loss: 1.97037 (semantic_loss: 0.02289, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19060 
Train Epoch: 8 [706/1000 22592/32000 (71%)] Loss: 1.96727 (semantic_loss: 0.02075, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.20932 
Train Epoch: 8 [711/1000 22752/32000 (71%)] Loss: 1.97031 (semantic_loss: 0.02283, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19997 
Train Epoch: 8 [716/1000 22912/32000 (72%)] Loss: 1.97364 (semantic_loss: 0.02615, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18694 
Train Epoch: 8 [721/1000 23072/32000 (72%)] Loss: 1.96631 (semantic_loss: 0.01981, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18684 
Train Epoch: 8 [726/1000 23232/32000 (73%)] Loss: 1.97214 (semantic_loss: 0.02368, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19683 
Train Epoch: 8 [731/1000 23392/32000 (73%)] Loss: 1.97300 (semantic_loss: 0.02650, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19354 
Train Epoch: 8 [736/1000 23552/32000 (74%)] Loss: 1.97104 (semantic_loss: 0.02355, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18951 
Train Epoch: 8 [741/1000 23712/32000 (74%)] Loss: 1.96983 (semantic_loss: 0.02235, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18899 
Train Epoch: 8 [746/1000 23872/32000 (75%)] Loss: 1.96957 (semantic_loss: 0.02112, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18588 
Train Epoch: 8 [751/1000 24032/32000 (75%)] Loss: 1.96517 (semantic_loss: 0.01866, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19450 
Train Epoch: 8 [756/1000 24192/32000 (76%)] Loss: 1.96927 (semantic_loss: 0.02277, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18675 
Train Epoch: 8 [761/1000 24352/32000 (76%)] Loss: 1.97352 (semantic_loss: 0.02603, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18616 
Train Epoch: 8 [766/1000 24512/32000 (77%)] Loss: 1.97129 (semantic_loss: 0.02380, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18705 
Train Epoch: 8 [771/1000 24672/32000 (77%)] Loss: 1.97161 (semantic_loss: 0.02413, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18966 
Train Epoch: 8 [776/1000 24832/32000 (78%)] Loss: 1.97497 (semantic_loss: 0.02749, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20970 
Train Epoch: 8 [781/1000 24992/32000 (78%)] Loss: 1.96775 (semantic_loss: 0.02027, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18884 
Train Epoch: 8 [786/1000 25152/32000 (79%)] Loss: 1.97088 (semantic_loss: 0.02339, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.56356 
Train Epoch: 8 [791/1000 25312/32000 (79%)] Loss: 1.96801 (semantic_loss: 0.02248, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.41826 
Train Epoch: 8 [796/1000 25472/32000 (80%)] Loss: 1.97358 (semantic_loss: 0.02609, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.28411 
Train Epoch: 8 [801/1000 25632/32000 (80%)] Loss: 1.96827 (semantic_loss: 0.02079, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.24591 
Train Epoch: 8 [806/1000 25792/32000 (81%)] Loss: 1.96605 (semantic_loss: 0.01954, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20344 
Train Epoch: 8 [811/1000 25952/32000 (81%)] Loss: 1.96846 (semantic_loss: 0.02098, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21777 
Train Epoch: 8 [816/1000 26112/32000 (82%)] Loss: 1.96737 (semantic_loss: 0.01989, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20760 
Train Epoch: 8 [821/1000 26272/32000 (82%)] Loss: 1.97232 (semantic_loss: 0.02581, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19580 
Train Epoch: 8 [826/1000 26432/32000 (83%)] Loss: 1.96990 (semantic_loss: 0.02340, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20820 
Train Epoch: 8 [831/1000 26592/32000 (83%)] Loss: 1.96806 (semantic_loss: 0.01961, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19141 
Train Epoch: 8 [836/1000 26752/32000 (84%)] Loss: 1.97395 (semantic_loss: 0.02646, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18538 
Train Epoch: 8 [841/1000 26912/32000 (84%)] Loss: 1.96845 (semantic_loss: 0.02195, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18471 
Train Epoch: 8 [846/1000 27072/32000 (85%)] Loss: 1.96694 (semantic_loss: 0.02141, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18385 
Train Epoch: 8 [851/1000 27232/32000 (85%)] Loss: 1.96870 (semantic_loss: 0.02219, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18823 
Train Epoch: 8 [856/1000 27392/32000 (86%)] Loss: 1.96640 (semantic_loss: 0.01989, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18989 
Train Epoch: 8 [861/1000 27552/32000 (86%)] Loss: 1.96698 (semantic_loss: 0.02047, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20895 
Train Epoch: 8 [866/1000 27712/32000 (87%)] Loss: 1.96873 (semantic_loss: 0.02125, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18965 
Train Epoch: 8 [871/1000 27872/32000 (87%)] Loss: 1.96842 (semantic_loss: 0.02094, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18800 
Train Epoch: 8 [876/1000 28032/32000 (88%)] Loss: 1.96730 (semantic_loss: 0.01885, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18534 
Train Epoch: 8 [881/1000 28192/32000 (88%)] Loss: 1.97201 (semantic_loss: 0.02454, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18656 
Train Epoch: 8 [886/1000 28352/32000 (89%)] Loss: 1.97147 (semantic_loss: 0.02301, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19826 
Train Epoch: 8 [891/1000 28512/32000 (89%)] Loss: 1.97275 (semantic_loss: 0.02429, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19235 
Train Epoch: 8 [896/1000 28672/32000 (90%)] Loss: 1.97139 (semantic_loss: 0.02293, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18822 
Train Epoch: 8 [901/1000 28832/32000 (90%)] Loss: 1.97135 (semantic_loss: 0.02387, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18888 
Train Epoch: 8 [906/1000 28992/32000 (91%)] Loss: 1.96748 (semantic_loss: 0.02097, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18700 
Train Epoch: 8 [911/1000 29152/32000 (91%)] Loss: 1.97450 (semantic_loss: 0.02702, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18790 
Train Epoch: 8 [916/1000 29312/32000 (92%)] Loss: 1.97347 (semantic_loss: 0.02599, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19390 
Train Epoch: 8 [921/1000 29472/32000 (92%)] Loss: 1.97122 (semantic_loss: 0.02472, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20902 
Train Epoch: 8 [926/1000 29632/32000 (93%)] Loss: 1.96381 (semantic_loss: 0.01828, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18678 
Train Epoch: 8 [931/1000 29792/32000 (93%)] Loss: 1.96686 (semantic_loss: 0.01938, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18692 
Train Epoch: 8 [936/1000 29952/32000 (94%)] Loss: 1.97409 (semantic_loss: 0.02662, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18772 
Train Epoch: 8 [941/1000 30112/32000 (94%)] Loss: 1.97147 (semantic_loss: 0.02300, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18859 
Train Epoch: 8 [946/1000 30272/32000 (95%)] Loss: 1.96853 (semantic_loss: 0.02202, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20343 
Train Epoch: 8 [951/1000 30432/32000 (95%)] Loss: 1.97354 (semantic_loss: 0.02606, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22500 
Train Epoch: 8 [956/1000 30592/32000 (96%)] Loss: 1.96511 (semantic_loss: 0.01763, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20753 
Train Epoch: 8 [961/1000 30752/32000 (96%)] Loss: 1.96880 (semantic_loss: 0.02131, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.37246 
Train Epoch: 8 [966/1000 30912/32000 (97%)] Loss: 1.97213 (semantic_loss: 0.02465, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21537 
Train Epoch: 8 [971/1000 31072/32000 (97%)] Loss: 1.96821 (semantic_loss: 0.02073, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.58133 
Train Epoch: 8 [976/1000 31232/32000 (98%)] Loss: 1.96925 (semantic_loss: 0.02177, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21472 
Train Epoch: 8 [981/1000 31392/32000 (98%)] Loss: 1.96863 (semantic_loss: 0.02115, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19825 
Train Epoch: 8 [986/1000 31552/32000 (99%)] Loss: 1.97200 (semantic_loss: 0.02451, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21449 
Train Epoch: 8 [991/1000 31712/32000 (99%)] Loss: 1.97312 (semantic_loss: 0.02660, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.18934 
Train Epoch: 8 [996/1000 31872/32000 (100%)] Loss: 1.97186 (semantic_loss: 0.02341, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18882 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch8.pth ...
Done in 4.917s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch8.pth ...
Done in 9.374s
removing stale ckpt [epoch 7] [took 0.00s]
 epoch          : 8
 loss           : 1.9695073229074478
 learning_rate  : 2.391484500000001e-05
 n_samples      : 256000
 n_steps        : 8000
 ActivityNet_val1_test/t2v_metrics/R1: 8.96888346552776
 ActivityNet_val1_test/t2v_metrics/R5: 32.153752287980474
 ActivityNet_val1_test/t2v_metrics/R10: 50.49827130364043
 ActivityNet_val1_test/t2v_metrics/R50: 85.29591214154972
 ActivityNet_val1_test/t2v_metrics/MedR: 10.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 48.54535285743339
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 24.420179470119937
 ActivityNet_val1_test/v2t_metrics/R1: 9.334960341671751
 ActivityNet_val1_test/v2t_metrics/R5: 34.00447427293065
 ActivityNet_val1_test/v2t_metrics/R10: 51.47447630669107
 ActivityNet_val1_test/v2t_metrics/R50: 85.72300183038438
 ActivityNet_val1_test/v2t_metrics/MedR: 10.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 49.99288183851942
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 25.375434828819866
 mnt_best       : 24.420179470119937
 not_improved_count: 0
Train Epoch: 9 [1/1000 32/32000 (0%)] Loss: 1.97007 (semantic_loss: 0.02454, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=22.32315 
Train Epoch: 9 [6/1000 192/32000 (1%)] Loss: 1.96526 (semantic_loss: 0.01876, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19202 
Train Epoch: 9 [11/1000 352/32000 (1%)] Loss: 1.96783 (semantic_loss: 0.02035, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18442 
Train Epoch: 9 [16/1000 512/32000 (2%)] Loss: 1.96659 (semantic_loss: 0.02106, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.25735 
Train Epoch: 9 [21/1000 672/32000 (2%)] Loss: 1.96797 (semantic_loss: 0.02146, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20472 
Train Epoch: 9 [26/1000 832/32000 (3%)] Loss: 1.97172 (semantic_loss: 0.02521, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18457 
Train Epoch: 9 [31/1000 992/32000 (3%)] Loss: 1.96757 (semantic_loss: 0.02009, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.61902 
Train Epoch: 9 [36/1000 1152/32000 (4%)] Loss: 1.96850 (semantic_loss: 0.02102, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19538 
Train Epoch: 9 [41/1000 1312/32000 (4%)] Loss: 1.97202 (semantic_loss: 0.02356, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20713 
Train Epoch: 9 [46/1000 1472/32000 (5%)] Loss: 1.96570 (semantic_loss: 0.01822, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20507 
Train Epoch: 9 [51/1000 1632/32000 (5%)] Loss: 1.96657 (semantic_loss: 0.01909, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18932 
Train Epoch: 9 [56/1000 1792/32000 (6%)] Loss: 1.97166 (semantic_loss: 0.02417, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20457 
Train Epoch: 9 [61/1000 1952/32000 (6%)] Loss: 1.97391 (semantic_loss: 0.02644, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21348 
Train Epoch: 9 [66/1000 2112/32000 (7%)] Loss: 1.97374 (semantic_loss: 0.02626, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22631 
Train Epoch: 9 [71/1000 2272/32000 (7%)] Loss: 1.96779 (semantic_loss: 0.02031, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19798 
Train Epoch: 9 [76/1000 2432/32000 (8%)] Loss: 1.96713 (semantic_loss: 0.01965, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.44230 
Train Epoch: 9 [81/1000 2592/32000 (8%)] Loss: 1.96458 (semantic_loss: 0.01710, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.58714 
Train Epoch: 9 [86/1000 2752/32000 (9%)] Loss: 1.97341 (semantic_loss: 0.02593, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20478 
Train Epoch: 9 [91/1000 2912/32000 (9%)] Loss: 1.96587 (semantic_loss: 0.01838, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19507 
Train Epoch: 9 [96/1000 3072/32000 (10%)] Loss: 1.96526 (semantic_loss: 0.01778, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18663 
Train Epoch: 9 [101/1000 3232/32000 (10%)] Loss: 1.96927 (semantic_loss: 0.02179, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19830 
Train Epoch: 9 [106/1000 3392/32000 (11%)] Loss: 1.96612 (semantic_loss: 0.01961, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19721 
Train Epoch: 9 [111/1000 3552/32000 (11%)] Loss: 1.97039 (semantic_loss: 0.02290, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18813 
Train Epoch: 9 [116/1000 3712/32000 (12%)] Loss: 1.96869 (semantic_loss: 0.01925, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.18843 
Train Epoch: 9 [121/1000 3872/32000 (12%)] Loss: 1.96949 (semantic_loss: 0.02201, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22033 
Train Epoch: 9 [126/1000 4032/32000 (13%)] Loss: 1.96906 (semantic_loss: 0.02158, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19223 
Train Epoch: 9 [131/1000 4192/32000 (13%)] Loss: 1.97619 (semantic_loss: 0.02773, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21673 
Train Epoch: 9 [136/1000 4352/32000 (14%)] Loss: 1.97374 (semantic_loss: 0.02625, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20621 
Train Epoch: 9 [141/1000 4512/32000 (14%)] Loss: 1.96617 (semantic_loss: 0.01868, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18492 
Train Epoch: 9 [146/1000 4672/32000 (15%)] Loss: 1.96674 (semantic_loss: 0.02023, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18626 
Train Epoch: 9 [151/1000 4832/32000 (15%)] Loss: 1.96978 (semantic_loss: 0.02230, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20250 
Train Epoch: 9 [156/1000 4992/32000 (16%)] Loss: 1.96752 (semantic_loss: 0.02102, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18551 
Train Epoch: 9 [161/1000 5152/32000 (16%)] Loss: 1.96854 (semantic_loss: 0.02204, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20372 
Train Epoch: 9 [166/1000 5312/32000 (17%)] Loss: 1.97219 (semantic_loss: 0.02568, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20792 
Train Epoch: 9 [171/1000 5472/32000 (17%)] Loss: 1.96761 (semantic_loss: 0.01915, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.25269 
Train Epoch: 9 [176/1000 5632/32000 (18%)] Loss: 1.97076 (semantic_loss: 0.02426, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20835 
Train Epoch: 9 [181/1000 5792/32000 (18%)] Loss: 1.96775 (semantic_loss: 0.02027, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20607 
Train Epoch: 9 [186/1000 5952/32000 (19%)] Loss: 1.96494 (semantic_loss: 0.01746, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21553 
Train Epoch: 9 [191/1000 6112/32000 (19%)] Loss: 1.96748 (semantic_loss: 0.02195, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20469 
Train Epoch: 9 [196/1000 6272/32000 (20%)] Loss: 1.96875 (semantic_loss: 0.02029, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18957 
Train Epoch: 9 [201/1000 6432/32000 (20%)] Loss: 1.97098 (semantic_loss: 0.02350, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18879 
Train Epoch: 9 [206/1000 6592/32000 (21%)] Loss: 1.96802 (semantic_loss: 0.02054, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21478 
Train Epoch: 9 [211/1000 6752/32000 (21%)] Loss: 1.96541 (semantic_loss: 0.01793, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21600 
Train Epoch: 9 [216/1000 6912/32000 (22%)] Loss: 1.96586 (semantic_loss: 0.01936, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21498 
Train Epoch: 9 [221/1000 7072/32000 (22%)] Loss: 1.97035 (semantic_loss: 0.02288, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21075 
Train Epoch: 9 [226/1000 7232/32000 (23%)] Loss: 1.96863 (semantic_loss: 0.02212, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19401 
Train Epoch: 9 [231/1000 7392/32000 (23%)] Loss: 1.96696 (semantic_loss: 0.02046, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20278 
Train Epoch: 9 [236/1000 7552/32000 (24%)] Loss: 1.97082 (semantic_loss: 0.02236, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19856 
Train Epoch: 9 [241/1000 7712/32000 (24%)] Loss: 1.96794 (semantic_loss: 0.02046, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19937 
Train Epoch: 9 [246/1000 7872/32000 (25%)] Loss: 1.96663 (semantic_loss: 0.02110, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18959 
Train Epoch: 9 [251/1000 8032/32000 (25%)] Loss: 1.96973 (semantic_loss: 0.02321, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.18417 
Train Epoch: 9 [256/1000 8192/32000 (26%)] Loss: 1.96715 (semantic_loss: 0.02064, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19107 
Train Epoch: 9 [261/1000 8352/32000 (26%)] Loss: 1.96545 (semantic_loss: 0.01796, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18778 
Train Epoch: 9 [266/1000 8512/32000 (27%)] Loss: 1.96773 (semantic_loss: 0.02123, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.25353 
Train Epoch: 9 [271/1000 8672/32000 (27%)] Loss: 1.96998 (semantic_loss: 0.02152, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19147 
Train Epoch: 9 [276/1000 8832/32000 (28%)] Loss: 1.97150 (semantic_loss: 0.02402, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19140 
Train Epoch: 9 [281/1000 8992/32000 (28%)] Loss: 1.96766 (semantic_loss: 0.01920, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19508 
Train Epoch: 9 [286/1000 9152/32000 (29%)] Loss: 1.97299 (semantic_loss: 0.02551, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18835 
Train Epoch: 9 [291/1000 9312/32000 (29%)] Loss: 1.96682 (semantic_loss: 0.01934, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18910 
Train Epoch: 9 [296/1000 9472/32000 (30%)] Loss: 1.96747 (semantic_loss: 0.01999, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18747 
Train Epoch: 9 [301/1000 9632/32000 (30%)] Loss: 1.97143 (semantic_loss: 0.02492, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18711 
Train Epoch: 9 [306/1000 9792/32000 (31%)] Loss: 1.96682 (semantic_loss: 0.02032, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18648 
Train Epoch: 9 [311/1000 9952/32000 (31%)] Loss: 1.96621 (semantic_loss: 0.01873, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22248 
Train Epoch: 9 [316/1000 10112/32000 (32%)] Loss: 1.96959 (semantic_loss: 0.02113, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18812 
Train Epoch: 9 [321/1000 10272/32000 (32%)] Loss: 1.96902 (semantic_loss: 0.02251, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20169 
Train Epoch: 9 [326/1000 10432/32000 (33%)] Loss: 1.96888 (semantic_loss: 0.02237, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18672 
Train Epoch: 9 [331/1000 10592/32000 (33%)] Loss: 1.97001 (semantic_loss: 0.02350, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18788 
Train Epoch: 9 [336/1000 10752/32000 (34%)] Loss: 1.97068 (semantic_loss: 0.02319, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.24809 
Train Epoch: 9 [341/1000 10912/32000 (34%)] Loss: 1.97360 (semantic_loss: 0.02611, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21367 
Train Epoch: 9 [346/1000 11072/32000 (35%)] Loss: 1.96791 (semantic_loss: 0.02140, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21272 
Train Epoch: 9 [351/1000 11232/32000 (35%)] Loss: 1.96918 (semantic_loss: 0.02072, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.59228 
Train Epoch: 9 [356/1000 11392/32000 (36%)] Loss: 1.96689 (semantic_loss: 0.01941, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19427 
Train Epoch: 9 [361/1000 11552/32000 (36%)] Loss: 1.96940 (semantic_loss: 0.02192, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19195 
Train Epoch: 9 [366/1000 11712/32000 (37%)] Loss: 1.96993 (semantic_loss: 0.02342, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20109 
Train Epoch: 9 [371/1000 11872/32000 (37%)] Loss: 1.96690 (semantic_loss: 0.01845, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22077 
Train Epoch: 9 [376/1000 12032/32000 (38%)] Loss: 1.96990 (semantic_loss: 0.02340, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21451 
Train Epoch: 9 [381/1000 12192/32000 (38%)] Loss: 1.97006 (semantic_loss: 0.02355, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22140 
Train Epoch: 9 [386/1000 12352/32000 (39%)] Loss: 1.96655 (semantic_loss: 0.01906, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21285 
Train Epoch: 9 [391/1000 12512/32000 (39%)] Loss: 1.96463 (semantic_loss: 0.01715, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21293 
Train Epoch: 9 [396/1000 12672/32000 (40%)] Loss: 1.96814 (semantic_loss: 0.02163, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.46273 
Train Epoch: 9 [401/1000 12832/32000 (40%)] Loss: 1.96797 (semantic_loss: 0.02147, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.60719 
Train Epoch: 9 [406/1000 12992/32000 (41%)] Loss: 1.97219 (semantic_loss: 0.02569, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19415 
Train Epoch: 9 [411/1000 13152/32000 (41%)] Loss: 1.96830 (semantic_loss: 0.01984, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18680 
Train Epoch: 9 [416/1000 13312/32000 (42%)] Loss: 1.96465 (semantic_loss: 0.01912, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18652 
Train Epoch: 9 [421/1000 13472/32000 (42%)] Loss: 1.97202 (semantic_loss: 0.02453, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18867 
Train Epoch: 9 [426/1000 13632/32000 (43%)] Loss: 1.96437 (semantic_loss: 0.01787, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18824 
Train Epoch: 9 [431/1000 13792/32000 (43%)] Loss: 1.96855 (semantic_loss: 0.02107, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20448 
Train Epoch: 9 [436/1000 13952/32000 (44%)] Loss: 1.96641 (semantic_loss: 0.01795, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19623 
Train Epoch: 9 [441/1000 14112/32000 (44%)] Loss: 1.96758 (semantic_loss: 0.02011, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20639 
Train Epoch: 9 [446/1000 14272/32000 (45%)] Loss: 1.96638 (semantic_loss: 0.01988, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20311 
Train Epoch: 9 [451/1000 14432/32000 (45%)] Loss: 1.96899 (semantic_loss: 0.02247, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.20550 
Train Epoch: 9 [456/1000 14592/32000 (46%)] Loss: 1.96719 (semantic_loss: 0.01971, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20327 
Train Epoch: 9 [461/1000 14752/32000 (46%)] Loss: 1.97010 (semantic_loss: 0.02262, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18267 
Train Epoch: 9 [466/1000 14912/32000 (47%)] Loss: 1.96426 (semantic_loss: 0.01775, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18621 
Train Epoch: 9 [471/1000 15072/32000 (47%)] Loss: 1.96860 (semantic_loss: 0.02112, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18522 
Train Epoch: 9 [476/1000 15232/32000 (48%)] Loss: 1.96593 (semantic_loss: 0.01943, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18798 
Train Epoch: 9 [481/1000 15392/32000 (48%)] Loss: 1.96737 (semantic_loss: 0.02183, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18557 
Train Epoch: 9 [486/1000 15552/32000 (49%)] Loss: 1.96710 (semantic_loss: 0.02060, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18744 
Train Epoch: 9 [491/1000 15712/32000 (49%)] Loss: 1.96882 (semantic_loss: 0.02036, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.23832 
Train Epoch: 9 [496/1000 15872/32000 (50%)] Loss: 1.96739 (semantic_loss: 0.02088, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18849 
Train Epoch: 9 [501/1000 16032/32000 (50%)] Loss: 1.96994 (semantic_loss: 0.02244, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.18619 
Train Epoch: 9 [506/1000 16192/32000 (51%)] Loss: 1.96110 (semantic_loss: 0.01655, quant_loss: 1.94434, bit_balance_loss: 0.00022) batch_time=0.19682 
Train Epoch: 9 [511/1000 16352/32000 (51%)] Loss: 1.96413 (semantic_loss: 0.01762, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20420 
Train Epoch: 9 [516/1000 16512/32000 (52%)] Loss: 1.96900 (semantic_loss: 0.02249, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18807 
Train Epoch: 9 [521/1000 16672/32000 (52%)] Loss: 1.96668 (semantic_loss: 0.01920, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20173 
Train Epoch: 9 [526/1000 16832/32000 (53%)] Loss: 1.96510 (semantic_loss: 0.01858, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.22145 
Train Epoch: 9 [531/1000 16992/32000 (53%)] Loss: 1.96984 (semantic_loss: 0.02335, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20204 
Train Epoch: 9 [536/1000 17152/32000 (54%)] Loss: 1.96866 (semantic_loss: 0.02119, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19711 
Train Epoch: 9 [541/1000 17312/32000 (54%)] Loss: 1.96889 (semantic_loss: 0.02043, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.21209 
Train Epoch: 9 [546/1000 17472/32000 (55%)] Loss: 1.96820 (semantic_loss: 0.02169, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.21452 
Train Epoch: 9 [551/1000 17632/32000 (55%)] Loss: 1.96873 (semantic_loss: 0.02125, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20201 
Train Epoch: 9 [556/1000 17792/32000 (56%)] Loss: 1.96702 (semantic_loss: 0.01954, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19002 
Train Epoch: 9 [561/1000 17952/32000 (56%)] Loss: 1.96604 (semantic_loss: 0.01954, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19290 
Train Epoch: 9 [566/1000 18112/32000 (57%)] Loss: 1.96911 (semantic_loss: 0.02163, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18743 
Train Epoch: 9 [571/1000 18272/32000 (57%)] Loss: 1.96565 (semantic_loss: 0.01914, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19580 
Train Epoch: 9 [576/1000 18432/32000 (58%)] Loss: 1.97213 (semantic_loss: 0.02464, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.19022 
Train Epoch: 9 [581/1000 18592/32000 (58%)] Loss: 1.97042 (semantic_loss: 0.02293, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21483 
Train Epoch: 9 [586/1000 18752/32000 (59%)] Loss: 1.97038 (semantic_loss: 0.02289, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.27879 
Train Epoch: 9 [591/1000 18912/32000 (59%)] Loss: 1.96875 (semantic_loss: 0.02029, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19221 
Train Epoch: 9 [596/1000 19072/32000 (60%)] Loss: 1.96475 (semantic_loss: 0.01825, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18585 
Train Epoch: 9 [601/1000 19232/32000 (60%)] Loss: 1.96588 (semantic_loss: 0.01840, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18757 
Train Epoch: 9 [606/1000 19392/32000 (61%)] Loss: 1.96872 (semantic_loss: 0.02123, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18647 
Train Epoch: 9 [611/1000 19552/32000 (61%)] Loss: 1.96808 (semantic_loss: 0.02060, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18583 
Train Epoch: 9 [616/1000 19712/32000 (62%)] Loss: 1.96655 (semantic_loss: 0.02004, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18459 
Train Epoch: 9 [621/1000 19872/32000 (62%)] Loss: 1.97447 (semantic_loss: 0.02601, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18540 
Train Epoch: 9 [626/1000 20032/32000 (63%)] Loss: 1.96463 (semantic_loss: 0.01812, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20159 
Train Epoch: 9 [631/1000 20192/32000 (63%)] Loss: 1.96870 (semantic_loss: 0.02220, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19863 
Train Epoch: 9 [636/1000 20352/32000 (64%)] Loss: 1.96827 (semantic_loss: 0.02079, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18708 
Train Epoch: 9 [641/1000 20512/32000 (64%)] Loss: 1.96790 (semantic_loss: 0.02139, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18942 
Train Epoch: 9 [646/1000 20672/32000 (65%)] Loss: 1.96717 (semantic_loss: 0.01970, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19532 
Train Epoch: 9 [651/1000 20832/32000 (65%)] Loss: 1.97334 (semantic_loss: 0.02683, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19483 
Train Epoch: 9 [656/1000 20992/32000 (66%)] Loss: 1.96352 (semantic_loss: 0.01799, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.25382 
Train Epoch: 9 [661/1000 21152/32000 (66%)] Loss: 1.96755 (semantic_loss: 0.01910, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19597 
Train Epoch: 9 [666/1000 21312/32000 (67%)] Loss: 1.96416 (semantic_loss: 0.01765, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18976 
Train Epoch: 9 [671/1000 21472/32000 (67%)] Loss: 1.96793 (semantic_loss: 0.02045, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.71847 
Train Epoch: 9 [676/1000 21632/32000 (68%)] Loss: 1.96857 (semantic_loss: 0.02206, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20103 
Train Epoch: 9 [681/1000 21792/32000 (68%)] Loss: 1.96755 (semantic_loss: 0.02007, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21816 
Train Epoch: 9 [686/1000 21952/32000 (69%)] Loss: 1.96397 (semantic_loss: 0.01845, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20656 
Train Epoch: 9 [691/1000 22112/32000 (69%)] Loss: 1.96931 (semantic_loss: 0.02182, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21209 
Train Epoch: 9 [696/1000 22272/32000 (70%)] Loss: 1.97146 (semantic_loss: 0.02398, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19457 
Train Epoch: 9 [701/1000 22432/32000 (70%)] Loss: 1.96963 (semantic_loss: 0.02214, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19678 
Train Epoch: 9 [706/1000 22592/32000 (71%)] Loss: 1.96745 (semantic_loss: 0.01996, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.22580 
Train Epoch: 9 [711/1000 22752/32000 (71%)] Loss: 1.96825 (semantic_loss: 0.02078, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19088 
Train Epoch: 9 [716/1000 22912/32000 (72%)] Loss: 1.96983 (semantic_loss: 0.02236, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.40676 
Train Epoch: 9 [721/1000 23072/32000 (72%)] Loss: 1.96726 (semantic_loss: 0.01978, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.53210 
Train Epoch: 9 [726/1000 23232/32000 (73%)] Loss: 1.96846 (semantic_loss: 0.02195, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18886 
Train Epoch: 9 [731/1000 23392/32000 (73%)] Loss: 1.96554 (semantic_loss: 0.01806, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18865 
Train Epoch: 9 [736/1000 23552/32000 (74%)] Loss: 1.96816 (semantic_loss: 0.02165, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19180 
Train Epoch: 9 [741/1000 23712/32000 (74%)] Loss: 1.96949 (semantic_loss: 0.02298, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19901 
Train Epoch: 9 [746/1000 23872/32000 (75%)] Loss: 1.97232 (semantic_loss: 0.02484, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19733 
Train Epoch: 9 [751/1000 24032/32000 (75%)] Loss: 1.96976 (semantic_loss: 0.02130, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18891 
Train Epoch: 9 [756/1000 24192/32000 (76%)] Loss: 1.97154 (semantic_loss: 0.02405, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18728 
Train Epoch: 9 [761/1000 24352/32000 (76%)] Loss: 1.96834 (semantic_loss: 0.02086, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20276 
Train Epoch: 9 [766/1000 24512/32000 (77%)] Loss: 1.97471 (semantic_loss: 0.02918, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18836 
Train Epoch: 9 [771/1000 24672/32000 (77%)] Loss: 1.97039 (semantic_loss: 0.02194, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20629 
Train Epoch: 9 [776/1000 24832/32000 (78%)] Loss: 1.96706 (semantic_loss: 0.02056, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20516 
Train Epoch: 9 [781/1000 24992/32000 (78%)] Loss: 1.97079 (semantic_loss: 0.02331, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18784 
Train Epoch: 9 [786/1000 25152/32000 (79%)] Loss: 1.96724 (semantic_loss: 0.01878, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18905 
Train Epoch: 9 [791/1000 25312/32000 (79%)] Loss: 1.97107 (semantic_loss: 0.02359, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18978 
Train Epoch: 9 [796/1000 25472/32000 (80%)] Loss: 1.97022 (semantic_loss: 0.02273, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18521 
Train Epoch: 9 [801/1000 25632/32000 (80%)] Loss: 1.97470 (semantic_loss: 0.02722, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18689 
Train Epoch: 9 [806/1000 25792/32000 (81%)] Loss: 1.96947 (semantic_loss: 0.02200, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18786 
Train Epoch: 9 [811/1000 25952/32000 (81%)] Loss: 1.96746 (semantic_loss: 0.01998, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.23880 
Train Epoch: 9 [816/1000 26112/32000 (82%)] Loss: 1.96832 (semantic_loss: 0.02084, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19057 
Train Epoch: 9 [821/1000 26272/32000 (82%)] Loss: 1.97109 (semantic_loss: 0.02263, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18944 
Train Epoch: 9 [826/1000 26432/32000 (83%)] Loss: 1.96713 (semantic_loss: 0.02062, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21349 
Train Epoch: 9 [831/1000 26592/32000 (83%)] Loss: 1.96851 (semantic_loss: 0.02103, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.24027 
Train Epoch: 9 [836/1000 26752/32000 (84%)] Loss: 1.96479 (semantic_loss: 0.01829, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21100 
Train Epoch: 9 [841/1000 26912/32000 (84%)] Loss: 1.96622 (semantic_loss: 0.01971, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22203 
Train Epoch: 9 [846/1000 27072/32000 (85%)] Loss: 1.96672 (semantic_loss: 0.02119, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.20049 
Train Epoch: 9 [851/1000 27232/32000 (85%)] Loss: 1.96391 (semantic_loss: 0.01838, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.19559 
Train Epoch: 9 [856/1000 27392/32000 (86%)] Loss: 1.96565 (semantic_loss: 0.01914, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19615 
Train Epoch: 9 [861/1000 27552/32000 (86%)] Loss: 1.96803 (semantic_loss: 0.01957, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19279 
Train Epoch: 9 [866/1000 27712/32000 (87%)] Loss: 1.97051 (semantic_loss: 0.02401, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20087 
Train Epoch: 9 [871/1000 27872/32000 (87%)] Loss: 1.96863 (semantic_loss: 0.02114, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19461 
Train Epoch: 9 [876/1000 28032/32000 (88%)] Loss: 1.97503 (semantic_loss: 0.02755, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18925 
Train Epoch: 9 [881/1000 28192/32000 (88%)] Loss: 1.96940 (semantic_loss: 0.02192, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18391 
Train Epoch: 9 [886/1000 28352/32000 (89%)] Loss: 1.96568 (semantic_loss: 0.01819, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20643 
Train Epoch: 9 [891/1000 28512/32000 (89%)] Loss: 1.96686 (semantic_loss: 0.01938, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18966 
Train Epoch: 9 [896/1000 28672/32000 (90%)] Loss: 1.97032 (semantic_loss: 0.02381, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19313 
Train Epoch: 9 [901/1000 28832/32000 (90%)] Loss: 1.96617 (semantic_loss: 0.01966, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18883 
Train Epoch: 9 [906/1000 28992/32000 (91%)] Loss: 1.97498 (semantic_loss: 0.02750, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25024 
Train Epoch: 9 [911/1000 29152/32000 (91%)] Loss: 1.96682 (semantic_loss: 0.02031, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.18536 
Train Epoch: 9 [916/1000 29312/32000 (92%)] Loss: 1.96822 (semantic_loss: 0.02074, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18462 
Train Epoch: 9 [921/1000 29472/32000 (92%)] Loss: 1.96502 (semantic_loss: 0.01852, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18548 
Train Epoch: 9 [926/1000 29632/32000 (93%)] Loss: 1.96575 (semantic_loss: 0.01827, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19076 
Train Epoch: 9 [931/1000 29792/32000 (93%)] Loss: 1.96493 (semantic_loss: 0.01843, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18802 
Train Epoch: 9 [936/1000 29952/32000 (94%)] Loss: 1.96867 (semantic_loss: 0.02216, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19631 
Train Epoch: 9 [941/1000 30112/32000 (94%)] Loss: 1.96707 (semantic_loss: 0.02056, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18313 
Train Epoch: 9 [946/1000 30272/32000 (95%)] Loss: 1.96752 (semantic_loss: 0.02101, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18348 
Train Epoch: 9 [951/1000 30432/32000 (95%)] Loss: 1.97190 (semantic_loss: 0.02442, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.27773 
Train Epoch: 9 [956/1000 30592/32000 (96%)] Loss: 1.96607 (semantic_loss: 0.01761, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19068 
Train Epoch: 9 [961/1000 30752/32000 (96%)] Loss: 1.97136 (semantic_loss: 0.02583, quant_loss: 1.94531, bit_balance_loss: 0.00023) batch_time=0.18892 
Train Epoch: 9 [966/1000 30912/32000 (97%)] Loss: 1.96825 (semantic_loss: 0.01980, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18876 
Train Epoch: 9 [971/1000 31072/32000 (97%)] Loss: 1.96734 (semantic_loss: 0.02083, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18496 
Train Epoch: 9 [976/1000 31232/32000 (98%)] Loss: 1.96760 (semantic_loss: 0.02109, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.29771 
Train Epoch: 9 [981/1000 31392/32000 (98%)] Loss: 1.96811 (semantic_loss: 0.02062, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22840 
Train Epoch: 9 [986/1000 31552/32000 (99%)] Loss: 1.96738 (semantic_loss: 0.02088, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20956 
Train Epoch: 9 [991/1000 31712/32000 (99%)] Loss: 1.96814 (semantic_loss: 0.02163, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.67229 
Train Epoch: 9 [996/1000 31872/32000 (100%)] Loss: 1.96465 (semantic_loss: 0.01814, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20312 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch9.pth ...
Done in 4.323s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch9.pth ...
Done in 8.409s
removing stale ckpt [epoch 8] [took 0.00s]
 epoch          : 9
 loss           : 1.9686110204458236
 learning_rate  : 2.152336050000001e-05
 n_samples      : 288000
 n_steps        : 9000
 ActivityNet_val1_test/t2v_metrics/R1: 9.599349196664633
 ActivityNet_val1_test/t2v_metrics/R5: 34.49257677445597
 ActivityNet_val1_test/t2v_metrics/R10: 52.043929225137276
 ActivityNet_val1_test/t2v_metrics/R50: 86.4958307911328
 ActivityNet_val1_test/t2v_metrics/MedR: 10.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 46.45261338214358
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 25.82929203094815
 ActivityNet_val1_test/v2t_metrics/R1: 10.311165344722392
 ActivityNet_val1_test/v2t_metrics/R5: 35.509456985967056
 ActivityNet_val1_test/v2t_metrics/R10: 52.572706935123044
 ActivityNet_val1_test/v2t_metrics/R50: 86.10941631075859
 ActivityNet_val1_test/v2t_metrics/MedR: 9.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 49.241814114297334
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 26.800159193466932
 mnt_best       : 25.82929203094815
 not_improved_count: 0
Train Epoch: 10 [1/1000 32/32000 (0%)] Loss: 1.96816 (semantic_loss: 0.02165, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=23.78179 
Train Epoch: 10 [6/1000 192/32000 (1%)] Loss: 1.96827 (semantic_loss: 0.02079, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18754 
Train Epoch: 10 [11/1000 352/32000 (1%)] Loss: 1.96536 (semantic_loss: 0.01885, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18996 
Train Epoch: 10 [16/1000 512/32000 (2%)] Loss: 1.96921 (semantic_loss: 0.02172, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.29671 
Train Epoch: 10 [21/1000 672/32000 (2%)] Loss: 1.96845 (semantic_loss: 0.02097, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19131 
Train Epoch: 10 [26/1000 832/32000 (3%)] Loss: 1.96713 (semantic_loss: 0.02062, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18627 
Train Epoch: 10 [31/1000 992/32000 (3%)] Loss: 1.96722 (semantic_loss: 0.01974, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18647 
Train Epoch: 10 [36/1000 1152/32000 (4%)] Loss: 1.96695 (semantic_loss: 0.02044, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18624 
Train Epoch: 10 [41/1000 1312/32000 (4%)] Loss: 1.96691 (semantic_loss: 0.01943, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18671 
Train Epoch: 10 [46/1000 1472/32000 (5%)] Loss: 1.96663 (semantic_loss: 0.02013, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19873 
Train Epoch: 10 [51/1000 1632/32000 (5%)] Loss: 1.96692 (semantic_loss: 0.02042, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18619 
Train Epoch: 10 [56/1000 1792/32000 (6%)] Loss: 1.96985 (semantic_loss: 0.02237, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18822 
Train Epoch: 10 [61/1000 1952/32000 (6%)] Loss: 1.96997 (semantic_loss: 0.02248, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18815 
Train Epoch: 10 [66/1000 2112/32000 (7%)] Loss: 1.96991 (semantic_loss: 0.02340, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.19911 
Train Epoch: 10 [71/1000 2272/32000 (7%)] Loss: 1.96920 (semantic_loss: 0.02172, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18516 
Train Epoch: 10 [76/1000 2432/32000 (8%)] Loss: 1.96415 (semantic_loss: 0.01667, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18565 
Train Epoch: 10 [81/1000 2592/32000 (8%)] Loss: 1.96626 (semantic_loss: 0.01878, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18523 
Train Epoch: 10 [86/1000 2752/32000 (9%)] Loss: 1.96690 (semantic_loss: 0.01942, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.68857 
Train Epoch: 10 [91/1000 2912/32000 (9%)] Loss: 1.96843 (semantic_loss: 0.02095, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21285 
Train Epoch: 10 [96/1000 3072/32000 (10%)] Loss: 1.96778 (semantic_loss: 0.01932, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20134 
Train Epoch: 10 [101/1000 3232/32000 (10%)] Loss: 1.96950 (semantic_loss: 0.02202, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20533 
Train Epoch: 10 [106/1000 3392/32000 (11%)] Loss: 1.96757 (semantic_loss: 0.02106, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20364 
Train Epoch: 10 [111/1000 3552/32000 (11%)] Loss: 1.96469 (semantic_loss: 0.01818, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19877 
Train Epoch: 10 [116/1000 3712/32000 (12%)] Loss: 1.96701 (semantic_loss: 0.01953, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19274 
Train Epoch: 10 [121/1000 3872/32000 (12%)] Loss: 1.96852 (semantic_loss: 0.02006, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19247 
Train Epoch: 10 [126/1000 4032/32000 (13%)] Loss: 1.96518 (semantic_loss: 0.01770, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19303 
Train Epoch: 10 [131/1000 4192/32000 (13%)] Loss: 1.96869 (semantic_loss: 0.02024, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.23214 
Train Epoch: 10 [136/1000 4352/32000 (14%)] Loss: 1.96834 (semantic_loss: 0.01989, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.23896 
Train Epoch: 10 [141/1000 4512/32000 (14%)] Loss: 1.96415 (semantic_loss: 0.01764, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18730 
Train Epoch: 10 [146/1000 4672/32000 (15%)] Loss: 1.96988 (semantic_loss: 0.02240, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18812 
Train Epoch: 10 [151/1000 4832/32000 (15%)] Loss: 1.96810 (semantic_loss: 0.02061, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19081 
Train Epoch: 10 [156/1000 4992/32000 (16%)] Loss: 1.96372 (semantic_loss: 0.01722, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19178 
Train Epoch: 10 [161/1000 5152/32000 (16%)] Loss: 1.96643 (semantic_loss: 0.01993, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20410 
Train Epoch: 10 [166/1000 5312/32000 (17%)] Loss: 1.96618 (semantic_loss: 0.01967, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18717 
Train Epoch: 10 [171/1000 5472/32000 (17%)] Loss: 1.97141 (semantic_loss: 0.02392, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18556 
Train Epoch: 10 [176/1000 5632/32000 (18%)] Loss: 1.96838 (semantic_loss: 0.02090, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18973 
Train Epoch: 10 [181/1000 5792/32000 (18%)] Loss: 1.96390 (semantic_loss: 0.01838, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18607 
Train Epoch: 10 [186/1000 5952/32000 (19%)] Loss: 1.96758 (semantic_loss: 0.02107, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19563 
Train Epoch: 10 [191/1000 6112/32000 (19%)] Loss: 1.96798 (semantic_loss: 0.02050, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18715 
Train Epoch: 10 [196/1000 6272/32000 (20%)] Loss: 1.96377 (semantic_loss: 0.01629, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18484 
Train Epoch: 10 [201/1000 6432/32000 (20%)] Loss: 1.96484 (semantic_loss: 0.01736, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18554 
Train Epoch: 10 [206/1000 6592/32000 (21%)] Loss: 1.96464 (semantic_loss: 0.01911, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.19441 
Train Epoch: 10 [211/1000 6752/32000 (21%)] Loss: 1.96453 (semantic_loss: 0.01705, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18651 
Train Epoch: 10 [216/1000 6912/32000 (22%)] Loss: 1.96890 (semantic_loss: 0.02044, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20369 
Train Epoch: 10 [221/1000 7072/32000 (22%)] Loss: 1.96845 (semantic_loss: 0.02000, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18964 
Train Epoch: 10 [226/1000 7232/32000 (23%)] Loss: 1.97097 (semantic_loss: 0.02251, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18797 
Train Epoch: 10 [231/1000 7392/32000 (23%)] Loss: 1.96598 (semantic_loss: 0.01947, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18957 
Train Epoch: 10 [236/1000 7552/32000 (24%)] Loss: 1.96985 (semantic_loss: 0.02139, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19214 
Train Epoch: 10 [241/1000 7712/32000 (24%)] Loss: 1.96767 (semantic_loss: 0.02019, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21139 
Train Epoch: 10 [246/1000 7872/32000 (25%)] Loss: 1.96895 (semantic_loss: 0.02049, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19660 
Train Epoch: 10 [251/1000 8032/32000 (25%)] Loss: 1.97474 (semantic_loss: 0.02725, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21709 
Train Epoch: 10 [256/1000 8192/32000 (26%)] Loss: 1.97017 (semantic_loss: 0.02366, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22038 
Train Epoch: 10 [261/1000 8352/32000 (26%)] Loss: 1.96934 (semantic_loss: 0.02185, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21986 
Train Epoch: 10 [266/1000 8512/32000 (27%)] Loss: 1.97134 (semantic_loss: 0.02386, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20934 
Train Epoch: 10 [271/1000 8672/32000 (27%)] Loss: 1.96365 (semantic_loss: 0.01715, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.23470 
Train Epoch: 10 [276/1000 8832/32000 (28%)] Loss: 1.96826 (semantic_loss: 0.02175, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=1.12167 
Train Epoch: 10 [281/1000 8992/32000 (28%)] Loss: 1.96693 (semantic_loss: 0.02042, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18848 
Train Epoch: 10 [286/1000 9152/32000 (29%)] Loss: 1.96549 (semantic_loss: 0.01899, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18931 
Train Epoch: 10 [291/1000 9312/32000 (29%)] Loss: 1.96764 (semantic_loss: 0.02016, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20456 
Train Epoch: 10 [296/1000 9472/32000 (30%)] Loss: 1.97340 (semantic_loss: 0.02591, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19005 
Train Epoch: 10 [301/1000 9632/32000 (30%)] Loss: 1.97090 (semantic_loss: 0.02341, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18926 
Train Epoch: 10 [306/1000 9792/32000 (31%)] Loss: 1.96785 (semantic_loss: 0.01940, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19238 
Train Epoch: 10 [311/1000 9952/32000 (31%)] Loss: 1.96976 (semantic_loss: 0.02228, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19220 
Train Epoch: 10 [316/1000 10112/32000 (32%)] Loss: 1.97299 (semantic_loss: 0.02550, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19863 
Train Epoch: 10 [321/1000 10272/32000 (32%)] Loss: 1.96954 (semantic_loss: 0.02303, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18881 
Train Epoch: 10 [326/1000 10432/32000 (33%)] Loss: 1.96737 (semantic_loss: 0.01989, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18678 
Train Epoch: 10 [331/1000 10592/32000 (33%)] Loss: 1.96969 (semantic_loss: 0.02319, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18775 
Train Epoch: 10 [336/1000 10752/32000 (34%)] Loss: 1.96705 (semantic_loss: 0.01957, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.29858 
Train Epoch: 10 [341/1000 10912/32000 (34%)] Loss: 1.96533 (semantic_loss: 0.01785, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18495 
Train Epoch: 10 [346/1000 11072/32000 (35%)] Loss: 1.96718 (semantic_loss: 0.01872, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18528 
Train Epoch: 10 [351/1000 11232/32000 (35%)] Loss: 1.96699 (semantic_loss: 0.01950, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20756 
Train Epoch: 10 [356/1000 11392/32000 (36%)] Loss: 1.96645 (semantic_loss: 0.01994, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.44908 
Train Epoch: 10 [361/1000 11552/32000 (36%)] Loss: 1.96537 (semantic_loss: 0.01983, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18412 
Train Epoch: 10 [366/1000 11712/32000 (37%)] Loss: 1.96765 (semantic_loss: 0.01918, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18639 
Train Epoch: 10 [371/1000 11872/32000 (37%)] Loss: 1.96317 (semantic_loss: 0.01666, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18795 
Train Epoch: 10 [376/1000 12032/32000 (38%)] Loss: 1.96529 (semantic_loss: 0.01781, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18780 
Train Epoch: 10 [381/1000 12192/32000 (38%)] Loss: 1.96975 (semantic_loss: 0.02323, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.18801 
Train Epoch: 10 [386/1000 12352/32000 (39%)] Loss: 1.96836 (semantic_loss: 0.01990, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19552 
Train Epoch: 10 [391/1000 12512/32000 (39%)] Loss: 1.96395 (semantic_loss: 0.01745, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18940 
Train Epoch: 10 [396/1000 12672/32000 (40%)] Loss: 1.96398 (semantic_loss: 0.01650, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21115 
Train Epoch: 10 [401/1000 12832/32000 (40%)] Loss: 1.96296 (semantic_loss: 0.01646, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21021 
Train Epoch: 10 [406/1000 12992/32000 (41%)] Loss: 1.97057 (semantic_loss: 0.02309, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.64219 
Train Epoch: 10 [411/1000 13152/32000 (41%)] Loss: 1.96373 (semantic_loss: 0.01722, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19975 
Train Epoch: 10 [416/1000 13312/32000 (42%)] Loss: 1.96928 (semantic_loss: 0.02179, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20188 
Train Epoch: 10 [421/1000 13472/32000 (42%)] Loss: 1.96480 (semantic_loss: 0.01830, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20686 
Train Epoch: 10 [426/1000 13632/32000 (43%)] Loss: 1.96500 (semantic_loss: 0.01850, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19264 
Train Epoch: 10 [431/1000 13792/32000 (43%)] Loss: 1.96634 (semantic_loss: 0.01887, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19237 
Train Epoch: 10 [436/1000 13952/32000 (44%)] Loss: 1.96486 (semantic_loss: 0.01738, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20545 
Train Epoch: 10 [441/1000 14112/32000 (44%)] Loss: 1.96508 (semantic_loss: 0.01759, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20640 
Train Epoch: 10 [446/1000 14272/32000 (45%)] Loss: 1.96852 (semantic_loss: 0.02299, quant_loss: 1.94531, bit_balance_loss: 0.00023) batch_time=0.20582 
Train Epoch: 10 [451/1000 14432/32000 (45%)] Loss: 1.96596 (semantic_loss: 0.02043, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.23302 
Train Epoch: 10 [456/1000 14592/32000 (46%)] Loss: 1.96393 (semantic_loss: 0.01742, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24014 
Train Epoch: 10 [461/1000 14752/32000 (46%)] Loss: 1.97458 (semantic_loss: 0.02711, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19087 
Train Epoch: 10 [466/1000 14912/32000 (47%)] Loss: 1.96382 (semantic_loss: 0.01634, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18853 
Train Epoch: 10 [471/1000 15072/32000 (47%)] Loss: 1.96297 (semantic_loss: 0.01647, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18726 
Train Epoch: 10 [476/1000 15232/32000 (48%)] Loss: 1.97266 (semantic_loss: 0.02517, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18903 
Train Epoch: 10 [481/1000 15392/32000 (48%)] Loss: 1.96792 (semantic_loss: 0.02043, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.18808 
Train Epoch: 10 [486/1000 15552/32000 (49%)] Loss: 1.96834 (semantic_loss: 0.02183, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18927 
Train Epoch: 10 [491/1000 15712/32000 (49%)] Loss: 1.97327 (semantic_loss: 0.02677, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18640 
Train Epoch: 10 [496/1000 15872/32000 (50%)] Loss: 1.96872 (semantic_loss: 0.02319, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18285 
Train Epoch: 10 [501/1000 16032/32000 (50%)] Loss: 1.96833 (semantic_loss: 0.01988, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18815 
Train Epoch: 10 [506/1000 16192/32000 (51%)] Loss: 1.96891 (semantic_loss: 0.02045, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18818 
Train Epoch: 10 [511/1000 16352/32000 (51%)] Loss: 1.96537 (semantic_loss: 0.01984, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19991 
Train Epoch: 10 [516/1000 16512/32000 (52%)] Loss: 1.96603 (semantic_loss: 0.01855, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18965 
Train Epoch: 10 [521/1000 16672/32000 (52%)] Loss: 1.96672 (semantic_loss: 0.01924, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18752 
Train Epoch: 10 [526/1000 16832/32000 (53%)] Loss: 1.96938 (semantic_loss: 0.02190, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22642 
Train Epoch: 10 [531/1000 16992/32000 (53%)] Loss: 1.96556 (semantic_loss: 0.01904, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.19053 
Train Epoch: 10 [536/1000 17152/32000 (54%)] Loss: 1.96661 (semantic_loss: 0.01912, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18293 
Train Epoch: 10 [541/1000 17312/32000 (54%)] Loss: 1.96648 (semantic_loss: 0.01900, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22027 
Train Epoch: 10 [546/1000 17472/32000 (55%)] Loss: 1.97031 (semantic_loss: 0.02283, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21863 
Train Epoch: 10 [551/1000 17632/32000 (55%)] Loss: 1.96693 (semantic_loss: 0.01847, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.21090 
Train Epoch: 10 [556/1000 17792/32000 (56%)] Loss: 1.96725 (semantic_loss: 0.01879, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21236 
Train Epoch: 10 [561/1000 17952/32000 (56%)] Loss: 1.96677 (semantic_loss: 0.01929, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20365 
Train Epoch: 10 [566/1000 18112/32000 (57%)] Loss: 1.96736 (semantic_loss: 0.01988, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20548 
Train Epoch: 10 [571/1000 18272/32000 (57%)] Loss: 1.96893 (semantic_loss: 0.02145, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19534 
Train Epoch: 10 [576/1000 18432/32000 (58%)] Loss: 1.96940 (semantic_loss: 0.02192, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19857 
Train Epoch: 10 [581/1000 18592/32000 (58%)] Loss: 1.96786 (semantic_loss: 0.02037, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19742 
Train Epoch: 10 [586/1000 18752/32000 (59%)] Loss: 1.96738 (semantic_loss: 0.02087, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18655 
Train Epoch: 10 [591/1000 18912/32000 (59%)] Loss: 1.96578 (semantic_loss: 0.01927, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22929 
Train Epoch: 10 [596/1000 19072/32000 (60%)] Loss: 1.96477 (semantic_loss: 0.01827, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.96227 
Train Epoch: 10 [601/1000 19232/32000 (60%)] Loss: 1.96784 (semantic_loss: 0.02134, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18869 
Train Epoch: 10 [606/1000 19392/32000 (61%)] Loss: 1.97138 (semantic_loss: 0.02487, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19085 
Train Epoch: 10 [611/1000 19552/32000 (61%)] Loss: 1.96677 (semantic_loss: 0.01928, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19291 
Train Epoch: 10 [616/1000 19712/32000 (62%)] Loss: 1.96405 (semantic_loss: 0.01754, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19717 
Train Epoch: 10 [621/1000 19872/32000 (62%)] Loss: 1.96869 (semantic_loss: 0.02121, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19273 
Train Epoch: 10 [626/1000 20032/32000 (63%)] Loss: 1.97024 (semantic_loss: 0.02275, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18901 
Train Epoch: 10 [631/1000 20192/32000 (63%)] Loss: 1.96561 (semantic_loss: 0.01910, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18536 
Train Epoch: 10 [636/1000 20352/32000 (64%)] Loss: 1.96629 (semantic_loss: 0.01881, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18607 
Train Epoch: 10 [641/1000 20512/32000 (64%)] Loss: 1.96753 (semantic_loss: 0.02102, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18613 
Train Epoch: 10 [646/1000 20672/32000 (65%)] Loss: 1.96841 (semantic_loss: 0.02190, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18873 
Train Epoch: 10 [651/1000 20832/32000 (65%)] Loss: 1.97535 (semantic_loss: 0.02786, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.18541 
Train Epoch: 10 [656/1000 20992/32000 (66%)] Loss: 1.96711 (semantic_loss: 0.01962, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.29606 
Train Epoch: 10 [661/1000 21152/32000 (66%)] Loss: 1.96735 (semantic_loss: 0.02181, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18384 
Train Epoch: 10 [666/1000 21312/32000 (67%)] Loss: 1.96890 (semantic_loss: 0.02044, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20063 
Train Epoch: 10 [671/1000 21472/32000 (67%)] Loss: 1.96604 (semantic_loss: 0.01953, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18966 
Train Epoch: 10 [676/1000 21632/32000 (68%)] Loss: 1.96875 (semantic_loss: 0.02127, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18793 
Train Epoch: 10 [681/1000 21792/32000 (68%)] Loss: 1.96912 (semantic_loss: 0.02262, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18705 
Train Epoch: 10 [686/1000 21952/32000 (69%)] Loss: 1.96648 (semantic_loss: 0.01999, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18946 
Train Epoch: 10 [691/1000 22112/32000 (69%)] Loss: 1.96644 (semantic_loss: 0.01896, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18649 
Train Epoch: 10 [696/1000 22272/32000 (70%)] Loss: 1.96747 (semantic_loss: 0.01999, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21020 
Train Epoch: 10 [701/1000 22432/32000 (70%)] Loss: 1.96961 (semantic_loss: 0.02213, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20915 
Train Epoch: 10 [706/1000 22592/32000 (71%)] Loss: 1.96408 (semantic_loss: 0.01659, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19597 
Train Epoch: 10 [711/1000 22752/32000 (71%)] Loss: 1.97007 (semantic_loss: 0.02356, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20123 
Train Epoch: 10 [716/1000 22912/32000 (72%)] Loss: 1.96480 (semantic_loss: 0.01731, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22421 
Train Epoch: 10 [721/1000 23072/32000 (72%)] Loss: 1.96949 (semantic_loss: 0.02298, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22390 
Train Epoch: 10 [726/1000 23232/32000 (73%)] Loss: 1.96519 (semantic_loss: 0.01869, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.57736 
Train Epoch: 10 [731/1000 23392/32000 (73%)] Loss: 1.96586 (semantic_loss: 0.01838, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19300 
Train Epoch: 10 [736/1000 23552/32000 (74%)] Loss: 1.96533 (semantic_loss: 0.01785, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19288 
Train Epoch: 10 [741/1000 23712/32000 (74%)] Loss: 1.96318 (semantic_loss: 0.01570, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21156 
Train Epoch: 10 [746/1000 23872/32000 (75%)] Loss: 1.96564 (semantic_loss: 0.02010, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18967 
Train Epoch: 10 [751/1000 24032/32000 (75%)] Loss: 1.96606 (semantic_loss: 0.02054, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18835 
Train Epoch: 10 [756/1000 24192/32000 (76%)] Loss: 1.96622 (semantic_loss: 0.01873, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19762 
Train Epoch: 10 [761/1000 24352/32000 (76%)] Loss: 1.96842 (semantic_loss: 0.02191, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18946 
Train Epoch: 10 [766/1000 24512/32000 (77%)] Loss: 1.96638 (semantic_loss: 0.01890, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19025 
Train Epoch: 10 [771/1000 24672/32000 (77%)] Loss: 1.96690 (semantic_loss: 0.02040, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24349 
Train Epoch: 10 [776/1000 24832/32000 (78%)] Loss: 1.96536 (semantic_loss: 0.01885, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.24329 
Train Epoch: 10 [781/1000 24992/32000 (78%)] Loss: 1.96508 (semantic_loss: 0.01759, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19642 
Train Epoch: 10 [786/1000 25152/32000 (79%)] Loss: 1.97069 (semantic_loss: 0.02321, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18993 
Train Epoch: 10 [791/1000 25312/32000 (79%)] Loss: 1.96816 (semantic_loss: 0.02165, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18469 
Train Epoch: 10 [796/1000 25472/32000 (80%)] Loss: 1.97795 (semantic_loss: 0.02947, quant_loss: 1.94824, bit_balance_loss: 0.00023) batch_time=0.18697 
Train Epoch: 10 [801/1000 25632/32000 (80%)] Loss: 1.96820 (semantic_loss: 0.02071, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18715 
Train Epoch: 10 [806/1000 25792/32000 (81%)] Loss: 1.96342 (semantic_loss: 0.01790, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18849 
Train Epoch: 10 [811/1000 25952/32000 (81%)] Loss: 1.96941 (semantic_loss: 0.02291, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18684 
Train Epoch: 10 [816/1000 26112/32000 (82%)] Loss: 1.96539 (semantic_loss: 0.01889, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19046 
Train Epoch: 10 [821/1000 26272/32000 (82%)] Loss: 1.96740 (semantic_loss: 0.01992, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18817 
Train Epoch: 10 [826/1000 26432/32000 (83%)] Loss: 1.96383 (semantic_loss: 0.01635, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18636 
Train Epoch: 10 [831/1000 26592/32000 (83%)] Loss: 1.96452 (semantic_loss: 0.01899, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18874 
Train Epoch: 10 [836/1000 26752/32000 (84%)] Loss: 1.96837 (semantic_loss: 0.02089, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19240 
Train Epoch: 10 [841/1000 26912/32000 (84%)] Loss: 1.96745 (semantic_loss: 0.01997, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19073 
Train Epoch: 10 [846/1000 27072/32000 (85%)] Loss: 1.97222 (semantic_loss: 0.02376, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20519 
Train Epoch: 10 [851/1000 27232/32000 (85%)] Loss: 1.96814 (semantic_loss: 0.02164, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18827 
Train Epoch: 10 [856/1000 27392/32000 (86%)] Loss: 1.96548 (semantic_loss: 0.01996, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19870 
Train Epoch: 10 [861/1000 27552/32000 (86%)] Loss: 1.96638 (semantic_loss: 0.01890, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20502 
Train Epoch: 10 [866/1000 27712/32000 (87%)] Loss: 1.97822 (semantic_loss: 0.02975, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20530 
Train Epoch: 10 [871/1000 27872/32000 (87%)] Loss: 1.96767 (semantic_loss: 0.02019, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22289 
Train Epoch: 10 [876/1000 28032/32000 (88%)] Loss: 1.96420 (semantic_loss: 0.01770, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23669 
Train Epoch: 10 [881/1000 28192/32000 (88%)] Loss: 1.96266 (semantic_loss: 0.01616, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20287 
Train Epoch: 10 [886/1000 28352/32000 (89%)] Loss: 1.96920 (semantic_loss: 0.02075, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20856 
Train Epoch: 10 [891/1000 28512/32000 (89%)] Loss: 1.96909 (semantic_loss: 0.02258, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20632 
Train Epoch: 10 [896/1000 28672/32000 (90%)] Loss: 1.96876 (semantic_loss: 0.02225, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19690 
Train Epoch: 10 [901/1000 28832/32000 (90%)] Loss: 1.96643 (semantic_loss: 0.01992, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19134 
Train Epoch: 10 [906/1000 28992/32000 (91%)] Loss: 1.96643 (semantic_loss: 0.01993, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18728 
Train Epoch: 10 [911/1000 29152/32000 (91%)] Loss: 1.96927 (semantic_loss: 0.02081, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22969 
Train Epoch: 10 [916/1000 29312/32000 (92%)] Loss: 1.96752 (semantic_loss: 0.02102, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.94644 
Train Epoch: 10 [921/1000 29472/32000 (92%)] Loss: 1.96417 (semantic_loss: 0.01668, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18641 
Train Epoch: 10 [926/1000 29632/32000 (93%)] Loss: 1.96915 (semantic_loss: 0.02166, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19263 
Train Epoch: 10 [931/1000 29792/32000 (93%)] Loss: 1.96461 (semantic_loss: 0.01713, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21412 
Train Epoch: 10 [936/1000 29952/32000 (94%)] Loss: 1.97291 (semantic_loss: 0.02543, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21147 
Train Epoch: 10 [941/1000 30112/32000 (94%)] Loss: 1.97211 (semantic_loss: 0.02560, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18966 
Train Epoch: 10 [946/1000 30272/32000 (95%)] Loss: 1.96587 (semantic_loss: 0.01936, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18901 
Train Epoch: 10 [951/1000 30432/32000 (95%)] Loss: 1.96739 (semantic_loss: 0.01990, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18483 
Train Epoch: 10 [956/1000 30592/32000 (96%)] Loss: 1.96981 (semantic_loss: 0.02234, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18728 
Train Epoch: 10 [961/1000 30752/32000 (96%)] Loss: 1.96863 (semantic_loss: 0.02114, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18909 
Train Epoch: 10 [966/1000 30912/32000 (97%)] Loss: 1.97055 (semantic_loss: 0.02209, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18844 
Train Epoch: 10 [971/1000 31072/32000 (97%)] Loss: 1.96629 (semantic_loss: 0.01880, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18723 
Train Epoch: 10 [976/1000 31232/32000 (98%)] Loss: 1.97010 (semantic_loss: 0.02360, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.30332 
Train Epoch: 10 [981/1000 31392/32000 (98%)] Loss: 1.96804 (semantic_loss: 0.02055, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18529 
Train Epoch: 10 [986/1000 31552/32000 (99%)] Loss: 1.96800 (semantic_loss: 0.02052, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18715 
Train Epoch: 10 [991/1000 31712/32000 (99%)] Loss: 1.96930 (semantic_loss: 0.02182, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18598 
Train Epoch: 10 [996/1000 31872/32000 (100%)] Loss: 1.96755 (semantic_loss: 0.01910, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18791 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch10.pth ...
Done in 4.423s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch10.pth ...
Done in 8.833s
removing stale ckpt [epoch 9] [took 0.00s]
 epoch          : 10
 loss           : 1.9677003294229507
 learning_rate  : 1.937102445000001e-05
 n_samples      : 320000
 n_steps        : 10000
 ActivityNet_val1_test/t2v_metrics/R1: 10.941631075859263
 ActivityNet_val1_test/t2v_metrics/R5: 35.163717714053284
 ActivityNet_val1_test/t2v_metrics/R10: 52.89810860280659
 ActivityNet_val1_test/t2v_metrics/R50: 86.73988204189547
 ActivityNet_val1_test/t2v_metrics/MedR: 10.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 45.91681919869839
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 27.302704294022565
 ActivityNet_val1_test/v2t_metrics/R1: 10.616229408175716
 ActivityNet_val1_test/v2t_metrics/R5: 36.97376449054301
 ActivityNet_val1_test/v2t_metrics/R10: 53.04047183241814
 ActivityNet_val1_test/v2t_metrics/R50: 86.35346756152126
 ActivityNet_val1_test/v2t_metrics/MedR: 9.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 48.84970510473866
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 27.509990982091537
 mnt_best       : 27.302704294022565
 not_improved_count: 0
Train Epoch: 11 [1/1000 32/32000 (0%)] Loss: 1.96491 (semantic_loss: 0.01841, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=22.94259 
Train Epoch: 11 [6/1000 192/32000 (1%)] Loss: 1.96823 (semantic_loss: 0.01977, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.46325 
Train Epoch: 11 [11/1000 352/32000 (1%)] Loss: 1.97034 (semantic_loss: 0.02188, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19221 
Train Epoch: 11 [16/1000 512/32000 (2%)] Loss: 1.96709 (semantic_loss: 0.01864, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.23839 
Train Epoch: 11 [21/1000 672/32000 (2%)] Loss: 1.96341 (semantic_loss: 0.01593, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18675 
Train Epoch: 11 [26/1000 832/32000 (3%)] Loss: 1.97166 (semantic_loss: 0.02417, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.23459 
Train Epoch: 11 [31/1000 992/32000 (3%)] Loss: 1.97115 (semantic_loss: 0.02171, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.86521 
Train Epoch: 11 [36/1000 1152/32000 (4%)] Loss: 1.96431 (semantic_loss: 0.01780, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20443 
Train Epoch: 11 [41/1000 1312/32000 (4%)] Loss: 1.96797 (semantic_loss: 0.02147, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18949 
Train Epoch: 11 [46/1000 1472/32000 (5%)] Loss: 1.96899 (semantic_loss: 0.02150, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22832 
Train Epoch: 11 [51/1000 1632/32000 (5%)] Loss: 1.96554 (semantic_loss: 0.01708, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19338 
Train Epoch: 11 [56/1000 1792/32000 (6%)] Loss: 1.96646 (semantic_loss: 0.01995, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19087 
Train Epoch: 11 [61/1000 1952/32000 (6%)] Loss: 1.96999 (semantic_loss: 0.02251, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18893 
Train Epoch: 11 [66/1000 2112/32000 (7%)] Loss: 1.96440 (semantic_loss: 0.01790, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21740 
Train Epoch: 11 [71/1000 2272/32000 (7%)] Loss: 1.96882 (semantic_loss: 0.02231, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18710 
Train Epoch: 11 [76/1000 2432/32000 (8%)] Loss: 1.96964 (semantic_loss: 0.02313, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18946 
Train Epoch: 11 [81/1000 2592/32000 (8%)] Loss: 1.96933 (semantic_loss: 0.02184, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.38585 
Train Epoch: 11 [86/1000 2752/32000 (9%)] Loss: 1.96387 (semantic_loss: 0.01736, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20601 
Train Epoch: 11 [91/1000 2912/32000 (9%)] Loss: 1.96700 (semantic_loss: 0.01952, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19261 
Train Epoch: 11 [96/1000 3072/32000 (10%)] Loss: 1.96528 (semantic_loss: 0.01878, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18552 
Train Epoch: 11 [101/1000 3232/32000 (10%)] Loss: 1.96917 (semantic_loss: 0.02169, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20228 
Train Epoch: 11 [106/1000 3392/32000 (11%)] Loss: 1.96914 (semantic_loss: 0.02166, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18757 
Train Epoch: 11 [111/1000 3552/32000 (11%)] Loss: 1.96828 (semantic_loss: 0.02079, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18836 
Train Epoch: 11 [116/1000 3712/32000 (12%)] Loss: 1.96564 (semantic_loss: 0.01816, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18636 
Train Epoch: 11 [121/1000 3872/32000 (12%)] Loss: 1.96755 (semantic_loss: 0.02104, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18716 
Train Epoch: 11 [126/1000 4032/32000 (13%)] Loss: 1.96393 (semantic_loss: 0.01548, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18396 
Train Epoch: 11 [131/1000 4192/32000 (13%)] Loss: 1.96564 (semantic_loss: 0.01816, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23002 
Train Epoch: 11 [136/1000 4352/32000 (14%)] Loss: 1.96702 (semantic_loss: 0.02051, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21896 
Train Epoch: 11 [141/1000 4512/32000 (14%)] Loss: 1.96798 (semantic_loss: 0.02051, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21451 
Train Epoch: 11 [146/1000 4672/32000 (15%)] Loss: 1.96608 (semantic_loss: 0.01860, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20941 
Train Epoch: 11 [151/1000 4832/32000 (15%)] Loss: 1.97046 (semantic_loss: 0.02297, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21252 
Train Epoch: 11 [156/1000 4992/32000 (16%)] Loss: 1.97377 (semantic_loss: 0.02629, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19752 
Train Epoch: 11 [161/1000 5152/32000 (16%)] Loss: 1.96358 (semantic_loss: 0.01805, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20370 
Train Epoch: 11 [166/1000 5312/32000 (17%)] Loss: 1.97387 (semantic_loss: 0.02541, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19133 
Train Epoch: 11 [171/1000 5472/32000 (17%)] Loss: 1.96504 (semantic_loss: 0.01756, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18688 
Train Epoch: 11 [176/1000 5632/32000 (18%)] Loss: 1.96850 (semantic_loss: 0.02003, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19062 
Train Epoch: 11 [181/1000 5792/32000 (18%)] Loss: 1.96529 (semantic_loss: 0.01781, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18537 
Train Epoch: 11 [186/1000 5952/32000 (19%)] Loss: 1.96823 (semantic_loss: 0.02172, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19622 
Train Epoch: 11 [191/1000 6112/32000 (19%)] Loss: 1.96964 (semantic_loss: 0.02314, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22634 
Train Epoch: 11 [196/1000 6272/32000 (20%)] Loss: 1.96231 (semantic_loss: 0.01579, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.26876 
Train Epoch: 11 [201/1000 6432/32000 (20%)] Loss: 1.96873 (semantic_loss: 0.02222, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19486 
Train Epoch: 11 [206/1000 6592/32000 (21%)] Loss: 1.96581 (semantic_loss: 0.01834, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21285 
Train Epoch: 11 [211/1000 6752/32000 (21%)] Loss: 1.96624 (semantic_loss: 0.01973, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18822 
Train Epoch: 11 [216/1000 6912/32000 (22%)] Loss: 1.97081 (semantic_loss: 0.02333, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18898 
Train Epoch: 11 [221/1000 7072/32000 (22%)] Loss: 1.96512 (semantic_loss: 0.01764, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18675 
Train Epoch: 11 [226/1000 7232/32000 (23%)] Loss: 1.96882 (semantic_loss: 0.02135, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18734 
Train Epoch: 11 [231/1000 7392/32000 (23%)] Loss: 1.96717 (semantic_loss: 0.01970, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18807 
Train Epoch: 11 [236/1000 7552/32000 (24%)] Loss: 1.96455 (semantic_loss: 0.01707, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19511 
Train Epoch: 11 [241/1000 7712/32000 (24%)] Loss: 1.97194 (semantic_loss: 0.02349, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18497 
Train Epoch: 11 [246/1000 7872/32000 (25%)] Loss: 1.96720 (semantic_loss: 0.01972, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18910 
Train Epoch: 11 [251/1000 8032/32000 (25%)] Loss: 1.96490 (semantic_loss: 0.01937, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18687 
Train Epoch: 11 [256/1000 8192/32000 (26%)] Loss: 1.96848 (semantic_loss: 0.02100, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18662 
Train Epoch: 11 [261/1000 8352/32000 (26%)] Loss: 1.96609 (semantic_loss: 0.01764, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20122 
Train Epoch: 11 [266/1000 8512/32000 (27%)] Loss: 1.97203 (semantic_loss: 0.02357, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18922 
Train Epoch: 11 [271/1000 8672/32000 (27%)] Loss: 1.96963 (semantic_loss: 0.02215, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18910 
Train Epoch: 11 [276/1000 8832/32000 (28%)] Loss: 1.96618 (semantic_loss: 0.01967, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18987 
Train Epoch: 11 [281/1000 8992/32000 (28%)] Loss: 1.96310 (semantic_loss: 0.01660, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21028 
Train Epoch: 11 [286/1000 9152/32000 (29%)] Loss: 1.97101 (semantic_loss: 0.02353, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20866 
Train Epoch: 11 [291/1000 9312/32000 (29%)] Loss: 1.97174 (semantic_loss: 0.02425, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20718 
Train Epoch: 11 [296/1000 9472/32000 (30%)] Loss: 1.96563 (semantic_loss: 0.01814, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=1.58946 
Train Epoch: 11 [301/1000 9632/32000 (30%)] Loss: 1.97141 (semantic_loss: 0.02393, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20996 
Train Epoch: 11 [306/1000 9792/32000 (31%)] Loss: 1.96635 (semantic_loss: 0.01984, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20475 
Train Epoch: 11 [311/1000 9952/32000 (31%)] Loss: 1.97397 (semantic_loss: 0.02746, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19669 
Train Epoch: 11 [316/1000 10112/32000 (32%)] Loss: 1.96381 (semantic_loss: 0.01633, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18841 
Train Epoch: 11 [321/1000 10272/32000 (32%)] Loss: 1.96553 (semantic_loss: 0.02000, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18657 
Train Epoch: 11 [326/1000 10432/32000 (33%)] Loss: 1.96554 (semantic_loss: 0.01806, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18889 
Train Epoch: 11 [331/1000 10592/32000 (33%)] Loss: 1.97452 (semantic_loss: 0.02704, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19094 
Train Epoch: 11 [336/1000 10752/32000 (34%)] Loss: 1.97587 (semantic_loss: 0.02936, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19679 
Train Epoch: 11 [341/1000 10912/32000 (34%)] Loss: 1.96698 (semantic_loss: 0.01949, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20713 
Train Epoch: 11 [346/1000 11072/32000 (35%)] Loss: 1.96368 (semantic_loss: 0.01816, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19268 
Train Epoch: 11 [351/1000 11232/32000 (35%)] Loss: 1.96835 (semantic_loss: 0.02087, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.35470 
Train Epoch: 11 [356/1000 11392/32000 (36%)] Loss: 1.96798 (semantic_loss: 0.02148, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18915 
Train Epoch: 11 [361/1000 11552/32000 (36%)] Loss: 1.96791 (semantic_loss: 0.02043, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18406 
Train Epoch: 11 [366/1000 11712/32000 (37%)] Loss: 1.96649 (semantic_loss: 0.01999, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18776 
Train Epoch: 11 [371/1000 11872/32000 (37%)] Loss: 1.96461 (semantic_loss: 0.01811, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18637 
Train Epoch: 11 [376/1000 12032/32000 (38%)] Loss: 1.97012 (semantic_loss: 0.02361, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18664 
Train Epoch: 11 [381/1000 12192/32000 (38%)] Loss: 1.96559 (semantic_loss: 0.01908, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18724 
Train Epoch: 11 [386/1000 12352/32000 (39%)] Loss: 1.96569 (semantic_loss: 0.01822, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20783 
Train Epoch: 11 [391/1000 12512/32000 (39%)] Loss: 1.96669 (semantic_loss: 0.02020, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20830 
Train Epoch: 11 [396/1000 12672/32000 (40%)] Loss: 1.96926 (semantic_loss: 0.02178, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18606 
Train Epoch: 11 [401/1000 12832/32000 (40%)] Loss: 1.96399 (semantic_loss: 0.01749, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18596 
Train Epoch: 11 [406/1000 12992/32000 (41%)] Loss: 1.96387 (semantic_loss: 0.01835, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19153 
Train Epoch: 11 [411/1000 13152/32000 (41%)] Loss: 1.97158 (semantic_loss: 0.02410, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26205 
Train Epoch: 11 [416/1000 13312/32000 (42%)] Loss: 1.96267 (semantic_loss: 0.01617, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18715 
Train Epoch: 11 [421/1000 13472/32000 (42%)] Loss: 1.96821 (semantic_loss: 0.02170, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20607 
Train Epoch: 11 [426/1000 13632/32000 (43%)] Loss: 1.96615 (semantic_loss: 0.02062, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18886 
Train Epoch: 11 [431/1000 13792/32000 (43%)] Loss: 1.96759 (semantic_loss: 0.01913, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21868 
Train Epoch: 11 [436/1000 13952/32000 (44%)] Loss: 1.96532 (semantic_loss: 0.01881, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21210 
Train Epoch: 11 [441/1000 14112/32000 (44%)] Loss: 1.96926 (semantic_loss: 0.02178, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22082 
Train Epoch: 11 [446/1000 14272/32000 (45%)] Loss: 1.96745 (semantic_loss: 0.02094, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20446 
Train Epoch: 11 [451/1000 14432/32000 (45%)] Loss: 1.96538 (semantic_loss: 0.01986, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.21055 
Train Epoch: 11 [456/1000 14592/32000 (46%)] Loss: 1.96432 (semantic_loss: 0.01782, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20562 
Train Epoch: 11 [461/1000 14752/32000 (46%)] Loss: 1.96963 (semantic_loss: 0.02215, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20461 
Train Epoch: 11 [466/1000 14912/32000 (47%)] Loss: 1.96882 (semantic_loss: 0.02036, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19306 
Train Epoch: 11 [471/1000 15072/32000 (47%)] Loss: 1.96830 (semantic_loss: 0.01985, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21207 
Train Epoch: 11 [476/1000 15232/32000 (48%)] Loss: 1.96622 (semantic_loss: 0.01776, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19042 
Train Epoch: 11 [481/1000 15392/32000 (48%)] Loss: 1.97171 (semantic_loss: 0.02423, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18977 
Train Epoch: 11 [486/1000 15552/32000 (49%)] Loss: 1.97024 (semantic_loss: 0.02276, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19247 
Train Epoch: 11 [491/1000 15712/32000 (49%)] Loss: 1.96466 (semantic_loss: 0.01621, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18719 
Train Epoch: 11 [496/1000 15872/32000 (50%)] Loss: 1.96826 (semantic_loss: 0.02175, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19159 
Train Epoch: 11 [501/1000 16032/32000 (50%)] Loss: 1.96697 (semantic_loss: 0.02047, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19477 
Train Epoch: 11 [506/1000 16192/32000 (51%)] Loss: 1.96522 (semantic_loss: 0.01774, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19415 
Train Epoch: 11 [511/1000 16352/32000 (51%)] Loss: 1.96600 (semantic_loss: 0.02144, quant_loss: 1.94434, bit_balance_loss: 0.00023) batch_time=0.19198 
Train Epoch: 11 [516/1000 16512/32000 (52%)] Loss: 1.96636 (semantic_loss: 0.01985, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18676 
Train Epoch: 11 [521/1000 16672/32000 (52%)] Loss: 1.96580 (semantic_loss: 0.01930, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18361 
Train Epoch: 11 [526/1000 16832/32000 (53%)] Loss: 1.96370 (semantic_loss: 0.01623, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18810 
Train Epoch: 11 [531/1000 16992/32000 (53%)] Loss: 1.97343 (semantic_loss: 0.02595, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18367 
Train Epoch: 11 [536/1000 17152/32000 (54%)] Loss: 1.96707 (semantic_loss: 0.01861, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18410 
Train Epoch: 11 [541/1000 17312/32000 (54%)] Loss: 1.96670 (semantic_loss: 0.02020, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20644 
Train Epoch: 11 [546/1000 17472/32000 (55%)] Loss: 1.96433 (semantic_loss: 0.01587, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18489 
Train Epoch: 11 [551/1000 17632/32000 (55%)] Loss: 1.96681 (semantic_loss: 0.01932, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.18749 
Train Epoch: 11 [556/1000 17792/32000 (56%)] Loss: 1.96278 (semantic_loss: 0.01530, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20563 
Train Epoch: 11 [561/1000 17952/32000 (56%)] Loss: 1.96778 (semantic_loss: 0.02029, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18905 
Train Epoch: 11 [566/1000 18112/32000 (57%)] Loss: 1.96705 (semantic_loss: 0.02151, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18775 
Train Epoch: 11 [571/1000 18272/32000 (57%)] Loss: 1.96714 (semantic_loss: 0.01868, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18684 
Train Epoch: 11 [576/1000 18432/32000 (58%)] Loss: 1.96902 (semantic_loss: 0.02057, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21046 
Train Epoch: 11 [581/1000 18592/32000 (58%)] Loss: 1.96732 (semantic_loss: 0.02081, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20919 
Train Epoch: 11 [586/1000 18752/32000 (59%)] Loss: 1.96682 (semantic_loss: 0.01934, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20923 
Train Epoch: 11 [591/1000 18912/32000 (59%)] Loss: 1.96436 (semantic_loss: 0.01687, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.21950 
Train Epoch: 11 [596/1000 19072/32000 (60%)] Loss: 1.96270 (semantic_loss: 0.01620, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22476 
Train Epoch: 11 [601/1000 19232/32000 (60%)] Loss: 1.96519 (semantic_loss: 0.01869, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22412 
Train Epoch: 11 [606/1000 19392/32000 (61%)] Loss: 1.96429 (semantic_loss: 0.01778, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22210 
Train Epoch: 11 [611/1000 19552/32000 (61%)] Loss: 1.96780 (semantic_loss: 0.02032, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22508 
Train Epoch: 11 [616/1000 19712/32000 (62%)] Loss: 1.96471 (semantic_loss: 0.01821, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.92806 
Train Epoch: 11 [621/1000 19872/32000 (62%)] Loss: 1.96565 (semantic_loss: 0.01915, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19819 
Train Epoch: 11 [626/1000 20032/32000 (63%)] Loss: 1.96348 (semantic_loss: 0.01600, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21754 
Train Epoch: 11 [631/1000 20192/32000 (63%)] Loss: 1.96705 (semantic_loss: 0.01957, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19461 
Train Epoch: 11 [636/1000 20352/32000 (64%)] Loss: 1.96883 (semantic_loss: 0.02331, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18692 
Train Epoch: 11 [641/1000 20512/32000 (64%)] Loss: 1.96643 (semantic_loss: 0.01895, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18636 
Train Epoch: 11 [646/1000 20672/32000 (65%)] Loss: 1.96922 (semantic_loss: 0.02271, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18724 
Train Epoch: 11 [651/1000 20832/32000 (65%)] Loss: 1.96600 (semantic_loss: 0.01852, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18787 
Train Epoch: 11 [656/1000 20992/32000 (66%)] Loss: 1.97122 (semantic_loss: 0.02374, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19014 
Train Epoch: 11 [661/1000 21152/32000 (66%)] Loss: 1.96794 (semantic_loss: 0.01949, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19338 
Train Epoch: 11 [666/1000 21312/32000 (67%)] Loss: 1.96890 (semantic_loss: 0.01947, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.21896 
Train Epoch: 11 [671/1000 21472/32000 (67%)] Loss: 1.96863 (semantic_loss: 0.02115, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.66673 
Train Epoch: 11 [676/1000 21632/32000 (68%)] Loss: 1.96552 (semantic_loss: 0.01804, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18099 
Train Epoch: 11 [681/1000 21792/32000 (68%)] Loss: 1.96475 (semantic_loss: 0.01727, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18566 
Train Epoch: 11 [686/1000 21952/32000 (69%)] Loss: 1.96688 (semantic_loss: 0.02037, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18922 
Train Epoch: 11 [691/1000 22112/32000 (69%)] Loss: 1.96625 (semantic_loss: 0.01877, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18957 
Train Epoch: 11 [696/1000 22272/32000 (70%)] Loss: 1.96697 (semantic_loss: 0.01950, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18827 
Train Epoch: 11 [701/1000 22432/32000 (70%)] Loss: 1.96620 (semantic_loss: 0.01969, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20178 
Train Epoch: 11 [706/1000 22592/32000 (71%)] Loss: 1.96962 (semantic_loss: 0.02117, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18788 
Train Epoch: 11 [711/1000 22752/32000 (71%)] Loss: 1.96467 (semantic_loss: 0.01816, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18749 
Train Epoch: 11 [716/1000 22912/32000 (72%)] Loss: 1.96382 (semantic_loss: 0.01731, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18668 
Train Epoch: 11 [721/1000 23072/32000 (72%)] Loss: 1.96532 (semantic_loss: 0.01783, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18723 
Train Epoch: 11 [726/1000 23232/32000 (73%)] Loss: 1.96722 (semantic_loss: 0.01974, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18768 
Train Epoch: 11 [731/1000 23392/32000 (73%)] Loss: 1.96473 (semantic_loss: 0.01628, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18791 
Train Epoch: 11 [736/1000 23552/32000 (74%)] Loss: 1.96610 (semantic_loss: 0.01862, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18456 
Train Epoch: 11 [741/1000 23712/32000 (74%)] Loss: 1.96549 (semantic_loss: 0.01801, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20215 
Train Epoch: 11 [746/1000 23872/32000 (75%)] Loss: 1.96439 (semantic_loss: 0.01690, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21790 
Train Epoch: 11 [751/1000 24032/32000 (75%)] Loss: 1.96660 (semantic_loss: 0.02010, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20647 
Train Epoch: 11 [756/1000 24192/32000 (76%)] Loss: 1.96639 (semantic_loss: 0.01891, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20908 
Train Epoch: 11 [761/1000 24352/32000 (76%)] Loss: 1.96614 (semantic_loss: 0.01865, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20717 
Train Epoch: 11 [766/1000 24512/32000 (77%)] Loss: 1.96500 (semantic_loss: 0.01752, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20089 
Train Epoch: 11 [771/1000 24672/32000 (77%)] Loss: 1.96600 (semantic_loss: 0.01852, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19689 
Train Epoch: 11 [776/1000 24832/32000 (78%)] Loss: 1.96230 (semantic_loss: 0.01482, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19920 
Train Epoch: 11 [781/1000 24992/32000 (78%)] Loss: 1.96791 (semantic_loss: 0.02140, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20563 
Train Epoch: 11 [786/1000 25152/32000 (79%)] Loss: 1.96733 (semantic_loss: 0.01985, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19296 
Train Epoch: 11 [791/1000 25312/32000 (79%)] Loss: 1.96508 (semantic_loss: 0.01759, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18790 
Train Epoch: 11 [796/1000 25472/32000 (80%)] Loss: 1.96344 (semantic_loss: 0.01693, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19674 
Train Epoch: 11 [801/1000 25632/32000 (80%)] Loss: 1.96884 (semantic_loss: 0.02136, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18830 
Train Epoch: 11 [806/1000 25792/32000 (81%)] Loss: 1.96976 (semantic_loss: 0.02228, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18657 
Train Epoch: 11 [811/1000 25952/32000 (81%)] Loss: 1.96664 (semantic_loss: 0.02111, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.19276 
Train Epoch: 11 [816/1000 26112/32000 (82%)] Loss: 1.96818 (semantic_loss: 0.02168, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19429 
Train Epoch: 11 [821/1000 26272/32000 (82%)] Loss: 1.96261 (semantic_loss: 0.01708, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19867 
Train Epoch: 11 [826/1000 26432/32000 (83%)] Loss: 1.96234 (semantic_loss: 0.01583, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18841 
Train Epoch: 11 [831/1000 26592/32000 (83%)] Loss: 1.96715 (semantic_loss: 0.02065, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18610 
Train Epoch: 11 [836/1000 26752/32000 (84%)] Loss: 1.96586 (semantic_loss: 0.01936, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18577 
Train Epoch: 11 [841/1000 26912/32000 (84%)] Loss: 1.96509 (semantic_loss: 0.01761, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18523 
Train Epoch: 11 [846/1000 27072/32000 (85%)] Loss: 1.96713 (semantic_loss: 0.01868, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19212 
Train Epoch: 11 [851/1000 27232/32000 (85%)] Loss: 1.97132 (semantic_loss: 0.02482, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18815 
Train Epoch: 11 [856/1000 27392/32000 (86%)] Loss: 1.96378 (semantic_loss: 0.01727, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18951 
Train Epoch: 11 [861/1000 27552/32000 (86%)] Loss: 1.96373 (semantic_loss: 0.01625, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18980 
Train Epoch: 11 [866/1000 27712/32000 (87%)] Loss: 1.96640 (semantic_loss: 0.01794, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19691 
Train Epoch: 11 [871/1000 27872/32000 (87%)] Loss: 1.96645 (semantic_loss: 0.01995, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20702 
Train Epoch: 11 [876/1000 28032/32000 (88%)] Loss: 1.96525 (semantic_loss: 0.01679, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.21228 
Train Epoch: 11 [881/1000 28192/32000 (88%)] Loss: 1.96441 (semantic_loss: 0.01693, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19309 
Train Epoch: 11 [886/1000 28352/32000 (89%)] Loss: 1.96932 (semantic_loss: 0.02184, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18825 
Train Epoch: 11 [891/1000 28512/32000 (89%)] Loss: 1.96632 (semantic_loss: 0.01884, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19150 
Train Epoch: 11 [896/1000 28672/32000 (90%)] Loss: 1.96596 (semantic_loss: 0.01945, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19049 
Train Epoch: 11 [901/1000 28832/32000 (90%)] Loss: 1.96607 (semantic_loss: 0.01859, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18811 
Train Epoch: 11 [906/1000 28992/32000 (91%)] Loss: 1.96583 (semantic_loss: 0.01835, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21020 
Train Epoch: 11 [911/1000 29152/32000 (91%)] Loss: 1.97134 (semantic_loss: 0.02288, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.21334 
Train Epoch: 11 [916/1000 29312/32000 (92%)] Loss: 1.96686 (semantic_loss: 0.01938, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21547 
Train Epoch: 11 [921/1000 29472/32000 (92%)] Loss: 1.96489 (semantic_loss: 0.01838, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19820 
Train Epoch: 11 [926/1000 29632/32000 (93%)] Loss: 1.96656 (semantic_loss: 0.01907, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19534 
Train Epoch: 11 [931/1000 29792/32000 (93%)] Loss: 1.96536 (semantic_loss: 0.01886, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21560 
Train Epoch: 11 [936/1000 29952/32000 (94%)] Loss: 1.96920 (semantic_loss: 0.02367, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=1.08990 
Train Epoch: 11 [941/1000 30112/32000 (94%)] Loss: 1.96858 (semantic_loss: 0.02109, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.19398 
Train Epoch: 11 [946/1000 30272/32000 (95%)] Loss: 1.97001 (semantic_loss: 0.02252, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18797 
Train Epoch: 11 [951/1000 30432/32000 (95%)] Loss: 1.97382 (semantic_loss: 0.02633, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18619 
Train Epoch: 11 [956/1000 30592/32000 (96%)] Loss: 1.96945 (semantic_loss: 0.02197, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19282 
Train Epoch: 11 [961/1000 30752/32000 (96%)] Loss: 1.96815 (semantic_loss: 0.02165, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20011 
Train Epoch: 11 [966/1000 30912/32000 (97%)] Loss: 1.96994 (semantic_loss: 0.02246, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18863 
Train Epoch: 11 [971/1000 31072/32000 (97%)] Loss: 1.96872 (semantic_loss: 0.02125, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19856 
Train Epoch: 11 [976/1000 31232/32000 (98%)] Loss: 1.96929 (semantic_loss: 0.02181, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19209 
Train Epoch: 11 [981/1000 31392/32000 (98%)] Loss: 1.96552 (semantic_loss: 0.01901, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20588 
Train Epoch: 11 [986/1000 31552/32000 (99%)] Loss: 1.97000 (semantic_loss: 0.02252, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20088 
Train Epoch: 11 [991/1000 31712/32000 (99%)] Loss: 1.96744 (semantic_loss: 0.01801, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.61077 
Train Epoch: 11 [996/1000 31872/32000 (100%)] Loss: 1.96700 (semantic_loss: 0.01952, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18534 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch11.pth ...
Done in 6.553s
removing stale ckpt [epoch 10] [took 0.00s]
 epoch          : 11
 loss           : 1.966845381140709
 learning_rate  : 1.7433922005000008e-05
 n_samples      : 352000
 n_steps        : 11000
 ActivityNet_val1_test/t2v_metrics/R1: 9.619686800894854
 ActivityNet_val1_test/t2v_metrics/R5: 35.265405735204396
 ActivityNet_val1_test/t2v_metrics/R10: 52.69473256050437
 ActivityNet_val1_test/t2v_metrics/R50: 86.43481797844214
 ActivityNet_val1_test/t2v_metrics/MedR: 10.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 47.10351840553183
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 26.147229210781173
 ActivityNet_val1_test/v2t_metrics/R1: 10.53487899125483
 ActivityNet_val1_test/v2t_metrics/R5: 35.753508236729715
 ActivityNet_val1_test/v2t_metrics/R10: 53.365873500101685
 ActivityNet_val1_test/v2t_metrics/R50: 86.41448037421192
 ActivityNet_val1_test/v2t_metrics/MedR: 9.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 49.7061216188733
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 27.189670553127392
 mnt_best       : 27.302704294022565
 not_improved_count: 1
Train Epoch: 12 [1/1000 32/32000 (0%)] Loss: 1.96948 (semantic_loss: 0.02200, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=21.74674 
Train Epoch: 12 [6/1000 192/32000 (1%)] Loss: 1.96439 (semantic_loss: 0.01789, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18815 
Train Epoch: 12 [11/1000 352/32000 (1%)] Loss: 1.96489 (semantic_loss: 0.01740, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.21464 
Train Epoch: 12 [16/1000 512/32000 (2%)] Loss: 1.96618 (semantic_loss: 0.01968, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23041 
Train Epoch: 12 [21/1000 672/32000 (2%)] Loss: 1.96660 (semantic_loss: 0.02010, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19189 
Train Epoch: 12 [26/1000 832/32000 (3%)] Loss: 1.96504 (semantic_loss: 0.01755, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20454 
Train Epoch: 12 [31/1000 992/32000 (3%)] Loss: 1.96540 (semantic_loss: 0.01792, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.55009 
Train Epoch: 12 [36/1000 1152/32000 (4%)] Loss: 1.96438 (semantic_loss: 0.01788, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22223 
Train Epoch: 12 [41/1000 1312/32000 (4%)] Loss: 1.96460 (semantic_loss: 0.01712, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22760 
Train Epoch: 12 [46/1000 1472/32000 (5%)] Loss: 1.96441 (semantic_loss: 0.01693, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21665 
Train Epoch: 12 [51/1000 1632/32000 (5%)] Loss: 1.96575 (semantic_loss: 0.01827, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.23024 
Train Epoch: 12 [56/1000 1792/32000 (6%)] Loss: 1.96853 (semantic_loss: 0.02106, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22333 
Train Epoch: 12 [61/1000 1952/32000 (6%)] Loss: 1.96633 (semantic_loss: 0.01982, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21233 
Train Epoch: 12 [66/1000 2112/32000 (7%)] Loss: 1.96476 (semantic_loss: 0.01826, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19186 
Train Epoch: 12 [71/1000 2272/32000 (7%)] Loss: 1.96298 (semantic_loss: 0.01648, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18692 
Train Epoch: 12 [76/1000 2432/32000 (8%)] Loss: 1.96618 (semantic_loss: 0.01870, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21242 
Train Epoch: 12 [81/1000 2592/32000 (8%)] Loss: 1.97016 (semantic_loss: 0.02170, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.67431 
Train Epoch: 12 [86/1000 2752/32000 (9%)] Loss: 1.96253 (semantic_loss: 0.01603, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19122 
Train Epoch: 12 [91/1000 2912/32000 (9%)] Loss: 1.96545 (semantic_loss: 0.01699, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.21020 
Train Epoch: 12 [96/1000 3072/32000 (10%)] Loss: 1.96798 (semantic_loss: 0.02050, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19134 
Train Epoch: 12 [101/1000 3232/32000 (10%)] Loss: 1.96579 (semantic_loss: 0.01929, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19433 
Train Epoch: 12 [106/1000 3392/32000 (11%)] Loss: 1.96733 (semantic_loss: 0.01888, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18866 
Train Epoch: 12 [111/1000 3552/32000 (11%)] Loss: 1.96834 (semantic_loss: 0.02183, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22531 
Train Epoch: 12 [116/1000 3712/32000 (12%)] Loss: 1.96226 (semantic_loss: 0.01576, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19104 
Train Epoch: 12 [121/1000 3872/32000 (12%)] Loss: 1.97279 (semantic_loss: 0.02335, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.19382 
Train Epoch: 12 [126/1000 4032/32000 (13%)] Loss: 1.96353 (semantic_loss: 0.01703, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23143 
Train Epoch: 12 [131/1000 4192/32000 (13%)] Loss: 1.96488 (semantic_loss: 0.01740, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19494 
Train Epoch: 12 [136/1000 4352/32000 (14%)] Loss: 1.96387 (semantic_loss: 0.01737, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19179 
Train Epoch: 12 [141/1000 4512/32000 (14%)] Loss: 1.96238 (semantic_loss: 0.01685, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18239 
Train Epoch: 12 [146/1000 4672/32000 (15%)] Loss: 1.96612 (semantic_loss: 0.01960, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18753 
Train Epoch: 12 [151/1000 4832/32000 (15%)] Loss: 1.96209 (semantic_loss: 0.01657, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18699 
Train Epoch: 12 [156/1000 4992/32000 (16%)] Loss: 1.96763 (semantic_loss: 0.02015, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18751 
Train Epoch: 12 [161/1000 5152/32000 (16%)] Loss: 1.96857 (semantic_loss: 0.02109, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18879 
Train Epoch: 12 [166/1000 5312/32000 (17%)] Loss: 1.96759 (semantic_loss: 0.01913, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18760 
Train Epoch: 12 [171/1000 5472/32000 (17%)] Loss: 1.96584 (semantic_loss: 0.01836, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19314 
Train Epoch: 12 [176/1000 5632/32000 (18%)] Loss: 1.96711 (semantic_loss: 0.01962, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19052 
Train Epoch: 12 [181/1000 5792/32000 (18%)] Loss: 1.96069 (semantic_loss: 0.01419, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21732 
Train Epoch: 12 [186/1000 5952/32000 (19%)] Loss: 1.96724 (semantic_loss: 0.01977, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22691 
Train Epoch: 12 [191/1000 6112/32000 (19%)] Loss: 1.96522 (semantic_loss: 0.01872, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.23942 
Train Epoch: 12 [196/1000 6272/32000 (20%)] Loss: 1.97125 (semantic_loss: 0.02279, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.21163 
Train Epoch: 12 [201/1000 6432/32000 (20%)] Loss: 1.96682 (semantic_loss: 0.01934, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20243 
Train Epoch: 12 [206/1000 6592/32000 (21%)] Loss: 1.96466 (semantic_loss: 0.01914, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.22446 
Train Epoch: 12 [211/1000 6752/32000 (21%)] Loss: 1.96433 (semantic_loss: 0.01685, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.66299 
Train Epoch: 12 [216/1000 6912/32000 (22%)] Loss: 1.96844 (semantic_loss: 0.02195, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20012 
Train Epoch: 12 [221/1000 7072/32000 (22%)] Loss: 1.96585 (semantic_loss: 0.01837, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18893 
Train Epoch: 12 [226/1000 7232/32000 (23%)] Loss: 1.96568 (semantic_loss: 0.01820, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19262 
Train Epoch: 12 [231/1000 7392/32000 (23%)] Loss: 1.96811 (semantic_loss: 0.01965, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18737 
Train Epoch: 12 [236/1000 7552/32000 (24%)] Loss: 1.96734 (semantic_loss: 0.02083, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18720 
Train Epoch: 12 [241/1000 7712/32000 (24%)] Loss: 1.96655 (semantic_loss: 0.01810, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18611 
Train Epoch: 12 [246/1000 7872/32000 (25%)] Loss: 1.96614 (semantic_loss: 0.01866, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19423 
Train Epoch: 12 [251/1000 8032/32000 (25%)] Loss: 1.96871 (semantic_loss: 0.02123, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20172 
Train Epoch: 12 [256/1000 8192/32000 (26%)] Loss: 1.96352 (semantic_loss: 0.01702, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20735 
Train Epoch: 12 [261/1000 8352/32000 (26%)] Loss: 1.96772 (semantic_loss: 0.01927, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.27909 
Train Epoch: 12 [266/1000 8512/32000 (27%)] Loss: 1.96820 (semantic_loss: 0.02073, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19527 
Train Epoch: 12 [271/1000 8672/32000 (27%)] Loss: 1.96461 (semantic_loss: 0.01811, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19710 
Train Epoch: 12 [276/1000 8832/32000 (28%)] Loss: 1.96611 (semantic_loss: 0.01863, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18703 
Train Epoch: 12 [281/1000 8992/32000 (28%)] Loss: 1.96794 (semantic_loss: 0.02046, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18901 
Train Epoch: 12 [286/1000 9152/32000 (29%)] Loss: 1.96149 (semantic_loss: 0.01596, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.20194 
Train Epoch: 12 [291/1000 9312/32000 (29%)] Loss: 1.96436 (semantic_loss: 0.01785, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18637 
Train Epoch: 12 [296/1000 9472/32000 (30%)] Loss: 1.96484 (semantic_loss: 0.01737, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18783 
Train Epoch: 12 [301/1000 9632/32000 (30%)] Loss: 1.96472 (semantic_loss: 0.01724, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18713 
Train Epoch: 12 [306/1000 9792/32000 (31%)] Loss: 1.96445 (semantic_loss: 0.01795, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18888 
Train Epoch: 12 [311/1000 9952/32000 (31%)] Loss: 1.96918 (semantic_loss: 0.02170, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21981 
Train Epoch: 12 [316/1000 10112/32000 (32%)] Loss: 1.96562 (semantic_loss: 0.01814, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.37513 
Train Epoch: 12 [321/1000 10272/32000 (32%)] Loss: 1.96635 (semantic_loss: 0.01985, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18774 
Train Epoch: 12 [326/1000 10432/32000 (33%)] Loss: 1.96399 (semantic_loss: 0.01652, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18811 
Train Epoch: 12 [331/1000 10592/32000 (33%)] Loss: 1.96655 (semantic_loss: 0.01906, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20476 
Train Epoch: 12 [336/1000 10752/32000 (34%)] Loss: 1.96518 (semantic_loss: 0.01769, quant_loss: 1.94727, bit_balance_loss: 0.00023) batch_time=0.24293 
Train Epoch: 12 [341/1000 10912/32000 (34%)] Loss: 1.96461 (semantic_loss: 0.01713, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22318 
Train Epoch: 12 [346/1000 11072/32000 (35%)] Loss: 1.96429 (semantic_loss: 0.01778, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19518 
Train Epoch: 12 [351/1000 11232/32000 (35%)] Loss: 1.96311 (semantic_loss: 0.01661, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20146 
Train Epoch: 12 [356/1000 11392/32000 (36%)] Loss: 1.96625 (semantic_loss: 0.01877, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21067 
Train Epoch: 12 [361/1000 11552/32000 (36%)] Loss: 1.96560 (semantic_loss: 0.01812, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20771 
Train Epoch: 12 [366/1000 11712/32000 (37%)] Loss: 1.96744 (semantic_loss: 0.01996, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21307 
Train Epoch: 12 [371/1000 11872/32000 (37%)] Loss: 1.96916 (semantic_loss: 0.02363, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18961 
Train Epoch: 12 [376/1000 12032/32000 (38%)] Loss: 1.96709 (semantic_loss: 0.02059, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18988 
Train Epoch: 12 [381/1000 12192/32000 (38%)] Loss: 1.96750 (semantic_loss: 0.02003, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20322 
Train Epoch: 12 [386/1000 12352/32000 (39%)] Loss: 1.96514 (semantic_loss: 0.01864, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19165 
Train Epoch: 12 [391/1000 12512/32000 (39%)] Loss: 1.96472 (semantic_loss: 0.01919, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18954 
Train Epoch: 12 [396/1000 12672/32000 (40%)] Loss: 1.96066 (semantic_loss: 0.01514, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19451 
Train Epoch: 12 [401/1000 12832/32000 (40%)] Loss: 1.96598 (semantic_loss: 0.01948, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.71480 
Train Epoch: 12 [406/1000 12992/32000 (41%)] Loss: 1.96574 (semantic_loss: 0.01923, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19130 
Train Epoch: 12 [411/1000 13152/32000 (41%)] Loss: 1.96320 (semantic_loss: 0.01670, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18874 
Train Epoch: 12 [416/1000 13312/32000 (42%)] Loss: 1.96848 (semantic_loss: 0.02198, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18545 
Train Epoch: 12 [421/1000 13472/32000 (42%)] Loss: 1.97007 (semantic_loss: 0.02258, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18762 
Train Epoch: 12 [426/1000 13632/32000 (43%)] Loss: 1.96292 (semantic_loss: 0.01641, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18670 
Train Epoch: 12 [431/1000 13792/32000 (43%)] Loss: 1.96495 (semantic_loss: 0.01845, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21912 
Train Epoch: 12 [436/1000 13952/32000 (44%)] Loss: 1.96612 (semantic_loss: 0.01865, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18615 
Train Epoch: 12 [441/1000 14112/32000 (44%)] Loss: 1.96612 (semantic_loss: 0.01962, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18793 
Train Epoch: 12 [446/1000 14272/32000 (45%)] Loss: 1.96287 (semantic_loss: 0.01636, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22506 
Train Epoch: 12 [451/1000 14432/32000 (45%)] Loss: 1.96861 (semantic_loss: 0.02210, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18809 
Train Epoch: 12 [456/1000 14592/32000 (46%)] Loss: 1.96630 (semantic_loss: 0.01882, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18885 
Train Epoch: 12 [461/1000 14752/32000 (46%)] Loss: 1.96565 (semantic_loss: 0.01915, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18756 
Train Epoch: 12 [466/1000 14912/32000 (47%)] Loss: 1.96696 (semantic_loss: 0.01850, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18862 
Train Epoch: 12 [471/1000 15072/32000 (47%)] Loss: 1.96621 (semantic_loss: 0.01872, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18920 
Train Epoch: 12 [476/1000 15232/32000 (48%)] Loss: 1.96416 (semantic_loss: 0.01766, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21144 
Train Epoch: 12 [481/1000 15392/32000 (48%)] Loss: 1.96201 (semantic_loss: 0.01551, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18832 
Train Epoch: 12 [486/1000 15552/32000 (49%)] Loss: 1.96683 (semantic_loss: 0.02032, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18889 
Train Epoch: 12 [491/1000 15712/32000 (49%)] Loss: 1.96360 (semantic_loss: 0.01710, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21643 
Train Epoch: 12 [496/1000 15872/32000 (50%)] Loss: 1.96732 (semantic_loss: 0.02081, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21189 
Train Epoch: 12 [501/1000 16032/32000 (50%)] Loss: 1.96532 (semantic_loss: 0.01784, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22264 
Train Epoch: 12 [506/1000 16192/32000 (51%)] Loss: 1.96386 (semantic_loss: 0.01736, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21778 
Train Epoch: 12 [511/1000 16352/32000 (51%)] Loss: 1.96432 (semantic_loss: 0.01782, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21366 
Train Epoch: 12 [516/1000 16512/32000 (52%)] Loss: 1.96454 (semantic_loss: 0.01804, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20356 
Train Epoch: 12 [521/1000 16672/32000 (52%)] Loss: 1.96754 (semantic_loss: 0.02103, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20389 
Train Epoch: 12 [526/1000 16832/32000 (53%)] Loss: 1.96304 (semantic_loss: 0.01556, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21044 
Train Epoch: 12 [531/1000 16992/32000 (53%)] Loss: 1.96352 (semantic_loss: 0.01701, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.60333 
Train Epoch: 12 [536/1000 17152/32000 (54%)] Loss: 1.96181 (semantic_loss: 0.01530, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18807 
Train Epoch: 12 [541/1000 17312/32000 (54%)] Loss: 1.96664 (semantic_loss: 0.01819, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18914 
Train Epoch: 12 [546/1000 17472/32000 (55%)] Loss: 1.96903 (semantic_loss: 0.02155, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21422 
Train Epoch: 12 [551/1000 17632/32000 (55%)] Loss: 1.96476 (semantic_loss: 0.01826, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19299 
Train Epoch: 12 [556/1000 17792/32000 (56%)] Loss: 1.96347 (semantic_loss: 0.01599, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19291 
Train Epoch: 12 [561/1000 17952/32000 (56%)] Loss: 1.96551 (semantic_loss: 0.01803, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19615 
Train Epoch: 12 [566/1000 18112/32000 (57%)] Loss: 1.96604 (semantic_loss: 0.01954, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20990 
Train Epoch: 12 [571/1000 18272/32000 (57%)] Loss: 1.96645 (semantic_loss: 0.01994, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19296 
Train Epoch: 12 [576/1000 18432/32000 (58%)] Loss: 1.96834 (semantic_loss: 0.02086, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21286 
Train Epoch: 12 [581/1000 18592/32000 (58%)] Loss: 1.96524 (semantic_loss: 0.01777, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.28600 
Train Epoch: 12 [586/1000 18752/32000 (59%)] Loss: 1.96345 (semantic_loss: 0.01597, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18719 
Train Epoch: 12 [591/1000 18912/32000 (59%)] Loss: 1.96634 (semantic_loss: 0.01789, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19673 
Train Epoch: 12 [596/1000 19072/32000 (60%)] Loss: 1.96795 (semantic_loss: 0.01949, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18677 
Train Epoch: 12 [601/1000 19232/32000 (60%)] Loss: 1.96590 (semantic_loss: 0.01745, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20205 
Train Epoch: 12 [606/1000 19392/32000 (61%)] Loss: 1.96493 (semantic_loss: 0.01939, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18846 
Train Epoch: 12 [611/1000 19552/32000 (61%)] Loss: 1.96752 (semantic_loss: 0.01907, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18770 
Train Epoch: 12 [616/1000 19712/32000 (62%)] Loss: 1.96415 (semantic_loss: 0.01764, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18649 
Train Epoch: 12 [621/1000 19872/32000 (62%)] Loss: 1.96634 (semantic_loss: 0.01887, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18657 
Train Epoch: 12 [626/1000 20032/32000 (63%)] Loss: 1.96466 (semantic_loss: 0.01815, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20035 
Train Epoch: 12 [631/1000 20192/32000 (63%)] Loss: 1.96224 (semantic_loss: 0.01574, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22613 
Train Epoch: 12 [636/1000 20352/32000 (64%)] Loss: 1.96637 (semantic_loss: 0.01889, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.57688 
Train Epoch: 12 [641/1000 20512/32000 (64%)] Loss: 1.96420 (semantic_loss: 0.01768, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19271 
Train Epoch: 12 [646/1000 20672/32000 (65%)] Loss: 1.96802 (semantic_loss: 0.02054, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18907 
Train Epoch: 12 [651/1000 20832/32000 (65%)] Loss: 1.96654 (semantic_loss: 0.01907, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22925 
Train Epoch: 12 [656/1000 20992/32000 (66%)] Loss: 1.96727 (semantic_loss: 0.01978, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.27273 
Train Epoch: 12 [661/1000 21152/32000 (66%)] Loss: 1.96415 (semantic_loss: 0.01764, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19412 
Train Epoch: 12 [666/1000 21312/32000 (67%)] Loss: 1.96739 (semantic_loss: 0.01991, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21917 
Train Epoch: 12 [671/1000 21472/32000 (67%)] Loss: 1.96518 (semantic_loss: 0.01868, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20982 
Train Epoch: 12 [676/1000 21632/32000 (68%)] Loss: 1.96635 (semantic_loss: 0.01985, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19811 
Train Epoch: 12 [681/1000 21792/32000 (68%)] Loss: 1.96495 (semantic_loss: 0.01747, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20023 
Train Epoch: 12 [686/1000 21952/32000 (69%)] Loss: 1.97034 (semantic_loss: 0.02286, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19353 
Train Epoch: 12 [691/1000 22112/32000 (69%)] Loss: 1.96337 (semantic_loss: 0.01588, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18669 
Train Epoch: 12 [696/1000 22272/32000 (70%)] Loss: 1.97101 (semantic_loss: 0.02255, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18850 
Train Epoch: 12 [701/1000 22432/32000 (70%)] Loss: 1.96132 (semantic_loss: 0.01580, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20376 
Train Epoch: 12 [706/1000 22592/32000 (71%)] Loss: 1.97241 (semantic_loss: 0.02395, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18714 
Train Epoch: 12 [711/1000 22752/32000 (71%)] Loss: 1.96154 (semantic_loss: 0.01504, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19354 
Train Epoch: 12 [716/1000 22912/32000 (72%)] Loss: 1.96238 (semantic_loss: 0.01686, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19136 
Train Epoch: 12 [721/1000 23072/32000 (72%)] Loss: 1.96629 (semantic_loss: 0.01979, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.71423 
Train Epoch: 12 [726/1000 23232/32000 (73%)] Loss: 1.96524 (semantic_loss: 0.01775, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18521 
Train Epoch: 12 [731/1000 23392/32000 (73%)] Loss: 1.96345 (semantic_loss: 0.01694, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18778 
Train Epoch: 12 [736/1000 23552/32000 (74%)] Loss: 1.96604 (semantic_loss: 0.01953, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18977 
Train Epoch: 12 [741/1000 23712/32000 (74%)] Loss: 1.96590 (semantic_loss: 0.01842, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18801 
Train Epoch: 12 [746/1000 23872/32000 (75%)] Loss: 1.96801 (semantic_loss: 0.02053, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18681 
Train Epoch: 12 [751/1000 24032/32000 (75%)] Loss: 1.96699 (semantic_loss: 0.02049, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22478 
Train Epoch: 12 [756/1000 24192/32000 (76%)] Loss: 1.96644 (semantic_loss: 0.01799, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18765 
Train Epoch: 12 [761/1000 24352/32000 (76%)] Loss: 1.96619 (semantic_loss: 0.01774, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18801 
Train Epoch: 12 [766/1000 24512/32000 (77%)] Loss: 1.96648 (semantic_loss: 0.01901, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23508 
Train Epoch: 12 [771/1000 24672/32000 (77%)] Loss: 1.96390 (semantic_loss: 0.01739, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18610 
Train Epoch: 12 [776/1000 24832/32000 (78%)] Loss: 1.96634 (semantic_loss: 0.01886, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18871 
Train Epoch: 12 [781/1000 24992/32000 (78%)] Loss: 1.97098 (semantic_loss: 0.02350, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18958 
Train Epoch: 12 [786/1000 25152/32000 (79%)] Loss: 1.96901 (semantic_loss: 0.02153, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18929 
Train Epoch: 12 [791/1000 25312/32000 (79%)] Loss: 1.96266 (semantic_loss: 0.01518, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18592 
Train Epoch: 12 [796/1000 25472/32000 (80%)] Loss: 1.96724 (semantic_loss: 0.01977, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19024 
Train Epoch: 12 [801/1000 25632/32000 (80%)] Loss: 1.96508 (semantic_loss: 0.01857, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21244 
Train Epoch: 12 [806/1000 25792/32000 (81%)] Loss: 1.97019 (semantic_loss: 0.02369, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22261 
Train Epoch: 12 [811/1000 25952/32000 (81%)] Loss: 1.96616 (semantic_loss: 0.01868, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22292 
Train Epoch: 12 [816/1000 26112/32000 (82%)] Loss: 1.96782 (semantic_loss: 0.02034, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20882 
Train Epoch: 12 [821/1000 26272/32000 (82%)] Loss: 1.96832 (semantic_loss: 0.02084, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21314 
Train Epoch: 12 [826/1000 26432/32000 (83%)] Loss: 1.96326 (semantic_loss: 0.01773, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19680 
Train Epoch: 12 [831/1000 26592/32000 (83%)] Loss: 1.96940 (semantic_loss: 0.02094, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19687 
Train Epoch: 12 [836/1000 26752/32000 (84%)] Loss: 1.96612 (semantic_loss: 0.01864, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20268 
Train Epoch: 12 [841/1000 26912/32000 (84%)] Loss: 1.96768 (semantic_loss: 0.02117, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20064 
Train Epoch: 12 [846/1000 27072/32000 (85%)] Loss: 1.96573 (semantic_loss: 0.01923, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20666 
Train Epoch: 12 [851/1000 27232/32000 (85%)] Loss: 1.96399 (semantic_loss: 0.01651, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.60629 
Train Epoch: 12 [856/1000 27392/32000 (86%)] Loss: 1.96685 (semantic_loss: 0.01937, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18937 
Train Epoch: 12 [861/1000 27552/32000 (86%)] Loss: 1.96318 (semantic_loss: 0.01473, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19054 
Train Epoch: 12 [866/1000 27712/32000 (87%)] Loss: 1.96699 (semantic_loss: 0.01951, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19871 
Train Epoch: 12 [871/1000 27872/32000 (87%)] Loss: 1.96175 (semantic_loss: 0.01525, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19831 
Train Epoch: 12 [876/1000 28032/32000 (88%)] Loss: 1.96561 (semantic_loss: 0.01910, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19217 
Train Epoch: 12 [881/1000 28192/32000 (88%)] Loss: 1.96681 (semantic_loss: 0.01934, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18929 
Train Epoch: 12 [886/1000 28352/32000 (89%)] Loss: 1.96405 (semantic_loss: 0.01754, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18765 
Train Epoch: 12 [891/1000 28512/32000 (89%)] Loss: 1.96965 (semantic_loss: 0.02217, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19414 
Train Epoch: 12 [896/1000 28672/32000 (90%)] Loss: 1.96400 (semantic_loss: 0.01750, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20749 
Train Epoch: 12 [901/1000 28832/32000 (90%)] Loss: 1.96368 (semantic_loss: 0.01620, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.28367 
Train Epoch: 12 [906/1000 28992/32000 (91%)] Loss: 1.96724 (semantic_loss: 0.01976, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20153 
Train Epoch: 12 [911/1000 29152/32000 (91%)] Loss: 1.96413 (semantic_loss: 0.01665, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19211 
Train Epoch: 12 [916/1000 29312/32000 (92%)] Loss: 1.96209 (semantic_loss: 0.01558, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18513 
Train Epoch: 12 [921/1000 29472/32000 (92%)] Loss: 1.96263 (semantic_loss: 0.01613, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18703 
Train Epoch: 12 [926/1000 29632/32000 (93%)] Loss: 1.96405 (semantic_loss: 0.01658, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18783 
Train Epoch: 12 [931/1000 29792/32000 (93%)] Loss: 1.96476 (semantic_loss: 0.01728, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19357 
Train Epoch: 12 [936/1000 29952/32000 (94%)] Loss: 1.96475 (semantic_loss: 0.01630, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19129 
Train Epoch: 12 [941/1000 30112/32000 (94%)] Loss: 1.96991 (semantic_loss: 0.02340, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19749 
Train Epoch: 12 [946/1000 30272/32000 (95%)] Loss: 1.96344 (semantic_loss: 0.01596, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19167 
Train Epoch: 12 [951/1000 30432/32000 (95%)] Loss: 1.96655 (semantic_loss: 0.01908, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22347 
Train Epoch: 12 [956/1000 30592/32000 (96%)] Loss: 1.96498 (semantic_loss: 0.01848, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.34933 
Train Epoch: 12 [961/1000 30752/32000 (96%)] Loss: 1.96566 (semantic_loss: 0.01916, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18994 
Train Epoch: 12 [966/1000 30912/32000 (97%)] Loss: 1.96677 (semantic_loss: 0.02026, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19890 
Train Epoch: 12 [971/1000 31072/32000 (97%)] Loss: 1.96320 (semantic_loss: 0.01572, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.24000 
Train Epoch: 12 [976/1000 31232/32000 (98%)] Loss: 1.96805 (semantic_loss: 0.01961, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.25984 
Train Epoch: 12 [981/1000 31392/32000 (98%)] Loss: 1.96315 (semantic_loss: 0.01567, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20028 
Train Epoch: 12 [986/1000 31552/32000 (99%)] Loss: 1.96345 (semantic_loss: 0.01500, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21455 
Train Epoch: 12 [991/1000 31712/32000 (99%)] Loss: 1.96632 (semantic_loss: 0.01982, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19575 
Train Epoch: 12 [996/1000 31872/32000 (100%)] Loss: 1.96420 (semantic_loss: 0.01673, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20717 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch12.pth ...
Done in 5.144s
removing stale ckpt [epoch 11] [took 0.01s]
 epoch          : 12
 loss           : 1.9660960540771484
 learning_rate  : 1.569052980450001e-05
 n_samples      : 384000
 n_steps        : 12000
 ActivityNet_val1_test/t2v_metrics/R1: 10.392515761643278
 ActivityNet_val1_test/t2v_metrics/R5: 35.95688427903193
 ActivityNet_val1_test/t2v_metrics/R10: 52.14561724628839
 ActivityNet_val1_test/t2v_metrics/R50: 86.57718120805369
 ActivityNet_val1_test/t2v_metrics/MedR: 10.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 49.09924750864348
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 26.909575838539077
 ActivityNet_val1_test/v2t_metrics/R1: 11.389058368924141
 ActivityNet_val1_test/v2t_metrics/R5: 35.71283302826927
 ActivityNet_val1_test/v2t_metrics/R10: 53.26418547895058
 ActivityNet_val1_test/v2t_metrics/R50: 86.0280658938377
 ActivityNet_val1_test/v2t_metrics/MedR: 9.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 53.053894651210086
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 27.877199134525185
 mnt_best       : 27.302704294022565
 not_improved_count: 2
Train Epoch: 13 [1/1000 32/32000 (0%)] Loss: 1.96596 (semantic_loss: 0.01848, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=21.88747 
Train Epoch: 13 [6/1000 192/32000 (1%)] Loss: 1.96404 (semantic_loss: 0.01755, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18753 
Train Epoch: 13 [11/1000 352/32000 (1%)] Loss: 1.96565 (semantic_loss: 0.01915, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.26106 
Train Epoch: 13 [16/1000 512/32000 (2%)] Loss: 1.96630 (semantic_loss: 0.01882, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.28719 
Train Epoch: 13 [21/1000 672/32000 (2%)] Loss: 1.96445 (semantic_loss: 0.01698, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21256 
Train Epoch: 13 [26/1000 832/32000 (3%)] Loss: 1.97015 (semantic_loss: 0.02267, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.31376 
Train Epoch: 13 [31/1000 992/32000 (3%)] Loss: 1.97119 (semantic_loss: 0.02274, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18714 
Train Epoch: 13 [36/1000 1152/32000 (4%)] Loss: 1.97084 (semantic_loss: 0.02337, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18560 
Train Epoch: 13 [41/1000 1312/32000 (4%)] Loss: 1.96507 (semantic_loss: 0.01759, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18467 
Train Epoch: 13 [46/1000 1472/32000 (5%)] Loss: 1.96497 (semantic_loss: 0.01749, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18729 
Train Epoch: 13 [51/1000 1632/32000 (5%)] Loss: 1.96674 (semantic_loss: 0.01927, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18675 
Train Epoch: 13 [56/1000 1792/32000 (6%)] Loss: 1.96990 (semantic_loss: 0.02242, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20974 
Train Epoch: 13 [61/1000 1952/32000 (6%)] Loss: 1.96396 (semantic_loss: 0.01647, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18716 
Train Epoch: 13 [66/1000 2112/32000 (7%)] Loss: 1.96713 (semantic_loss: 0.01966, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19988 
Train Epoch: 13 [71/1000 2272/32000 (7%)] Loss: 1.96616 (semantic_loss: 0.01966, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20507 
Train Epoch: 13 [76/1000 2432/32000 (8%)] Loss: 1.97153 (semantic_loss: 0.02405, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18508 
Train Epoch: 13 [81/1000 2592/32000 (8%)] Loss: 1.96486 (semantic_loss: 0.01836, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.47881 
Train Epoch: 13 [86/1000 2752/32000 (9%)] Loss: 1.96739 (semantic_loss: 0.02185, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18542 
Train Epoch: 13 [91/1000 2912/32000 (9%)] Loss: 1.96966 (semantic_loss: 0.02219, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19037 
Train Epoch: 13 [96/1000 3072/32000 (10%)] Loss: 1.96680 (semantic_loss: 0.02030, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20400 
Train Epoch: 13 [101/1000 3232/32000 (10%)] Loss: 1.96791 (semantic_loss: 0.02141, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22070 
Train Epoch: 13 [106/1000 3392/32000 (11%)] Loss: 1.96523 (semantic_loss: 0.01871, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21878 
Train Epoch: 13 [111/1000 3552/32000 (11%)] Loss: 1.96576 (semantic_loss: 0.01926, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20984 
Train Epoch: 13 [116/1000 3712/32000 (12%)] Loss: 1.96849 (semantic_loss: 0.02004, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22460 
Train Epoch: 13 [121/1000 3872/32000 (12%)] Loss: 1.96463 (semantic_loss: 0.01813, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22988 
Train Epoch: 13 [126/1000 4032/32000 (13%)] Loss: 1.96991 (semantic_loss: 0.02242, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.23626 
Train Epoch: 13 [131/1000 4192/32000 (13%)] Loss: 1.96472 (semantic_loss: 0.01821, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20829 
Train Epoch: 13 [136/1000 4352/32000 (14%)] Loss: 1.96319 (semantic_loss: 0.01670, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18928 
Train Epoch: 13 [141/1000 4512/32000 (14%)] Loss: 1.96549 (semantic_loss: 0.01802, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18609 
Train Epoch: 13 [146/1000 4672/32000 (15%)] Loss: 1.96678 (semantic_loss: 0.02028, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18788 
Train Epoch: 13 [151/1000 4832/32000 (15%)] Loss: 1.96327 (semantic_loss: 0.01579, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18632 
Train Epoch: 13 [156/1000 4992/32000 (16%)] Loss: 1.96468 (semantic_loss: 0.01622, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18670 
Train Epoch: 13 [161/1000 5152/32000 (16%)] Loss: 1.96338 (semantic_loss: 0.01591, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.34142 
Train Epoch: 13 [166/1000 5312/32000 (17%)] Loss: 1.96450 (semantic_loss: 0.01702, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24093 
Train Epoch: 13 [171/1000 5472/32000 (17%)] Loss: 1.96538 (semantic_loss: 0.01791, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19178 
Train Epoch: 13 [176/1000 5632/32000 (18%)] Loss: 1.96590 (semantic_loss: 0.01940, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18436 
Train Epoch: 13 [181/1000 5792/32000 (18%)] Loss: 1.96426 (semantic_loss: 0.01580, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18517 
Train Epoch: 13 [186/1000 5952/32000 (19%)] Loss: 1.96643 (semantic_loss: 0.01896, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18933 
Train Epoch: 13 [191/1000 6112/32000 (19%)] Loss: 1.96879 (semantic_loss: 0.02132, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19320 
Train Epoch: 13 [196/1000 6272/32000 (20%)] Loss: 1.96613 (semantic_loss: 0.01962, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18781 
Train Epoch: 13 [201/1000 6432/32000 (20%)] Loss: 1.97305 (semantic_loss: 0.02557, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19098 
Train Epoch: 13 [206/1000 6592/32000 (21%)] Loss: 1.96485 (semantic_loss: 0.01737, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18958 
Train Epoch: 13 [211/1000 6752/32000 (21%)] Loss: 1.96538 (semantic_loss: 0.01790, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19358 
Train Epoch: 13 [216/1000 6912/32000 (22%)] Loss: 1.96814 (semantic_loss: 0.01968, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20871 
Train Epoch: 13 [221/1000 7072/32000 (22%)] Loss: 1.96304 (semantic_loss: 0.01654, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18683 
Train Epoch: 13 [226/1000 7232/32000 (23%)] Loss: 1.96657 (semantic_loss: 0.01910, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18724 
Train Epoch: 13 [231/1000 7392/32000 (23%)] Loss: 1.96709 (semantic_loss: 0.02059, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19252 
Train Epoch: 13 [236/1000 7552/32000 (24%)] Loss: 1.96746 (semantic_loss: 0.01900, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19157 
Train Epoch: 13 [241/1000 7712/32000 (24%)] Loss: 1.96279 (semantic_loss: 0.01629, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18796 
Train Epoch: 13 [246/1000 7872/32000 (25%)] Loss: 1.96692 (semantic_loss: 0.01944, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18702 
Train Epoch: 13 [251/1000 8032/32000 (25%)] Loss: 1.96536 (semantic_loss: 0.01886, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19704 
Train Epoch: 13 [256/1000 8192/32000 (26%)] Loss: 1.96578 (semantic_loss: 0.01928, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20358 
Train Epoch: 13 [261/1000 8352/32000 (26%)] Loss: 1.96547 (semantic_loss: 0.01798, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.26556 
Train Epoch: 13 [266/1000 8512/32000 (27%)] Loss: 1.96801 (semantic_loss: 0.02054, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.29027 
Train Epoch: 13 [271/1000 8672/32000 (27%)] Loss: 1.96868 (semantic_loss: 0.02121, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.61006 
Train Epoch: 13 [276/1000 8832/32000 (28%)] Loss: 1.96274 (semantic_loss: 0.01721, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.19702 
Train Epoch: 13 [281/1000 8992/32000 (28%)] Loss: 1.96418 (semantic_loss: 0.01768, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20712 
Train Epoch: 13 [286/1000 9152/32000 (29%)] Loss: 1.96370 (semantic_loss: 0.01622, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20146 
Train Epoch: 13 [291/1000 9312/32000 (29%)] Loss: 1.96491 (semantic_loss: 0.01841, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18616 
Train Epoch: 13 [296/1000 9472/32000 (30%)] Loss: 1.96515 (semantic_loss: 0.01865, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.32412 
Train Epoch: 13 [301/1000 9632/32000 (30%)] Loss: 1.96724 (semantic_loss: 0.01878, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18929 
Train Epoch: 13 [306/1000 9792/32000 (31%)] Loss: 1.96861 (semantic_loss: 0.02015, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18824 
Train Epoch: 13 [311/1000 9952/32000 (31%)] Loss: 1.97076 (semantic_loss: 0.02328, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18778 
Train Epoch: 13 [316/1000 10112/32000 (32%)] Loss: 1.96346 (semantic_loss: 0.01793, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19355 
Train Epoch: 13 [321/1000 10272/32000 (32%)] Loss: 1.96672 (semantic_loss: 0.01924, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19298 
Train Epoch: 13 [326/1000 10432/32000 (33%)] Loss: 1.96618 (semantic_loss: 0.01870, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19248 
Train Epoch: 13 [331/1000 10592/32000 (33%)] Loss: 1.96828 (semantic_loss: 0.02080, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26055 
Train Epoch: 13 [336/1000 10752/32000 (34%)] Loss: 1.96496 (semantic_loss: 0.01845, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.27482 
Train Epoch: 13 [341/1000 10912/32000 (34%)] Loss: 1.96534 (semantic_loss: 0.01688, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20955 
Train Epoch: 13 [346/1000 11072/32000 (35%)] Loss: 1.96794 (semantic_loss: 0.02046, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.30965 
Train Epoch: 13 [351/1000 11232/32000 (35%)] Loss: 1.96684 (semantic_loss: 0.01937, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18452 
Train Epoch: 13 [356/1000 11392/32000 (36%)] Loss: 1.96683 (semantic_loss: 0.01935, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18427 
Train Epoch: 13 [361/1000 11552/32000 (36%)] Loss: 1.97172 (semantic_loss: 0.02424, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18194 
Train Epoch: 13 [366/1000 11712/32000 (37%)] Loss: 1.96430 (semantic_loss: 0.01682, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19276 
Train Epoch: 13 [371/1000 11872/32000 (37%)] Loss: 1.96670 (semantic_loss: 0.02020, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19178 
Train Epoch: 13 [376/1000 12032/32000 (38%)] Loss: 1.96361 (semantic_loss: 0.01711, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18638 
Train Epoch: 13 [381/1000 12192/32000 (38%)] Loss: 1.96511 (semantic_loss: 0.01957, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18721 
Train Epoch: 13 [386/1000 12352/32000 (39%)] Loss: 1.96557 (semantic_loss: 0.01712, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19795 
Train Epoch: 13 [391/1000 12512/32000 (39%)] Loss: 1.96918 (semantic_loss: 0.02170, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19522 
Train Epoch: 13 [396/1000 12672/32000 (40%)] Loss: 1.96463 (semantic_loss: 0.01812, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20018 
Train Epoch: 13 [401/1000 12832/32000 (40%)] Loss: 1.96507 (semantic_loss: 0.01856, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.47035 
Train Epoch: 13 [406/1000 12992/32000 (41%)] Loss: 1.97214 (semantic_loss: 0.02369, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20165 
Train Epoch: 13 [411/1000 13152/32000 (41%)] Loss: 1.96719 (semantic_loss: 0.02166, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20682 
Train Epoch: 13 [416/1000 13312/32000 (42%)] Loss: 1.96380 (semantic_loss: 0.01729, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22957 
Train Epoch: 13 [421/1000 13472/32000 (42%)] Loss: 1.96775 (semantic_loss: 0.02028, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20970 
Train Epoch: 13 [426/1000 13632/32000 (43%)] Loss: 1.96573 (semantic_loss: 0.01922, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20131 
Train Epoch: 13 [431/1000 13792/32000 (43%)] Loss: 1.96603 (semantic_loss: 0.01757, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21112 
Train Epoch: 13 [436/1000 13952/32000 (44%)] Loss: 1.96425 (semantic_loss: 0.01774, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22365 
Train Epoch: 13 [441/1000 14112/32000 (44%)] Loss: 1.96810 (semantic_loss: 0.02160, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20063 
Train Epoch: 13 [446/1000 14272/32000 (45%)] Loss: 1.96524 (semantic_loss: 0.01873, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22827 
Train Epoch: 13 [451/1000 14432/32000 (45%)] Loss: 1.96524 (semantic_loss: 0.01776, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18651 
Train Epoch: 13 [456/1000 14592/32000 (46%)] Loss: 1.97094 (semantic_loss: 0.02345, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19266 
Train Epoch: 13 [461/1000 14752/32000 (46%)] Loss: 1.96885 (semantic_loss: 0.02235, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18640 
Train Epoch: 13 [466/1000 14912/32000 (47%)] Loss: 1.96295 (semantic_loss: 0.01644, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18555 
Train Epoch: 13 [471/1000 15072/32000 (47%)] Loss: 1.96577 (semantic_loss: 0.01828, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20070 
Train Epoch: 13 [476/1000 15232/32000 (48%)] Loss: 1.96726 (semantic_loss: 0.01978, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20758 
Train Epoch: 13 [481/1000 15392/32000 (48%)] Loss: 1.96606 (semantic_loss: 0.01956, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.34892 
Train Epoch: 13 [486/1000 15552/32000 (49%)] Loss: 1.96649 (semantic_loss: 0.02194, quant_loss: 1.94434, bit_balance_loss: 0.00021) batch_time=0.19014 
Train Epoch: 13 [491/1000 15712/32000 (49%)] Loss: 1.96544 (semantic_loss: 0.01894, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18864 
Train Epoch: 13 [496/1000 15872/32000 (50%)] Loss: 1.96375 (semantic_loss: 0.01627, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18706 
Train Epoch: 13 [501/1000 16032/32000 (50%)] Loss: 1.96329 (semantic_loss: 0.01777, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18654 
Train Epoch: 13 [506/1000 16192/32000 (51%)] Loss: 1.96812 (semantic_loss: 0.02065, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18964 
Train Epoch: 13 [511/1000 16352/32000 (51%)] Loss: 1.96997 (semantic_loss: 0.02152, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18851 
Train Epoch: 13 [516/1000 16512/32000 (52%)] Loss: 1.96514 (semantic_loss: 0.01766, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18857 
Train Epoch: 13 [521/1000 16672/32000 (52%)] Loss: 1.96307 (semantic_loss: 0.01657, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18522 
Train Epoch: 13 [526/1000 16832/32000 (53%)] Loss: 1.96708 (semantic_loss: 0.01863, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18514 
Train Epoch: 13 [531/1000 16992/32000 (53%)] Loss: 1.96580 (semantic_loss: 0.01930, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18726 
Train Epoch: 13 [536/1000 17152/32000 (54%)] Loss: 1.96688 (semantic_loss: 0.01842, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18750 
Train Epoch: 13 [541/1000 17312/32000 (54%)] Loss: 1.96370 (semantic_loss: 0.01622, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18906 
Train Epoch: 13 [546/1000 17472/32000 (55%)] Loss: 1.96309 (semantic_loss: 0.01659, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18356 
Train Epoch: 13 [551/1000 17632/32000 (55%)] Loss: 1.96717 (semantic_loss: 0.01970, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18836 
Train Epoch: 13 [556/1000 17792/32000 (56%)] Loss: 1.96712 (semantic_loss: 0.01963, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18984 
Train Epoch: 13 [561/1000 17952/32000 (56%)] Loss: 1.96198 (semantic_loss: 0.01548, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19279 
Train Epoch: 13 [566/1000 18112/32000 (57%)] Loss: 1.96404 (semantic_loss: 0.01852, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18977 
Train Epoch: 13 [571/1000 18272/32000 (57%)] Loss: 1.96599 (semantic_loss: 0.01753, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.23408 
Train Epoch: 13 [576/1000 18432/32000 (58%)] Loss: 1.96621 (semantic_loss: 0.01874, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21974 
Train Epoch: 13 [581/1000 18592/32000 (58%)] Loss: 1.96510 (semantic_loss: 0.01763, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26466 
Train Epoch: 13 [586/1000 18752/32000 (59%)] Loss: 1.96361 (semantic_loss: 0.01613, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25222 
Train Epoch: 13 [591/1000 18912/32000 (59%)] Loss: 1.96694 (semantic_loss: 0.02044, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.52461 
Train Epoch: 13 [596/1000 19072/32000 (60%)] Loss: 1.96701 (semantic_loss: 0.01856, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20256 
Train Epoch: 13 [601/1000 19232/32000 (60%)] Loss: 1.96477 (semantic_loss: 0.01827, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21449 
Train Epoch: 13 [606/1000 19392/32000 (61%)] Loss: 1.96552 (semantic_loss: 0.01804, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19199 
Train Epoch: 13 [611/1000 19552/32000 (61%)] Loss: 1.97024 (semantic_loss: 0.02374, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18868 
Train Epoch: 13 [616/1000 19712/32000 (62%)] Loss: 1.96600 (semantic_loss: 0.01950, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.33199 
Train Epoch: 13 [621/1000 19872/32000 (62%)] Loss: 1.96618 (semantic_loss: 0.01772, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18712 
Train Epoch: 13 [626/1000 20032/32000 (63%)] Loss: 1.96695 (semantic_loss: 0.01946, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19239 
Train Epoch: 13 [631/1000 20192/32000 (63%)] Loss: 1.96985 (semantic_loss: 0.02432, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18735 
Train Epoch: 13 [636/1000 20352/32000 (64%)] Loss: 1.96711 (semantic_loss: 0.01963, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19702 
Train Epoch: 13 [641/1000 20512/32000 (64%)] Loss: 1.96637 (semantic_loss: 0.01889, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19190 
Train Epoch: 13 [646/1000 20672/32000 (65%)] Loss: 1.96665 (semantic_loss: 0.01820, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19929 
Train Epoch: 13 [651/1000 20832/32000 (65%)] Loss: 1.96768 (semantic_loss: 0.02020, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25496 
Train Epoch: 13 [656/1000 20992/32000 (66%)] Loss: 1.96480 (semantic_loss: 0.01731, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.27345 
Train Epoch: 13 [661/1000 21152/32000 (66%)] Loss: 1.96776 (semantic_loss: 0.02028, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20669 
Train Epoch: 13 [666/1000 21312/32000 (67%)] Loss: 1.96827 (semantic_loss: 0.01981, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.31705 
Train Epoch: 13 [671/1000 21472/32000 (67%)] Loss: 1.96324 (semantic_loss: 0.01673, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18880 
Train Epoch: 13 [676/1000 21632/32000 (68%)] Loss: 1.96329 (semantic_loss: 0.01680, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18341 
Train Epoch: 13 [681/1000 21792/32000 (68%)] Loss: 1.96354 (semantic_loss: 0.01607, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18493 
Train Epoch: 13 [686/1000 21952/32000 (69%)] Loss: 1.96334 (semantic_loss: 0.01781, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18896 
Train Epoch: 13 [691/1000 22112/32000 (69%)] Loss: 1.96778 (semantic_loss: 0.01933, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18794 
Train Epoch: 13 [696/1000 22272/32000 (70%)] Loss: 1.96290 (semantic_loss: 0.01542, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18767 
Train Epoch: 13 [701/1000 22432/32000 (70%)] Loss: 1.96315 (semantic_loss: 0.01665, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18987 
Train Epoch: 13 [706/1000 22592/32000 (71%)] Loss: 1.97126 (semantic_loss: 0.02378, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18771 
Train Epoch: 13 [711/1000 22752/32000 (71%)] Loss: 1.96843 (semantic_loss: 0.01998, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19563 
Train Epoch: 13 [716/1000 22912/32000 (72%)] Loss: 1.96821 (semantic_loss: 0.02073, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18821 
Train Epoch: 13 [721/1000 23072/32000 (72%)] Loss: 1.96459 (semantic_loss: 0.01712, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.46541 
Train Epoch: 13 [726/1000 23232/32000 (73%)] Loss: 1.96611 (semantic_loss: 0.01864, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18524 
Train Epoch: 13 [731/1000 23392/32000 (73%)] Loss: 1.96454 (semantic_loss: 0.01803, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21043 
Train Epoch: 13 [736/1000 23552/32000 (74%)] Loss: 1.96308 (semantic_loss: 0.01560, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22634 
Train Epoch: 13 [741/1000 23712/32000 (74%)] Loss: 1.96810 (semantic_loss: 0.02159, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21050 
Train Epoch: 13 [746/1000 23872/32000 (75%)] Loss: 1.96774 (semantic_loss: 0.02026, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21454 
Train Epoch: 13 [751/1000 24032/32000 (75%)] Loss: 1.96558 (semantic_loss: 0.01809, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19697 
Train Epoch: 13 [756/1000 24192/32000 (76%)] Loss: 1.96804 (semantic_loss: 0.02154, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20097 
Train Epoch: 13 [761/1000 24352/32000 (76%)] Loss: 1.96418 (semantic_loss: 0.01768, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19478 
Train Epoch: 13 [766/1000 24512/32000 (77%)] Loss: 1.96202 (semantic_loss: 0.01552, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18557 
Train Epoch: 13 [771/1000 24672/32000 (77%)] Loss: 1.96076 (semantic_loss: 0.01426, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18906 
Train Epoch: 13 [776/1000 24832/32000 (78%)] Loss: 1.96675 (semantic_loss: 0.01926, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18888 
Train Epoch: 13 [781/1000 24992/32000 (78%)] Loss: 1.96541 (semantic_loss: 0.01988, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19054 
Train Epoch: 13 [786/1000 25152/32000 (79%)] Loss: 1.96210 (semantic_loss: 0.01462, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18638 
Train Epoch: 13 [791/1000 25312/32000 (79%)] Loss: 1.96809 (semantic_loss: 0.01964, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19857 
Train Epoch: 13 [796/1000 25472/32000 (80%)] Loss: 1.96862 (semantic_loss: 0.02212, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19356 
Train Epoch: 13 [801/1000 25632/32000 (80%)] Loss: 1.96475 (semantic_loss: 0.01825, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.33048 
Train Epoch: 13 [806/1000 25792/32000 (81%)] Loss: 1.96651 (semantic_loss: 0.01903, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.76179 
Train Epoch: 13 [811/1000 25952/32000 (81%)] Loss: 1.97013 (semantic_loss: 0.02363, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18431 
Train Epoch: 13 [816/1000 26112/32000 (82%)] Loss: 1.96693 (semantic_loss: 0.01946, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18612 
Train Epoch: 13 [821/1000 26272/32000 (82%)] Loss: 1.96396 (semantic_loss: 0.01843, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18644 
Train Epoch: 13 [826/1000 26432/32000 (83%)] Loss: 1.96578 (semantic_loss: 0.01831, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18555 
Train Epoch: 13 [831/1000 26592/32000 (83%)] Loss: 1.96452 (semantic_loss: 0.01606, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18977 
Train Epoch: 13 [836/1000 26752/32000 (84%)] Loss: 1.96509 (semantic_loss: 0.01859, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18691 
Train Epoch: 13 [841/1000 26912/32000 (84%)] Loss: 1.96295 (semantic_loss: 0.01548, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18732 
Train Epoch: 13 [846/1000 27072/32000 (85%)] Loss: 1.96363 (semantic_loss: 0.01615, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19539 
Train Epoch: 13 [851/1000 27232/32000 (85%)] Loss: 1.96366 (semantic_loss: 0.01715, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18873 
Train Epoch: 13 [856/1000 27392/32000 (86%)] Loss: 1.96908 (semantic_loss: 0.02160, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18786 
Train Epoch: 13 [861/1000 27552/32000 (86%)] Loss: 1.96415 (semantic_loss: 0.01668, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18941 
Train Epoch: 13 [866/1000 27712/32000 (87%)] Loss: 1.96793 (semantic_loss: 0.01947, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18749 
Train Epoch: 13 [871/1000 27872/32000 (87%)] Loss: 1.96926 (semantic_loss: 0.02276, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18824 
Train Epoch: 13 [876/1000 28032/32000 (88%)] Loss: 1.96493 (semantic_loss: 0.01940, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18920 
Train Epoch: 13 [881/1000 28192/32000 (88%)] Loss: 1.96960 (semantic_loss: 0.02211, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18920 
Train Epoch: 13 [886/1000 28352/32000 (89%)] Loss: 1.96203 (semantic_loss: 0.01553, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19936 
Train Epoch: 13 [891/1000 28512/32000 (89%)] Loss: 1.96611 (semantic_loss: 0.01863, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20881 
Train Epoch: 13 [896/1000 28672/32000 (90%)] Loss: 1.96469 (semantic_loss: 0.01722, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20245 
Train Epoch: 13 [901/1000 28832/32000 (90%)] Loss: 1.96566 (semantic_loss: 0.01721, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.23866 
Train Epoch: 13 [906/1000 28992/32000 (91%)] Loss: 1.96550 (semantic_loss: 0.01899, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22357 
Train Epoch: 13 [911/1000 29152/32000 (91%)] Loss: 1.96550 (semantic_loss: 0.01900, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23207 
Train Epoch: 13 [916/1000 29312/32000 (92%)] Loss: 1.96882 (semantic_loss: 0.02232, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21221 
Train Epoch: 13 [921/1000 29472/32000 (92%)] Loss: 1.97411 (semantic_loss: 0.02663, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21610 
Train Epoch: 13 [926/1000 29632/32000 (93%)] Loss: 1.96530 (semantic_loss: 0.01879, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21754 
Train Epoch: 13 [931/1000 29792/32000 (93%)] Loss: 1.97024 (semantic_loss: 0.02374, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19006 
Train Epoch: 13 [936/1000 29952/32000 (94%)] Loss: 1.96702 (semantic_loss: 0.01856, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18988 
Train Epoch: 13 [941/1000 30112/32000 (94%)] Loss: 1.97277 (semantic_loss: 0.02529, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19013 
Train Epoch: 13 [946/1000 30272/32000 (95%)] Loss: 1.96645 (semantic_loss: 0.01897, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19138 
Train Epoch: 13 [951/1000 30432/32000 (95%)] Loss: 1.96569 (semantic_loss: 0.01918, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19597 
Train Epoch: 13 [956/1000 30592/32000 (96%)] Loss: 1.96536 (semantic_loss: 0.01789, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19222 
Train Epoch: 13 [961/1000 30752/32000 (96%)] Loss: 1.96540 (semantic_loss: 0.01793, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20619 
Train Epoch: 13 [966/1000 30912/32000 (97%)] Loss: 1.96455 (semantic_loss: 0.01707, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19543 
Train Epoch: 13 [971/1000 31072/32000 (97%)] Loss: 1.96665 (semantic_loss: 0.01918, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18867 
Train Epoch: 13 [976/1000 31232/32000 (98%)] Loss: 1.96692 (semantic_loss: 0.01846, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19370 
Train Epoch: 13 [981/1000 31392/32000 (98%)] Loss: 1.96341 (semantic_loss: 0.01593, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19009 
Train Epoch: 13 [986/1000 31552/32000 (99%)] Loss: 1.96644 (semantic_loss: 0.01896, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.52603 
Train Epoch: 13 [991/1000 31712/32000 (99%)] Loss: 1.96885 (semantic_loss: 0.02137, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18765 
Train Epoch: 13 [996/1000 31872/32000 (100%)] Loss: 1.96641 (semantic_loss: 0.01894, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20508 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch13.pth ...
Done in 13.715s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch13.pth ...
Done in 18.031s
removing stale ckpt [epoch 12] [took 0.00s]
 epoch          : 13
 loss           : 1.9657518669366836
 learning_rate  : 1.4121476824050009e-05
 n_samples      : 416000
 n_steps        : 13000
 ActivityNet_val1_test/t2v_metrics/R1: 10.311165344722392
 ActivityNet_val1_test/t2v_metrics/R5: 36.587350010168805
 ActivityNet_val1_test/t2v_metrics/R10: 53.97600162700834
 ActivityNet_val1_test/t2v_metrics/R50: 85.94671547691682
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 50.14795607077486
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 27.307365637924285
 ActivityNet_val1_test/v2t_metrics/R1: 11.104331909701036
 ActivityNet_val1_test/v2t_metrics/R5: 37.46186699206834
 ActivityNet_val1_test/v2t_metrics/R10: 54.464104128533656
 ActivityNet_val1_test/v2t_metrics/R50: 85.94671547691682
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 51.899532235102704
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 28.29637085763665
 mnt_best       : 27.307365637924285
 not_improved_count: 0
Train Epoch: 14 [1/1000 32/32000 (0%)] Loss: 1.96412 (semantic_loss: 0.01665, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=22.40853 
Train Epoch: 14 [6/1000 192/32000 (1%)] Loss: 1.96678 (semantic_loss: 0.02029, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20096 
Train Epoch: 14 [11/1000 352/32000 (1%)] Loss: 1.96791 (semantic_loss: 0.02043, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19874 
Train Epoch: 14 [16/1000 512/32000 (2%)] Loss: 1.96428 (semantic_loss: 0.01680, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22788 
Train Epoch: 14 [21/1000 672/32000 (2%)] Loss: 1.96927 (semantic_loss: 0.02277, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18452 
Train Epoch: 14 [26/1000 832/32000 (3%)] Loss: 1.96350 (semantic_loss: 0.01700, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18802 
Train Epoch: 14 [31/1000 992/32000 (3%)] Loss: 1.97131 (semantic_loss: 0.02383, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19197 
Train Epoch: 14 [36/1000 1152/32000 (4%)] Loss: 1.96314 (semantic_loss: 0.01663, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19789 
Train Epoch: 14 [41/1000 1312/32000 (4%)] Loss: 1.96632 (semantic_loss: 0.02080, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19444 
Train Epoch: 14 [46/1000 1472/32000 (5%)] Loss: 1.96878 (semantic_loss: 0.02130, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18656 
Train Epoch: 14 [51/1000 1632/32000 (5%)] Loss: 1.96229 (semantic_loss: 0.01579, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18626 
Train Epoch: 14 [56/1000 1792/32000 (6%)] Loss: 1.96363 (semantic_loss: 0.01615, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18741 
Train Epoch: 14 [61/1000 1952/32000 (6%)] Loss: 1.96480 (semantic_loss: 0.01830, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18903 
Train Epoch: 14 [66/1000 2112/32000 (7%)] Loss: 1.96289 (semantic_loss: 0.01639, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22506 
Train Epoch: 14 [71/1000 2272/32000 (7%)] Loss: 1.96515 (semantic_loss: 0.01864, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18925 
Train Epoch: 14 [76/1000 2432/32000 (8%)] Loss: 1.96524 (semantic_loss: 0.01679, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.23628 
Train Epoch: 14 [81/1000 2592/32000 (8%)] Loss: 1.96609 (semantic_loss: 0.01861, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.62362 
Train Epoch: 14 [86/1000 2752/32000 (9%)] Loss: 1.96498 (semantic_loss: 0.01751, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18731 
Train Epoch: 14 [91/1000 2912/32000 (9%)] Loss: 1.96897 (semantic_loss: 0.02149, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18464 
Train Epoch: 14 [96/1000 3072/32000 (10%)] Loss: 1.96351 (semantic_loss: 0.01604, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21579 
Train Epoch: 14 [101/1000 3232/32000 (10%)] Loss: 1.96656 (semantic_loss: 0.01908, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18997 
Train Epoch: 14 [106/1000 3392/32000 (11%)] Loss: 1.96288 (semantic_loss: 0.01637, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18895 
Train Epoch: 14 [111/1000 3552/32000 (11%)] Loss: 1.97001 (semantic_loss: 0.02254, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18576 
Train Epoch: 14 [116/1000 3712/32000 (12%)] Loss: 1.97055 (semantic_loss: 0.02404, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19174 
Train Epoch: 14 [121/1000 3872/32000 (12%)] Loss: 1.96422 (semantic_loss: 0.01772, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20214 
Train Epoch: 14 [126/1000 4032/32000 (13%)] Loss: 1.96522 (semantic_loss: 0.01775, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21311 
Train Epoch: 14 [131/1000 4192/32000 (13%)] Loss: 1.96495 (semantic_loss: 0.01845, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.38521 
Train Epoch: 14 [136/1000 4352/32000 (14%)] Loss: 1.96520 (semantic_loss: 0.01772, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23126 
Train Epoch: 14 [141/1000 4512/32000 (14%)] Loss: 1.96730 (semantic_loss: 0.01982, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20611 
Train Epoch: 14 [146/1000 4672/32000 (15%)] Loss: 1.96241 (semantic_loss: 0.01591, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20051 
Train Epoch: 14 [151/1000 4832/32000 (15%)] Loss: 1.96212 (semantic_loss: 0.01659, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19939 
Train Epoch: 14 [156/1000 4992/32000 (16%)] Loss: 1.96127 (semantic_loss: 0.01379, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19923 
Train Epoch: 14 [161/1000 5152/32000 (16%)] Loss: 1.96614 (semantic_loss: 0.01867, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19678 
Train Epoch: 14 [166/1000 5312/32000 (17%)] Loss: 1.96848 (semantic_loss: 0.02100, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20561 
Train Epoch: 14 [171/1000 5472/32000 (17%)] Loss: 1.96276 (semantic_loss: 0.01724, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18881 
Train Epoch: 14 [176/1000 5632/32000 (18%)] Loss: 1.96799 (semantic_loss: 0.02149, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18778 
Train Epoch: 14 [181/1000 5792/32000 (18%)] Loss: 1.96176 (semantic_loss: 0.01428, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18847 
Train Epoch: 14 [186/1000 5952/32000 (19%)] Loss: 1.96547 (semantic_loss: 0.01800, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19366 
Train Epoch: 14 [191/1000 6112/32000 (19%)] Loss: 1.97350 (semantic_loss: 0.02700, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19268 
Train Epoch: 14 [196/1000 6272/32000 (20%)] Loss: 1.96477 (semantic_loss: 0.01730, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26855 
Train Epoch: 14 [201/1000 6432/32000 (20%)] Loss: 1.96205 (semantic_loss: 0.01555, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20559 
Train Epoch: 14 [206/1000 6592/32000 (21%)] Loss: 1.96580 (semantic_loss: 0.01832, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19411 
Train Epoch: 14 [211/1000 6752/32000 (21%)] Loss: 1.96344 (semantic_loss: 0.01596, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18758 
Train Epoch: 14 [216/1000 6912/32000 (22%)] Loss: 1.96845 (semantic_loss: 0.02195, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18865 
Train Epoch: 14 [221/1000 7072/32000 (22%)] Loss: 1.96498 (semantic_loss: 0.01750, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18979 
Train Epoch: 14 [226/1000 7232/32000 (23%)] Loss: 1.96517 (semantic_loss: 0.01867, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18832 
Train Epoch: 14 [231/1000 7392/32000 (23%)] Loss: 1.96502 (semantic_loss: 0.01754, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18970 
Train Epoch: 14 [236/1000 7552/32000 (24%)] Loss: 1.96688 (semantic_loss: 0.01939, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18676 
Train Epoch: 14 [241/1000 7712/32000 (24%)] Loss: 1.96243 (semantic_loss: 0.01690, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18425 
Train Epoch: 14 [246/1000 7872/32000 (25%)] Loss: 1.96847 (semantic_loss: 0.02100, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19344 
Train Epoch: 14 [251/1000 8032/32000 (25%)] Loss: 1.96593 (semantic_loss: 0.01943, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18627 
Train Epoch: 14 [256/1000 8192/32000 (26%)] Loss: 1.96772 (semantic_loss: 0.02025, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20182 
Train Epoch: 14 [261/1000 8352/32000 (26%)] Loss: 1.96069 (semantic_loss: 0.01517, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18462 
Train Epoch: 14 [266/1000 8512/32000 (27%)] Loss: 1.96348 (semantic_loss: 0.01698, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18718 
Train Epoch: 14 [271/1000 8672/32000 (27%)] Loss: 1.96443 (semantic_loss: 0.01793, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18990 
Train Epoch: 14 [276/1000 8832/32000 (28%)] Loss: 1.96152 (semantic_loss: 0.01502, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.48553 
Train Epoch: 14 [281/1000 8992/32000 (28%)] Loss: 1.96726 (semantic_loss: 0.02076, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20677 
Train Epoch: 14 [286/1000 9152/32000 (29%)] Loss: 1.96319 (semantic_loss: 0.01571, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.37963 
Train Epoch: 14 [291/1000 9312/32000 (29%)] Loss: 1.96541 (semantic_loss: 0.01794, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21373 
Train Epoch: 14 [296/1000 9472/32000 (30%)] Loss: 1.96690 (semantic_loss: 0.01943, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20566 
Train Epoch: 14 [301/1000 9632/32000 (30%)] Loss: 1.96397 (semantic_loss: 0.01747, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19352 
Train Epoch: 14 [306/1000 9792/32000 (31%)] Loss: 1.96681 (semantic_loss: 0.01836, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.25073 
Train Epoch: 14 [311/1000 9952/32000 (31%)] Loss: 1.96585 (semantic_loss: 0.01838, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25513 
Train Epoch: 14 [316/1000 10112/32000 (32%)] Loss: 1.96821 (semantic_loss: 0.02074, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19656 
Train Epoch: 14 [321/1000 10272/32000 (32%)] Loss: 1.96432 (semantic_loss: 0.01683, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21274 
Train Epoch: 14 [326/1000 10432/32000 (33%)] Loss: 1.96979 (semantic_loss: 0.02230, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19005 
Train Epoch: 14 [331/1000 10592/32000 (33%)] Loss: 1.96074 (semantic_loss: 0.01424, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18703 
Train Epoch: 14 [336/1000 10752/32000 (34%)] Loss: 1.96523 (semantic_loss: 0.01873, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21840 
Train Epoch: 14 [341/1000 10912/32000 (34%)] Loss: 1.96717 (semantic_loss: 0.01969, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18958 
Train Epoch: 14 [346/1000 11072/32000 (35%)] Loss: 1.96387 (semantic_loss: 0.01737, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20482 
Train Epoch: 14 [351/1000 11232/32000 (35%)] Loss: 1.96768 (semantic_loss: 0.02020, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19071 
Train Epoch: 14 [356/1000 11392/32000 (36%)] Loss: 1.96487 (semantic_loss: 0.01739, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21058 
Train Epoch: 14 [361/1000 11552/32000 (36%)] Loss: 1.96556 (semantic_loss: 0.02004, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.21037 
Train Epoch: 14 [366/1000 11712/32000 (37%)] Loss: 1.96876 (semantic_loss: 0.02226, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18758 
Train Epoch: 14 [371/1000 11872/32000 (37%)] Loss: 1.96320 (semantic_loss: 0.01670, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19560 
Train Epoch: 14 [376/1000 12032/32000 (38%)] Loss: 1.96606 (semantic_loss: 0.01857, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19750 
Train Epoch: 14 [381/1000 12192/32000 (38%)] Loss: 1.96524 (semantic_loss: 0.01874, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18890 
Train Epoch: 14 [386/1000 12352/32000 (39%)] Loss: 1.96661 (semantic_loss: 0.02011, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22596 
Train Epoch: 14 [391/1000 12512/32000 (39%)] Loss: 1.96115 (semantic_loss: 0.01466, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18741 
Train Epoch: 14 [396/1000 12672/32000 (40%)] Loss: 1.96456 (semantic_loss: 0.01708, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23919 
Train Epoch: 14 [401/1000 12832/32000 (40%)] Loss: 1.96869 (semantic_loss: 0.02218, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.64821 
Train Epoch: 14 [406/1000 12992/32000 (41%)] Loss: 1.97097 (semantic_loss: 0.02251, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19833 
Train Epoch: 14 [411/1000 13152/32000 (41%)] Loss: 1.96268 (semantic_loss: 0.01521, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18860 
Train Epoch: 14 [416/1000 13312/32000 (42%)] Loss: 1.96723 (semantic_loss: 0.01877, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18737 
Train Epoch: 14 [421/1000 13472/32000 (42%)] Loss: 1.96394 (semantic_loss: 0.01647, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19375 
Train Epoch: 14 [426/1000 13632/32000 (43%)] Loss: 1.97061 (semantic_loss: 0.02313, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22831 
Train Epoch: 14 [431/1000 13792/32000 (43%)] Loss: 1.96533 (semantic_loss: 0.01883, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21229 
Train Epoch: 14 [436/1000 13952/32000 (44%)] Loss: 1.96705 (semantic_loss: 0.01860, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21352 
Train Epoch: 14 [441/1000 14112/32000 (44%)] Loss: 1.96155 (semantic_loss: 0.01505, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22445 
Train Epoch: 14 [446/1000 14272/32000 (45%)] Loss: 1.97021 (semantic_loss: 0.02176, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19993 
Train Epoch: 14 [451/1000 14432/32000 (45%)] Loss: 1.96513 (semantic_loss: 0.01765, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.31186 
Train Epoch: 14 [456/1000 14592/32000 (46%)] Loss: 1.96679 (semantic_loss: 0.01931, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19554 
Train Epoch: 14 [461/1000 14752/32000 (46%)] Loss: 1.97305 (semantic_loss: 0.02557, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19987 
Train Epoch: 14 [466/1000 14912/32000 (47%)] Loss: 1.96570 (semantic_loss: 0.01822, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18693 
Train Epoch: 14 [471/1000 15072/32000 (47%)] Loss: 1.96705 (semantic_loss: 0.01958, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18578 
Train Epoch: 14 [476/1000 15232/32000 (48%)] Loss: 1.96181 (semantic_loss: 0.01629, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18614 
Train Epoch: 14 [481/1000 15392/32000 (48%)] Loss: 1.96177 (semantic_loss: 0.01430, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18656 
Train Epoch: 14 [486/1000 15552/32000 (49%)] Loss: 1.96697 (semantic_loss: 0.01949, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18780 
Train Epoch: 14 [491/1000 15712/32000 (49%)] Loss: 1.96215 (semantic_loss: 0.01565, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20873 
Train Epoch: 14 [496/1000 15872/32000 (50%)] Loss: 1.96338 (semantic_loss: 0.01590, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22903 
Train Epoch: 14 [501/1000 16032/32000 (50%)] Loss: 1.96340 (semantic_loss: 0.01690, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19152 
Train Epoch: 14 [506/1000 16192/32000 (51%)] Loss: 1.96981 (semantic_loss: 0.02429, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18656 
Train Epoch: 14 [511/1000 16352/32000 (51%)] Loss: 1.96643 (semantic_loss: 0.01896, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18725 
Train Epoch: 14 [516/1000 16512/32000 (52%)] Loss: 1.96560 (semantic_loss: 0.01812, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23868 
Train Epoch: 14 [521/1000 16672/32000 (52%)] Loss: 1.96478 (semantic_loss: 0.01730, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20326 
Train Epoch: 14 [526/1000 16832/32000 (53%)] Loss: 1.96414 (semantic_loss: 0.01764, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18576 
Train Epoch: 14 [531/1000 16992/32000 (53%)] Loss: 1.96290 (semantic_loss: 0.01542, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18666 
Train Epoch: 14 [536/1000 17152/32000 (54%)] Loss: 1.96056 (semantic_loss: 0.01407, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18648 
Train Epoch: 14 [541/1000 17312/32000 (54%)] Loss: 1.96586 (semantic_loss: 0.01838, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18701 
Train Epoch: 14 [546/1000 17472/32000 (55%)] Loss: 1.96696 (semantic_loss: 0.01850, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18968 
Train Epoch: 14 [551/1000 17632/32000 (55%)] Loss: 1.96528 (semantic_loss: 0.01781, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20258 
Train Epoch: 14 [556/1000 17792/32000 (56%)] Loss: 1.96395 (semantic_loss: 0.01549, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18750 
Train Epoch: 14 [561/1000 17952/32000 (56%)] Loss: 1.96396 (semantic_loss: 0.01648, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19025 
Train Epoch: 14 [566/1000 18112/32000 (57%)] Loss: 1.96208 (semantic_loss: 0.01558, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19480 
Train Epoch: 14 [571/1000 18272/32000 (57%)] Loss: 1.96338 (semantic_loss: 0.01590, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18914 
Train Epoch: 14 [576/1000 18432/32000 (58%)] Loss: 1.96350 (semantic_loss: 0.01602, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18686 
Train Epoch: 14 [581/1000 18592/32000 (58%)] Loss: 1.96583 (semantic_loss: 0.01834, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19330 
Train Epoch: 14 [586/1000 18752/32000 (59%)] Loss: 1.96435 (semantic_loss: 0.01688, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18621 
Train Epoch: 14 [591/1000 18912/32000 (59%)] Loss: 1.96571 (semantic_loss: 0.01921, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20998 
Train Epoch: 14 [596/1000 19072/32000 (60%)] Loss: 1.96438 (semantic_loss: 0.01886, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.52892 
Train Epoch: 14 [601/1000 19232/32000 (60%)] Loss: 1.96449 (semantic_loss: 0.01799, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22567 
Train Epoch: 14 [606/1000 19392/32000 (61%)] Loss: 1.96081 (semantic_loss: 0.01528, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.35743 
Train Epoch: 14 [611/1000 19552/32000 (61%)] Loss: 1.96903 (semantic_loss: 0.02057, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19298 
Train Epoch: 14 [616/1000 19712/32000 (62%)] Loss: 1.96888 (semantic_loss: 0.02238, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21402 
Train Epoch: 14 [621/1000 19872/32000 (62%)] Loss: 1.96226 (semantic_loss: 0.01478, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22860 
Train Epoch: 14 [626/1000 20032/32000 (63%)] Loss: 1.96379 (semantic_loss: 0.01632, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24381 
Train Epoch: 14 [631/1000 20192/32000 (63%)] Loss: 1.96537 (semantic_loss: 0.01692, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21228 
Train Epoch: 14 [636/1000 20352/32000 (64%)] Loss: 1.96357 (semantic_loss: 0.01707, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19617 
Train Epoch: 14 [641/1000 20512/32000 (64%)] Loss: 1.96755 (semantic_loss: 0.02007, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19077 
Train Epoch: 14 [646/1000 20672/32000 (65%)] Loss: 1.96760 (semantic_loss: 0.01915, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21325 
Train Epoch: 14 [651/1000 20832/32000 (65%)] Loss: 1.96108 (semantic_loss: 0.01458, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19701 
Train Epoch: 14 [656/1000 20992/32000 (66%)] Loss: 1.96342 (semantic_loss: 0.01788, quant_loss: 1.94531, bit_balance_loss: 0.00023) batch_time=0.19311 
Train Epoch: 14 [661/1000 21152/32000 (66%)] Loss: 1.96429 (semantic_loss: 0.01876, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19109 
Train Epoch: 14 [666/1000 21312/32000 (67%)] Loss: 1.96695 (semantic_loss: 0.01849, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18592 
Train Epoch: 14 [671/1000 21472/32000 (67%)] Loss: 1.96822 (semantic_loss: 0.02171, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18733 
Train Epoch: 14 [676/1000 21632/32000 (68%)] Loss: 1.96326 (semantic_loss: 0.01676, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18740 
Train Epoch: 14 [681/1000 21792/32000 (68%)] Loss: 1.96527 (semantic_loss: 0.01876, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18655 
Train Epoch: 14 [686/1000 21952/32000 (69%)] Loss: 1.96578 (semantic_loss: 0.01927, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18642 
Train Epoch: 14 [691/1000 22112/32000 (69%)] Loss: 1.96282 (semantic_loss: 0.01534, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18711 
Train Epoch: 14 [696/1000 22272/32000 (70%)] Loss: 1.96305 (semantic_loss: 0.01557, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20914 
Train Epoch: 14 [701/1000 22432/32000 (70%)] Loss: 1.96036 (semantic_loss: 0.01386, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18674 
Train Epoch: 14 [706/1000 22592/32000 (71%)] Loss: 1.96681 (semantic_loss: 0.02030, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18684 
Train Epoch: 14 [711/1000 22752/32000 (71%)] Loss: 1.96637 (semantic_loss: 0.01890, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18813 
Train Epoch: 14 [716/1000 22912/32000 (72%)] Loss: 1.96654 (semantic_loss: 0.02004, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18752 
Train Epoch: 14 [721/1000 23072/32000 (72%)] Loss: 1.96048 (semantic_loss: 0.01496, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18669 
Train Epoch: 14 [726/1000 23232/32000 (73%)] Loss: 1.96607 (semantic_loss: 0.01860, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18643 
Train Epoch: 14 [731/1000 23392/32000 (73%)] Loss: 1.96390 (semantic_loss: 0.01740, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20845 
Train Epoch: 14 [736/1000 23552/32000 (74%)] Loss: 1.96567 (semantic_loss: 0.01916, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19607 
Train Epoch: 14 [741/1000 23712/32000 (74%)] Loss: 1.96374 (semantic_loss: 0.01723, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20960 
Train Epoch: 14 [746/1000 23872/32000 (75%)] Loss: 1.96617 (semantic_loss: 0.01771, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.23950 
Train Epoch: 14 [751/1000 24032/32000 (75%)] Loss: 1.96645 (semantic_loss: 0.01898, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22430 
Train Epoch: 14 [756/1000 24192/32000 (76%)] Loss: 1.96229 (semantic_loss: 0.01482, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20042 
Train Epoch: 14 [761/1000 24352/32000 (76%)] Loss: 1.96365 (semantic_loss: 0.01618, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22515 
Train Epoch: 14 [766/1000 24512/32000 (77%)] Loss: 1.97087 (semantic_loss: 0.02241, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20008 
Train Epoch: 14 [771/1000 24672/32000 (77%)] Loss: 1.96828 (semantic_loss: 0.02081, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20035 
Train Epoch: 14 [776/1000 24832/32000 (78%)] Loss: 1.96579 (semantic_loss: 0.01928, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19011 
Train Epoch: 14 [781/1000 24992/32000 (78%)] Loss: 1.96424 (semantic_loss: 0.01676, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19087 
Train Epoch: 14 [786/1000 25152/32000 (79%)] Loss: 1.96438 (semantic_loss: 0.01690, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18853 
Train Epoch: 14 [791/1000 25312/32000 (79%)] Loss: 1.96423 (semantic_loss: 0.01675, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19040 
Train Epoch: 14 [796/1000 25472/32000 (80%)] Loss: 1.96518 (semantic_loss: 0.01770, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18990 
Train Epoch: 14 [801/1000 25632/32000 (80%)] Loss: 1.96553 (semantic_loss: 0.01805, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21708 
Train Epoch: 14 [806/1000 25792/32000 (81%)] Loss: 1.96780 (semantic_loss: 0.02130, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19961 
Train Epoch: 14 [811/1000 25952/32000 (81%)] Loss: 1.96594 (semantic_loss: 0.01944, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19209 
Train Epoch: 14 [816/1000 26112/32000 (82%)] Loss: 1.96338 (semantic_loss: 0.01687, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19749 
Train Epoch: 14 [821/1000 26272/32000 (82%)] Loss: 1.96284 (semantic_loss: 0.01634, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=1.04885 
Train Epoch: 14 [826/1000 26432/32000 (83%)] Loss: 1.96817 (semantic_loss: 0.02070, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18917 
Train Epoch: 14 [831/1000 26592/32000 (83%)] Loss: 1.96051 (semantic_loss: 0.01401, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18813 
Train Epoch: 14 [836/1000 26752/32000 (84%)] Loss: 1.96507 (semantic_loss: 0.01955, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18984 
Train Epoch: 14 [841/1000 26912/32000 (84%)] Loss: 1.96793 (semantic_loss: 0.01948, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18847 
Train Epoch: 14 [846/1000 27072/32000 (85%)] Loss: 1.96456 (semantic_loss: 0.01904, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19056 
Train Epoch: 14 [851/1000 27232/32000 (85%)] Loss: 1.96525 (semantic_loss: 0.01874, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18923 
Train Epoch: 14 [856/1000 27392/32000 (86%)] Loss: 1.96553 (semantic_loss: 0.01707, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18739 
Train Epoch: 14 [861/1000 27552/32000 (86%)] Loss: 1.96514 (semantic_loss: 0.01669, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18796 
Train Epoch: 14 [866/1000 27712/32000 (87%)] Loss: 1.96808 (semantic_loss: 0.01963, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18823 
Train Epoch: 14 [871/1000 27872/32000 (87%)] Loss: 1.96239 (semantic_loss: 0.01490, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19455 
Train Epoch: 14 [876/1000 28032/32000 (88%)] Loss: 1.96357 (semantic_loss: 0.01707, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19649 
Train Epoch: 14 [881/1000 28192/32000 (88%)] Loss: 1.96543 (semantic_loss: 0.01795, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19360 
Train Epoch: 14 [886/1000 28352/32000 (89%)] Loss: 1.96821 (semantic_loss: 0.02171, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18988 
Train Epoch: 14 [891/1000 28512/32000 (89%)] Loss: 1.96262 (semantic_loss: 0.01515, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20585 
Train Epoch: 14 [896/1000 28672/32000 (90%)] Loss: 1.96211 (semantic_loss: 0.01561, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22190 
Train Epoch: 14 [901/1000 28832/32000 (90%)] Loss: 1.96157 (semantic_loss: 0.01605, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.21340 
Train Epoch: 14 [906/1000 28992/32000 (91%)] Loss: 1.96358 (semantic_loss: 0.01709, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21620 
Train Epoch: 14 [911/1000 29152/32000 (91%)] Loss: 1.96720 (semantic_loss: 0.02070, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19541 
Train Epoch: 14 [916/1000 29312/32000 (92%)] Loss: 1.96410 (semantic_loss: 0.01662, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.42229 
Train Epoch: 14 [921/1000 29472/32000 (92%)] Loss: 1.96118 (semantic_loss: 0.01566, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20671 
Train Epoch: 14 [926/1000 29632/32000 (93%)] Loss: 1.96344 (semantic_loss: 0.01693, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.34974 
Train Epoch: 14 [931/1000 29792/32000 (93%)] Loss: 1.96485 (semantic_loss: 0.01737, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19662 
Train Epoch: 14 [936/1000 29952/32000 (94%)] Loss: 1.96661 (semantic_loss: 0.02011, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18795 
Train Epoch: 14 [941/1000 30112/32000 (94%)] Loss: 1.96521 (semantic_loss: 0.01871, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18798 
Train Epoch: 14 [946/1000 30272/32000 (95%)] Loss: 1.96380 (semantic_loss: 0.01632, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23877 
Train Epoch: 14 [951/1000 30432/32000 (95%)] Loss: 1.96498 (semantic_loss: 0.01945, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18948 
Train Epoch: 14 [956/1000 30592/32000 (96%)] Loss: 1.96176 (semantic_loss: 0.01526, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19776 
Train Epoch: 14 [961/1000 30752/32000 (96%)] Loss: 1.96729 (semantic_loss: 0.01980, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19601 
Train Epoch: 14 [966/1000 30912/32000 (97%)] Loss: 1.96882 (semantic_loss: 0.02134, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19219 
Train Epoch: 14 [971/1000 31072/32000 (97%)] Loss: 1.96433 (semantic_loss: 0.01686, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18647 
Train Epoch: 14 [976/1000 31232/32000 (98%)] Loss: 1.96708 (semantic_loss: 0.02058, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18977 
Train Epoch: 14 [981/1000 31392/32000 (98%)] Loss: 1.96397 (semantic_loss: 0.01747, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21043 
Train Epoch: 14 [986/1000 31552/32000 (99%)] Loss: 1.96205 (semantic_loss: 0.01555, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18635 
Train Epoch: 14 [991/1000 31712/32000 (99%)] Loss: 1.96833 (semantic_loss: 0.01987, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19042 
Train Epoch: 14 [996/1000 31872/32000 (100%)] Loss: 1.96761 (semantic_loss: 0.02012, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18427 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch14.pth ...
Done in 5.182s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch14.pth ...
Done in 9.879s
removing stale ckpt [epoch 13] [took 0.01s]
 epoch          : 14
 loss           : 1.965178817987442
 learning_rate  : 1.2709329141645008e-05
 n_samples      : 448000
 n_steps        : 14000
 ActivityNet_val1_test/t2v_metrics/R1: 11.328045556233477
 ActivityNet_val1_test/t2v_metrics/R5: 36.42464917632703
 ActivityNet_val1_test/t2v_metrics/R10: 53.89465121008745
 ActivityNet_val1_test/t2v_metrics/R50: 86.23144193613992
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 50.210494203782794
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 28.12108091485615
 ActivityNet_val1_test/v2t_metrics/R1: 12.365263371974782
 ActivityNet_val1_test/v2t_metrics/R5: 37.74659345129144
 ActivityNet_val1_test/v2t_metrics/R10: 55.1555826723612
 ActivityNet_val1_test/v2t_metrics/R50: 85.98739068537726
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 51.279743746186696
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.52728644400903
 mnt_best       : 28.12108091485615
 not_improved_count: 0
Train Epoch: 15 [1/1000 32/32000 (0%)] Loss: 1.96460 (semantic_loss: 0.01809, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=21.33155 
Train Epoch: 15 [6/1000 192/32000 (1%)] Loss: 1.96776 (semantic_loss: 0.02029, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20777 
Train Epoch: 15 [11/1000 352/32000 (1%)] Loss: 1.96538 (semantic_loss: 0.01693, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22371 
Train Epoch: 15 [16/1000 512/32000 (2%)] Loss: 1.96299 (semantic_loss: 0.01551, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=1.22524 
Train Epoch: 15 [21/1000 672/32000 (2%)] Loss: 1.96699 (semantic_loss: 0.02048, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20432 
Train Epoch: 15 [26/1000 832/32000 (3%)] Loss: 1.96776 (semantic_loss: 0.02223, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.19471 
Train Epoch: 15 [31/1000 992/32000 (3%)] Loss: 1.96518 (semantic_loss: 0.01672, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20009 
Train Epoch: 15 [36/1000 1152/32000 (4%)] Loss: 1.96420 (semantic_loss: 0.01770, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19979 
Train Epoch: 15 [41/1000 1312/32000 (4%)] Loss: 1.96537 (semantic_loss: 0.01790, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19005 
Train Epoch: 15 [46/1000 1472/32000 (5%)] Loss: 1.96956 (semantic_loss: 0.02209, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18708 
Train Epoch: 15 [51/1000 1632/32000 (5%)] Loss: 1.96302 (semantic_loss: 0.01555, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18521 
Train Epoch: 15 [56/1000 1792/32000 (6%)] Loss: 1.96125 (semantic_loss: 0.01475, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21842 
Train Epoch: 15 [61/1000 1952/32000 (6%)] Loss: 1.96284 (semantic_loss: 0.01634, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24240 
Train Epoch: 15 [66/1000 2112/32000 (7%)] Loss: 1.95856 (semantic_loss: 0.01304, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.24571 
Train Epoch: 15 [71/1000 2272/32000 (7%)] Loss: 1.97222 (semantic_loss: 0.02377, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19902 
Train Epoch: 15 [76/1000 2432/32000 (8%)] Loss: 1.96975 (semantic_loss: 0.02325, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21435 
Train Epoch: 15 [81/1000 2592/32000 (8%)] Loss: 1.96171 (semantic_loss: 0.01424, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18586 
Train Epoch: 15 [86/1000 2752/32000 (9%)] Loss: 1.96987 (semantic_loss: 0.02141, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18661 
Train Epoch: 15 [91/1000 2912/32000 (9%)] Loss: 1.96502 (semantic_loss: 0.01753, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18708 
Train Epoch: 15 [96/1000 3072/32000 (10%)] Loss: 1.96422 (semantic_loss: 0.01674, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19285 
Train Epoch: 15 [101/1000 3232/32000 (10%)] Loss: 1.97010 (semantic_loss: 0.02263, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18760 
Train Epoch: 15 [106/1000 3392/32000 (11%)] Loss: 1.96308 (semantic_loss: 0.01560, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18510 
Train Epoch: 15 [111/1000 3552/32000 (11%)] Loss: 1.96539 (semantic_loss: 0.01792, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18621 
Train Epoch: 15 [116/1000 3712/32000 (12%)] Loss: 1.96232 (semantic_loss: 0.01582, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18396 
Train Epoch: 15 [121/1000 3872/32000 (12%)] Loss: 1.96372 (semantic_loss: 0.01625, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18932 
Train Epoch: 15 [126/1000 4032/32000 (13%)] Loss: 1.96252 (semantic_loss: 0.01504, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18714 
Train Epoch: 15 [131/1000 4192/32000 (13%)] Loss: 1.96843 (semantic_loss: 0.02096, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.43615 
Train Epoch: 15 [136/1000 4352/32000 (14%)] Loss: 1.96381 (semantic_loss: 0.01634, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18775 
Train Epoch: 15 [141/1000 4512/32000 (14%)] Loss: 1.96966 (semantic_loss: 0.02219, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18761 
Train Epoch: 15 [146/1000 4672/32000 (15%)] Loss: 1.96391 (semantic_loss: 0.01643, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18844 
Train Epoch: 15 [151/1000 4832/32000 (15%)] Loss: 1.96547 (semantic_loss: 0.01799, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19676 
Train Epoch: 15 [156/1000 4992/32000 (16%)] Loss: 1.97268 (semantic_loss: 0.02422, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22411 
Train Epoch: 15 [161/1000 5152/32000 (16%)] Loss: 1.96322 (semantic_loss: 0.01477, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.26408 
Train Epoch: 15 [166/1000 5312/32000 (17%)] Loss: 1.96603 (semantic_loss: 0.01757, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20080 
Train Epoch: 15 [171/1000 5472/32000 (17%)] Loss: 1.96284 (semantic_loss: 0.01537, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19787 
Train Epoch: 15 [176/1000 5632/32000 (18%)] Loss: 1.96550 (semantic_loss: 0.01802, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22074 
Train Epoch: 15 [181/1000 5792/32000 (18%)] Loss: 1.96652 (semantic_loss: 0.01806, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19868 
Train Epoch: 15 [186/1000 5952/32000 (19%)] Loss: 1.96333 (semantic_loss: 0.01585, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22763 
Train Epoch: 15 [191/1000 6112/32000 (19%)] Loss: 1.96329 (semantic_loss: 0.01386, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.44325 
Train Epoch: 15 [196/1000 6272/32000 (20%)] Loss: 1.96377 (semantic_loss: 0.01630, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18586 
Train Epoch: 15 [201/1000 6432/32000 (20%)] Loss: 1.96672 (semantic_loss: 0.01827, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18641 
Train Epoch: 15 [206/1000 6592/32000 (21%)] Loss: 1.96443 (semantic_loss: 0.01598, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18677 
Train Epoch: 15 [211/1000 6752/32000 (21%)] Loss: 1.96382 (semantic_loss: 0.01635, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21135 
Train Epoch: 15 [216/1000 6912/32000 (22%)] Loss: 1.96402 (semantic_loss: 0.01751, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19877 
Train Epoch: 15 [221/1000 7072/32000 (22%)] Loss: 1.97092 (semantic_loss: 0.02245, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19936 
Train Epoch: 15 [226/1000 7232/32000 (23%)] Loss: 1.96854 (semantic_loss: 0.02106, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18832 
Train Epoch: 15 [231/1000 7392/32000 (23%)] Loss: 1.96435 (semantic_loss: 0.01589, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.26855 
Train Epoch: 15 [236/1000 7552/32000 (24%)] Loss: 1.96309 (semantic_loss: 0.01561, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18781 
Train Epoch: 15 [241/1000 7712/32000 (24%)] Loss: 1.97040 (semantic_loss: 0.02292, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18564 
Train Epoch: 15 [246/1000 7872/32000 (25%)] Loss: 1.96469 (semantic_loss: 0.01819, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20307 
Train Epoch: 15 [251/1000 8032/32000 (25%)] Loss: 1.96251 (semantic_loss: 0.01601, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20323 
Train Epoch: 15 [256/1000 8192/32000 (26%)] Loss: 1.96497 (semantic_loss: 0.01750, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18956 
Train Epoch: 15 [261/1000 8352/32000 (26%)] Loss: 1.96509 (semantic_loss: 0.01859, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21764 
Train Epoch: 15 [266/1000 8512/32000 (27%)] Loss: 1.96311 (semantic_loss: 0.01660, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18612 
Train Epoch: 15 [271/1000 8672/32000 (27%)] Loss: 1.96444 (semantic_loss: 0.01794, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19285 
Train Epoch: 15 [276/1000 8832/32000 (28%)] Loss: 1.96555 (semantic_loss: 0.01905, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18450 
Train Epoch: 15 [281/1000 8992/32000 (28%)] Loss: 1.96276 (semantic_loss: 0.01529, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21055 
Train Epoch: 15 [286/1000 9152/32000 (29%)] Loss: 1.96870 (semantic_loss: 0.01927, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.18524 
Train Epoch: 15 [291/1000 9312/32000 (29%)] Loss: 1.96255 (semantic_loss: 0.01605, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18888 
Train Epoch: 15 [296/1000 9472/32000 (30%)] Loss: 1.96746 (semantic_loss: 0.02097, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19661 
Train Epoch: 15 [301/1000 9632/32000 (30%)] Loss: 1.96530 (semantic_loss: 0.01586, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.19048 
Train Epoch: 15 [306/1000 9792/32000 (31%)] Loss: 1.96620 (semantic_loss: 0.01872, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22447 
Train Epoch: 15 [311/1000 9952/32000 (31%)] Loss: 1.96492 (semantic_loss: 0.01744, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23572 
Train Epoch: 15 [316/1000 10112/32000 (32%)] Loss: 1.96209 (semantic_loss: 0.01558, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21021 
Train Epoch: 15 [321/1000 10272/32000 (32%)] Loss: 1.96502 (semantic_loss: 0.01852, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.27849 
Train Epoch: 15 [326/1000 10432/32000 (33%)] Loss: 1.96523 (semantic_loss: 0.01775, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19465 
Train Epoch: 15 [331/1000 10592/32000 (33%)] Loss: 1.96813 (semantic_loss: 0.02065, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19627 
Train Epoch: 15 [336/1000 10752/32000 (34%)] Loss: 1.96653 (semantic_loss: 0.02002, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=1.08199 
Train Epoch: 15 [341/1000 10912/32000 (34%)] Loss: 1.96467 (semantic_loss: 0.01817, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20091 
Train Epoch: 15 [346/1000 11072/32000 (35%)] Loss: 1.96256 (semantic_loss: 0.01606, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18821 
Train Epoch: 15 [351/1000 11232/32000 (35%)] Loss: 1.96588 (semantic_loss: 0.01938, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18889 
Train Epoch: 15 [356/1000 11392/32000 (36%)] Loss: 1.96310 (semantic_loss: 0.01562, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18888 
Train Epoch: 15 [361/1000 11552/32000 (36%)] Loss: 1.96402 (semantic_loss: 0.01654, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18957 
Train Epoch: 15 [366/1000 11712/32000 (37%)] Loss: 1.96615 (semantic_loss: 0.01964, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18783 
Train Epoch: 15 [371/1000 11872/32000 (37%)] Loss: 1.96493 (semantic_loss: 0.01843, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19114 
Train Epoch: 15 [376/1000 12032/32000 (38%)] Loss: 1.96573 (semantic_loss: 0.01824, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22711 
Train Epoch: 15 [381/1000 12192/32000 (38%)] Loss: 1.96348 (semantic_loss: 0.01600, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24024 
Train Epoch: 15 [386/1000 12352/32000 (39%)] Loss: 1.96819 (semantic_loss: 0.01974, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.24587 
Train Epoch: 15 [391/1000 12512/32000 (39%)] Loss: 1.96929 (semantic_loss: 0.02278, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18383 
Train Epoch: 15 [396/1000 12672/32000 (40%)] Loss: 1.96533 (semantic_loss: 0.01785, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20504 
Train Epoch: 15 [401/1000 12832/32000 (40%)] Loss: 1.96734 (semantic_loss: 0.01986, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18683 
Train Epoch: 15 [406/1000 12992/32000 (41%)] Loss: 1.96418 (semantic_loss: 0.01670, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18516 
Train Epoch: 15 [411/1000 13152/32000 (41%)] Loss: 1.96417 (semantic_loss: 0.01768, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18572 
Train Epoch: 15 [416/1000 13312/32000 (42%)] Loss: 1.96497 (semantic_loss: 0.01749, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20016 
Train Epoch: 15 [421/1000 13472/32000 (42%)] Loss: 1.96476 (semantic_loss: 0.01728, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19207 
Train Epoch: 15 [426/1000 13632/32000 (43%)] Loss: 1.96264 (semantic_loss: 0.01614, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18804 
Train Epoch: 15 [431/1000 13792/32000 (43%)] Loss: 1.96280 (semantic_loss: 0.01533, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18682 
Train Epoch: 15 [436/1000 13952/32000 (44%)] Loss: 1.96800 (semantic_loss: 0.02148, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18636 
Train Epoch: 15 [441/1000 14112/32000 (44%)] Loss: 1.96485 (semantic_loss: 0.01835, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18578 
Train Epoch: 15 [446/1000 14272/32000 (45%)] Loss: 1.96210 (semantic_loss: 0.01657, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18841 
Train Epoch: 15 [451/1000 14432/32000 (45%)] Loss: 1.96390 (semantic_loss: 0.01643, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.34659 
Train Epoch: 15 [456/1000 14592/32000 (46%)] Loss: 1.96299 (semantic_loss: 0.01454, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18512 
Train Epoch: 15 [461/1000 14752/32000 (46%)] Loss: 1.96482 (semantic_loss: 0.01735, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19870 
Train Epoch: 15 [466/1000 14912/32000 (47%)] Loss: 1.96699 (semantic_loss: 0.01952, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21934 
Train Epoch: 15 [471/1000 15072/32000 (47%)] Loss: 1.96712 (semantic_loss: 0.01866, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.23188 
Train Epoch: 15 [476/1000 15232/32000 (48%)] Loss: 1.96306 (semantic_loss: 0.01558, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19399 
Train Epoch: 15 [481/1000 15392/32000 (48%)] Loss: 1.96262 (semantic_loss: 0.01515, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23732 
Train Epoch: 15 [486/1000 15552/32000 (49%)] Loss: 1.96465 (semantic_loss: 0.01815, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22043 
Train Epoch: 15 [491/1000 15712/32000 (49%)] Loss: 1.96166 (semantic_loss: 0.01419, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21205 
Train Epoch: 15 [496/1000 15872/32000 (50%)] Loss: 1.96108 (semantic_loss: 0.01458, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19797 
Train Epoch: 15 [501/1000 16032/32000 (50%)] Loss: 1.97042 (semantic_loss: 0.02294, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18961 
Train Epoch: 15 [506/1000 16192/32000 (51%)] Loss: 1.96469 (semantic_loss: 0.01623, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20395 
Train Epoch: 15 [511/1000 16352/32000 (51%)] Loss: 1.96160 (semantic_loss: 0.01510, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19693 
Train Epoch: 15 [516/1000 16512/32000 (52%)] Loss: 1.96636 (semantic_loss: 0.01986, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18800 
Train Epoch: 15 [521/1000 16672/32000 (52%)] Loss: 1.96319 (semantic_loss: 0.01670, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19000 
Train Epoch: 15 [526/1000 16832/32000 (53%)] Loss: 1.96093 (semantic_loss: 0.01443, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19082 
Train Epoch: 15 [531/1000 16992/32000 (53%)] Loss: 1.96409 (semantic_loss: 0.01661, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18976 
Train Epoch: 15 [536/1000 17152/32000 (54%)] Loss: 1.96423 (semantic_loss: 0.01772, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18678 
Train Epoch: 15 [541/1000 17312/32000 (54%)] Loss: 1.96357 (semantic_loss: 0.01512, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18674 
Train Epoch: 15 [546/1000 17472/32000 (55%)] Loss: 1.96945 (semantic_loss: 0.02198, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18877 
Train Epoch: 15 [551/1000 17632/32000 (55%)] Loss: 1.96519 (semantic_loss: 0.01771, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.27332 
Train Epoch: 15 [556/1000 17792/32000 (56%)] Loss: 1.96325 (semantic_loss: 0.01674, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19135 
Train Epoch: 15 [561/1000 17952/32000 (56%)] Loss: 1.96451 (semantic_loss: 0.01801, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19486 
Train Epoch: 15 [566/1000 18112/32000 (57%)] Loss: 1.96472 (semantic_loss: 0.01822, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20600 
Train Epoch: 15 [571/1000 18272/32000 (57%)] Loss: 1.96191 (semantic_loss: 0.01542, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20433 
Train Epoch: 15 [576/1000 18432/32000 (58%)] Loss: 1.96260 (semantic_loss: 0.01610, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18765 
Train Epoch: 15 [581/1000 18592/32000 (58%)] Loss: 1.96344 (semantic_loss: 0.01596, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22121 
Train Epoch: 15 [586/1000 18752/32000 (59%)] Loss: 1.96778 (semantic_loss: 0.02226, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18554 
Train Epoch: 15 [591/1000 18912/32000 (59%)] Loss: 1.96355 (semantic_loss: 0.01704, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19401 
Train Epoch: 15 [596/1000 19072/32000 (60%)] Loss: 1.96319 (semantic_loss: 0.01669, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19325 
Train Epoch: 15 [601/1000 19232/32000 (60%)] Loss: 1.96431 (semantic_loss: 0.01879, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20360 
Train Epoch: 15 [606/1000 19392/32000 (61%)] Loss: 1.96530 (semantic_loss: 0.01782, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18536 
Train Epoch: 15 [611/1000 19552/32000 (61%)] Loss: 1.96836 (semantic_loss: 0.02187, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18735 
Train Epoch: 15 [616/1000 19712/32000 (62%)] Loss: 1.96728 (semantic_loss: 0.02078, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23012 
Train Epoch: 15 [621/1000 19872/32000 (62%)] Loss: 1.96367 (semantic_loss: 0.01717, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22716 
Train Epoch: 15 [626/1000 20032/32000 (63%)] Loss: 1.96264 (semantic_loss: 0.01516, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22535 
Train Epoch: 15 [631/1000 20192/32000 (63%)] Loss: 1.96943 (semantic_loss: 0.02293, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20089 
Train Epoch: 15 [636/1000 20352/32000 (64%)] Loss: 1.96180 (semantic_loss: 0.01530, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20594 
Train Epoch: 15 [641/1000 20512/32000 (64%)] Loss: 1.96469 (semantic_loss: 0.01624, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.23530 
Train Epoch: 15 [646/1000 20672/32000 (65%)] Loss: 1.96479 (semantic_loss: 0.01732, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20424 
Train Epoch: 15 [651/1000 20832/32000 (65%)] Loss: 1.96600 (semantic_loss: 0.01755, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18913 
Train Epoch: 15 [656/1000 20992/32000 (66%)] Loss: 1.96507 (semantic_loss: 0.01857, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.95584 
Train Epoch: 15 [661/1000 21152/32000 (66%)] Loss: 1.96410 (semantic_loss: 0.01662, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18645 
Train Epoch: 15 [666/1000 21312/32000 (67%)] Loss: 1.96831 (semantic_loss: 0.02083, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18405 
Train Epoch: 15 [671/1000 21472/32000 (67%)] Loss: 1.96315 (semantic_loss: 0.01664, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18502 
Train Epoch: 15 [676/1000 21632/32000 (68%)] Loss: 1.96469 (semantic_loss: 0.01721, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21018 
Train Epoch: 15 [681/1000 21792/32000 (68%)] Loss: 1.96539 (semantic_loss: 0.01694, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19141 
Train Epoch: 15 [686/1000 21952/32000 (69%)] Loss: 1.96915 (semantic_loss: 0.02070, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19910 
Train Epoch: 15 [691/1000 22112/32000 (69%)] Loss: 1.96922 (semantic_loss: 0.02271, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20925 
Train Epoch: 15 [696/1000 22272/32000 (70%)] Loss: 1.96455 (semantic_loss: 0.01610, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22216 
Train Epoch: 15 [701/1000 22432/32000 (70%)] Loss: 1.96828 (semantic_loss: 0.02177, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.24528 
Train Epoch: 15 [706/1000 22592/32000 (71%)] Loss: 1.96454 (semantic_loss: 0.01804, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23561 
Train Epoch: 15 [711/1000 22752/32000 (71%)] Loss: 1.95959 (semantic_loss: 0.01309, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20708 
Train Epoch: 15 [716/1000 22912/32000 (72%)] Loss: 1.96211 (semantic_loss: 0.01464, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22111 
Train Epoch: 15 [721/1000 23072/32000 (72%)] Loss: 1.96617 (semantic_loss: 0.01771, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18618 
Train Epoch: 15 [726/1000 23232/32000 (73%)] Loss: 1.96619 (semantic_loss: 0.01872, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18548 
Train Epoch: 15 [731/1000 23392/32000 (73%)] Loss: 1.97041 (semantic_loss: 0.02294, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18511 
Train Epoch: 15 [736/1000 23552/32000 (74%)] Loss: 1.96324 (semantic_loss: 0.01673, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18620 
Train Epoch: 15 [741/1000 23712/32000 (74%)] Loss: 1.96560 (semantic_loss: 0.01812, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20763 
Train Epoch: 15 [746/1000 23872/32000 (75%)] Loss: 1.96453 (semantic_loss: 0.01705, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21117 
Train Epoch: 15 [751/1000 24032/32000 (75%)] Loss: 1.96268 (semantic_loss: 0.01520, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20173 
Train Epoch: 15 [756/1000 24192/32000 (76%)] Loss: 1.96336 (semantic_loss: 0.01587, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18783 
Train Epoch: 15 [761/1000 24352/32000 (76%)] Loss: 1.96406 (semantic_loss: 0.01658, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18580 
Train Epoch: 15 [766/1000 24512/32000 (77%)] Loss: 1.96229 (semantic_loss: 0.01676, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18944 
Train Epoch: 15 [771/1000 24672/32000 (77%)] Loss: 1.96675 (semantic_loss: 0.02025, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.34778 
Train Epoch: 15 [776/1000 24832/32000 (78%)] Loss: 1.96516 (semantic_loss: 0.01768, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20738 
Train Epoch: 15 [781/1000 24992/32000 (78%)] Loss: 1.96415 (semantic_loss: 0.01667, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22935 
Train Epoch: 15 [786/1000 25152/32000 (79%)] Loss: 1.96861 (semantic_loss: 0.02015, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20413 
Train Epoch: 15 [791/1000 25312/32000 (79%)] Loss: 1.96824 (semantic_loss: 0.02075, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21951 
Train Epoch: 15 [796/1000 25472/32000 (80%)] Loss: 1.96510 (semantic_loss: 0.01665, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20207 
Train Epoch: 15 [801/1000 25632/32000 (80%)] Loss: 1.96348 (semantic_loss: 0.01601, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23817 
Train Epoch: 15 [806/1000 25792/32000 (81%)] Loss: 1.96203 (semantic_loss: 0.01553, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19820 
Train Epoch: 15 [811/1000 25952/32000 (81%)] Loss: 1.96184 (semantic_loss: 0.01533, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18930 
Train Epoch: 15 [816/1000 26112/32000 (82%)] Loss: 1.96657 (semantic_loss: 0.02006, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18832 
Train Epoch: 15 [821/1000 26272/32000 (82%)] Loss: 1.96466 (semantic_loss: 0.01815, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19422 
Train Epoch: 15 [826/1000 26432/32000 (83%)] Loss: 1.96122 (semantic_loss: 0.01570, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20354 
Train Epoch: 15 [831/1000 26592/32000 (83%)] Loss: 1.96171 (semantic_loss: 0.01521, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18593 
Train Epoch: 15 [836/1000 26752/32000 (84%)] Loss: 1.96313 (semantic_loss: 0.01565, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19649 
Train Epoch: 15 [841/1000 26912/32000 (84%)] Loss: 1.96635 (semantic_loss: 0.01985, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20941 
Train Epoch: 15 [846/1000 27072/32000 (85%)] Loss: 1.96669 (semantic_loss: 0.01921, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20421 
Train Epoch: 15 [851/1000 27232/32000 (85%)] Loss: 1.96745 (semantic_loss: 0.01998, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18998 
Train Epoch: 15 [856/1000 27392/32000 (86%)] Loss: 1.96327 (semantic_loss: 0.01677, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18590 
Train Epoch: 15 [861/1000 27552/32000 (86%)] Loss: 1.96301 (semantic_loss: 0.01651, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18823 
Train Epoch: 15 [866/1000 27712/32000 (87%)] Loss: 1.96533 (semantic_loss: 0.01687, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18669 
Train Epoch: 15 [871/1000 27872/32000 (87%)] Loss: 1.96653 (semantic_loss: 0.02003, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.26817 
Train Epoch: 15 [876/1000 28032/32000 (88%)] Loss: 1.96373 (semantic_loss: 0.01625, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18846 
Train Epoch: 15 [881/1000 28192/32000 (88%)] Loss: 1.96407 (semantic_loss: 0.01756, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18699 
Train Epoch: 15 [886/1000 28352/32000 (89%)] Loss: 1.96220 (semantic_loss: 0.01472, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20492 
Train Epoch: 15 [891/1000 28512/32000 (89%)] Loss: 1.96594 (semantic_loss: 0.01944, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20412 
Train Epoch: 15 [896/1000 28672/32000 (90%)] Loss: 1.96269 (semantic_loss: 0.01619, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18840 
Train Epoch: 15 [901/1000 28832/32000 (90%)] Loss: 1.96420 (semantic_loss: 0.01673, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24447 
Train Epoch: 15 [906/1000 28992/32000 (91%)] Loss: 1.96445 (semantic_loss: 0.01794, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18891 
Train Epoch: 15 [911/1000 29152/32000 (91%)] Loss: 1.96173 (semantic_loss: 0.01523, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20706 
Train Epoch: 15 [916/1000 29312/32000 (92%)] Loss: 1.96472 (semantic_loss: 0.01626, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19726 
Train Epoch: 15 [921/1000 29472/32000 (92%)] Loss: 1.96228 (semantic_loss: 0.01480, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21139 
Train Epoch: 15 [926/1000 29632/32000 (93%)] Loss: 1.96386 (semantic_loss: 0.01834, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.21846 
Train Epoch: 15 [931/1000 29792/32000 (93%)] Loss: 1.96225 (semantic_loss: 0.01574, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.22682 
Train Epoch: 15 [936/1000 29952/32000 (94%)] Loss: 1.96649 (semantic_loss: 0.01998, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20517 
Train Epoch: 15 [941/1000 30112/32000 (94%)] Loss: 1.96376 (semantic_loss: 0.01726, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19825 
Train Epoch: 15 [946/1000 30272/32000 (95%)] Loss: 1.96356 (semantic_loss: 0.01706, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19406 
Train Epoch: 15 [951/1000 30432/32000 (95%)] Loss: 1.96917 (semantic_loss: 0.02168, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20636 
Train Epoch: 15 [956/1000 30592/32000 (96%)] Loss: 1.96716 (semantic_loss: 0.01968, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19735 
Train Epoch: 15 [961/1000 30752/32000 (96%)] Loss: 1.96301 (semantic_loss: 0.01553, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22556 
Train Epoch: 15 [966/1000 30912/32000 (97%)] Loss: 1.96453 (semantic_loss: 0.01608, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18765 
Train Epoch: 15 [971/1000 31072/32000 (97%)] Loss: 1.96667 (semantic_loss: 0.01918, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18900 
Train Epoch: 15 [976/1000 31232/32000 (98%)] Loss: 1.96725 (semantic_loss: 0.02075, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.95491 
Train Epoch: 15 [981/1000 31392/32000 (98%)] Loss: 1.96276 (semantic_loss: 0.01625, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18826 
Train Epoch: 15 [986/1000 31552/32000 (99%)] Loss: 1.96188 (semantic_loss: 0.01538, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21074 
Train Epoch: 15 [991/1000 31712/32000 (99%)] Loss: 1.96462 (semantic_loss: 0.01811, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19370 
Train Epoch: 15 [996/1000 31872/32000 (100%)] Loss: 1.96467 (semantic_loss: 0.01720, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18990 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch15.pth ...
Done in 4.161s
removing stale ckpt [epoch 14] [took 0.00s]
 epoch          : 15
 loss           : 1.9647134767770766
 learning_rate  : 1.1438396227480508e-05
 n_samples      : 480000
 n_steps        : 15000
 ActivityNet_val1_test/t2v_metrics/R1: 10.636567012405939
 ActivityNet_val1_test/t2v_metrics/R5: 37.29916615822656
 ActivityNet_val1_test/t2v_metrics/R10: 53.77262558470612
 ActivityNet_val1_test/t2v_metrics/R50: 86.12975391498881
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 54.13148261134838
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 27.734517535933996
 ActivityNet_val1_test/v2t_metrics/R1: 11.592434411226357
 ActivityNet_val1_test/v2t_metrics/R5: 37.929631889363435
 ActivityNet_val1_test/v2t_metrics/R10: 54.85051860890787
 ActivityNet_val1_test/v2t_metrics/R50: 85.51962578808217
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 56.26062639821029
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 28.892026495987963
 mnt_best       : 28.12108091485615
 not_improved_count: 1
Train Epoch: 16 [1/1000 32/32000 (0%)] Loss: 1.95930 (semantic_loss: 0.01280, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=23.42959 
Train Epoch: 16 [6/1000 192/32000 (1%)] Loss: 1.96530 (semantic_loss: 0.01880, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18416 
Train Epoch: 16 [11/1000 352/32000 (1%)] Loss: 1.96566 (semantic_loss: 0.01720, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18517 
Train Epoch: 16 [16/1000 512/32000 (2%)] Loss: 1.96381 (semantic_loss: 0.01633, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.74674 
Train Epoch: 16 [21/1000 672/32000 (2%)] Loss: 1.96638 (semantic_loss: 0.01890, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18252 
Train Epoch: 16 [26/1000 832/32000 (3%)] Loss: 1.96612 (semantic_loss: 0.01864, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18412 
Train Epoch: 16 [31/1000 992/32000 (3%)] Loss: 1.96495 (semantic_loss: 0.01844, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18947 
Train Epoch: 16 [36/1000 1152/32000 (4%)] Loss: 1.96549 (semantic_loss: 0.01802, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18583 
Train Epoch: 16 [41/1000 1312/32000 (4%)] Loss: 1.96465 (semantic_loss: 0.01815, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20479 
Train Epoch: 16 [46/1000 1472/32000 (5%)] Loss: 1.96635 (semantic_loss: 0.01887, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19168 
Train Epoch: 16 [51/1000 1632/32000 (5%)] Loss: 1.96610 (semantic_loss: 0.01863, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21422 
Train Epoch: 16 [56/1000 1792/32000 (6%)] Loss: 1.96292 (semantic_loss: 0.01545, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20527 
Train Epoch: 16 [61/1000 1952/32000 (6%)] Loss: 1.96489 (semantic_loss: 0.01839, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20465 
Train Epoch: 16 [66/1000 2112/32000 (7%)] Loss: 1.96171 (semantic_loss: 0.01423, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.30465 
Train Epoch: 16 [71/1000 2272/32000 (7%)] Loss: 1.96389 (semantic_loss: 0.01641, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19421 
Train Epoch: 16 [76/1000 2432/32000 (8%)] Loss: 1.96441 (semantic_loss: 0.01791, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20590 
Train Epoch: 16 [81/1000 2592/32000 (8%)] Loss: 1.96430 (semantic_loss: 0.01682, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19834 
Train Epoch: 16 [86/1000 2752/32000 (9%)] Loss: 1.96731 (semantic_loss: 0.01983, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19614 
Train Epoch: 16 [91/1000 2912/32000 (9%)] Loss: 1.96472 (semantic_loss: 0.01822, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18919 
Train Epoch: 16 [96/1000 3072/32000 (10%)] Loss: 1.96686 (semantic_loss: 0.01938, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18850 
Train Epoch: 16 [101/1000 3232/32000 (10%)] Loss: 1.96002 (semantic_loss: 0.01352, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19256 
Train Epoch: 16 [106/1000 3392/32000 (11%)] Loss: 1.96427 (semantic_loss: 0.01679, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18629 
Train Epoch: 16 [111/1000 3552/32000 (11%)] Loss: 1.96342 (semantic_loss: 0.01691, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18543 
Train Epoch: 16 [116/1000 3712/32000 (12%)] Loss: 1.96506 (semantic_loss: 0.01759, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19213 
Train Epoch: 16 [121/1000 3872/32000 (12%)] Loss: 1.96220 (semantic_loss: 0.01569, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18932 
Train Epoch: 16 [126/1000 4032/32000 (13%)] Loss: 1.96198 (semantic_loss: 0.01546, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20105 
Train Epoch: 16 [131/1000 4192/32000 (13%)] Loss: 1.96617 (semantic_loss: 0.01869, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20569 
Train Epoch: 16 [136/1000 4352/32000 (14%)] Loss: 1.96124 (semantic_loss: 0.01571, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18387 
Train Epoch: 16 [141/1000 4512/32000 (14%)] Loss: 1.96218 (semantic_loss: 0.01471, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18850 
Train Epoch: 16 [146/1000 4672/32000 (15%)] Loss: 1.96516 (semantic_loss: 0.01670, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18584 
Train Epoch: 16 [151/1000 4832/32000 (15%)] Loss: 1.96568 (semantic_loss: 0.01820, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19391 
Train Epoch: 16 [156/1000 4992/32000 (16%)] Loss: 1.96087 (semantic_loss: 0.01436, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18722 
Train Epoch: 16 [161/1000 5152/32000 (16%)] Loss: 1.96525 (semantic_loss: 0.01777, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18553 
Train Epoch: 16 [166/1000 5312/32000 (17%)] Loss: 1.96392 (semantic_loss: 0.01742, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18623 
Train Epoch: 16 [171/1000 5472/32000 (17%)] Loss: 1.96708 (semantic_loss: 0.02058, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18726 
Train Epoch: 16 [176/1000 5632/32000 (18%)] Loss: 1.96799 (semantic_loss: 0.02051, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18601 
Train Epoch: 16 [181/1000 5792/32000 (18%)] Loss: 1.96557 (semantic_loss: 0.01907, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19056 
Train Epoch: 16 [186/1000 5952/32000 (19%)] Loss: 1.96363 (semantic_loss: 0.01615, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18762 
Train Epoch: 16 [191/1000 6112/32000 (19%)] Loss: 1.96486 (semantic_loss: 0.01836, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18818 
Train Epoch: 16 [196/1000 6272/32000 (20%)] Loss: 1.96108 (semantic_loss: 0.01360, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.27347 
Train Epoch: 16 [201/1000 6432/32000 (20%)] Loss: 1.96296 (semantic_loss: 0.01451, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18358 
Train Epoch: 16 [206/1000 6592/32000 (21%)] Loss: 1.96520 (semantic_loss: 0.01772, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20456 
Train Epoch: 16 [211/1000 6752/32000 (21%)] Loss: 1.96436 (semantic_loss: 0.01688, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.30513 
Train Epoch: 16 [216/1000 6912/32000 (22%)] Loss: 1.96355 (semantic_loss: 0.01608, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22107 
Train Epoch: 16 [221/1000 7072/32000 (22%)] Loss: 1.96427 (semantic_loss: 0.01679, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23950 
Train Epoch: 16 [226/1000 7232/32000 (23%)] Loss: 1.96620 (semantic_loss: 0.01872, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19287 
Train Epoch: 16 [231/1000 7392/32000 (23%)] Loss: 1.96575 (semantic_loss: 0.01730, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.24684 
Train Epoch: 16 [236/1000 7552/32000 (24%)] Loss: 1.96244 (semantic_loss: 0.01691, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20149 
Train Epoch: 16 [241/1000 7712/32000 (24%)] Loss: 1.96252 (semantic_loss: 0.01505, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20957 
Train Epoch: 16 [246/1000 7872/32000 (25%)] Loss: 1.96308 (semantic_loss: 0.01561, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21297 
Train Epoch: 16 [251/1000 8032/32000 (25%)] Loss: 1.96032 (semantic_loss: 0.01480, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19736 
Train Epoch: 16 [256/1000 8192/32000 (26%)] Loss: 1.96423 (semantic_loss: 0.01675, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19174 
Train Epoch: 16 [261/1000 8352/32000 (26%)] Loss: 1.96416 (semantic_loss: 0.01863, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.18817 
Train Epoch: 16 [266/1000 8512/32000 (27%)] Loss: 1.96601 (semantic_loss: 0.01853, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19650 
Train Epoch: 16 [271/1000 8672/32000 (27%)] Loss: 1.96071 (semantic_loss: 0.01421, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21948 
Train Epoch: 16 [276/1000 8832/32000 (28%)] Loss: 1.96093 (semantic_loss: 0.01443, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.64513 
Train Epoch: 16 [281/1000 8992/32000 (28%)] Loss: 1.96850 (semantic_loss: 0.02298, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19476 
Train Epoch: 16 [286/1000 9152/32000 (29%)] Loss: 1.96477 (semantic_loss: 0.01730, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20375 
Train Epoch: 16 [291/1000 9312/32000 (29%)] Loss: 1.96179 (semantic_loss: 0.01529, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18391 
Train Epoch: 16 [296/1000 9472/32000 (30%)] Loss: 1.96549 (semantic_loss: 0.01900, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.25167 
Train Epoch: 16 [301/1000 9632/32000 (30%)] Loss: 1.96551 (semantic_loss: 0.01900, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18461 
Train Epoch: 16 [306/1000 9792/32000 (31%)] Loss: 1.96332 (semantic_loss: 0.01584, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18596 
Train Epoch: 16 [311/1000 9952/32000 (31%)] Loss: 1.96284 (semantic_loss: 0.01536, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18836 
Train Epoch: 16 [316/1000 10112/32000 (32%)] Loss: 1.96459 (semantic_loss: 0.01809, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18881 
Train Epoch: 16 [321/1000 10272/32000 (32%)] Loss: 1.96606 (semantic_loss: 0.01663, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.19679 
Train Epoch: 16 [326/1000 10432/32000 (33%)] Loss: 1.96432 (semantic_loss: 0.01685, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18888 
Train Epoch: 16 [331/1000 10592/32000 (33%)] Loss: 1.96707 (semantic_loss: 0.01959, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19034 
Train Epoch: 16 [336/1000 10752/32000 (34%)] Loss: 1.96723 (semantic_loss: 0.01976, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.70817 
Train Epoch: 16 [341/1000 10912/32000 (34%)] Loss: 1.96528 (semantic_loss: 0.01780, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18577 
Train Epoch: 16 [346/1000 11072/32000 (35%)] Loss: 1.96134 (semantic_loss: 0.01484, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18744 
Train Epoch: 16 [351/1000 11232/32000 (35%)] Loss: 1.96621 (semantic_loss: 0.01873, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18987 
Train Epoch: 16 [356/1000 11392/32000 (36%)] Loss: 1.96558 (semantic_loss: 0.01810, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20779 
Train Epoch: 16 [361/1000 11552/32000 (36%)] Loss: 1.96436 (semantic_loss: 0.01785, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20462 
Train Epoch: 16 [366/1000 11712/32000 (37%)] Loss: 1.96296 (semantic_loss: 0.01743, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.22838 
Train Epoch: 16 [371/1000 11872/32000 (37%)] Loss: 1.96456 (semantic_loss: 0.01708, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20264 
Train Epoch: 16 [376/1000 12032/32000 (38%)] Loss: 1.96321 (semantic_loss: 0.01574, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19315 
Train Epoch: 16 [381/1000 12192/32000 (38%)] Loss: 1.96731 (semantic_loss: 0.02081, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20716 
Train Epoch: 16 [386/1000 12352/32000 (39%)] Loss: 1.96747 (semantic_loss: 0.01999, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.28469 
Train Epoch: 16 [391/1000 12512/32000 (39%)] Loss: 1.96272 (semantic_loss: 0.01524, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20168 
Train Epoch: 16 [396/1000 12672/32000 (40%)] Loss: 1.96163 (semantic_loss: 0.01513, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19355 
Train Epoch: 16 [401/1000 12832/32000 (40%)] Loss: 1.96335 (semantic_loss: 0.01587, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19195 
Train Epoch: 16 [406/1000 12992/32000 (41%)] Loss: 1.96591 (semantic_loss: 0.01745, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18288 
Train Epoch: 16 [411/1000 13152/32000 (41%)] Loss: 1.96369 (semantic_loss: 0.01817, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18944 
Train Epoch: 16 [416/1000 13312/32000 (42%)] Loss: 1.97197 (semantic_loss: 0.02546, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18666 
Train Epoch: 16 [421/1000 13472/32000 (42%)] Loss: 1.96170 (semantic_loss: 0.01423, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18442 
Train Epoch: 16 [426/1000 13632/32000 (43%)] Loss: 1.96564 (semantic_loss: 0.01719, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18763 
Train Epoch: 16 [431/1000 13792/32000 (43%)] Loss: 1.96705 (semantic_loss: 0.01957, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19497 
Train Epoch: 16 [436/1000 13952/32000 (44%)] Loss: 1.96288 (semantic_loss: 0.01540, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21220 
Train Epoch: 16 [441/1000 14112/32000 (44%)] Loss: 1.96334 (semantic_loss: 0.01684, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18484 
Train Epoch: 16 [446/1000 14272/32000 (45%)] Loss: 1.96396 (semantic_loss: 0.01746, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18364 
Train Epoch: 16 [451/1000 14432/32000 (45%)] Loss: 1.96653 (semantic_loss: 0.01906, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20783 
Train Epoch: 16 [456/1000 14592/32000 (46%)] Loss: 1.96264 (semantic_loss: 0.01615, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18902 
Train Epoch: 16 [461/1000 14752/32000 (46%)] Loss: 1.96592 (semantic_loss: 0.01844, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18840 
Train Epoch: 16 [466/1000 14912/32000 (47%)] Loss: 1.96343 (semantic_loss: 0.01693, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18794 
Train Epoch: 16 [471/1000 15072/32000 (47%)] Loss: 1.96188 (semantic_loss: 0.01440, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18542 
Train Epoch: 16 [476/1000 15232/32000 (48%)] Loss: 1.96490 (semantic_loss: 0.01840, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18710 
Train Epoch: 16 [481/1000 15392/32000 (48%)] Loss: 1.96173 (semantic_loss: 0.01523, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18759 
Train Epoch: 16 [486/1000 15552/32000 (49%)] Loss: 1.96056 (semantic_loss: 0.01406, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20181 
Train Epoch: 16 [491/1000 15712/32000 (49%)] Loss: 1.96356 (semantic_loss: 0.01705, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18453 
Train Epoch: 16 [496/1000 15872/32000 (50%)] Loss: 1.96112 (semantic_loss: 0.01365, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20653 
Train Epoch: 16 [501/1000 16032/32000 (50%)] Loss: 1.96126 (semantic_loss: 0.01476, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18597 
Train Epoch: 16 [506/1000 16192/32000 (51%)] Loss: 1.96388 (semantic_loss: 0.01543, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18567 
Train Epoch: 16 [511/1000 16352/32000 (51%)] Loss: 1.96454 (semantic_loss: 0.01608, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20920 
Train Epoch: 16 [516/1000 16512/32000 (52%)] Loss: 1.96569 (semantic_loss: 0.01918, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.31365 
Train Epoch: 16 [521/1000 16672/32000 (52%)] Loss: 1.96166 (semantic_loss: 0.01516, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23517 
Train Epoch: 16 [526/1000 16832/32000 (53%)] Loss: 1.96707 (semantic_loss: 0.01959, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23828 
Train Epoch: 16 [531/1000 16992/32000 (53%)] Loss: 1.96938 (semantic_loss: 0.02093, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.33243 
Train Epoch: 16 [536/1000 17152/32000 (54%)] Loss: 1.96416 (semantic_loss: 0.01571, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22149 
Train Epoch: 16 [541/1000 17312/32000 (54%)] Loss: 1.96267 (semantic_loss: 0.01617, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20525 
Train Epoch: 16 [546/1000 17472/32000 (55%)] Loss: 1.96151 (semantic_loss: 0.01501, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20776 
Train Epoch: 16 [551/1000 17632/32000 (55%)] Loss: 1.96477 (semantic_loss: 0.01729, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22462 
Train Epoch: 16 [556/1000 17792/32000 (56%)] Loss: 1.96358 (semantic_loss: 0.01707, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18474 
Train Epoch: 16 [561/1000 17952/32000 (56%)] Loss: 1.96637 (semantic_loss: 0.01792, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19665 
Train Epoch: 16 [566/1000 18112/32000 (57%)] Loss: 1.96289 (semantic_loss: 0.01541, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18663 
Train Epoch: 16 [571/1000 18272/32000 (57%)] Loss: 1.96785 (semantic_loss: 0.01939, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19113 
Train Epoch: 16 [576/1000 18432/32000 (58%)] Loss: 1.96767 (semantic_loss: 0.02117, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18379 
Train Epoch: 16 [581/1000 18592/32000 (58%)] Loss: 1.96726 (semantic_loss: 0.02076, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20341 
Train Epoch: 16 [586/1000 18752/32000 (59%)] Loss: 1.96510 (semantic_loss: 0.01859, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18684 
Train Epoch: 16 [591/1000 18912/32000 (59%)] Loss: 1.96010 (semantic_loss: 0.01262, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22876 
Train Epoch: 16 [596/1000 19072/32000 (60%)] Loss: 1.96102 (semantic_loss: 0.01550, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.58208 
Train Epoch: 16 [601/1000 19232/32000 (60%)] Loss: 1.96501 (semantic_loss: 0.01753, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18582 
Train Epoch: 16 [606/1000 19392/32000 (61%)] Loss: 1.96838 (semantic_loss: 0.02090, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19320 
Train Epoch: 16 [611/1000 19552/32000 (61%)] Loss: 1.96223 (semantic_loss: 0.01574, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18378 
Train Epoch: 16 [616/1000 19712/32000 (62%)] Loss: 1.96311 (semantic_loss: 0.01661, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.25291 
Train Epoch: 16 [621/1000 19872/32000 (62%)] Loss: 1.96344 (semantic_loss: 0.01791, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19054 
Train Epoch: 16 [626/1000 20032/32000 (63%)] Loss: 1.96471 (semantic_loss: 0.01723, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18213 
Train Epoch: 16 [631/1000 20192/32000 (63%)] Loss: 1.96650 (semantic_loss: 0.02000, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18443 
Train Epoch: 16 [636/1000 20352/32000 (64%)] Loss: 1.96216 (semantic_loss: 0.01469, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18439 
Train Epoch: 16 [641/1000 20512/32000 (64%)] Loss: 1.97179 (semantic_loss: 0.02430, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19491 
Train Epoch: 16 [646/1000 20672/32000 (65%)] Loss: 1.96403 (semantic_loss: 0.01851, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19099 
Train Epoch: 16 [651/1000 20832/32000 (65%)] Loss: 1.96693 (semantic_loss: 0.01946, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19449 
Train Epoch: 16 [656/1000 20992/32000 (66%)] Loss: 1.96157 (semantic_loss: 0.01506, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.72072 
Train Epoch: 16 [661/1000 21152/32000 (66%)] Loss: 1.96246 (semantic_loss: 0.01596, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18917 
Train Epoch: 16 [666/1000 21312/32000 (67%)] Loss: 1.96924 (semantic_loss: 0.02079, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18655 
Train Epoch: 16 [671/1000 21472/32000 (67%)] Loss: 1.96355 (semantic_loss: 0.01706, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20035 
Train Epoch: 16 [676/1000 21632/32000 (68%)] Loss: 1.96739 (semantic_loss: 0.02089, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19762 
Train Epoch: 16 [681/1000 21792/32000 (68%)] Loss: 1.96307 (semantic_loss: 0.01754, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.23398 
Train Epoch: 16 [686/1000 21952/32000 (69%)] Loss: 1.96666 (semantic_loss: 0.01918, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22257 
Train Epoch: 16 [691/1000 22112/32000 (69%)] Loss: 1.95987 (semantic_loss: 0.01337, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20527 
Train Epoch: 16 [696/1000 22272/32000 (70%)] Loss: 1.97175 (semantic_loss: 0.02428, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19162 
Train Epoch: 16 [701/1000 22432/32000 (70%)] Loss: 1.96804 (semantic_loss: 0.02056, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20945 
Train Epoch: 16 [706/1000 22592/32000 (71%)] Loss: 1.96350 (semantic_loss: 0.01700, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.27987 
Train Epoch: 16 [711/1000 22752/32000 (71%)] Loss: 1.96966 (semantic_loss: 0.02121, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19939 
Train Epoch: 16 [716/1000 22912/32000 (72%)] Loss: 1.96545 (semantic_loss: 0.01700, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19301 
Train Epoch: 16 [721/1000 23072/32000 (72%)] Loss: 1.96510 (semantic_loss: 0.01763, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20026 
Train Epoch: 16 [726/1000 23232/32000 (73%)] Loss: 1.96627 (semantic_loss: 0.01782, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19051 
Train Epoch: 16 [731/1000 23392/32000 (73%)] Loss: 1.96707 (semantic_loss: 0.01959, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18464 
Train Epoch: 16 [736/1000 23552/32000 (74%)] Loss: 1.96156 (semantic_loss: 0.01408, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19804 
Train Epoch: 16 [741/1000 23712/32000 (74%)] Loss: 1.96190 (semantic_loss: 0.01540, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19405 
Train Epoch: 16 [746/1000 23872/32000 (75%)] Loss: 1.96685 (semantic_loss: 0.01840, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20042 
Train Epoch: 16 [751/1000 24032/32000 (75%)] Loss: 1.96288 (semantic_loss: 0.01541, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18385 
Train Epoch: 16 [756/1000 24192/32000 (76%)] Loss: 1.96662 (semantic_loss: 0.01915, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18280 
Train Epoch: 16 [761/1000 24352/32000 (76%)] Loss: 1.96326 (semantic_loss: 0.01677, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18448 
Train Epoch: 16 [766/1000 24512/32000 (77%)] Loss: 1.96461 (semantic_loss: 0.01616, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18234 
Train Epoch: 16 [771/1000 24672/32000 (77%)] Loss: 1.96126 (semantic_loss: 0.01379, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20107 
Train Epoch: 16 [776/1000 24832/32000 (78%)] Loss: 1.96176 (semantic_loss: 0.01526, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18990 
Train Epoch: 16 [781/1000 24992/32000 (78%)] Loss: 1.96307 (semantic_loss: 0.01656, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19162 
Train Epoch: 16 [786/1000 25152/32000 (79%)] Loss: 1.96300 (semantic_loss: 0.01650, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18703 
Train Epoch: 16 [791/1000 25312/32000 (79%)] Loss: 1.96627 (semantic_loss: 0.01977, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18917 
Train Epoch: 16 [796/1000 25472/32000 (80%)] Loss: 1.96167 (semantic_loss: 0.01517, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18731 
Train Epoch: 16 [801/1000 25632/32000 (80%)] Loss: 1.96597 (semantic_loss: 0.01947, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19743 
Train Epoch: 16 [806/1000 25792/32000 (81%)] Loss: 1.96233 (semantic_loss: 0.01583, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20843 
Train Epoch: 16 [811/1000 25952/32000 (81%)] Loss: 1.96483 (semantic_loss: 0.01735, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18907 
Train Epoch: 16 [816/1000 26112/32000 (82%)] Loss: 1.96337 (semantic_loss: 0.01590, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18661 
Train Epoch: 16 [821/1000 26272/32000 (82%)] Loss: 1.96456 (semantic_loss: 0.01806, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18571 
Train Epoch: 16 [826/1000 26432/32000 (83%)] Loss: 1.96202 (semantic_loss: 0.01552, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19616 
Train Epoch: 16 [831/1000 26592/32000 (83%)] Loss: 1.96207 (semantic_loss: 0.01460, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21789 
Train Epoch: 16 [836/1000 26752/32000 (84%)] Loss: 1.96161 (semantic_loss: 0.01414, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.32808 
Train Epoch: 16 [841/1000 26912/32000 (84%)] Loss: 1.96534 (semantic_loss: 0.01883, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.23227 
Train Epoch: 16 [846/1000 27072/32000 (85%)] Loss: 1.96240 (semantic_loss: 0.01590, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20277 
Train Epoch: 16 [851/1000 27232/32000 (85%)] Loss: 1.96377 (semantic_loss: 0.01630, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.30003 
Train Epoch: 16 [856/1000 27392/32000 (86%)] Loss: 1.96220 (semantic_loss: 0.01569, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19223 
Train Epoch: 16 [861/1000 27552/32000 (86%)] Loss: 1.97021 (semantic_loss: 0.02274, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20940 
Train Epoch: 16 [866/1000 27712/32000 (87%)] Loss: 1.96220 (semantic_loss: 0.01570, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19134 
Train Epoch: 16 [871/1000 27872/32000 (87%)] Loss: 1.96886 (semantic_loss: 0.02040, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.22206 
Train Epoch: 16 [876/1000 28032/32000 (88%)] Loss: 1.96577 (semantic_loss: 0.01829, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18697 
Train Epoch: 16 [881/1000 28192/32000 (88%)] Loss: 1.96465 (semantic_loss: 0.01717, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18669 
Train Epoch: 16 [886/1000 28352/32000 (89%)] Loss: 1.96183 (semantic_loss: 0.01533, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18793 
Train Epoch: 16 [891/1000 28512/32000 (89%)] Loss: 1.96534 (semantic_loss: 0.01688, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19903 
Train Epoch: 16 [896/1000 28672/32000 (90%)] Loss: 1.96725 (semantic_loss: 0.01977, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19951 
Train Epoch: 16 [901/1000 28832/32000 (90%)] Loss: 1.96399 (semantic_loss: 0.01651, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19991 
Train Epoch: 16 [906/1000 28992/32000 (91%)] Loss: 1.96180 (semantic_loss: 0.01432, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19398 
Train Epoch: 16 [911/1000 29152/32000 (91%)] Loss: 1.96080 (semantic_loss: 0.01430, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21274 
Train Epoch: 16 [916/1000 29312/32000 (92%)] Loss: 1.96486 (semantic_loss: 0.01836, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.58430 
Train Epoch: 16 [921/1000 29472/32000 (92%)] Loss: 1.96204 (semantic_loss: 0.01457, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19941 
Train Epoch: 16 [926/1000 29632/32000 (93%)] Loss: 1.96729 (semantic_loss: 0.01981, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18634 
Train Epoch: 16 [931/1000 29792/32000 (93%)] Loss: 1.96556 (semantic_loss: 0.01906, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19271 
Train Epoch: 16 [936/1000 29952/32000 (94%)] Loss: 1.96200 (semantic_loss: 0.01550, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.25708 
Train Epoch: 16 [941/1000 30112/32000 (94%)] Loss: 1.96280 (semantic_loss: 0.01533, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18815 
Train Epoch: 16 [946/1000 30272/32000 (95%)] Loss: 1.96448 (semantic_loss: 0.01798, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18998 
Train Epoch: 16 [951/1000 30432/32000 (95%)] Loss: 1.96224 (semantic_loss: 0.01477, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18420 
Train Epoch: 16 [956/1000 30592/32000 (96%)] Loss: 1.96721 (semantic_loss: 0.01974, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20950 
Train Epoch: 16 [961/1000 30752/32000 (96%)] Loss: 1.96441 (semantic_loss: 0.01693, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21756 
Train Epoch: 16 [966/1000 30912/32000 (97%)] Loss: 1.96222 (semantic_loss: 0.01474, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18860 
Train Epoch: 16 [971/1000 31072/32000 (97%)] Loss: 1.96499 (semantic_loss: 0.01751, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19037 
Train Epoch: 16 [976/1000 31232/32000 (98%)] Loss: 1.96411 (semantic_loss: 0.01664, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.73455 
Train Epoch: 16 [981/1000 31392/32000 (98%)] Loss: 1.96804 (semantic_loss: 0.01959, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19164 
Train Epoch: 16 [986/1000 31552/32000 (99%)] Loss: 1.96358 (semantic_loss: 0.01513, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21459 
Train Epoch: 16 [991/1000 31712/32000 (99%)] Loss: 1.96404 (semantic_loss: 0.01754, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21914 
Train Epoch: 16 [996/1000 31872/32000 (100%)] Loss: 1.96571 (semantic_loss: 0.01922, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24053 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch16.pth ...
Done in 4.094s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch16.pth ...
Done in 22.762s
removing stale ckpt [epoch 15] [took 0.01s]
 epoch          : 16
 loss           : 1.9643650294542312
 learning_rate  : 1.0294556604732457e-05
 n_samples      : 512000
 n_steps        : 16000
 ActivityNet_val1_test/t2v_metrics/R1: 11.185682326621924
 ActivityNet_val1_test/t2v_metrics/R5: 36.99410209477323
 ActivityNet_val1_test/t2v_metrics/R10: 54.36241610738255
 ActivityNet_val1_test/t2v_metrics/R50: 85.56030099654261
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 52.452003254016674
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 28.229156893677047
 ActivityNet_val1_test/v2t_metrics/R1: 12.772015456579215
 ActivityNet_val1_test/v2t_metrics/R5: 37.50254220052878
 ActivityNet_val1_test/v2t_metrics/R10: 55.11490746390075
 ActivityNet_val1_test/v2t_metrics/R50: 85.96705308114704
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 54.79611551759203
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.775775097632206
 mnt_best       : 28.229156893677047
 not_improved_count: 0
Train Epoch: 17 [1/1000 32/32000 (0%)] Loss: 1.96402 (semantic_loss: 0.01655, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=21.20361 
Train Epoch: 17 [6/1000 192/32000 (1%)] Loss: 1.96557 (semantic_loss: 0.01907, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18561 
Train Epoch: 17 [11/1000 352/32000 (1%)] Loss: 1.96328 (semantic_loss: 0.01678, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.28748 
Train Epoch: 17 [16/1000 512/32000 (2%)] Loss: 1.96431 (semantic_loss: 0.01684, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.35654 
Train Epoch: 17 [21/1000 672/32000 (2%)] Loss: 1.96274 (semantic_loss: 0.01527, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19401 
Train Epoch: 17 [26/1000 832/32000 (3%)] Loss: 1.96347 (semantic_loss: 0.01600, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18649 
Train Epoch: 17 [31/1000 992/32000 (3%)] Loss: 1.96279 (semantic_loss: 0.01630, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20704 
Train Epoch: 17 [36/1000 1152/32000 (4%)] Loss: 1.96247 (semantic_loss: 0.01597, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19391 
Train Epoch: 17 [41/1000 1312/32000 (4%)] Loss: 1.96523 (semantic_loss: 0.01775, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20601 
Train Epoch: 17 [46/1000 1472/32000 (5%)] Loss: 1.96285 (semantic_loss: 0.01636, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21505 
Train Epoch: 17 [51/1000 1632/32000 (5%)] Loss: 1.96331 (semantic_loss: 0.01583, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24567 
Train Epoch: 17 [56/1000 1792/32000 (6%)] Loss: 1.96670 (semantic_loss: 0.01922, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23308 
Train Epoch: 17 [61/1000 1952/32000 (6%)] Loss: 1.96367 (semantic_loss: 0.01717, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20402 
Train Epoch: 17 [66/1000 2112/32000 (7%)] Loss: 1.96341 (semantic_loss: 0.01594, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25322 
Train Epoch: 17 [71/1000 2272/32000 (7%)] Loss: 1.96275 (semantic_loss: 0.01625, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20876 
Train Epoch: 17 [76/1000 2432/32000 (8%)] Loss: 1.96378 (semantic_loss: 0.01727, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19258 
Train Epoch: 17 [81/1000 2592/32000 (8%)] Loss: 1.96125 (semantic_loss: 0.01474, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18958 
Train Epoch: 17 [86/1000 2752/32000 (9%)] Loss: 1.96298 (semantic_loss: 0.01647, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18685 
Train Epoch: 17 [91/1000 2912/32000 (9%)] Loss: 1.96273 (semantic_loss: 0.01623, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19433 
Train Epoch: 17 [96/1000 3072/32000 (10%)] Loss: 1.96433 (semantic_loss: 0.01685, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25159 
Train Epoch: 17 [101/1000 3232/32000 (10%)] Loss: 1.96605 (semantic_loss: 0.01954, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.18623 
Train Epoch: 17 [106/1000 3392/32000 (11%)] Loss: 1.96497 (semantic_loss: 0.01749, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23149 
Train Epoch: 17 [111/1000 3552/32000 (11%)] Loss: 1.96126 (semantic_loss: 0.01476, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21514 
Train Epoch: 17 [116/1000 3712/32000 (12%)] Loss: 1.96436 (semantic_loss: 0.01687, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19046 
Train Epoch: 17 [121/1000 3872/32000 (12%)] Loss: 1.96224 (semantic_loss: 0.01574, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19025 
Train Epoch: 17 [126/1000 4032/32000 (13%)] Loss: 1.96195 (semantic_loss: 0.01545, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18471 
Train Epoch: 17 [131/1000 4192/32000 (13%)] Loss: 1.96166 (semantic_loss: 0.01515, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18589 
Train Epoch: 17 [136/1000 4352/32000 (14%)] Loss: 1.96306 (semantic_loss: 0.01461, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22048 
Train Epoch: 17 [141/1000 4512/32000 (14%)] Loss: 1.96527 (semantic_loss: 0.01682, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.23344 
Train Epoch: 17 [146/1000 4672/32000 (15%)] Loss: 1.96039 (semantic_loss: 0.01389, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18935 
Train Epoch: 17 [151/1000 4832/32000 (15%)] Loss: 1.96440 (semantic_loss: 0.01790, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18546 
Train Epoch: 17 [156/1000 4992/32000 (16%)] Loss: 1.96136 (semantic_loss: 0.01388, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18485 
Train Epoch: 17 [161/1000 5152/32000 (16%)] Loss: 1.96362 (semantic_loss: 0.01615, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20133 
Train Epoch: 17 [166/1000 5312/32000 (17%)] Loss: 1.96223 (semantic_loss: 0.01476, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20650 
Train Epoch: 17 [171/1000 5472/32000 (17%)] Loss: 1.96263 (semantic_loss: 0.01710, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.21823 
Train Epoch: 17 [176/1000 5632/32000 (18%)] Loss: 1.96782 (semantic_loss: 0.01839, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.18407 
Train Epoch: 17 [181/1000 5792/32000 (18%)] Loss: 1.96558 (semantic_loss: 0.01907, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.23313 
Train Epoch: 17 [186/1000 5952/32000 (19%)] Loss: 1.96604 (semantic_loss: 0.01758, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18549 
Train Epoch: 17 [191/1000 6112/32000 (19%)] Loss: 1.96612 (semantic_loss: 0.01864, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18669 
Train Epoch: 17 [196/1000 6272/32000 (20%)] Loss: 1.96501 (semantic_loss: 0.01851, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20845 
Train Epoch: 17 [201/1000 6432/32000 (20%)] Loss: 1.96652 (semantic_loss: 0.01710, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.22757 
Train Epoch: 17 [206/1000 6592/32000 (21%)] Loss: 1.96215 (semantic_loss: 0.01662, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.22304 
Train Epoch: 17 [211/1000 6752/32000 (21%)] Loss: 1.96469 (semantic_loss: 0.01818, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19506 
Train Epoch: 17 [216/1000 6912/32000 (22%)] Loss: 1.96479 (semantic_loss: 0.01634, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20303 
Train Epoch: 17 [221/1000 7072/32000 (22%)] Loss: 1.96659 (semantic_loss: 0.01814, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19449 
Train Epoch: 17 [226/1000 7232/32000 (23%)] Loss: 1.96081 (semantic_loss: 0.01432, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19178 
Train Epoch: 17 [231/1000 7392/32000 (23%)] Loss: 1.96724 (semantic_loss: 0.01977, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18681 
Train Epoch: 17 [236/1000 7552/32000 (24%)] Loss: 1.96339 (semantic_loss: 0.01592, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18656 
Train Epoch: 17 [241/1000 7712/32000 (24%)] Loss: 1.96969 (semantic_loss: 0.02123, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19977 
Train Epoch: 17 [246/1000 7872/32000 (25%)] Loss: 1.96192 (semantic_loss: 0.01541, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18862 
Train Epoch: 17 [251/1000 8032/32000 (25%)] Loss: 1.96397 (semantic_loss: 0.01649, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18786 
Train Epoch: 17 [256/1000 8192/32000 (26%)] Loss: 1.96355 (semantic_loss: 0.01607, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20973 
Train Epoch: 17 [261/1000 8352/32000 (26%)] Loss: 1.96001 (semantic_loss: 0.01351, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19853 
Train Epoch: 17 [266/1000 8512/32000 (27%)] Loss: 1.96505 (semantic_loss: 0.01758, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19001 
Train Epoch: 17 [271/1000 8672/32000 (27%)] Loss: 1.96127 (semantic_loss: 0.01380, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21868 
Train Epoch: 17 [276/1000 8832/32000 (28%)] Loss: 1.96386 (semantic_loss: 0.01638, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19342 
Train Epoch: 17 [281/1000 8992/32000 (28%)] Loss: 1.96459 (semantic_loss: 0.01711, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18817 
Train Epoch: 17 [286/1000 9152/32000 (29%)] Loss: 1.96038 (semantic_loss: 0.01388, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19662 
Train Epoch: 17 [291/1000 9312/32000 (29%)] Loss: 1.96458 (semantic_loss: 0.01808, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18950 
Train Epoch: 17 [296/1000 9472/32000 (30%)] Loss: 1.96272 (semantic_loss: 0.01621, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18491 
Train Epoch: 17 [301/1000 9632/32000 (30%)] Loss: 1.96243 (semantic_loss: 0.01495, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18640 
Train Epoch: 17 [306/1000 9792/32000 (31%)] Loss: 1.96374 (semantic_loss: 0.01626, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20457 
Train Epoch: 17 [311/1000 9952/32000 (31%)] Loss: 1.96786 (semantic_loss: 0.02038, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20664 
Train Epoch: 17 [316/1000 10112/32000 (32%)] Loss: 1.96586 (semantic_loss: 0.01740, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19039 
Train Epoch: 17 [321/1000 10272/32000 (32%)] Loss: 1.97255 (semantic_loss: 0.02409, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18574 
Train Epoch: 17 [326/1000 10432/32000 (33%)] Loss: 1.96581 (semantic_loss: 0.01931, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18507 
Train Epoch: 17 [331/1000 10592/32000 (33%)] Loss: 1.96102 (semantic_loss: 0.01452, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18591 
Train Epoch: 17 [336/1000 10752/32000 (34%)] Loss: 1.96388 (semantic_loss: 0.01737, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.28711 
Train Epoch: 17 [341/1000 10912/32000 (34%)] Loss: 1.96183 (semantic_loss: 0.01533, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18784 
Train Epoch: 17 [346/1000 11072/32000 (35%)] Loss: 1.96790 (semantic_loss: 0.02140, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20542 
Train Epoch: 17 [351/1000 11232/32000 (35%)] Loss: 1.96341 (semantic_loss: 0.01691, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21473 
Train Epoch: 17 [356/1000 11392/32000 (36%)] Loss: 1.96536 (semantic_loss: 0.01788, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22254 
Train Epoch: 17 [361/1000 11552/32000 (36%)] Loss: 1.96425 (semantic_loss: 0.01676, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.22263 
Train Epoch: 17 [366/1000 11712/32000 (37%)] Loss: 1.96252 (semantic_loss: 0.01603, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20435 
Train Epoch: 17 [371/1000 11872/32000 (37%)] Loss: 1.96337 (semantic_loss: 0.01492, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19099 
Train Epoch: 17 [376/1000 12032/32000 (38%)] Loss: 1.96396 (semantic_loss: 0.01746, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20051 
Train Epoch: 17 [381/1000 12192/32000 (38%)] Loss: 1.96903 (semantic_loss: 0.02155, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20088 
Train Epoch: 17 [386/1000 12352/32000 (39%)] Loss: 1.96180 (semantic_loss: 0.01530, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20470 
Train Epoch: 17 [391/1000 12512/32000 (39%)] Loss: 1.96339 (semantic_loss: 0.01495, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19485 
Train Epoch: 17 [396/1000 12672/32000 (40%)] Loss: 1.96698 (semantic_loss: 0.01951, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18473 
Train Epoch: 17 [401/1000 12832/32000 (40%)] Loss: 1.96406 (semantic_loss: 0.01659, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18939 
Train Epoch: 17 [406/1000 12992/32000 (41%)] Loss: 1.96610 (semantic_loss: 0.01765, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=1.13911 
Train Epoch: 17 [411/1000 13152/32000 (41%)] Loss: 1.96637 (semantic_loss: 0.01889, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18848 
Train Epoch: 17 [416/1000 13312/32000 (42%)] Loss: 1.96365 (semantic_loss: 0.01617, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19152 
Train Epoch: 17 [421/1000 13472/32000 (42%)] Loss: 1.96371 (semantic_loss: 0.01623, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24521 
Train Epoch: 17 [426/1000 13632/32000 (43%)] Loss: 1.96398 (semantic_loss: 0.01651, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19584 
Train Epoch: 17 [431/1000 13792/32000 (43%)] Loss: 1.96077 (semantic_loss: 0.01330, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18722 
Train Epoch: 17 [436/1000 13952/32000 (44%)] Loss: 1.96153 (semantic_loss: 0.01503, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18817 
Train Epoch: 17 [441/1000 14112/32000 (44%)] Loss: 1.96244 (semantic_loss: 0.01594, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18467 
Train Epoch: 17 [446/1000 14272/32000 (45%)] Loss: 1.96457 (semantic_loss: 0.01710, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18633 
Train Epoch: 17 [451/1000 14432/32000 (45%)] Loss: 1.96844 (semantic_loss: 0.02194, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18413 
Train Epoch: 17 [456/1000 14592/32000 (46%)] Loss: 1.96444 (semantic_loss: 0.01697, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18473 
Train Epoch: 17 [461/1000 14752/32000 (46%)] Loss: 1.96307 (semantic_loss: 0.01657, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18408 
Train Epoch: 17 [466/1000 14912/32000 (47%)] Loss: 1.96582 (semantic_loss: 0.01834, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19129 
Train Epoch: 17 [471/1000 15072/32000 (47%)] Loss: 1.96418 (semantic_loss: 0.01767, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18432 
Train Epoch: 17 [476/1000 15232/32000 (48%)] Loss: 1.96292 (semantic_loss: 0.01642, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18690 
Train Epoch: 17 [481/1000 15392/32000 (48%)] Loss: 1.96357 (semantic_loss: 0.01609, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18886 
Train Epoch: 17 [486/1000 15552/32000 (49%)] Loss: 1.96517 (semantic_loss: 0.01867, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18495 
Train Epoch: 17 [491/1000 15712/32000 (49%)] Loss: 1.96588 (semantic_loss: 0.01841, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18500 
Train Epoch: 17 [496/1000 15872/32000 (50%)] Loss: 1.96602 (semantic_loss: 0.01853, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18836 
Train Epoch: 17 [501/1000 16032/32000 (50%)] Loss: 1.96231 (semantic_loss: 0.01483, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18806 
Train Epoch: 17 [506/1000 16192/32000 (51%)] Loss: 1.96522 (semantic_loss: 0.01774, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18883 
Train Epoch: 17 [511/1000 16352/32000 (51%)] Loss: 1.96256 (semantic_loss: 0.01509, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22466 
Train Epoch: 17 [516/1000 16512/32000 (52%)] Loss: 1.96306 (semantic_loss: 0.01657, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22334 
Train Epoch: 17 [521/1000 16672/32000 (52%)] Loss: 1.96265 (semantic_loss: 0.01615, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21439 
Train Epoch: 17 [526/1000 16832/32000 (53%)] Loss: 1.96621 (semantic_loss: 0.01874, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19675 
Train Epoch: 17 [531/1000 16992/32000 (53%)] Loss: 1.96632 (semantic_loss: 0.01786, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20315 
Train Epoch: 17 [536/1000 17152/32000 (54%)] Loss: 1.96261 (semantic_loss: 0.01611, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19844 
Train Epoch: 17 [541/1000 17312/32000 (54%)] Loss: 1.96058 (semantic_loss: 0.01408, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19479 
Train Epoch: 17 [546/1000 17472/32000 (55%)] Loss: 1.96185 (semantic_loss: 0.01535, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19116 
Train Epoch: 17 [551/1000 17632/32000 (55%)] Loss: 1.96456 (semantic_loss: 0.01806, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21006 
Train Epoch: 17 [556/1000 17792/32000 (56%)] Loss: 1.96658 (semantic_loss: 0.02008, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18703 
Train Epoch: 17 [561/1000 17952/32000 (56%)] Loss: 1.96732 (semantic_loss: 0.01887, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20283 
Train Epoch: 17 [566/1000 18112/32000 (57%)] Loss: 1.96427 (semantic_loss: 0.01582, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18714 
Train Epoch: 17 [571/1000 18272/32000 (57%)] Loss: 1.95987 (semantic_loss: 0.01337, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18529 
Train Epoch: 17 [576/1000 18432/32000 (58%)] Loss: 1.96518 (semantic_loss: 0.01771, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19697 
Train Epoch: 17 [581/1000 18592/32000 (58%)] Loss: 1.96316 (semantic_loss: 0.01471, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19745 
Train Epoch: 17 [586/1000 18752/32000 (59%)] Loss: 1.96545 (semantic_loss: 0.01700, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20899 
Train Epoch: 17 [591/1000 18912/32000 (59%)] Loss: 1.96306 (semantic_loss: 0.01656, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18632 
Train Epoch: 17 [596/1000 19072/32000 (60%)] Loss: 1.96238 (semantic_loss: 0.01491, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19643 
Train Epoch: 17 [601/1000 19232/32000 (60%)] Loss: 1.96345 (semantic_loss: 0.01597, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18834 
Train Epoch: 17 [606/1000 19392/32000 (61%)] Loss: 1.96167 (semantic_loss: 0.01614, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18477 
Train Epoch: 17 [611/1000 19552/32000 (61%)] Loss: 1.96185 (semantic_loss: 0.01438, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19415 
Train Epoch: 17 [616/1000 19712/32000 (62%)] Loss: 1.96305 (semantic_loss: 0.01558, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18842 
Train Epoch: 17 [621/1000 19872/32000 (62%)] Loss: 1.96360 (semantic_loss: 0.01612, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18676 
Train Epoch: 17 [626/1000 20032/32000 (63%)] Loss: 1.96693 (semantic_loss: 0.02043, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19522 
Train Epoch: 17 [631/1000 20192/32000 (63%)] Loss: 1.96281 (semantic_loss: 0.01631, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18650 
Train Epoch: 17 [636/1000 20352/32000 (64%)] Loss: 1.96415 (semantic_loss: 0.01570, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18897 
Train Epoch: 17 [641/1000 20512/32000 (64%)] Loss: 1.96265 (semantic_loss: 0.01517, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18936 
Train Epoch: 17 [646/1000 20672/32000 (65%)] Loss: 1.96615 (semantic_loss: 0.01964, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19495 
Train Epoch: 17 [651/1000 20832/32000 (65%)] Loss: 1.96793 (semantic_loss: 0.02045, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18806 
Train Epoch: 17 [656/1000 20992/32000 (66%)] Loss: 1.96167 (semantic_loss: 0.01517, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.28674 
Train Epoch: 17 [661/1000 21152/32000 (66%)] Loss: 1.96603 (semantic_loss: 0.01855, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20431 
Train Epoch: 17 [666/1000 21312/32000 (67%)] Loss: 1.96249 (semantic_loss: 0.01501, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22449 
Train Epoch: 17 [671/1000 21472/32000 (67%)] Loss: 1.96627 (semantic_loss: 0.01782, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.25854 
Train Epoch: 17 [676/1000 21632/32000 (68%)] Loss: 1.96140 (semantic_loss: 0.01393, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20983 
Train Epoch: 17 [681/1000 21792/32000 (68%)] Loss: 1.96325 (semantic_loss: 0.01674, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20285 
Train Epoch: 17 [686/1000 21952/32000 (69%)] Loss: 1.96537 (semantic_loss: 0.01789, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19666 
Train Epoch: 17 [691/1000 22112/32000 (69%)] Loss: 1.96411 (semantic_loss: 0.01762, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20192 
Train Epoch: 17 [696/1000 22272/32000 (70%)] Loss: 1.96461 (semantic_loss: 0.01713, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19859 
Train Epoch: 17 [701/1000 22432/32000 (70%)] Loss: 1.96791 (semantic_loss: 0.02044, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18661 
Train Epoch: 17 [706/1000 22592/32000 (71%)] Loss: 1.96192 (semantic_loss: 0.01542, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18900 
Train Epoch: 17 [711/1000 22752/32000 (71%)] Loss: 1.96225 (semantic_loss: 0.01575, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18528 
Train Epoch: 17 [716/1000 22912/32000 (72%)] Loss: 1.96314 (semantic_loss: 0.01566, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19115 
Train Epoch: 17 [721/1000 23072/32000 (72%)] Loss: 1.96310 (semantic_loss: 0.01563, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18674 
Train Epoch: 17 [726/1000 23232/32000 (73%)] Loss: 1.96261 (semantic_loss: 0.01514, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=1.18764 
Train Epoch: 17 [731/1000 23392/32000 (73%)] Loss: 1.96260 (semantic_loss: 0.01610, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18916 
Train Epoch: 17 [736/1000 23552/32000 (74%)] Loss: 1.96341 (semantic_loss: 0.01691, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19291 
Train Epoch: 17 [741/1000 23712/32000 (74%)] Loss: 1.96354 (semantic_loss: 0.01606, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18742 
Train Epoch: 17 [746/1000 23872/32000 (75%)] Loss: 1.96503 (semantic_loss: 0.01755, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19139 
Train Epoch: 17 [751/1000 24032/32000 (75%)] Loss: 1.96445 (semantic_loss: 0.01697, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18602 
Train Epoch: 17 [756/1000 24192/32000 (76%)] Loss: 1.96313 (semantic_loss: 0.01566, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18720 
Train Epoch: 17 [761/1000 24352/32000 (76%)] Loss: 1.97133 (semantic_loss: 0.02385, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18461 
Train Epoch: 17 [766/1000 24512/32000 (77%)] Loss: 1.96513 (semantic_loss: 0.01766, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18424 
Train Epoch: 17 [771/1000 24672/32000 (77%)] Loss: 1.96387 (semantic_loss: 0.01640, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18759 
Train Epoch: 17 [776/1000 24832/32000 (78%)] Loss: 1.96079 (semantic_loss: 0.01429, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19719 
Train Epoch: 17 [781/1000 24992/32000 (78%)] Loss: 1.96439 (semantic_loss: 0.01691, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18795 
Train Epoch: 17 [786/1000 25152/32000 (79%)] Loss: 1.96089 (semantic_loss: 0.01537, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20368 
Train Epoch: 17 [791/1000 25312/32000 (79%)] Loss: 1.96119 (semantic_loss: 0.01274, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18722 
Train Epoch: 17 [796/1000 25472/32000 (80%)] Loss: 1.96126 (semantic_loss: 0.01378, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19502 
Train Epoch: 17 [801/1000 25632/32000 (80%)] Loss: 1.96941 (semantic_loss: 0.02194, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21762 
Train Epoch: 17 [806/1000 25792/32000 (81%)] Loss: 1.96069 (semantic_loss: 0.01419, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18664 
Train Epoch: 17 [811/1000 25952/32000 (81%)] Loss: 1.96506 (semantic_loss: 0.01660, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18828 
Train Epoch: 17 [816/1000 26112/32000 (82%)] Loss: 1.96429 (semantic_loss: 0.01681, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19137 
Train Epoch: 17 [821/1000 26272/32000 (82%)] Loss: 1.96278 (semantic_loss: 0.01531, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20086 
Train Epoch: 17 [826/1000 26432/32000 (83%)] Loss: 1.96049 (semantic_loss: 0.01400, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23292 
Train Epoch: 17 [831/1000 26592/32000 (83%)] Loss: 1.96603 (semantic_loss: 0.01952, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22912 
Train Epoch: 17 [836/1000 26752/32000 (84%)] Loss: 1.96340 (semantic_loss: 0.01592, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19793 
Train Epoch: 17 [841/1000 26912/32000 (84%)] Loss: 1.96531 (semantic_loss: 0.01881, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19920 
Train Epoch: 17 [846/1000 27072/32000 (85%)] Loss: 1.96351 (semantic_loss: 0.01797, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.20394 
Train Epoch: 17 [851/1000 27232/32000 (85%)] Loss: 1.96767 (semantic_loss: 0.01921, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.20210 
Train Epoch: 17 [856/1000 27392/32000 (86%)] Loss: 1.96761 (semantic_loss: 0.02013, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19578 
Train Epoch: 17 [861/1000 27552/32000 (86%)] Loss: 1.96631 (semantic_loss: 0.01883, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19902 
Train Epoch: 17 [866/1000 27712/32000 (87%)] Loss: 1.96172 (semantic_loss: 0.01522, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18875 
Train Epoch: 17 [871/1000 27872/32000 (87%)] Loss: 1.96594 (semantic_loss: 0.01944, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18720 
Train Epoch: 17 [876/1000 28032/32000 (88%)] Loss: 1.96165 (semantic_loss: 0.01515, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19060 
Train Epoch: 17 [881/1000 28192/32000 (88%)] Loss: 1.96391 (semantic_loss: 0.01643, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18801 
Train Epoch: 17 [886/1000 28352/32000 (89%)] Loss: 1.96371 (semantic_loss: 0.01721, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19181 
Train Epoch: 17 [891/1000 28512/32000 (89%)] Loss: 1.96501 (semantic_loss: 0.01850, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19469 
Train Epoch: 17 [896/1000 28672/32000 (90%)] Loss: 1.96164 (semantic_loss: 0.01417, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19321 
Train Epoch: 17 [901/1000 28832/32000 (90%)] Loss: 1.96628 (semantic_loss: 0.01880, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19433 
Train Epoch: 17 [906/1000 28992/32000 (91%)] Loss: 1.96070 (semantic_loss: 0.01420, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18868 
Train Epoch: 17 [911/1000 29152/32000 (91%)] Loss: 1.96272 (semantic_loss: 0.01621, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18558 
Train Epoch: 17 [916/1000 29312/32000 (92%)] Loss: 1.96096 (semantic_loss: 0.01446, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18514 
Train Epoch: 17 [921/1000 29472/32000 (92%)] Loss: 1.96054 (semantic_loss: 0.01403, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18780 
Train Epoch: 17 [926/1000 29632/32000 (93%)] Loss: 1.96248 (semantic_loss: 0.01500, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18943 
Train Epoch: 17 [931/1000 29792/32000 (93%)] Loss: 1.96770 (semantic_loss: 0.02120, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19166 
Train Epoch: 17 [936/1000 29952/32000 (94%)] Loss: 1.95996 (semantic_loss: 0.01346, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19060 
Train Epoch: 17 [941/1000 30112/32000 (94%)] Loss: 1.96463 (semantic_loss: 0.01813, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19049 
Train Epoch: 17 [946/1000 30272/32000 (95%)] Loss: 1.96230 (semantic_loss: 0.01678, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19134 
Train Epoch: 17 [951/1000 30432/32000 (95%)] Loss: 1.96178 (semantic_loss: 0.01431, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18812 
Train Epoch: 17 [956/1000 30592/32000 (96%)] Loss: 1.96763 (semantic_loss: 0.02015, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20330 
Train Epoch: 17 [961/1000 30752/32000 (96%)] Loss: 1.96093 (semantic_loss: 0.01541, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19573 
Train Epoch: 17 [966/1000 30912/32000 (97%)] Loss: 1.96294 (semantic_loss: 0.01546, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19510 
Train Epoch: 17 [971/1000 31072/32000 (97%)] Loss: 1.96371 (semantic_loss: 0.01623, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19031 
Train Epoch: 17 [976/1000 31232/32000 (98%)] Loss: 1.96208 (semantic_loss: 0.01558, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.28724 
Train Epoch: 17 [981/1000 31392/32000 (98%)] Loss: 1.96132 (semantic_loss: 0.01384, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20570 
Train Epoch: 17 [986/1000 31552/32000 (99%)] Loss: 1.96325 (semantic_loss: 0.01578, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20216 
Train Epoch: 17 [991/1000 31712/32000 (99%)] Loss: 1.96365 (semantic_loss: 0.01715, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23500 
Train Epoch: 17 [996/1000 31872/32000 (100%)] Loss: 1.96369 (semantic_loss: 0.01719, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19177 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch17.pth ...
Done in 4.141s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch17.pth ...
Done in 8.639s
removing stale ckpt [epoch 16] [took 0.11s]
 epoch          : 17
 loss           : 1.9640655781030656
 learning_rate  : 9.265100944259211e-06
 n_samples      : 544000
 n_steps        : 17000
 ActivityNet_val1_test/t2v_metrics/R1: 11.246695139312589
 ActivityNet_val1_test/t2v_metrics/R5: 38.15334553589587
 ActivityNet_val1_test/t2v_metrics/R10: 55.17592027659142
 ActivityNet_val1_test/t2v_metrics/R50: 85.56030099654261
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 52.8672971323978
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 28.714574030751553
 ActivityNet_val1_test/v2t_metrics/R1: 12.141549725442342
 ActivityNet_val1_test/v2t_metrics/R5: 38.966849705104735
 ActivityNet_val1_test/v2t_metrics/R10: 55.23693308928208
 ActivityNet_val1_test/v2t_metrics/R50: 85.4586129753915
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 54.92607280862315
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.675610271099618
 mnt_best       : 28.714574030751553
 not_improved_count: 0
Train Epoch: 18 [1/1000 32/32000 (0%)] Loss: 1.96331 (semantic_loss: 0.01779, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=24.30571 
Train Epoch: 18 [6/1000 192/32000 (1%)] Loss: 1.96166 (semantic_loss: 0.01419, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.30183 
Train Epoch: 18 [11/1000 352/32000 (1%)] Loss: 1.96488 (semantic_loss: 0.01741, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18576 
Train Epoch: 18 [16/1000 512/32000 (2%)] Loss: 1.96213 (semantic_loss: 0.01563, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23703 
Train Epoch: 18 [21/1000 672/32000 (2%)] Loss: 1.96474 (semantic_loss: 0.01824, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18791 
Train Epoch: 18 [26/1000 832/32000 (3%)] Loss: 1.96265 (semantic_loss: 0.01517, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25426 
Train Epoch: 18 [31/1000 992/32000 (3%)] Loss: 1.96729 (semantic_loss: 0.01884, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18479 
Train Epoch: 18 [36/1000 1152/32000 (4%)] Loss: 1.96525 (semantic_loss: 0.01777, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18704 
Train Epoch: 18 [41/1000 1312/32000 (4%)] Loss: 1.96683 (semantic_loss: 0.01935, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.27157 
Train Epoch: 18 [46/1000 1472/32000 (5%)] Loss: 1.96176 (semantic_loss: 0.01525, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18845 
Train Epoch: 18 [51/1000 1632/32000 (5%)] Loss: 1.95985 (semantic_loss: 0.01237, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18666 
Train Epoch: 18 [56/1000 1792/32000 (6%)] Loss: 1.96632 (semantic_loss: 0.01982, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21511 
Train Epoch: 18 [61/1000 1952/32000 (6%)] Loss: 1.96348 (semantic_loss: 0.01503, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18797 
Train Epoch: 18 [66/1000 2112/32000 (7%)] Loss: 1.96515 (semantic_loss: 0.01670, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18765 
Train Epoch: 18 [71/1000 2272/32000 (7%)] Loss: 1.96656 (semantic_loss: 0.01908, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18807 
Train Epoch: 18 [76/1000 2432/32000 (8%)] Loss: 1.96287 (semantic_loss: 0.01540, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21195 
Train Epoch: 18 [81/1000 2592/32000 (8%)] Loss: 1.96006 (semantic_loss: 0.01356, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.63166 
Train Epoch: 18 [86/1000 2752/32000 (9%)] Loss: 1.96282 (semantic_loss: 0.01535, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22919 
Train Epoch: 18 [91/1000 2912/32000 (9%)] Loss: 1.96019 (semantic_loss: 0.01369, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20549 
Train Epoch: 18 [96/1000 3072/32000 (10%)] Loss: 1.96395 (semantic_loss: 0.01550, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20266 
Train Epoch: 18 [101/1000 3232/32000 (10%)] Loss: 1.96520 (semantic_loss: 0.01772, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19910 
Train Epoch: 18 [106/1000 3392/32000 (11%)] Loss: 1.96331 (semantic_loss: 0.01583, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21861 
Train Epoch: 18 [111/1000 3552/32000 (11%)] Loss: 1.96813 (semantic_loss: 0.01967, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19119 
Train Epoch: 18 [116/1000 3712/32000 (12%)] Loss: 1.96416 (semantic_loss: 0.01766, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20985 
Train Epoch: 18 [121/1000 3872/32000 (12%)] Loss: 1.96574 (semantic_loss: 0.01728, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18985 
Train Epoch: 18 [126/1000 4032/32000 (13%)] Loss: 1.96557 (semantic_loss: 0.01907, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18671 
Train Epoch: 18 [131/1000 4192/32000 (13%)] Loss: 1.96139 (semantic_loss: 0.01489, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18793 
Train Epoch: 18 [136/1000 4352/32000 (14%)] Loss: 1.96188 (semantic_loss: 0.01538, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20958 
Train Epoch: 18 [141/1000 4512/32000 (14%)] Loss: 1.96754 (semantic_loss: 0.01909, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20462 
Train Epoch: 18 [146/1000 4672/32000 (15%)] Loss: 1.96524 (semantic_loss: 0.01678, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.25546 
Train Epoch: 18 [151/1000 4832/32000 (15%)] Loss: 1.96065 (semantic_loss: 0.01415, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18910 
Train Epoch: 18 [156/1000 4992/32000 (16%)] Loss: 1.96488 (semantic_loss: 0.01838, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22705 
Train Epoch: 18 [161/1000 5152/32000 (16%)] Loss: 1.96766 (semantic_loss: 0.02017, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.23836 
Train Epoch: 18 [166/1000 5312/32000 (17%)] Loss: 1.96262 (semantic_loss: 0.01514, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.32828 
Train Epoch: 18 [171/1000 5472/32000 (17%)] Loss: 1.96381 (semantic_loss: 0.01634, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18793 
Train Epoch: 18 [176/1000 5632/32000 (18%)] Loss: 1.96435 (semantic_loss: 0.01686, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18581 
Train Epoch: 18 [181/1000 5792/32000 (18%)] Loss: 1.96323 (semantic_loss: 0.01673, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20354 
Train Epoch: 18 [186/1000 5952/32000 (19%)] Loss: 1.95958 (semantic_loss: 0.01308, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18627 
Train Epoch: 18 [191/1000 6112/32000 (19%)] Loss: 1.96336 (semantic_loss: 0.01686, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19489 
Train Epoch: 18 [196/1000 6272/32000 (20%)] Loss: 1.96346 (semantic_loss: 0.01599, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18834 
Train Epoch: 18 [201/1000 6432/32000 (20%)] Loss: 1.96327 (semantic_loss: 0.01579, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18633 
Train Epoch: 18 [206/1000 6592/32000 (21%)] Loss: 1.96434 (semantic_loss: 0.01589, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19149 
Train Epoch: 18 [211/1000 6752/32000 (21%)] Loss: 1.96249 (semantic_loss: 0.01599, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18820 
Train Epoch: 18 [216/1000 6912/32000 (22%)] Loss: 1.96755 (semantic_loss: 0.01910, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18783 
Train Epoch: 18 [221/1000 7072/32000 (22%)] Loss: 1.96456 (semantic_loss: 0.01708, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18778 
Train Epoch: 18 [226/1000 7232/32000 (23%)] Loss: 1.96520 (semantic_loss: 0.01772, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.27235 
Train Epoch: 18 [231/1000 7392/32000 (23%)] Loss: 1.96382 (semantic_loss: 0.01732, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.23202 
Train Epoch: 18 [236/1000 7552/32000 (24%)] Loss: 1.97121 (semantic_loss: 0.02276, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20244 
Train Epoch: 18 [241/1000 7712/32000 (24%)] Loss: 1.96378 (semantic_loss: 0.01825, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.23132 
Train Epoch: 18 [246/1000 7872/32000 (25%)] Loss: 1.95944 (semantic_loss: 0.01392, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.22717 
Train Epoch: 18 [251/1000 8032/32000 (25%)] Loss: 1.96380 (semantic_loss: 0.01632, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19321 
Train Epoch: 18 [256/1000 8192/32000 (26%)] Loss: 1.96302 (semantic_loss: 0.01555, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20403 
Train Epoch: 18 [261/1000 8352/32000 (26%)] Loss: 1.96631 (semantic_loss: 0.01883, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20964 
Train Epoch: 18 [266/1000 8512/32000 (27%)] Loss: 1.96314 (semantic_loss: 0.01761, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.20923 
Train Epoch: 18 [271/1000 8672/32000 (27%)] Loss: 1.96713 (semantic_loss: 0.02063, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21365 
Train Epoch: 18 [276/1000 8832/32000 (28%)] Loss: 1.96180 (semantic_loss: 0.01531, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18520 
Train Epoch: 18 [281/1000 8992/32000 (28%)] Loss: 1.96270 (semantic_loss: 0.01620, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20184 
Train Epoch: 18 [286/1000 9152/32000 (29%)] Loss: 1.96478 (semantic_loss: 0.01632, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19915 
Train Epoch: 18 [291/1000 9312/32000 (29%)] Loss: 1.95896 (semantic_loss: 0.01442, quant_loss: 1.94434, bit_balance_loss: 0.00021) batch_time=0.18665 
Train Epoch: 18 [296/1000 9472/32000 (30%)] Loss: 1.96371 (semantic_loss: 0.01721, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19764 
Train Epoch: 18 [301/1000 9632/32000 (30%)] Loss: 1.96524 (semantic_loss: 0.01874, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.26631 
Train Epoch: 18 [306/1000 9792/32000 (31%)] Loss: 1.96397 (semantic_loss: 0.01650, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19051 
Train Epoch: 18 [311/1000 9952/32000 (31%)] Loss: 1.96524 (semantic_loss: 0.01777, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19400 
Train Epoch: 18 [316/1000 10112/32000 (32%)] Loss: 1.96253 (semantic_loss: 0.01408, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20304 
Train Epoch: 18 [321/1000 10272/32000 (32%)] Loss: 1.96979 (semantic_loss: 0.02036, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.30157 
Train Epoch: 18 [326/1000 10432/32000 (33%)] Loss: 1.96368 (semantic_loss: 0.01718, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18626 
Train Epoch: 18 [331/1000 10592/32000 (33%)] Loss: 1.96377 (semantic_loss: 0.01629, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19249 
Train Epoch: 18 [336/1000 10752/32000 (34%)] Loss: 1.96249 (semantic_loss: 0.01599, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.23677 
Train Epoch: 18 [341/1000 10912/32000 (34%)] Loss: 1.96305 (semantic_loss: 0.01558, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20033 
Train Epoch: 18 [346/1000 11072/32000 (35%)] Loss: 1.96231 (semantic_loss: 0.01580, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.25857 
Train Epoch: 18 [351/1000 11232/32000 (35%)] Loss: 1.96323 (semantic_loss: 0.01673, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20221 
Train Epoch: 18 [356/1000 11392/32000 (36%)] Loss: 1.96529 (semantic_loss: 0.01782, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19581 
Train Epoch: 18 [361/1000 11552/32000 (36%)] Loss: 1.96437 (semantic_loss: 0.01495, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.27586 
Train Epoch: 18 [366/1000 11712/32000 (37%)] Loss: 1.96444 (semantic_loss: 0.01891, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19244 
Train Epoch: 18 [371/1000 11872/32000 (37%)] Loss: 1.96335 (semantic_loss: 0.01685, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20401 
Train Epoch: 18 [376/1000 12032/32000 (38%)] Loss: 1.96298 (semantic_loss: 0.01453, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21923 
Train Epoch: 18 [381/1000 12192/32000 (38%)] Loss: 1.96228 (semantic_loss: 0.01578, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19383 
Train Epoch: 18 [386/1000 12352/32000 (39%)] Loss: 1.96234 (semantic_loss: 0.01487, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19395 
Train Epoch: 18 [391/1000 12512/32000 (39%)] Loss: 1.96644 (semantic_loss: 0.01897, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20281 
Train Epoch: 18 [396/1000 12672/32000 (40%)] Loss: 1.96206 (semantic_loss: 0.01361, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21217 
Train Epoch: 18 [401/1000 12832/32000 (40%)] Loss: 1.96209 (semantic_loss: 0.01559, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.67588 
Train Epoch: 18 [406/1000 12992/32000 (41%)] Loss: 1.96259 (semantic_loss: 0.01608, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22706 
Train Epoch: 18 [411/1000 13152/32000 (41%)] Loss: 1.96355 (semantic_loss: 0.01608, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21453 
Train Epoch: 18 [416/1000 13312/32000 (42%)] Loss: 1.96441 (semantic_loss: 0.01596, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20625 
Train Epoch: 18 [421/1000 13472/32000 (42%)] Loss: 1.96225 (semantic_loss: 0.01477, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20658 
Train Epoch: 18 [426/1000 13632/32000 (43%)] Loss: 1.95955 (semantic_loss: 0.01403, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19791 
Train Epoch: 18 [431/1000 13792/32000 (43%)] Loss: 1.96209 (semantic_loss: 0.01462, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21875 
Train Epoch: 18 [436/1000 13952/32000 (44%)] Loss: 1.96708 (semantic_loss: 0.01961, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20685 
Train Epoch: 18 [441/1000 14112/32000 (44%)] Loss: 1.96059 (semantic_loss: 0.01409, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18820 
Train Epoch: 18 [446/1000 14272/32000 (45%)] Loss: 1.96237 (semantic_loss: 0.01587, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19088 
Train Epoch: 18 [451/1000 14432/32000 (45%)] Loss: 1.96665 (semantic_loss: 0.02015, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19904 
Train Epoch: 18 [456/1000 14592/32000 (46%)] Loss: 1.96338 (semantic_loss: 0.01688, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20556 
Train Epoch: 18 [461/1000 14752/32000 (46%)] Loss: 1.96423 (semantic_loss: 0.01675, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20297 
Train Epoch: 18 [466/1000 14912/32000 (47%)] Loss: 1.96534 (semantic_loss: 0.01786, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25774 
Train Epoch: 18 [471/1000 15072/32000 (47%)] Loss: 1.96723 (semantic_loss: 0.01976, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18906 
Train Epoch: 18 [476/1000 15232/32000 (48%)] Loss: 1.96570 (semantic_loss: 0.01823, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22081 
Train Epoch: 18 [481/1000 15392/32000 (48%)] Loss: 1.96324 (semantic_loss: 0.01576, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23543 
Train Epoch: 18 [486/1000 15552/32000 (49%)] Loss: 1.96314 (semantic_loss: 0.01469, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.32143 
Train Epoch: 18 [491/1000 15712/32000 (49%)] Loss: 1.96234 (semantic_loss: 0.01486, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18544 
Train Epoch: 18 [496/1000 15872/32000 (50%)] Loss: 1.96484 (semantic_loss: 0.01932, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18477 
Train Epoch: 18 [501/1000 16032/32000 (50%)] Loss: 1.96519 (semantic_loss: 0.01771, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21599 
Train Epoch: 18 [506/1000 16192/32000 (51%)] Loss: 1.96451 (semantic_loss: 0.01703, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18733 
Train Epoch: 18 [511/1000 16352/32000 (51%)] Loss: 1.96159 (semantic_loss: 0.01509, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21684 
Train Epoch: 18 [516/1000 16512/32000 (52%)] Loss: 1.96671 (semantic_loss: 0.01924, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21076 
Train Epoch: 18 [521/1000 16672/32000 (52%)] Loss: 1.96396 (semantic_loss: 0.01747, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19646 
Train Epoch: 18 [526/1000 16832/32000 (53%)] Loss: 1.96423 (semantic_loss: 0.01675, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18863 
Train Epoch: 18 [531/1000 16992/32000 (53%)] Loss: 1.96404 (semantic_loss: 0.01657, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20155 
Train Epoch: 18 [536/1000 17152/32000 (54%)] Loss: 1.96182 (semantic_loss: 0.01532, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20911 
Train Epoch: 18 [541/1000 17312/32000 (54%)] Loss: 1.96541 (semantic_loss: 0.01793, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20169 
Train Epoch: 18 [546/1000 17472/32000 (55%)] Loss: 1.96717 (semantic_loss: 0.01970, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.31382 
Train Epoch: 18 [551/1000 17632/32000 (55%)] Loss: 1.96157 (semantic_loss: 0.01507, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.26720 
Train Epoch: 18 [556/1000 17792/32000 (56%)] Loss: 1.96472 (semantic_loss: 0.01725, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20914 
Train Epoch: 18 [561/1000 17952/32000 (56%)] Loss: 1.96137 (semantic_loss: 0.01487, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19456 
Train Epoch: 18 [566/1000 18112/32000 (57%)] Loss: 1.96138 (semantic_loss: 0.01488, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20011 
Train Epoch: 18 [571/1000 18272/32000 (57%)] Loss: 1.96288 (semantic_loss: 0.01540, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21818 
Train Epoch: 18 [576/1000 18432/32000 (58%)] Loss: 1.96257 (semantic_loss: 0.01607, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19219 
Train Epoch: 18 [581/1000 18592/32000 (58%)] Loss: 1.96511 (semantic_loss: 0.01860, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19082 
Train Epoch: 18 [586/1000 18752/32000 (59%)] Loss: 1.96312 (semantic_loss: 0.01662, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18936 
Train Epoch: 18 [591/1000 18912/32000 (59%)] Loss: 1.96583 (semantic_loss: 0.01835, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20524 
Train Epoch: 18 [596/1000 19072/32000 (60%)] Loss: 1.96335 (semantic_loss: 0.01588, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21287 
Train Epoch: 18 [601/1000 19232/32000 (60%)] Loss: 1.96378 (semantic_loss: 0.01826, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.22408 
Train Epoch: 18 [606/1000 19392/32000 (61%)] Loss: 1.96338 (semantic_loss: 0.01590, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20355 
Train Epoch: 18 [611/1000 19552/32000 (61%)] Loss: 1.96328 (semantic_loss: 0.01580, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19751 
Train Epoch: 18 [616/1000 19712/32000 (62%)] Loss: 1.96142 (semantic_loss: 0.01492, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19201 
Train Epoch: 18 [621/1000 19872/32000 (62%)] Loss: 1.96518 (semantic_loss: 0.01770, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26771 
Train Epoch: 18 [626/1000 20032/32000 (63%)] Loss: 1.96418 (semantic_loss: 0.01671, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18739 
Train Epoch: 18 [631/1000 20192/32000 (63%)] Loss: 1.96144 (semantic_loss: 0.01494, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19002 
Train Epoch: 18 [636/1000 20352/32000 (64%)] Loss: 1.96404 (semantic_loss: 0.01656, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19038 
Train Epoch: 18 [641/1000 20512/32000 (64%)] Loss: 1.96631 (semantic_loss: 0.01884, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.30181 
Train Epoch: 18 [646/1000 20672/32000 (65%)] Loss: 1.96428 (semantic_loss: 0.01680, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18822 
Train Epoch: 18 [651/1000 20832/32000 (65%)] Loss: 1.96255 (semantic_loss: 0.01507, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18535 
Train Epoch: 18 [656/1000 20992/32000 (66%)] Loss: 1.96756 (semantic_loss: 0.02007, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.23746 
Train Epoch: 18 [661/1000 21152/32000 (66%)] Loss: 1.96297 (semantic_loss: 0.01549, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18712 
Train Epoch: 18 [666/1000 21312/32000 (67%)] Loss: 1.96232 (semantic_loss: 0.01582, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24745 
Train Epoch: 18 [671/1000 21472/32000 (67%)] Loss: 1.96593 (semantic_loss: 0.01845, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21026 
Train Epoch: 18 [676/1000 21632/32000 (68%)] Loss: 1.96101 (semantic_loss: 0.01452, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19007 
Train Epoch: 18 [681/1000 21792/32000 (68%)] Loss: 1.96702 (semantic_loss: 0.01955, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26470 
Train Epoch: 18 [686/1000 21952/32000 (69%)] Loss: 1.96633 (semantic_loss: 0.01983, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18933 
Train Epoch: 18 [691/1000 22112/32000 (69%)] Loss: 1.96585 (semantic_loss: 0.01838, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18732 
Train Epoch: 18 [696/1000 22272/32000 (70%)] Loss: 1.96224 (semantic_loss: 0.01477, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20441 
Train Epoch: 18 [701/1000 22432/32000 (70%)] Loss: 1.96200 (semantic_loss: 0.01452, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23579 
Train Epoch: 18 [706/1000 22592/32000 (71%)] Loss: 1.96446 (semantic_loss: 0.01699, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20668 
Train Epoch: 18 [711/1000 22752/32000 (71%)] Loss: 1.96673 (semantic_loss: 0.02023, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21023 
Train Epoch: 18 [716/1000 22912/32000 (72%)] Loss: 1.96730 (semantic_loss: 0.02080, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22189 
Train Epoch: 18 [721/1000 23072/32000 (72%)] Loss: 1.95888 (semantic_loss: 0.01336, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.56870 
Train Epoch: 18 [726/1000 23232/32000 (73%)] Loss: 1.96023 (semantic_loss: 0.01373, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20789 
Train Epoch: 18 [731/1000 23392/32000 (73%)] Loss: 1.96332 (semantic_loss: 0.01585, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20973 
Train Epoch: 18 [736/1000 23552/32000 (74%)] Loss: 1.96190 (semantic_loss: 0.01541, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18811 
Train Epoch: 18 [741/1000 23712/32000 (74%)] Loss: 1.96366 (semantic_loss: 0.01618, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18806 
Train Epoch: 18 [746/1000 23872/32000 (75%)] Loss: 1.96126 (semantic_loss: 0.01379, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18611 
Train Epoch: 18 [751/1000 24032/32000 (75%)] Loss: 1.96409 (semantic_loss: 0.01661, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18807 
Train Epoch: 18 [756/1000 24192/32000 (76%)] Loss: 1.96059 (semantic_loss: 0.01409, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18351 
Train Epoch: 18 [761/1000 24352/32000 (76%)] Loss: 1.96141 (semantic_loss: 0.01394, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19268 
Train Epoch: 18 [766/1000 24512/32000 (77%)] Loss: 1.96090 (semantic_loss: 0.01343, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19265 
Train Epoch: 18 [771/1000 24672/32000 (77%)] Loss: 1.96305 (semantic_loss: 0.01558, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21351 
Train Epoch: 18 [776/1000 24832/32000 (78%)] Loss: 1.96112 (semantic_loss: 0.01462, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20071 
Train Epoch: 18 [781/1000 24992/32000 (78%)] Loss: 1.96508 (semantic_loss: 0.01662, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19562 
Train Epoch: 18 [786/1000 25152/32000 (79%)] Loss: 1.96404 (semantic_loss: 0.01656, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25093 
Train Epoch: 18 [791/1000 25312/32000 (79%)] Loss: 1.96346 (semantic_loss: 0.01598, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20977 
Train Epoch: 18 [796/1000 25472/32000 (80%)] Loss: 1.96406 (semantic_loss: 0.01561, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22217 
Train Epoch: 18 [801/1000 25632/32000 (80%)] Loss: 1.96285 (semantic_loss: 0.01538, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23127 
Train Epoch: 18 [806/1000 25792/32000 (81%)] Loss: 1.96184 (semantic_loss: 0.01534, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.33108 
Train Epoch: 18 [811/1000 25952/32000 (81%)] Loss: 1.96432 (semantic_loss: 0.01685, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20265 
Train Epoch: 18 [816/1000 26112/32000 (82%)] Loss: 1.96121 (semantic_loss: 0.01471, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19009 
Train Epoch: 18 [821/1000 26272/32000 (82%)] Loss: 1.96203 (semantic_loss: 0.01358, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20480 
Train Epoch: 18 [826/1000 26432/32000 (83%)] Loss: 1.96904 (semantic_loss: 0.02156, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20015 
Train Epoch: 18 [831/1000 26592/32000 (83%)] Loss: 1.96267 (semantic_loss: 0.01617, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19563 
Train Epoch: 18 [836/1000 26752/32000 (84%)] Loss: 1.96642 (semantic_loss: 0.01797, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18767 
Train Epoch: 18 [841/1000 26912/32000 (84%)] Loss: 1.96345 (semantic_loss: 0.01597, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18851 
Train Epoch: 18 [846/1000 27072/32000 (85%)] Loss: 1.96336 (semantic_loss: 0.01589, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18827 
Train Epoch: 18 [851/1000 27232/32000 (85%)] Loss: 1.96139 (semantic_loss: 0.01391, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20873 
Train Epoch: 18 [856/1000 27392/32000 (86%)] Loss: 1.96435 (semantic_loss: 0.01980, quant_loss: 1.94434, bit_balance_loss: 0.00021) batch_time=0.22365 
Train Epoch: 18 [861/1000 27552/32000 (86%)] Loss: 1.96633 (semantic_loss: 0.01885, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20161 
Train Epoch: 18 [866/1000 27712/32000 (87%)] Loss: 1.96541 (semantic_loss: 0.01696, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.31865 
Train Epoch: 18 [871/1000 27872/32000 (87%)] Loss: 1.96324 (semantic_loss: 0.01576, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.27555 
Train Epoch: 18 [876/1000 28032/32000 (88%)] Loss: 1.96517 (semantic_loss: 0.01770, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20395 
Train Epoch: 18 [881/1000 28192/32000 (88%)] Loss: 1.96568 (semantic_loss: 0.01722, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21733 
Train Epoch: 18 [886/1000 28352/32000 (89%)] Loss: 1.96501 (semantic_loss: 0.01850, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19004 
Train Epoch: 18 [891/1000 28512/32000 (89%)] Loss: 1.96165 (semantic_loss: 0.01418, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18885 
Train Epoch: 18 [896/1000 28672/32000 (90%)] Loss: 1.96347 (semantic_loss: 0.01697, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20961 
Train Epoch: 18 [901/1000 28832/32000 (90%)] Loss: 1.96164 (semantic_loss: 0.01514, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19328 
Train Epoch: 18 [906/1000 28992/32000 (91%)] Loss: 1.96793 (semantic_loss: 0.01948, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18624 
Train Epoch: 18 [911/1000 29152/32000 (91%)] Loss: 1.96332 (semantic_loss: 0.01681, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22399 
Train Epoch: 18 [916/1000 29312/32000 (92%)] Loss: 1.96618 (semantic_loss: 0.02066, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.21133 
Train Epoch: 18 [921/1000 29472/32000 (92%)] Loss: 1.96575 (semantic_loss: 0.01925, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21182 
Train Epoch: 18 [926/1000 29632/32000 (93%)] Loss: 1.96374 (semantic_loss: 0.01626, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19901 
Train Epoch: 18 [931/1000 29792/32000 (93%)] Loss: 1.95959 (semantic_loss: 0.01309, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18814 
Train Epoch: 18 [936/1000 29952/32000 (94%)] Loss: 1.96333 (semantic_loss: 0.01781, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18647 
Train Epoch: 18 [941/1000 30112/32000 (94%)] Loss: 1.96315 (semantic_loss: 0.01470, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.25903 
Train Epoch: 18 [946/1000 30272/32000 (95%)] Loss: 1.96755 (semantic_loss: 0.02105, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19035 
Train Epoch: 18 [951/1000 30432/32000 (95%)] Loss: 1.96596 (semantic_loss: 0.02044, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18864 
Train Epoch: 18 [956/1000 30592/32000 (96%)] Loss: 1.96894 (semantic_loss: 0.02147, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18661 
Train Epoch: 18 [961/1000 30752/32000 (96%)] Loss: 1.96726 (semantic_loss: 0.01978, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.31436 
Train Epoch: 18 [966/1000 30912/32000 (97%)] Loss: 1.96293 (semantic_loss: 0.01644, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18865 
Train Epoch: 18 [971/1000 31072/32000 (97%)] Loss: 1.96052 (semantic_loss: 0.01305, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18921 
Train Epoch: 18 [976/1000 31232/32000 (98%)] Loss: 1.96279 (semantic_loss: 0.01532, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23938 
Train Epoch: 18 [981/1000 31392/32000 (98%)] Loss: 1.96315 (semantic_loss: 0.01567, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19324 
Train Epoch: 18 [986/1000 31552/32000 (99%)] Loss: 1.96192 (semantic_loss: 0.01541, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.31187 
Train Epoch: 18 [991/1000 31712/32000 (99%)] Loss: 1.96113 (semantic_loss: 0.01365, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19012 
Train Epoch: 18 [996/1000 31872/32000 (100%)] Loss: 1.96472 (semantic_loss: 0.01627, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18909 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch18.pth ...
Done in 4.435s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch18.pth ...
Done in 8.615s
removing stale ckpt [epoch 17] [took 0.00s]
 epoch          : 18
 loss           : 1.963857043504715
 learning_rate  : 8.33859084983329e-06
 n_samples      : 576000
 n_steps        : 18000
 ActivityNet_val1_test/t2v_metrics/R1: 11.61277201545658
 ActivityNet_val1_test/t2v_metrics/R5: 37.685580638600776
 ActivityNet_val1_test/t2v_metrics/R10: 54.60646735814521
 ActivityNet_val1_test/t2v_metrics/R50: 85.53996339231239
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 55.318690258287575
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 28.80392894496838
 ActivityNet_val1_test/v2t_metrics/R1: 12.22290014236323
 ActivityNet_val1_test/v2t_metrics/R5: 38.763473662802525
 ActivityNet_val1_test/v2t_metrics/R10: 55.19625788082164
 ActivityNet_val1_test/v2t_metrics/R50: 85.37726255847062
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 57.74150905023388
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.68261749543259
 mnt_best       : 28.80392894496838
 not_improved_count: 0
Train Epoch: 19 [1/1000 32/32000 (0%)] Loss: 1.96524 (semantic_loss: 0.01776, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=22.33232 
Train Epoch: 19 [6/1000 192/32000 (1%)] Loss: 1.96326 (semantic_loss: 0.01578, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18841 
Train Epoch: 19 [11/1000 352/32000 (1%)] Loss: 1.96122 (semantic_loss: 0.01472, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18745 
Train Epoch: 19 [16/1000 512/32000 (2%)] Loss: 1.96207 (semantic_loss: 0.01460, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20257 
Train Epoch: 19 [21/1000 672/32000 (2%)] Loss: 1.96397 (semantic_loss: 0.01845, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18560 
Train Epoch: 19 [26/1000 832/32000 (3%)] Loss: 1.96670 (semantic_loss: 0.01922, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19451 
Train Epoch: 19 [31/1000 992/32000 (3%)] Loss: 1.96297 (semantic_loss: 0.01549, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18940 
Train Epoch: 19 [36/1000 1152/32000 (4%)] Loss: 1.96207 (semantic_loss: 0.01460, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19190 
Train Epoch: 19 [41/1000 1312/32000 (4%)] Loss: 1.96750 (semantic_loss: 0.02002, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18641 
Train Epoch: 19 [46/1000 1472/32000 (5%)] Loss: 1.96399 (semantic_loss: 0.01749, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.50093 
Train Epoch: 19 [51/1000 1632/32000 (5%)] Loss: 1.96856 (semantic_loss: 0.02108, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18924 
Train Epoch: 19 [56/1000 1792/32000 (6%)] Loss: 1.96270 (semantic_loss: 0.01522, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18633 
Train Epoch: 19 [61/1000 1952/32000 (6%)] Loss: 1.96297 (semantic_loss: 0.01550, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18805 
Train Epoch: 19 [66/1000 2112/32000 (7%)] Loss: 1.96274 (semantic_loss: 0.01429, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.25565 
Train Epoch: 19 [71/1000 2272/32000 (7%)] Loss: 1.96887 (semantic_loss: 0.02237, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.77660 
Train Epoch: 19 [76/1000 2432/32000 (8%)] Loss: 1.96565 (semantic_loss: 0.01914, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18619 
Train Epoch: 19 [81/1000 2592/32000 (8%)] Loss: 1.96116 (semantic_loss: 0.01662, quant_loss: 1.94434, bit_balance_loss: 0.00021) batch_time=0.18545 
Train Epoch: 19 [86/1000 2752/32000 (9%)] Loss: 1.96202 (semantic_loss: 0.01454, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19364 
Train Epoch: 19 [91/1000 2912/32000 (9%)] Loss: 1.96444 (semantic_loss: 0.01697, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18504 
Train Epoch: 19 [96/1000 3072/32000 (10%)] Loss: 1.96257 (semantic_loss: 0.01510, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18760 
Train Epoch: 19 [101/1000 3232/32000 (10%)] Loss: 1.97050 (semantic_loss: 0.02401, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18654 
Train Epoch: 19 [106/1000 3392/32000 (11%)] Loss: 1.96243 (semantic_loss: 0.01594, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22736 
Train Epoch: 19 [111/1000 3552/32000 (11%)] Loss: 1.96859 (semantic_loss: 0.02112, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18592 
Train Epoch: 19 [116/1000 3712/32000 (12%)] Loss: 1.96314 (semantic_loss: 0.01664, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21395 
Train Epoch: 19 [121/1000 3872/32000 (12%)] Loss: 1.96150 (semantic_loss: 0.01401, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.21749 
Train Epoch: 19 [126/1000 4032/32000 (13%)] Loss: 1.96652 (semantic_loss: 0.01904, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.23389 
Train Epoch: 19 [131/1000 4192/32000 (13%)] Loss: 1.96447 (semantic_loss: 0.01699, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20929 
Train Epoch: 19 [136/1000 4352/32000 (14%)] Loss: 1.96262 (semantic_loss: 0.01514, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20531 
Train Epoch: 19 [141/1000 4512/32000 (14%)] Loss: 1.96502 (semantic_loss: 0.01754, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19090 
Train Epoch: 19 [146/1000 4672/32000 (15%)] Loss: 1.96385 (semantic_loss: 0.01735, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19237 
Train Epoch: 19 [151/1000 4832/32000 (15%)] Loss: 1.96394 (semantic_loss: 0.01549, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19047 
Train Epoch: 19 [156/1000 4992/32000 (16%)] Loss: 1.96243 (semantic_loss: 0.01495, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19543 
Train Epoch: 19 [161/1000 5152/32000 (16%)] Loss: 1.96690 (semantic_loss: 0.01942, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18742 
Train Epoch: 19 [166/1000 5312/32000 (17%)] Loss: 1.96483 (semantic_loss: 0.01833, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18891 
Train Epoch: 19 [171/1000 5472/32000 (17%)] Loss: 1.96119 (semantic_loss: 0.01469, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.32996 
Train Epoch: 19 [176/1000 5632/32000 (18%)] Loss: 1.96026 (semantic_loss: 0.01376, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19387 
Train Epoch: 19 [181/1000 5792/32000 (18%)] Loss: 1.96505 (semantic_loss: 0.01757, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19387 
Train Epoch: 19 [186/1000 5952/32000 (19%)] Loss: 1.96261 (semantic_loss: 0.01611, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19839 
Train Epoch: 19 [191/1000 6112/32000 (19%)] Loss: 1.96798 (semantic_loss: 0.02147, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20482 
Train Epoch: 19 [196/1000 6272/32000 (20%)] Loss: 1.96247 (semantic_loss: 0.01402, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18732 
Train Epoch: 19 [201/1000 6432/32000 (20%)] Loss: 1.96359 (semantic_loss: 0.01611, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18773 
Train Epoch: 19 [206/1000 6592/32000 (21%)] Loss: 1.96172 (semantic_loss: 0.01522, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18514 
Train Epoch: 19 [211/1000 6752/32000 (21%)] Loss: 1.95965 (semantic_loss: 0.01413, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18705 
Train Epoch: 19 [216/1000 6912/32000 (22%)] Loss: 1.96471 (semantic_loss: 0.01723, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18505 
Train Epoch: 19 [221/1000 7072/32000 (22%)] Loss: 1.96684 (semantic_loss: 0.02034, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18298 
Train Epoch: 19 [226/1000 7232/32000 (23%)] Loss: 1.96121 (semantic_loss: 0.01373, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18453 
Train Epoch: 19 [231/1000 7392/32000 (23%)] Loss: 1.96487 (semantic_loss: 0.01739, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18641 
Train Epoch: 19 [236/1000 7552/32000 (24%)] Loss: 1.96766 (semantic_loss: 0.01921, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18595 
Train Epoch: 19 [241/1000 7712/32000 (24%)] Loss: 1.96462 (semantic_loss: 0.01713, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18502 
Train Epoch: 19 [246/1000 7872/32000 (25%)] Loss: 1.96362 (semantic_loss: 0.01614, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19462 
Train Epoch: 19 [251/1000 8032/32000 (25%)] Loss: 1.96473 (semantic_loss: 0.01726, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21741 
Train Epoch: 19 [256/1000 8192/32000 (26%)] Loss: 1.96154 (semantic_loss: 0.01504, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18639 
Train Epoch: 19 [261/1000 8352/32000 (26%)] Loss: 1.96335 (semantic_loss: 0.01588, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18766 
Train Epoch: 19 [266/1000 8512/32000 (27%)] Loss: 1.96267 (semantic_loss: 0.01618, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18781 
Train Epoch: 19 [271/1000 8672/32000 (27%)] Loss: 1.96313 (semantic_loss: 0.01468, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18940 
Train Epoch: 19 [276/1000 8832/32000 (28%)] Loss: 1.96002 (semantic_loss: 0.01352, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21551 
Train Epoch: 19 [281/1000 8992/32000 (28%)] Loss: 1.96325 (semantic_loss: 0.01578, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20562 
Train Epoch: 19 [286/1000 9152/32000 (29%)] Loss: 1.96185 (semantic_loss: 0.01438, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21518 
Train Epoch: 19 [291/1000 9312/32000 (29%)] Loss: 1.96174 (semantic_loss: 0.01524, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22508 
Train Epoch: 19 [296/1000 9472/32000 (30%)] Loss: 1.96418 (semantic_loss: 0.01671, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.36406 
Train Epoch: 19 [301/1000 9632/32000 (30%)] Loss: 1.96360 (semantic_loss: 0.01612, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19084 
Train Epoch: 19 [306/1000 9792/32000 (31%)] Loss: 1.96148 (semantic_loss: 0.01498, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.27628 
Train Epoch: 19 [311/1000 9952/32000 (31%)] Loss: 1.96376 (semantic_loss: 0.01628, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.23671 
Train Epoch: 19 [316/1000 10112/32000 (32%)] Loss: 1.96155 (semantic_loss: 0.01505, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19985 
Train Epoch: 19 [321/1000 10272/32000 (32%)] Loss: 1.96076 (semantic_loss: 0.01425, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19179 
Train Epoch: 19 [326/1000 10432/32000 (33%)] Loss: 1.96294 (semantic_loss: 0.01547, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18825 
Train Epoch: 19 [331/1000 10592/32000 (33%)] Loss: 1.96385 (semantic_loss: 0.01735, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18902 
Train Epoch: 19 [336/1000 10752/32000 (34%)] Loss: 1.96568 (semantic_loss: 0.01821, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18571 
Train Epoch: 19 [341/1000 10912/32000 (34%)] Loss: 1.96435 (semantic_loss: 0.01688, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19564 
Train Epoch: 19 [346/1000 11072/32000 (35%)] Loss: 1.96487 (semantic_loss: 0.01837, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20686 
Train Epoch: 19 [351/1000 11232/32000 (35%)] Loss: 1.96223 (semantic_loss: 0.01476, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20299 
Train Epoch: 19 [356/1000 11392/32000 (36%)] Loss: 1.96109 (semantic_loss: 0.01557, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18783 
Train Epoch: 19 [361/1000 11552/32000 (36%)] Loss: 1.96154 (semantic_loss: 0.01504, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19040 
Train Epoch: 19 [366/1000 11712/32000 (37%)] Loss: 1.96207 (semantic_loss: 0.01557, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.48868 
Train Epoch: 19 [371/1000 11872/32000 (37%)] Loss: 1.96355 (semantic_loss: 0.01510, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18434 
Train Epoch: 19 [376/1000 12032/32000 (38%)] Loss: 1.96260 (semantic_loss: 0.01708, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18746 
Train Epoch: 19 [381/1000 12192/32000 (38%)] Loss: 1.96502 (semantic_loss: 0.01755, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18928 
Train Epoch: 19 [386/1000 12352/32000 (39%)] Loss: 1.96565 (semantic_loss: 0.01817, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25530 
Train Epoch: 19 [391/1000 12512/32000 (39%)] Loss: 1.96756 (semantic_loss: 0.02008, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.78303 
Train Epoch: 19 [396/1000 12672/32000 (40%)] Loss: 1.96426 (semantic_loss: 0.01678, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21802 
Train Epoch: 19 [401/1000 12832/32000 (40%)] Loss: 1.96353 (semantic_loss: 0.01605, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19213 
Train Epoch: 19 [406/1000 12992/32000 (41%)] Loss: 1.96264 (semantic_loss: 0.01516, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18402 
Train Epoch: 19 [411/1000 13152/32000 (41%)] Loss: 1.96280 (semantic_loss: 0.01532, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.23278 
Train Epoch: 19 [416/1000 13312/32000 (42%)] Loss: 1.96189 (semantic_loss: 0.01539, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18587 
Train Epoch: 19 [421/1000 13472/32000 (42%)] Loss: 1.95989 (semantic_loss: 0.01436, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18687 
Train Epoch: 19 [426/1000 13632/32000 (43%)] Loss: 1.96288 (semantic_loss: 0.01541, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21996 
Train Epoch: 19 [431/1000 13792/32000 (43%)] Loss: 1.96220 (semantic_loss: 0.01473, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19452 
Train Epoch: 19 [436/1000 13952/32000 (44%)] Loss: 1.96386 (semantic_loss: 0.01541, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21755 
Train Epoch: 19 [441/1000 14112/32000 (44%)] Loss: 1.96688 (semantic_loss: 0.01843, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.21009 
Train Epoch: 19 [446/1000 14272/32000 (45%)] Loss: 1.96600 (semantic_loss: 0.01853, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20117 
Train Epoch: 19 [451/1000 14432/32000 (45%)] Loss: 1.96331 (semantic_loss: 0.01681, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19339 
Train Epoch: 19 [456/1000 14592/32000 (46%)] Loss: 1.96427 (semantic_loss: 0.01777, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18858 
Train Epoch: 19 [461/1000 14752/32000 (46%)] Loss: 1.96391 (semantic_loss: 0.01643, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21989 
Train Epoch: 19 [466/1000 14912/32000 (47%)] Loss: 1.96341 (semantic_loss: 0.01496, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19745 
Train Epoch: 19 [471/1000 15072/32000 (47%)] Loss: 1.96057 (semantic_loss: 0.01212, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19039 
Train Epoch: 19 [476/1000 15232/32000 (48%)] Loss: 1.96422 (semantic_loss: 0.01675, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20012 
Train Epoch: 19 [481/1000 15392/32000 (48%)] Loss: 1.96000 (semantic_loss: 0.01351, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18775 
Train Epoch: 19 [486/1000 15552/32000 (49%)] Loss: 1.96295 (semantic_loss: 0.01645, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18719 
Train Epoch: 19 [491/1000 15712/32000 (49%)] Loss: 1.96193 (semantic_loss: 0.01347, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.30547 
Train Epoch: 19 [496/1000 15872/32000 (50%)] Loss: 1.96231 (semantic_loss: 0.01581, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18564 
Train Epoch: 19 [501/1000 16032/32000 (50%)] Loss: 1.96387 (semantic_loss: 0.01542, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20381 
Train Epoch: 19 [506/1000 16192/32000 (51%)] Loss: 1.96344 (semantic_loss: 0.01498, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18968 
Train Epoch: 19 [511/1000 16352/32000 (51%)] Loss: 1.96088 (semantic_loss: 0.01438, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20524 
Train Epoch: 19 [516/1000 16512/32000 (52%)] Loss: 1.96312 (semantic_loss: 0.01466, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18386 
Train Epoch: 19 [521/1000 16672/32000 (52%)] Loss: 1.96582 (semantic_loss: 0.01834, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19132 
Train Epoch: 19 [526/1000 16832/32000 (53%)] Loss: 1.96945 (semantic_loss: 0.02295, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18711 
Train Epoch: 19 [531/1000 16992/32000 (53%)] Loss: 1.96293 (semantic_loss: 0.01643, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19564 
Train Epoch: 19 [536/1000 17152/32000 (54%)] Loss: 1.96465 (semantic_loss: 0.01717, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18637 
Train Epoch: 19 [541/1000 17312/32000 (54%)] Loss: 1.96651 (semantic_loss: 0.01903, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18641 
Train Epoch: 19 [546/1000 17472/32000 (55%)] Loss: 1.95965 (semantic_loss: 0.01413, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18998 
Train Epoch: 19 [551/1000 17632/32000 (55%)] Loss: 1.95954 (semantic_loss: 0.01304, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18833 
Train Epoch: 19 [556/1000 17792/32000 (56%)] Loss: 1.96297 (semantic_loss: 0.01549, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19148 
Train Epoch: 19 [561/1000 17952/32000 (56%)] Loss: 1.96220 (semantic_loss: 0.01375, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18701 
Train Epoch: 19 [566/1000 18112/32000 (57%)] Loss: 1.96629 (semantic_loss: 0.01881, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20207 
Train Epoch: 19 [571/1000 18272/32000 (57%)] Loss: 1.96279 (semantic_loss: 0.01628, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18771 
Train Epoch: 19 [576/1000 18432/32000 (58%)] Loss: 1.96340 (semantic_loss: 0.01689, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19011 
Train Epoch: 19 [581/1000 18592/32000 (58%)] Loss: 1.96203 (semantic_loss: 0.01455, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18787 
Train Epoch: 19 [586/1000 18752/32000 (59%)] Loss: 1.96280 (semantic_loss: 0.01630, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20548 
Train Epoch: 19 [591/1000 18912/32000 (59%)] Loss: 1.96192 (semantic_loss: 0.01542, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20698 
Train Epoch: 19 [596/1000 19072/32000 (60%)] Loss: 1.95993 (semantic_loss: 0.01343, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22150 
Train Epoch: 19 [601/1000 19232/32000 (60%)] Loss: 1.96389 (semantic_loss: 0.01641, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21936 
Train Epoch: 19 [606/1000 19392/32000 (61%)] Loss: 1.96665 (semantic_loss: 0.01918, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19350 
Train Epoch: 19 [611/1000 19552/32000 (61%)] Loss: 1.96356 (semantic_loss: 0.01608, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20038 
Train Epoch: 19 [616/1000 19712/32000 (62%)] Loss: 1.96356 (semantic_loss: 0.01609, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.32127 
Train Epoch: 19 [621/1000 19872/32000 (62%)] Loss: 1.96457 (semantic_loss: 0.01710, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19253 
Train Epoch: 19 [626/1000 20032/32000 (63%)] Loss: 1.96169 (semantic_loss: 0.01618, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.28220 
Train Epoch: 19 [631/1000 20192/32000 (63%)] Loss: 1.96834 (semantic_loss: 0.01989, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19414 
Train Epoch: 19 [636/1000 20352/32000 (64%)] Loss: 1.96626 (semantic_loss: 0.01780, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.18542 
Train Epoch: 19 [641/1000 20512/32000 (64%)] Loss: 1.96414 (semantic_loss: 0.01666, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19918 
Train Epoch: 19 [646/1000 20672/32000 (65%)] Loss: 1.96315 (semantic_loss: 0.01470, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18959 
Train Epoch: 19 [651/1000 20832/32000 (65%)] Loss: 1.96395 (semantic_loss: 0.01550, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19061 
Train Epoch: 19 [656/1000 20992/32000 (66%)] Loss: 1.95965 (semantic_loss: 0.01315, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24632 
Train Epoch: 19 [661/1000 21152/32000 (66%)] Loss: 1.96684 (semantic_loss: 0.02035, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21433 
Train Epoch: 19 [666/1000 21312/32000 (67%)] Loss: 1.96310 (semantic_loss: 0.01758, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18687 
Train Epoch: 19 [671/1000 21472/32000 (67%)] Loss: 1.96588 (semantic_loss: 0.01839, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18987 
Train Epoch: 19 [676/1000 21632/32000 (68%)] Loss: 1.96495 (semantic_loss: 0.01846, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18760 
Train Epoch: 19 [681/1000 21792/32000 (68%)] Loss: 1.96570 (semantic_loss: 0.01920, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19322 
Train Epoch: 19 [686/1000 21952/32000 (69%)] Loss: 1.96170 (semantic_loss: 0.01422, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.49766 
Train Epoch: 19 [691/1000 22112/32000 (69%)] Loss: 1.96453 (semantic_loss: 0.01706, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18795 
Train Epoch: 19 [696/1000 22272/32000 (70%)] Loss: 1.96747 (semantic_loss: 0.01999, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18661 
Train Epoch: 19 [701/1000 22432/32000 (70%)] Loss: 1.96241 (semantic_loss: 0.01591, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19004 
Train Epoch: 19 [706/1000 22592/32000 (71%)] Loss: 1.96294 (semantic_loss: 0.01644, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24960 
Train Epoch: 19 [711/1000 22752/32000 (71%)] Loss: 1.96434 (semantic_loss: 0.01882, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.76909 
Train Epoch: 19 [716/1000 22912/32000 (72%)] Loss: 1.96188 (semantic_loss: 0.01441, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18899 
Train Epoch: 19 [721/1000 23072/32000 (72%)] Loss: 1.96297 (semantic_loss: 0.01647, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18625 
Train Epoch: 19 [726/1000 23232/32000 (73%)] Loss: 1.96324 (semantic_loss: 0.01577, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19322 
Train Epoch: 19 [731/1000 23392/32000 (73%)] Loss: 1.96202 (semantic_loss: 0.01650, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18546 
Train Epoch: 19 [736/1000 23552/32000 (74%)] Loss: 1.96179 (semantic_loss: 0.01529, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18715 
Train Epoch: 19 [741/1000 23712/32000 (74%)] Loss: 1.96108 (semantic_loss: 0.01458, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18642 
Train Epoch: 19 [746/1000 23872/32000 (75%)] Loss: 1.96477 (semantic_loss: 0.01729, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25053 
Train Epoch: 19 [751/1000 24032/32000 (75%)] Loss: 1.96326 (semantic_loss: 0.01578, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23219 
Train Epoch: 19 [756/1000 24192/32000 (76%)] Loss: 1.96916 (semantic_loss: 0.02169, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21475 
Train Epoch: 19 [761/1000 24352/32000 (76%)] Loss: 1.96125 (semantic_loss: 0.01378, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21495 
Train Epoch: 19 [766/1000 24512/32000 (77%)] Loss: 1.96411 (semantic_loss: 0.01663, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20503 
Train Epoch: 19 [771/1000 24672/32000 (77%)] Loss: 1.96358 (semantic_loss: 0.01611, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19448 
Train Epoch: 19 [776/1000 24832/32000 (78%)] Loss: 1.95976 (semantic_loss: 0.01327, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22044 
Train Epoch: 19 [781/1000 24992/32000 (78%)] Loss: 1.96401 (semantic_loss: 0.01751, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19160 
Train Epoch: 19 [786/1000 25152/32000 (79%)] Loss: 1.96911 (semantic_loss: 0.02065, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18407 
Train Epoch: 19 [791/1000 25312/32000 (79%)] Loss: 1.96426 (semantic_loss: 0.01678, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18562 
Train Epoch: 19 [796/1000 25472/32000 (80%)] Loss: 1.96372 (semantic_loss: 0.01625, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18510 
Train Epoch: 19 [801/1000 25632/32000 (80%)] Loss: 1.96445 (semantic_loss: 0.01600, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19047 
Train Epoch: 19 [806/1000 25792/32000 (81%)] Loss: 1.96359 (semantic_loss: 0.01515, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19357 
Train Epoch: 19 [811/1000 25952/32000 (81%)] Loss: 1.96288 (semantic_loss: 0.01541, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.31789 
Train Epoch: 19 [816/1000 26112/32000 (82%)] Loss: 1.96072 (semantic_loss: 0.01325, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19889 
Train Epoch: 19 [821/1000 26272/32000 (82%)] Loss: 1.96033 (semantic_loss: 0.01383, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19102 
Train Epoch: 19 [826/1000 26432/32000 (83%)] Loss: 1.96315 (semantic_loss: 0.01665, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20441 
Train Epoch: 19 [831/1000 26592/32000 (83%)] Loss: 1.96019 (semantic_loss: 0.01369, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19803 
Train Epoch: 19 [836/1000 26752/32000 (84%)] Loss: 1.96821 (semantic_loss: 0.02171, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18819 
Train Epoch: 19 [841/1000 26912/32000 (84%)] Loss: 1.96395 (semantic_loss: 0.01648, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19366 
Train Epoch: 19 [846/1000 27072/32000 (85%)] Loss: 1.96477 (semantic_loss: 0.01729, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18524 
Train Epoch: 19 [851/1000 27232/32000 (85%)] Loss: 1.96309 (semantic_loss: 0.01561, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18691 
Train Epoch: 19 [856/1000 27392/32000 (86%)] Loss: 1.96426 (semantic_loss: 0.01679, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18588 
Train Epoch: 19 [861/1000 27552/32000 (86%)] Loss: 1.96492 (semantic_loss: 0.01647, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19798 
Train Epoch: 19 [866/1000 27712/32000 (87%)] Loss: 1.96824 (semantic_loss: 0.01979, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18356 
Train Epoch: 19 [871/1000 27872/32000 (87%)] Loss: 1.96903 (semantic_loss: 0.02156, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18808 
Train Epoch: 19 [876/1000 28032/32000 (88%)] Loss: 1.96440 (semantic_loss: 0.01595, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18763 
Train Epoch: 19 [881/1000 28192/32000 (88%)] Loss: 1.97092 (semantic_loss: 0.02344, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18527 
Train Epoch: 19 [886/1000 28352/32000 (89%)] Loss: 1.96182 (semantic_loss: 0.01631, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18653 
Train Epoch: 19 [891/1000 28512/32000 (89%)] Loss: 1.96409 (semantic_loss: 0.01759, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18853 
Train Epoch: 19 [896/1000 28672/32000 (90%)] Loss: 1.96208 (semantic_loss: 0.01557, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.19110 
Train Epoch: 19 [901/1000 28832/32000 (90%)] Loss: 1.96498 (semantic_loss: 0.01653, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21409 
Train Epoch: 19 [906/1000 28992/32000 (91%)] Loss: 1.96389 (semantic_loss: 0.01740, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23425 
Train Epoch: 19 [911/1000 29152/32000 (91%)] Loss: 1.96232 (semantic_loss: 0.01485, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21937 
Train Epoch: 19 [916/1000 29312/32000 (92%)] Loss: 1.96254 (semantic_loss: 0.01506, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19067 
Train Epoch: 19 [921/1000 29472/32000 (92%)] Loss: 1.95836 (semantic_loss: 0.01285, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19972 
Train Epoch: 19 [926/1000 29632/32000 (93%)] Loss: 1.96367 (semantic_loss: 0.01814, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19284 
Train Epoch: 19 [931/1000 29792/32000 (93%)] Loss: 1.96086 (semantic_loss: 0.01339, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20063 
Train Epoch: 19 [936/1000 29952/32000 (94%)] Loss: 1.96272 (semantic_loss: 0.01524, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.31932 
Train Epoch: 19 [941/1000 30112/32000 (94%)] Loss: 1.96398 (semantic_loss: 0.01748, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18806 
Train Epoch: 19 [946/1000 30272/32000 (95%)] Loss: 1.96307 (semantic_loss: 0.01559, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26042 
Train Epoch: 19 [951/1000 30432/32000 (95%)] Loss: 1.96440 (semantic_loss: 0.01692, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18913 
Train Epoch: 19 [956/1000 30592/32000 (96%)] Loss: 1.96347 (semantic_loss: 0.01599, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18743 
Train Epoch: 19 [961/1000 30752/32000 (96%)] Loss: 1.96272 (semantic_loss: 0.01525, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20228 
Train Epoch: 19 [966/1000 30912/32000 (97%)] Loss: 1.96149 (semantic_loss: 0.01401, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21033 
Train Epoch: 19 [971/1000 31072/32000 (97%)] Loss: 1.96280 (semantic_loss: 0.01532, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19356 
Train Epoch: 19 [976/1000 31232/32000 (98%)] Loss: 1.96065 (semantic_loss: 0.01415, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18756 
Train Epoch: 19 [981/1000 31392/32000 (98%)] Loss: 1.96494 (semantic_loss: 0.01747, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18865 
Train Epoch: 19 [986/1000 31552/32000 (99%)] Loss: 1.96729 (semantic_loss: 0.01982, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19433 
Train Epoch: 19 [991/1000 31712/32000 (99%)] Loss: 1.96347 (semantic_loss: 0.01600, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18593 
Train Epoch: 19 [996/1000 31872/32000 (100%)] Loss: 1.96828 (semantic_loss: 0.02081, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18711 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch19.pth ...
Done in 5.475s
removing stale ckpt [epoch 18] [took 0.04s]
 epoch          : 19
 loss           : 1.963636019229889
 learning_rate  : 7.504731764849961e-06
 n_samples      : 608000
 n_steps        : 19000
 ActivityNet_val1_test/t2v_metrics/R1: 11.429733577384583
 ActivityNet_val1_test/t2v_metrics/R5: 37.50254220052878
 ActivityNet_val1_test/t2v_metrics/R10: 54.687817775066094
 ActivityNet_val1_test/t2v_metrics/R50: 85.19422412039862
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 56.685580638600776
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 28.619528190048978
 ActivityNet_val1_test/v2t_metrics/R1: 11.592434411226357
 ActivityNet_val1_test/v2t_metrics/R5: 37.58389261744966
 ActivityNet_val1_test/v2t_metrics/R10: 54.260728086231445
 ActivityNet_val1_test/v2t_metrics/R50: 84.54342078503153
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 60.141753101484646
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 28.70035978064753
 mnt_best       : 28.80392894496838
 not_improved_count: 1
Train Epoch: 20 [1/1000 32/32000 (0%)] Loss: 1.96109 (semantic_loss: 0.01460, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=20.45937 
Train Epoch: 20 [6/1000 192/32000 (1%)] Loss: 1.96265 (semantic_loss: 0.01420, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22393 
Train Epoch: 20 [11/1000 352/32000 (1%)] Loss: 1.96239 (semantic_loss: 0.01590, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18282 
Train Epoch: 20 [16/1000 512/32000 (2%)] Loss: 1.96242 (semantic_loss: 0.01593, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.33222 
Train Epoch: 20 [21/1000 672/32000 (2%)] Loss: 1.96612 (semantic_loss: 0.01864, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18928 
Train Epoch: 20 [26/1000 832/32000 (3%)] Loss: 1.96002 (semantic_loss: 0.01254, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19198 
Train Epoch: 20 [31/1000 992/32000 (3%)] Loss: 1.96535 (semantic_loss: 0.01788, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20308 
Train Epoch: 20 [36/1000 1152/32000 (4%)] Loss: 1.96376 (semantic_loss: 0.01628, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20640 
Train Epoch: 20 [41/1000 1312/32000 (4%)] Loss: 1.96971 (semantic_loss: 0.02126, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20705 
Train Epoch: 20 [46/1000 1472/32000 (5%)] Loss: 1.96814 (semantic_loss: 0.01969, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22483 
Train Epoch: 20 [51/1000 1632/32000 (5%)] Loss: 1.96315 (semantic_loss: 0.01470, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19814 
Train Epoch: 20 [56/1000 1792/32000 (6%)] Loss: 1.96395 (semantic_loss: 0.01746, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19697 
Train Epoch: 20 [61/1000 1952/32000 (6%)] Loss: 1.96421 (semantic_loss: 0.01673, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19965 
Train Epoch: 20 [66/1000 2112/32000 (7%)] Loss: 1.96309 (semantic_loss: 0.01562, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21170 
Train Epoch: 20 [71/1000 2272/32000 (7%)] Loss: 1.96215 (semantic_loss: 0.01565, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19029 
Train Epoch: 20 [76/1000 2432/32000 (8%)] Loss: 1.96334 (semantic_loss: 0.01488, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19707 
Train Epoch: 20 [81/1000 2592/32000 (8%)] Loss: 1.96183 (semantic_loss: 0.01436, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19872 
Train Epoch: 20 [86/1000 2752/32000 (9%)] Loss: 1.96474 (semantic_loss: 0.01628, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18610 
Train Epoch: 20 [91/1000 2912/32000 (9%)] Loss: 1.96429 (semantic_loss: 0.01682, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.36156 
Train Epoch: 20 [96/1000 3072/32000 (10%)] Loss: 1.96776 (semantic_loss: 0.01931, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.58226 
Train Epoch: 20 [101/1000 3232/32000 (10%)] Loss: 1.96025 (semantic_loss: 0.01278, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19461 
Train Epoch: 20 [106/1000 3392/32000 (11%)] Loss: 1.96105 (semantic_loss: 0.01455, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20684 
Train Epoch: 20 [111/1000 3552/32000 (11%)] Loss: 1.96327 (semantic_loss: 0.01678, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18792 
Train Epoch: 20 [116/1000 3712/32000 (12%)] Loss: 1.96407 (semantic_loss: 0.01757, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19329 
Train Epoch: 20 [121/1000 3872/32000 (12%)] Loss: 1.96761 (semantic_loss: 0.02111, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18746 
Train Epoch: 20 [126/1000 4032/32000 (13%)] Loss: 1.95921 (semantic_loss: 0.01368, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18699 
Train Epoch: 20 [131/1000 4192/32000 (13%)] Loss: 1.96113 (semantic_loss: 0.01366, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18838 
Train Epoch: 20 [136/1000 4352/32000 (14%)] Loss: 1.96962 (semantic_loss: 0.02214, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18896 
Train Epoch: 20 [141/1000 4512/32000 (14%)] Loss: 1.96380 (semantic_loss: 0.01632, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18696 
Train Epoch: 20 [146/1000 4672/32000 (15%)] Loss: 1.96284 (semantic_loss: 0.01634, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19678 
Train Epoch: 20 [151/1000 4832/32000 (15%)] Loss: 1.96248 (semantic_loss: 0.01500, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18455 
Train Epoch: 20 [156/1000 4992/32000 (16%)] Loss: 1.96870 (semantic_loss: 0.02220, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19851 
Train Epoch: 20 [161/1000 5152/32000 (16%)] Loss: 1.96237 (semantic_loss: 0.01588, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18945 
Train Epoch: 20 [166/1000 5312/32000 (17%)] Loss: 1.96303 (semantic_loss: 0.01555, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18659 
Train Epoch: 20 [171/1000 5472/32000 (17%)] Loss: 1.96012 (semantic_loss: 0.01362, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21981 
Train Epoch: 20 [176/1000 5632/32000 (18%)] Loss: 1.95970 (semantic_loss: 0.01223, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.29205 
Train Epoch: 20 [181/1000 5792/32000 (18%)] Loss: 1.96011 (semantic_loss: 0.01361, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18785 
Train Epoch: 20 [186/1000 5952/32000 (19%)] Loss: 1.96265 (semantic_loss: 0.01616, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18709 
Train Epoch: 20 [191/1000 6112/32000 (19%)] Loss: 1.96218 (semantic_loss: 0.01471, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18722 
Train Epoch: 20 [196/1000 6272/32000 (20%)] Loss: 1.96008 (semantic_loss: 0.01358, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.27612 
Train Epoch: 20 [201/1000 6432/32000 (20%)] Loss: 1.96175 (semantic_loss: 0.01525, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21810 
Train Epoch: 20 [206/1000 6592/32000 (21%)] Loss: 1.96462 (semantic_loss: 0.01715, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23129 
Train Epoch: 20 [211/1000 6752/32000 (21%)] Loss: 1.96053 (semantic_loss: 0.01404, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19832 
Train Epoch: 20 [216/1000 6912/32000 (22%)] Loss: 1.96010 (semantic_loss: 0.01262, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20938 
Train Epoch: 20 [221/1000 7072/32000 (22%)] Loss: 1.96197 (semantic_loss: 0.01548, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.30724 
Train Epoch: 20 [226/1000 7232/32000 (23%)] Loss: 1.96284 (semantic_loss: 0.01537, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19516 
Train Epoch: 20 [231/1000 7392/32000 (23%)] Loss: 1.96198 (semantic_loss: 0.01548, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18972 
Train Epoch: 20 [236/1000 7552/32000 (24%)] Loss: 1.96256 (semantic_loss: 0.01509, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19007 
Train Epoch: 20 [241/1000 7712/32000 (24%)] Loss: 1.96185 (semantic_loss: 0.01535, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18513 
Train Epoch: 20 [246/1000 7872/32000 (25%)] Loss: 1.96183 (semantic_loss: 0.01437, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18790 
Train Epoch: 20 [251/1000 8032/32000 (25%)] Loss: 1.96584 (semantic_loss: 0.01836, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21188 
Train Epoch: 20 [256/1000 8192/32000 (26%)] Loss: 1.96098 (semantic_loss: 0.01351, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19298 
Train Epoch: 20 [261/1000 8352/32000 (26%)] Loss: 1.95914 (semantic_loss: 0.01265, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.26291 
Train Epoch: 20 [266/1000 8512/32000 (27%)] Loss: 1.96302 (semantic_loss: 0.01456, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21226 
Train Epoch: 20 [271/1000 8672/32000 (27%)] Loss: 1.96107 (semantic_loss: 0.01554, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20822 
Train Epoch: 20 [276/1000 8832/32000 (28%)] Loss: 1.96325 (semantic_loss: 0.01773, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.30423 
Train Epoch: 20 [281/1000 8992/32000 (28%)] Loss: 1.96126 (semantic_loss: 0.01281, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18732 
Train Epoch: 20 [286/1000 9152/32000 (29%)] Loss: 1.96127 (semantic_loss: 0.01477, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18670 
Train Epoch: 20 [291/1000 9312/32000 (29%)] Loss: 1.96680 (semantic_loss: 0.02030, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20451 
Train Epoch: 20 [296/1000 9472/32000 (30%)] Loss: 1.96288 (semantic_loss: 0.01540, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18713 
Train Epoch: 20 [301/1000 9632/32000 (30%)] Loss: 1.96432 (semantic_loss: 0.01782, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19727 
Train Epoch: 20 [306/1000 9792/32000 (31%)] Loss: 1.96656 (semantic_loss: 0.01909, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18961 
Train Epoch: 20 [311/1000 9952/32000 (31%)] Loss: 1.96508 (semantic_loss: 0.01858, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18760 
Train Epoch: 20 [316/1000 10112/32000 (32%)] Loss: 1.96422 (semantic_loss: 0.01675, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18778 
Train Epoch: 20 [321/1000 10272/32000 (32%)] Loss: 1.96680 (semantic_loss: 0.01933, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19130 
Train Epoch: 20 [326/1000 10432/32000 (33%)] Loss: 1.96272 (semantic_loss: 0.01524, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22443 
Train Epoch: 20 [331/1000 10592/32000 (33%)] Loss: 1.96326 (semantic_loss: 0.01481, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19457 
Train Epoch: 20 [336/1000 10752/32000 (34%)] Loss: 1.96367 (semantic_loss: 0.01717, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.35437 
Train Epoch: 20 [341/1000 10912/32000 (34%)] Loss: 1.96711 (semantic_loss: 0.01964, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19456 
Train Epoch: 20 [346/1000 11072/32000 (35%)] Loss: 1.96586 (semantic_loss: 0.01742, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18675 
Train Epoch: 20 [351/1000 11232/32000 (35%)] Loss: 1.96257 (semantic_loss: 0.01510, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20520 
Train Epoch: 20 [356/1000 11392/32000 (36%)] Loss: 1.96150 (semantic_loss: 0.01305, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22078 
Train Epoch: 20 [361/1000 11552/32000 (36%)] Loss: 1.96414 (semantic_loss: 0.01569, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22116 
Train Epoch: 20 [366/1000 11712/32000 (37%)] Loss: 1.96126 (semantic_loss: 0.01476, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22825 
Train Epoch: 20 [371/1000 11872/32000 (37%)] Loss: 1.96207 (semantic_loss: 0.01558, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20376 
Train Epoch: 20 [376/1000 12032/32000 (38%)] Loss: 1.96186 (semantic_loss: 0.01438, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20082 
Train Epoch: 20 [381/1000 12192/32000 (38%)] Loss: 1.96235 (semantic_loss: 0.01487, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19852 
Train Epoch: 20 [386/1000 12352/32000 (39%)] Loss: 1.96872 (semantic_loss: 0.02222, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19265 
Train Epoch: 20 [391/1000 12512/32000 (39%)] Loss: 1.96133 (semantic_loss: 0.01386, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19322 
Train Epoch: 20 [396/1000 12672/32000 (40%)] Loss: 1.96085 (semantic_loss: 0.01436, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18824 
Train Epoch: 20 [401/1000 12832/32000 (40%)] Loss: 1.96402 (semantic_loss: 0.01557, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18450 
Train Epoch: 20 [406/1000 12992/32000 (41%)] Loss: 1.96558 (semantic_loss: 0.01908, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18561 
Train Epoch: 20 [411/1000 13152/32000 (41%)] Loss: 1.96315 (semantic_loss: 0.01568, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.36420 
Train Epoch: 20 [416/1000 13312/32000 (42%)] Loss: 1.96190 (semantic_loss: 0.01442, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.60113 
Train Epoch: 20 [421/1000 13472/32000 (42%)] Loss: 1.96177 (semantic_loss: 0.01528, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19225 
Train Epoch: 20 [426/1000 13632/32000 (43%)] Loss: 1.96350 (semantic_loss: 0.01603, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19755 
Train Epoch: 20 [431/1000 13792/32000 (43%)] Loss: 1.96003 (semantic_loss: 0.01353, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18988 
Train Epoch: 20 [436/1000 13952/32000 (44%)] Loss: 1.96111 (semantic_loss: 0.01364, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18637 
Train Epoch: 20 [441/1000 14112/32000 (44%)] Loss: 1.96341 (semantic_loss: 0.01691, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18830 
Train Epoch: 20 [446/1000 14272/32000 (45%)] Loss: 1.96603 (semantic_loss: 0.01855, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18735 
Train Epoch: 20 [451/1000 14432/32000 (45%)] Loss: 1.96520 (semantic_loss: 0.01870, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18691 
Train Epoch: 20 [456/1000 14592/32000 (46%)] Loss: 1.95988 (semantic_loss: 0.01338, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18779 
Train Epoch: 20 [461/1000 14752/32000 (46%)] Loss: 1.96258 (semantic_loss: 0.01608, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18472 
Train Epoch: 20 [466/1000 14912/32000 (47%)] Loss: 1.96375 (semantic_loss: 0.01725, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18839 
Train Epoch: 20 [471/1000 15072/32000 (47%)] Loss: 1.96649 (semantic_loss: 0.01901, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18700 
Train Epoch: 20 [476/1000 15232/32000 (48%)] Loss: 1.96032 (semantic_loss: 0.01480, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18488 
Train Epoch: 20 [481/1000 15392/32000 (48%)] Loss: 1.96224 (semantic_loss: 0.01574, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19349 
Train Epoch: 20 [486/1000 15552/32000 (49%)] Loss: 1.96406 (semantic_loss: 0.01659, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19009 
Train Epoch: 20 [491/1000 15712/32000 (49%)] Loss: 1.96268 (semantic_loss: 0.01618, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18823 
Train Epoch: 20 [496/1000 15872/32000 (50%)] Loss: 1.96041 (semantic_loss: 0.01392, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.29198 
Train Epoch: 20 [501/1000 16032/32000 (50%)] Loss: 1.96007 (semantic_loss: 0.01357, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20770 
Train Epoch: 20 [506/1000 16192/32000 (51%)] Loss: 1.96191 (semantic_loss: 0.01347, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19367 
Train Epoch: 20 [511/1000 16352/32000 (51%)] Loss: 1.96272 (semantic_loss: 0.01428, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21363 
Train Epoch: 20 [516/1000 16512/32000 (52%)] Loss: 1.96193 (semantic_loss: 0.01543, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.28625 
Train Epoch: 20 [521/1000 16672/32000 (52%)] Loss: 1.96172 (semantic_loss: 0.01522, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22378 
Train Epoch: 20 [526/1000 16832/32000 (53%)] Loss: 1.96286 (semantic_loss: 0.01539, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21208 
Train Epoch: 20 [531/1000 16992/32000 (53%)] Loss: 1.96165 (semantic_loss: 0.01515, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20020 
Train Epoch: 20 [536/1000 17152/32000 (54%)] Loss: 1.96945 (semantic_loss: 0.02294, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.21734 
Train Epoch: 20 [541/1000 17312/32000 (54%)] Loss: 1.96275 (semantic_loss: 0.01527, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.32834 
Train Epoch: 20 [546/1000 17472/32000 (55%)] Loss: 1.96226 (semantic_loss: 0.01577, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18798 
Train Epoch: 20 [551/1000 17632/32000 (55%)] Loss: 1.96673 (semantic_loss: 0.01925, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18755 
Train Epoch: 20 [556/1000 17792/32000 (56%)] Loss: 1.96584 (semantic_loss: 0.01837, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19041 
Train Epoch: 20 [561/1000 17952/32000 (56%)] Loss: 1.96620 (semantic_loss: 0.01872, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18381 
Train Epoch: 20 [566/1000 18112/32000 (57%)] Loss: 1.96495 (semantic_loss: 0.01649, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18659 
Train Epoch: 20 [571/1000 18272/32000 (57%)] Loss: 1.96681 (semantic_loss: 0.01933, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19952 
Train Epoch: 20 [576/1000 18432/32000 (58%)] Loss: 1.96259 (semantic_loss: 0.01609, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20160 
Train Epoch: 20 [581/1000 18592/32000 (58%)] Loss: 1.96401 (semantic_loss: 0.01654, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23878 
Train Epoch: 20 [586/1000 18752/32000 (59%)] Loss: 1.96179 (semantic_loss: 0.01627, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20526 
Train Epoch: 20 [591/1000 18912/32000 (59%)] Loss: 1.96267 (semantic_loss: 0.01715, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19988 
Train Epoch: 20 [596/1000 19072/32000 (60%)] Loss: 1.96067 (semantic_loss: 0.01320, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.29139 
Train Epoch: 20 [601/1000 19232/32000 (60%)] Loss: 1.96547 (semantic_loss: 0.01702, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18404 
Train Epoch: 20 [606/1000 19392/32000 (61%)] Loss: 1.96846 (semantic_loss: 0.02196, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18572 
Train Epoch: 20 [611/1000 19552/32000 (61%)] Loss: 1.96493 (semantic_loss: 0.01746, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18634 
Train Epoch: 20 [616/1000 19712/32000 (62%)] Loss: 1.96593 (semantic_loss: 0.01845, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18370 
Train Epoch: 20 [621/1000 19872/32000 (62%)] Loss: 1.96542 (semantic_loss: 0.01794, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18699 
Train Epoch: 20 [626/1000 20032/32000 (63%)] Loss: 1.96298 (semantic_loss: 0.01453, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18940 
Train Epoch: 20 [631/1000 20192/32000 (63%)] Loss: 1.96593 (semantic_loss: 0.01748, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18496 
Train Epoch: 20 [636/1000 20352/32000 (64%)] Loss: 1.96438 (semantic_loss: 0.01691, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18509 
Train Epoch: 20 [641/1000 20512/32000 (64%)] Loss: 1.96008 (semantic_loss: 0.01456, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19632 
Train Epoch: 20 [646/1000 20672/32000 (65%)] Loss: 1.96180 (semantic_loss: 0.01335, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.23096 
Train Epoch: 20 [651/1000 20832/32000 (65%)] Loss: 1.95882 (semantic_loss: 0.01232, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18629 
Train Epoch: 20 [656/1000 20992/32000 (66%)] Loss: 1.96064 (semantic_loss: 0.01317, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.35430 
Train Epoch: 20 [661/1000 21152/32000 (66%)] Loss: 1.96069 (semantic_loss: 0.01321, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19126 
Train Epoch: 20 [666/1000 21312/32000 (67%)] Loss: 1.96217 (semantic_loss: 0.01470, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18697 
Train Epoch: 20 [671/1000 21472/32000 (67%)] Loss: 1.96336 (semantic_loss: 0.01686, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21402 
Train Epoch: 20 [676/1000 21632/32000 (68%)] Loss: 1.96995 (semantic_loss: 0.02150, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21080 
Train Epoch: 20 [681/1000 21792/32000 (68%)] Loss: 1.96054 (semantic_loss: 0.01307, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23330 
Train Epoch: 20 [686/1000 21952/32000 (69%)] Loss: 1.95965 (semantic_loss: 0.01315, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20067 
Train Epoch: 20 [691/1000 22112/32000 (69%)] Loss: 1.96311 (semantic_loss: 0.01661, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21175 
Train Epoch: 20 [696/1000 22272/32000 (70%)] Loss: 1.96528 (semantic_loss: 0.01781, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20636 
Train Epoch: 20 [701/1000 22432/32000 (70%)] Loss: 1.96078 (semantic_loss: 0.01331, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19199 
Train Epoch: 20 [706/1000 22592/32000 (71%)] Loss: 1.96044 (semantic_loss: 0.01395, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23416 
Train Epoch: 20 [711/1000 22752/32000 (71%)] Loss: 1.96169 (semantic_loss: 0.01519, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18882 
Train Epoch: 20 [716/1000 22912/32000 (72%)] Loss: 1.96601 (semantic_loss: 0.01757, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18612 
Train Epoch: 20 [721/1000 23072/32000 (72%)] Loss: 1.96331 (semantic_loss: 0.01584, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19012 
Train Epoch: 20 [726/1000 23232/32000 (73%)] Loss: 1.96011 (semantic_loss: 0.01459, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18613 
Train Epoch: 20 [731/1000 23392/32000 (73%)] Loss: 1.96639 (semantic_loss: 0.01989, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.35437 
Train Epoch: 20 [736/1000 23552/32000 (74%)] Loss: 1.96475 (semantic_loss: 0.01825, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.59588 
Train Epoch: 20 [741/1000 23712/32000 (74%)] Loss: 1.96019 (semantic_loss: 0.01370, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19308 
Train Epoch: 20 [746/1000 23872/32000 (75%)] Loss: 1.96130 (semantic_loss: 0.01480, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19489 
Train Epoch: 20 [751/1000 24032/32000 (75%)] Loss: 1.96092 (semantic_loss: 0.01344, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18738 
Train Epoch: 20 [756/1000 24192/32000 (76%)] Loss: 1.96238 (semantic_loss: 0.01589, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18759 
Train Epoch: 20 [761/1000 24352/32000 (76%)] Loss: 1.96437 (semantic_loss: 0.01690, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18467 
Train Epoch: 20 [766/1000 24512/32000 (77%)] Loss: 1.96002 (semantic_loss: 0.01352, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18692 
Train Epoch: 20 [771/1000 24672/32000 (77%)] Loss: 1.96599 (semantic_loss: 0.01852, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18424 
Train Epoch: 20 [776/1000 24832/32000 (78%)] Loss: 1.96150 (semantic_loss: 0.01500, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18495 
Train Epoch: 20 [781/1000 24992/32000 (78%)] Loss: 1.96510 (semantic_loss: 0.01763, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18737 
Train Epoch: 20 [786/1000 25152/32000 (79%)] Loss: 1.96131 (semantic_loss: 0.01482, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20385 
Train Epoch: 20 [791/1000 25312/32000 (79%)] Loss: 1.96373 (semantic_loss: 0.01627, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18947 
Train Epoch: 20 [796/1000 25472/32000 (80%)] Loss: 1.96230 (semantic_loss: 0.01385, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18618 
Train Epoch: 20 [801/1000 25632/32000 (80%)] Loss: 1.96808 (semantic_loss: 0.02158, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18859 
Train Epoch: 20 [806/1000 25792/32000 (81%)] Loss: 1.96145 (semantic_loss: 0.01495, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18597 
Train Epoch: 20 [811/1000 25952/32000 (81%)] Loss: 1.96089 (semantic_loss: 0.01440, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18732 
Train Epoch: 20 [816/1000 26112/32000 (82%)] Loss: 1.96000 (semantic_loss: 0.01350, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.29886 
Train Epoch: 20 [821/1000 26272/32000 (82%)] Loss: 1.96196 (semantic_loss: 0.01449, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18967 
Train Epoch: 20 [826/1000 26432/32000 (83%)] Loss: 1.96360 (semantic_loss: 0.01612, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21606 
Train Epoch: 20 [831/1000 26592/32000 (83%)] Loss: 1.96626 (semantic_loss: 0.01977, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22038 
Train Epoch: 20 [836/1000 26752/32000 (84%)] Loss: 1.96090 (semantic_loss: 0.01441, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.28780 
Train Epoch: 20 [841/1000 26912/32000 (84%)] Loss: 1.96712 (semantic_loss: 0.02061, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21369 
Train Epoch: 20 [846/1000 27072/32000 (85%)] Loss: 1.96293 (semantic_loss: 0.01644, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20072 
Train Epoch: 20 [851/1000 27232/32000 (85%)] Loss: 1.96515 (semantic_loss: 0.01767, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22728 
Train Epoch: 20 [856/1000 27392/32000 (86%)] Loss: 1.96301 (semantic_loss: 0.01651, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20045 
Train Epoch: 20 [861/1000 27552/32000 (86%)] Loss: 1.96418 (semantic_loss: 0.01671, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.30247 
Train Epoch: 20 [866/1000 27712/32000 (87%)] Loss: 1.96570 (semantic_loss: 0.01823, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18548 
Train Epoch: 20 [871/1000 27872/32000 (87%)] Loss: 1.96101 (semantic_loss: 0.01451, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18695 
Train Epoch: 20 [876/1000 28032/32000 (88%)] Loss: 1.96166 (semantic_loss: 0.01418, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18668 
Train Epoch: 20 [881/1000 28192/32000 (88%)] Loss: 1.95975 (semantic_loss: 0.01326, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19013 
Train Epoch: 20 [886/1000 28352/32000 (89%)] Loss: 1.96304 (semantic_loss: 0.01557, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18900 
Train Epoch: 20 [891/1000 28512/32000 (89%)] Loss: 1.96442 (semantic_loss: 0.01792, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19056 
Train Epoch: 20 [896/1000 28672/32000 (90%)] Loss: 1.96146 (semantic_loss: 0.01302, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19638 
Train Epoch: 20 [901/1000 28832/32000 (90%)] Loss: 1.96231 (semantic_loss: 0.01484, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23807 
Train Epoch: 20 [906/1000 28992/32000 (91%)] Loss: 1.96083 (semantic_loss: 0.01433, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20082 
Train Epoch: 20 [911/1000 29152/32000 (91%)] Loss: 1.96167 (semantic_loss: 0.01517, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20113 
Train Epoch: 20 [916/1000 29312/32000 (92%)] Loss: 1.96671 (semantic_loss: 0.01826, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.29748 
Train Epoch: 20 [921/1000 29472/32000 (92%)] Loss: 1.96304 (semantic_loss: 0.01654, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18512 
Train Epoch: 20 [926/1000 29632/32000 (93%)] Loss: 1.96745 (semantic_loss: 0.01998, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19863 
Train Epoch: 20 [931/1000 29792/32000 (93%)] Loss: 1.95994 (semantic_loss: 0.01344, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18855 
Train Epoch: 20 [936/1000 29952/32000 (94%)] Loss: 1.96312 (semantic_loss: 0.01661, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19269 
Train Epoch: 20 [941/1000 30112/32000 (94%)] Loss: 1.96074 (semantic_loss: 0.01326, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18539 
Train Epoch: 20 [946/1000 30272/32000 (95%)] Loss: 1.96208 (semantic_loss: 0.01461, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18976 
Train Epoch: 20 [951/1000 30432/32000 (95%)] Loss: 1.96527 (semantic_loss: 0.01780, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18903 
Train Epoch: 20 [956/1000 30592/32000 (96%)] Loss: 1.96169 (semantic_loss: 0.01422, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20668 
Train Epoch: 20 [961/1000 30752/32000 (96%)] Loss: 1.96072 (semantic_loss: 0.01325, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19575 
Train Epoch: 20 [966/1000 30912/32000 (97%)] Loss: 1.96378 (semantic_loss: 0.01630, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22189 
Train Epoch: 20 [971/1000 31072/32000 (97%)] Loss: 1.96437 (semantic_loss: 0.01690, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18614 
Train Epoch: 20 [976/1000 31232/32000 (98%)] Loss: 1.96186 (semantic_loss: 0.01536, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.35506 
Train Epoch: 20 [981/1000 31392/32000 (98%)] Loss: 1.96076 (semantic_loss: 0.01426, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18707 
Train Epoch: 20 [986/1000 31552/32000 (99%)] Loss: 1.96272 (semantic_loss: 0.01525, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18767 
Train Epoch: 20 [991/1000 31712/32000 (99%)] Loss: 1.96550 (semantic_loss: 0.01706, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.23217 
Train Epoch: 20 [996/1000 31872/32000 (100%)] Loss: 1.96299 (semantic_loss: 0.01552, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22922 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch20.pth ...
Done in 4.855s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch20.pth ...
Done in 9.349s
removing stale ckpt [epoch 19] [took 0.00s]
 epoch          : 20
 loss           : 1.9633752416372299
 learning_rate  : 6.754258588364965e-06
 n_samples      : 640000
 n_steps        : 20000
 ActivityNet_val1_test/t2v_metrics/R1: 11.714460036607688
 ActivityNet_val1_test/t2v_metrics/R5: 37.624567825910106
 ActivityNet_val1_test/t2v_metrics/R10: 54.83018100467765
 ActivityNet_val1_test/t2v_metrics/R50: 86.00772828960748
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 55.638804148871266
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 28.911535532251342
 ActivityNet_val1_test/v2t_metrics/R1: 12.629652226967663
 ActivityNet_val1_test/v2t_metrics/R5: 37.97030709782388
 ActivityNet_val1_test/v2t_metrics/R10: 54.91153142159853
 ActivityNet_val1_test/v2t_metrics/R50: 85.2755745373195
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 58.13524506813098
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.75087106519512
 mnt_best       : 28.911535532251342
 not_improved_count: 0
Train Epoch: 21 [1/1000 32/32000 (0%)] Loss: 1.96390 (semantic_loss: 0.01740, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=23.34282 
Train Epoch: 21 [6/1000 192/32000 (1%)] Loss: 1.96674 (semantic_loss: 0.01927, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21085 
Train Epoch: 21 [11/1000 352/32000 (1%)] Loss: 1.96256 (semantic_loss: 0.01509, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.53419 
Train Epoch: 21 [16/1000 512/32000 (2%)] Loss: 1.96421 (semantic_loss: 0.01673, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21479 
Train Epoch: 21 [21/1000 672/32000 (2%)] Loss: 1.96219 (semantic_loss: 0.01569, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19476 
Train Epoch: 21 [26/1000 832/32000 (3%)] Loss: 1.96069 (semantic_loss: 0.01322, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18399 
Train Epoch: 21 [31/1000 992/32000 (3%)] Loss: 1.95901 (semantic_loss: 0.01349, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18384 
Train Epoch: 21 [36/1000 1152/32000 (4%)] Loss: 1.96271 (semantic_loss: 0.01622, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18394 
Train Epoch: 21 [41/1000 1312/32000 (4%)] Loss: 1.96345 (semantic_loss: 0.01598, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18932 
Train Epoch: 21 [46/1000 1472/32000 (5%)] Loss: 1.96468 (semantic_loss: 0.01721, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18794 
Train Epoch: 21 [51/1000 1632/32000 (5%)] Loss: 1.96876 (semantic_loss: 0.02128, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19146 
Train Epoch: 21 [56/1000 1792/32000 (6%)] Loss: 1.96343 (semantic_loss: 0.01498, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18769 
Train Epoch: 21 [61/1000 1952/32000 (6%)] Loss: 1.96764 (semantic_loss: 0.02115, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18960 
Train Epoch: 21 [66/1000 2112/32000 (7%)] Loss: 1.96295 (semantic_loss: 0.01548, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18688 
Train Epoch: 21 [71/1000 2272/32000 (7%)] Loss: 1.96126 (semantic_loss: 0.01476, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21817 
Train Epoch: 21 [76/1000 2432/32000 (8%)] Loss: 1.95998 (semantic_loss: 0.01250, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18821 
Train Epoch: 21 [81/1000 2592/32000 (8%)] Loss: 1.96112 (semantic_loss: 0.01365, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22805 
Train Epoch: 21 [86/1000 2752/32000 (9%)] Loss: 1.96471 (semantic_loss: 0.01724, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18895 
Train Epoch: 21 [91/1000 2912/32000 (9%)] Loss: 1.96018 (semantic_loss: 0.01368, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20462 
Train Epoch: 21 [96/1000 3072/32000 (10%)] Loss: 1.96212 (semantic_loss: 0.01563, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.68889 
Train Epoch: 21 [101/1000 3232/32000 (10%)] Loss: 1.96285 (semantic_loss: 0.01635, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22467 
Train Epoch: 21 [106/1000 3392/32000 (11%)] Loss: 1.96882 (semantic_loss: 0.02134, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19876 
Train Epoch: 21 [111/1000 3552/32000 (11%)] Loss: 1.96718 (semantic_loss: 0.02067, quant_loss: 1.94629, bit_balance_loss: 0.00023) batch_time=0.21591 
Train Epoch: 21 [116/1000 3712/32000 (12%)] Loss: 1.96105 (semantic_loss: 0.01456, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19989 
Train Epoch: 21 [121/1000 3872/32000 (12%)] Loss: 1.95948 (semantic_loss: 0.01397, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19920 
Train Epoch: 21 [126/1000 4032/32000 (13%)] Loss: 1.96303 (semantic_loss: 0.01653, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19924 
Train Epoch: 21 [131/1000 4192/32000 (13%)] Loss: 1.96335 (semantic_loss: 0.01588, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19547 
Train Epoch: 21 [136/1000 4352/32000 (14%)] Loss: 1.96352 (semantic_loss: 0.01604, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22155 
Train Epoch: 21 [141/1000 4512/32000 (14%)] Loss: 1.96682 (semantic_loss: 0.01935, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20057 
Train Epoch: 21 [146/1000 4672/32000 (15%)] Loss: 1.96308 (semantic_loss: 0.01560, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20333 
Train Epoch: 21 [151/1000 4832/32000 (15%)] Loss: 1.96285 (semantic_loss: 0.01538, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18646 
Train Epoch: 21 [156/1000 4992/32000 (16%)] Loss: 1.96631 (semantic_loss: 0.01883, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19052 
Train Epoch: 21 [161/1000 5152/32000 (16%)] Loss: 1.96377 (semantic_loss: 0.01629, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20399 
Train Epoch: 21 [166/1000 5312/32000 (17%)] Loss: 1.96247 (semantic_loss: 0.01597, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18788 
Train Epoch: 21 [171/1000 5472/32000 (17%)] Loss: 1.96059 (semantic_loss: 0.01410, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18755 
Train Epoch: 21 [176/1000 5632/32000 (18%)] Loss: 1.96347 (semantic_loss: 0.01502, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18579 
Train Epoch: 21 [181/1000 5792/32000 (18%)] Loss: 1.96839 (semantic_loss: 0.02189, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18914 
Train Epoch: 21 [186/1000 5952/32000 (19%)] Loss: 1.96126 (semantic_loss: 0.01476, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19341 
Train Epoch: 21 [191/1000 6112/32000 (19%)] Loss: 1.96275 (semantic_loss: 0.01527, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18557 
Train Epoch: 21 [196/1000 6272/32000 (20%)] Loss: 1.96139 (semantic_loss: 0.01392, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18664 
Train Epoch: 21 [201/1000 6432/32000 (20%)] Loss: 1.96196 (semantic_loss: 0.01546, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20334 
Train Epoch: 21 [206/1000 6592/32000 (21%)] Loss: 1.96217 (semantic_loss: 0.01567, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19129 
Train Epoch: 21 [211/1000 6752/32000 (21%)] Loss: 1.96550 (semantic_loss: 0.01803, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18527 
Train Epoch: 21 [216/1000 6912/32000 (22%)] Loss: 1.96186 (semantic_loss: 0.01439, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18631 
Train Epoch: 21 [221/1000 7072/32000 (22%)] Loss: 1.95924 (semantic_loss: 0.01274, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18727 
Train Epoch: 21 [226/1000 7232/32000 (23%)] Loss: 1.96166 (semantic_loss: 0.01516, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19544 
Train Epoch: 21 [231/1000 7392/32000 (23%)] Loss: 1.96192 (semantic_loss: 0.01542, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19244 
Train Epoch: 21 [236/1000 7552/32000 (24%)] Loss: 1.96136 (semantic_loss: 0.01486, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18811 
Train Epoch: 21 [241/1000 7712/32000 (24%)] Loss: 1.96280 (semantic_loss: 0.01533, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21488 
Train Epoch: 21 [246/1000 7872/32000 (25%)] Loss: 1.96017 (semantic_loss: 0.01270, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24506 
Train Epoch: 21 [251/1000 8032/32000 (25%)] Loss: 1.96820 (semantic_loss: 0.01877, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.21496 
Train Epoch: 21 [256/1000 8192/32000 (26%)] Loss: 1.96103 (semantic_loss: 0.01355, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21185 
Train Epoch: 21 [261/1000 8352/32000 (26%)] Loss: 1.96018 (semantic_loss: 0.01368, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23203 
Train Epoch: 21 [266/1000 8512/32000 (27%)] Loss: 1.96157 (semantic_loss: 0.01409, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26795 
Train Epoch: 21 [271/1000 8672/32000 (27%)] Loss: 1.96125 (semantic_loss: 0.01476, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19576 
Train Epoch: 21 [276/1000 8832/32000 (28%)] Loss: 1.96385 (semantic_loss: 0.01638, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.32728 
Train Epoch: 21 [281/1000 8992/32000 (28%)] Loss: 1.96202 (semantic_loss: 0.01455, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.55250 
Train Epoch: 21 [286/1000 9152/32000 (29%)] Loss: 1.96176 (semantic_loss: 0.01429, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18830 
Train Epoch: 21 [291/1000 9312/32000 (29%)] Loss: 1.96339 (semantic_loss: 0.01494, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19115 
Train Epoch: 21 [296/1000 9472/32000 (30%)] Loss: 1.96591 (semantic_loss: 0.01941, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18913 
Train Epoch: 21 [301/1000 9632/32000 (30%)] Loss: 1.96142 (semantic_loss: 0.01298, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20304 
Train Epoch: 21 [306/1000 9792/32000 (31%)] Loss: 1.96549 (semantic_loss: 0.01899, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19983 
Train Epoch: 21 [311/1000 9952/32000 (31%)] Loss: 1.96368 (semantic_loss: 0.01718, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20481 
Train Epoch: 21 [316/1000 10112/32000 (32%)] Loss: 1.96110 (semantic_loss: 0.01460, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19706 
Train Epoch: 21 [321/1000 10272/32000 (32%)] Loss: 1.96583 (semantic_loss: 0.01933, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18874 
Train Epoch: 21 [326/1000 10432/32000 (33%)] Loss: 1.96411 (semantic_loss: 0.01663, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20223 
Train Epoch: 21 [331/1000 10592/32000 (33%)] Loss: 1.96640 (semantic_loss: 0.01892, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.48791 
Train Epoch: 21 [336/1000 10752/32000 (34%)] Loss: 1.96228 (semantic_loss: 0.01480, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20063 
Train Epoch: 21 [341/1000 10912/32000 (34%)] Loss: 1.96048 (semantic_loss: 0.01301, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18663 
Train Epoch: 21 [346/1000 11072/32000 (35%)] Loss: 1.96112 (semantic_loss: 0.01560, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18627 
Train Epoch: 21 [351/1000 11232/32000 (35%)] Loss: 1.96351 (semantic_loss: 0.01603, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18603 
Train Epoch: 21 [356/1000 11392/32000 (36%)] Loss: 1.96311 (semantic_loss: 0.01662, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19755 
Train Epoch: 21 [361/1000 11552/32000 (36%)] Loss: 1.96101 (semantic_loss: 0.01450, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19066 
Train Epoch: 21 [366/1000 11712/32000 (37%)] Loss: 1.96399 (semantic_loss: 0.01652, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18548 
Train Epoch: 21 [371/1000 11872/32000 (37%)] Loss: 1.96315 (semantic_loss: 0.01469, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21079 
Train Epoch: 21 [376/1000 12032/32000 (38%)] Loss: 1.96193 (semantic_loss: 0.01348, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18745 
Train Epoch: 21 [381/1000 12192/32000 (38%)] Loss: 1.96587 (semantic_loss: 0.01840, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18597 
Train Epoch: 21 [386/1000 12352/32000 (39%)] Loss: 1.96700 (semantic_loss: 0.01953, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18623 
Train Epoch: 21 [391/1000 12512/32000 (39%)] Loss: 1.96286 (semantic_loss: 0.01539, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21995 
Train Epoch: 21 [396/1000 12672/32000 (40%)] Loss: 1.96472 (semantic_loss: 0.01725, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21622 
Train Epoch: 21 [401/1000 12832/32000 (40%)] Loss: 1.96765 (semantic_loss: 0.02114, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.27163 
Train Epoch: 21 [406/1000 12992/32000 (41%)] Loss: 1.96482 (semantic_loss: 0.01734, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22270 
Train Epoch: 21 [411/1000 13152/32000 (41%)] Loss: 1.96370 (semantic_loss: 0.01623, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20278 
Train Epoch: 21 [416/1000 13312/32000 (42%)] Loss: 1.96226 (semantic_loss: 0.01478, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.59627 
Train Epoch: 21 [421/1000 13472/32000 (42%)] Loss: 1.96405 (semantic_loss: 0.01658, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20500 
Train Epoch: 21 [426/1000 13632/32000 (43%)] Loss: 1.96472 (semantic_loss: 0.01626, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19038 
Train Epoch: 21 [431/1000 13792/32000 (43%)] Loss: 1.96773 (semantic_loss: 0.01929, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18666 
Train Epoch: 21 [436/1000 13952/32000 (44%)] Loss: 1.96125 (semantic_loss: 0.01475, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18706 
Train Epoch: 21 [441/1000 14112/32000 (44%)] Loss: 1.96654 (semantic_loss: 0.01809, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18764 
Train Epoch: 21 [446/1000 14272/32000 (45%)] Loss: 1.96440 (semantic_loss: 0.01596, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18522 
Train Epoch: 21 [451/1000 14432/32000 (45%)] Loss: 1.96133 (semantic_loss: 0.01386, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19111 
Train Epoch: 21 [456/1000 14592/32000 (46%)] Loss: 1.96249 (semantic_loss: 0.01599, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22589 
Train Epoch: 21 [461/1000 14752/32000 (46%)] Loss: 1.96386 (semantic_loss: 0.01638, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19188 
Train Epoch: 21 [466/1000 14912/32000 (47%)] Loss: 1.96040 (semantic_loss: 0.01293, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23343 
Train Epoch: 21 [471/1000 15072/32000 (47%)] Loss: 1.96189 (semantic_loss: 0.01539, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19974 
Train Epoch: 21 [476/1000 15232/32000 (48%)] Loss: 1.96337 (semantic_loss: 0.01590, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19453 
Train Epoch: 21 [481/1000 15392/32000 (48%)] Loss: 1.96297 (semantic_loss: 0.01647, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18703 
Train Epoch: 21 [486/1000 15552/32000 (49%)] Loss: 1.96286 (semantic_loss: 0.01636, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19053 
Train Epoch: 21 [491/1000 15712/32000 (49%)] Loss: 1.96315 (semantic_loss: 0.01568, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18493 
Train Epoch: 21 [496/1000 15872/32000 (50%)] Loss: 1.96050 (semantic_loss: 0.01401, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18744 
Train Epoch: 21 [501/1000 16032/32000 (50%)] Loss: 1.96097 (semantic_loss: 0.01349, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18649 
Train Epoch: 21 [506/1000 16192/32000 (51%)] Loss: 1.96294 (semantic_loss: 0.01547, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18907 
Train Epoch: 21 [511/1000 16352/32000 (51%)] Loss: 1.96398 (semantic_loss: 0.01748, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18848 
Train Epoch: 21 [516/1000 16512/32000 (52%)] Loss: 1.96146 (semantic_loss: 0.01593, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18932 
Train Epoch: 21 [521/1000 16672/32000 (52%)] Loss: 1.96075 (semantic_loss: 0.01328, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20521 
Train Epoch: 21 [526/1000 16832/32000 (53%)] Loss: 1.96474 (semantic_loss: 0.01825, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19516 
Train Epoch: 21 [531/1000 16992/32000 (53%)] Loss: 1.96405 (semantic_loss: 0.01658, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19396 
Train Epoch: 21 [536/1000 17152/32000 (54%)] Loss: 1.96271 (semantic_loss: 0.01426, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18918 
Train Epoch: 21 [541/1000 17312/32000 (54%)] Loss: 1.96557 (semantic_loss: 0.01810, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18726 
Train Epoch: 21 [546/1000 17472/32000 (55%)] Loss: 1.96279 (semantic_loss: 0.01531, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20809 
Train Epoch: 21 [551/1000 17632/32000 (55%)] Loss: 1.96028 (semantic_loss: 0.01281, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21030 
Train Epoch: 21 [556/1000 17792/32000 (56%)] Loss: 1.96336 (semantic_loss: 0.01491, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20852 
Train Epoch: 21 [561/1000 17952/32000 (56%)] Loss: 1.96404 (semantic_loss: 0.01560, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22329 
Train Epoch: 21 [566/1000 18112/32000 (57%)] Loss: 1.96426 (semantic_loss: 0.01679, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19375 
Train Epoch: 21 [571/1000 18272/32000 (57%)] Loss: 1.96173 (semantic_loss: 0.01426, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21769 
Train Epoch: 21 [576/1000 18432/32000 (58%)] Loss: 1.96311 (semantic_loss: 0.01563, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20431 
Train Epoch: 21 [581/1000 18592/32000 (58%)] Loss: 1.96225 (semantic_loss: 0.01477, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23454 
Train Epoch: 21 [586/1000 18752/32000 (59%)] Loss: 1.96033 (semantic_loss: 0.01385, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24978 
Train Epoch: 21 [591/1000 18912/32000 (59%)] Loss: 1.96545 (semantic_loss: 0.01700, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19021 
Train Epoch: 21 [596/1000 19072/32000 (60%)] Loss: 1.96045 (semantic_loss: 0.01297, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.28462 
Train Epoch: 21 [601/1000 19232/32000 (60%)] Loss: 1.96341 (semantic_loss: 0.01593, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.57485 
Train Epoch: 21 [606/1000 19392/32000 (61%)] Loss: 1.96619 (semantic_loss: 0.01773, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18965 
Train Epoch: 21 [611/1000 19552/32000 (61%)] Loss: 1.96780 (semantic_loss: 0.01935, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21691 
Train Epoch: 21 [616/1000 19712/32000 (62%)] Loss: 1.96318 (semantic_loss: 0.01474, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20717 
Train Epoch: 21 [621/1000 19872/32000 (62%)] Loss: 1.95958 (semantic_loss: 0.01307, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19872 
Train Epoch: 21 [626/1000 20032/32000 (63%)] Loss: 1.96544 (semantic_loss: 0.01895, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19120 
Train Epoch: 21 [631/1000 20192/32000 (63%)] Loss: 1.96320 (semantic_loss: 0.01475, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18563 
Train Epoch: 21 [636/1000 20352/32000 (64%)] Loss: 1.96192 (semantic_loss: 0.01542, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18587 
Train Epoch: 21 [641/1000 20512/32000 (64%)] Loss: 1.95960 (semantic_loss: 0.01311, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18863 
Train Epoch: 21 [646/1000 20672/32000 (65%)] Loss: 1.96175 (semantic_loss: 0.01526, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20335 
Train Epoch: 21 [651/1000 20832/32000 (65%)] Loss: 1.96127 (semantic_loss: 0.01476, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.48629 
Train Epoch: 21 [656/1000 20992/32000 (66%)] Loss: 1.96107 (semantic_loss: 0.01360, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18554 
Train Epoch: 21 [661/1000 21152/32000 (66%)] Loss: 1.96673 (semantic_loss: 0.01926, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18772 
Train Epoch: 21 [666/1000 21312/32000 (67%)] Loss: 1.95792 (semantic_loss: 0.01240, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18823 
Train Epoch: 21 [671/1000 21472/32000 (67%)] Loss: 1.95946 (semantic_loss: 0.01296, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18711 
Train Epoch: 21 [676/1000 21632/32000 (68%)] Loss: 1.96434 (semantic_loss: 0.01686, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18833 
Train Epoch: 21 [681/1000 21792/32000 (68%)] Loss: 1.96871 (semantic_loss: 0.02026, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18644 
Train Epoch: 21 [686/1000 21952/32000 (69%)] Loss: 1.96210 (semantic_loss: 0.01560, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18782 
Train Epoch: 21 [691/1000 22112/32000 (69%)] Loss: 1.96553 (semantic_loss: 0.01903, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18581 
Train Epoch: 21 [696/1000 22272/32000 (70%)] Loss: 1.96295 (semantic_loss: 0.01547, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18727 
Train Epoch: 21 [701/1000 22432/32000 (70%)] Loss: 1.96419 (semantic_loss: 0.01574, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18535 
Train Epoch: 21 [706/1000 22592/32000 (71%)] Loss: 1.96405 (semantic_loss: 0.01462, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.21407 
Train Epoch: 21 [711/1000 22752/32000 (71%)] Loss: 1.96314 (semantic_loss: 0.01664, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.25314 
Train Epoch: 21 [716/1000 22912/32000 (72%)] Loss: 1.96295 (semantic_loss: 0.01645, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22597 
Train Epoch: 21 [721/1000 23072/32000 (72%)] Loss: 1.96499 (semantic_loss: 0.01654, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.47629 
Train Epoch: 21 [726/1000 23232/32000 (73%)] Loss: 1.96483 (semantic_loss: 0.01735, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22601 
Train Epoch: 21 [731/1000 23392/32000 (73%)] Loss: 1.95974 (semantic_loss: 0.01324, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20661 
Train Epoch: 21 [736/1000 23552/32000 (74%)] Loss: 1.96542 (semantic_loss: 0.01794, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.57658 
Train Epoch: 21 [741/1000 23712/32000 (74%)] Loss: 1.96907 (semantic_loss: 0.02160, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19261 
Train Epoch: 21 [746/1000 23872/32000 (75%)] Loss: 1.96234 (semantic_loss: 0.01584, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19764 
Train Epoch: 21 [751/1000 24032/32000 (75%)] Loss: 1.96057 (semantic_loss: 0.01505, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19093 
Train Epoch: 21 [756/1000 24192/32000 (76%)] Loss: 1.96538 (semantic_loss: 0.01888, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18650 
Train Epoch: 21 [761/1000 24352/32000 (76%)] Loss: 1.96364 (semantic_loss: 0.01715, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18702 
Train Epoch: 21 [766/1000 24512/32000 (77%)] Loss: 1.96687 (semantic_loss: 0.01842, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18713 
Train Epoch: 21 [771/1000 24672/32000 (77%)] Loss: 1.95929 (semantic_loss: 0.01279, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20159 
Train Epoch: 21 [776/1000 24832/32000 (78%)] Loss: 1.96624 (semantic_loss: 0.01877, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25503 
Train Epoch: 21 [781/1000 24992/32000 (78%)] Loss: 1.96069 (semantic_loss: 0.01420, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18899 
Train Epoch: 21 [786/1000 25152/32000 (79%)] Loss: 1.96514 (semantic_loss: 0.01865, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21085 
Train Epoch: 21 [791/1000 25312/32000 (79%)] Loss: 1.96682 (semantic_loss: 0.01935, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19240 
Train Epoch: 21 [796/1000 25472/32000 (80%)] Loss: 1.96186 (semantic_loss: 0.01536, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18700 
Train Epoch: 21 [801/1000 25632/32000 (80%)] Loss: 1.96437 (semantic_loss: 0.01592, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18926 
Train Epoch: 21 [806/1000 25792/32000 (81%)] Loss: 1.96485 (semantic_loss: 0.01738, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19063 
Train Epoch: 21 [811/1000 25952/32000 (81%)] Loss: 1.96254 (semantic_loss: 0.01702, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18683 
Train Epoch: 21 [816/1000 26112/32000 (82%)] Loss: 1.96175 (semantic_loss: 0.01525, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18781 
Train Epoch: 21 [821/1000 26272/32000 (82%)] Loss: 1.96324 (semantic_loss: 0.01480, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18343 
Train Epoch: 21 [826/1000 26432/32000 (83%)] Loss: 1.96259 (semantic_loss: 0.01512, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18773 
Train Epoch: 21 [831/1000 26592/32000 (83%)] Loss: 1.96316 (semantic_loss: 0.01569, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18455 
Train Epoch: 21 [836/1000 26752/32000 (84%)] Loss: 1.96196 (semantic_loss: 0.01546, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22221 
Train Epoch: 21 [841/1000 26912/32000 (84%)] Loss: 1.96149 (semantic_loss: 0.01401, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20587 
Train Epoch: 21 [846/1000 27072/32000 (85%)] Loss: 1.96247 (semantic_loss: 0.01598, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18647 
Train Epoch: 21 [851/1000 27232/32000 (85%)] Loss: 1.96144 (semantic_loss: 0.01495, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18497 
Train Epoch: 21 [856/1000 27392/32000 (86%)] Loss: 1.96028 (semantic_loss: 0.01476, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19610 
Train Epoch: 21 [861/1000 27552/32000 (86%)] Loss: 1.96148 (semantic_loss: 0.01498, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23239 
Train Epoch: 21 [866/1000 27712/32000 (87%)] Loss: 1.96055 (semantic_loss: 0.01307, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25757 
Train Epoch: 21 [871/1000 27872/32000 (87%)] Loss: 1.95967 (semantic_loss: 0.01318, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21821 
Train Epoch: 21 [876/1000 28032/32000 (88%)] Loss: 1.96918 (semantic_loss: 0.02171, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20106 
Train Epoch: 21 [881/1000 28192/32000 (88%)] Loss: 1.96482 (semantic_loss: 0.01734, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21062 
Train Epoch: 21 [886/1000 28352/32000 (89%)] Loss: 1.96151 (semantic_loss: 0.01404, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19318 
Train Epoch: 21 [891/1000 28512/32000 (89%)] Loss: 1.96305 (semantic_loss: 0.01557, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19886 
Train Epoch: 21 [896/1000 28672/32000 (90%)] Loss: 1.96262 (semantic_loss: 0.01416, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19023 
Train Epoch: 21 [901/1000 28832/32000 (90%)] Loss: 1.96103 (semantic_loss: 0.01453, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20293 
Train Epoch: 21 [906/1000 28992/32000 (91%)] Loss: 1.96560 (semantic_loss: 0.01812, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22442 
Train Epoch: 21 [911/1000 29152/32000 (91%)] Loss: 1.96632 (semantic_loss: 0.01983, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19036 
Train Epoch: 21 [916/1000 29312/32000 (92%)] Loss: 1.97180 (semantic_loss: 0.02433, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.31057 
Train Epoch: 21 [921/1000 29472/32000 (92%)] Loss: 1.96007 (semantic_loss: 0.01260, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.60154 
Train Epoch: 21 [926/1000 29632/32000 (93%)] Loss: 1.96399 (semantic_loss: 0.01554, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21303 
Train Epoch: 21 [931/1000 29792/32000 (93%)] Loss: 1.96029 (semantic_loss: 0.01380, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19196 
Train Epoch: 21 [936/1000 29952/32000 (94%)] Loss: 1.95980 (semantic_loss: 0.01330, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18543 
Train Epoch: 21 [941/1000 30112/32000 (94%)] Loss: 1.96049 (semantic_loss: 0.01497, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18845 
Train Epoch: 21 [946/1000 30272/32000 (95%)] Loss: 1.96436 (semantic_loss: 0.01786, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19444 
Train Epoch: 21 [951/1000 30432/32000 (95%)] Loss: 1.96288 (semantic_loss: 0.01541, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19011 
Train Epoch: 21 [956/1000 30592/32000 (96%)] Loss: 1.96301 (semantic_loss: 0.01553, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18628 
Train Epoch: 21 [961/1000 30752/32000 (96%)] Loss: 1.96229 (semantic_loss: 0.01482, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18847 
Train Epoch: 21 [966/1000 30912/32000 (97%)] Loss: 1.96193 (semantic_loss: 0.01446, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20294 
Train Epoch: 21 [971/1000 31072/32000 (97%)] Loss: 1.96257 (semantic_loss: 0.01705, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.50962 
Train Epoch: 21 [976/1000 31232/32000 (98%)] Loss: 1.96137 (semantic_loss: 0.01487, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18738 
Train Epoch: 21 [981/1000 31392/32000 (98%)] Loss: 1.96854 (semantic_loss: 0.02106, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19322 
Train Epoch: 21 [986/1000 31552/32000 (99%)] Loss: 1.96021 (semantic_loss: 0.01274, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20973 
Train Epoch: 21 [991/1000 31712/32000 (99%)] Loss: 1.96859 (semantic_loss: 0.02112, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19168 
Train Epoch: 21 [996/1000 31872/32000 (100%)] Loss: 1.96064 (semantic_loss: 0.01415, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18966 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch21.pth ...
Done in 4.623s
removing stale ckpt [epoch 20] [took 0.00s]
 epoch          : 21
 loss           : 1.9631768773794174
 learning_rate  : 6.078832729528469e-06
 n_samples      : 672000
 n_steps        : 21000
 ActivityNet_val1_test/t2v_metrics/R1: 11.592434411226357
 ActivityNet_val1_test/t2v_metrics/R5: 36.79072605247102
 ActivityNet_val1_test/t2v_metrics/R10: 53.99633923123856
 ActivityNet_val1_test/t2v_metrics/R50: 85.56030099654261
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 56.581858857026646
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 28.450666249060525
 ActivityNet_val1_test/v2t_metrics/R1: 12.385600976205003
 ActivityNet_val1_test/v2t_metrics/R5: 38.98718730933496
 ActivityNet_val1_test/v2t_metrics/R10: 55.1555826723612
 ActivityNet_val1_test/v2t_metrics/R50: 85.29591214154972
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 57.701647345942646
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.86364552346606
 mnt_best       : 28.911535532251342
 not_improved_count: 1
Train Epoch: 22 [1/1000 32/32000 (0%)] Loss: 1.96433 (semantic_loss: 0.01686, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=21.80156 
Train Epoch: 22 [6/1000 192/32000 (1%)] Loss: 1.96573 (semantic_loss: 0.01825, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22763 
Train Epoch: 22 [11/1000 352/32000 (1%)] Loss: 1.96315 (semantic_loss: 0.01568, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20299 
Train Epoch: 22 [16/1000 512/32000 (2%)] Loss: 1.96042 (semantic_loss: 0.01392, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.24224 
Train Epoch: 22 [21/1000 672/32000 (2%)] Loss: 1.96171 (semantic_loss: 0.01521, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19381 
Train Epoch: 22 [26/1000 832/32000 (3%)] Loss: 1.96166 (semantic_loss: 0.01614, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.61384 
Train Epoch: 22 [31/1000 992/32000 (3%)] Loss: 1.96178 (semantic_loss: 0.01529, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18855 
Train Epoch: 22 [36/1000 1152/32000 (4%)] Loss: 1.96413 (semantic_loss: 0.01764, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18721 
Train Epoch: 22 [41/1000 1312/32000 (4%)] Loss: 1.96099 (semantic_loss: 0.01351, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18856 
Train Epoch: 22 [46/1000 1472/32000 (5%)] Loss: 1.96277 (semantic_loss: 0.01530, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19816 
Train Epoch: 22 [51/1000 1632/32000 (5%)] Loss: 1.96101 (semantic_loss: 0.01353, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19015 
Train Epoch: 22 [56/1000 1792/32000 (6%)] Loss: 1.96171 (semantic_loss: 0.01521, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18879 
Train Epoch: 22 [61/1000 1952/32000 (6%)] Loss: 1.96382 (semantic_loss: 0.01635, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18899 
Train Epoch: 22 [66/1000 2112/32000 (7%)] Loss: 1.96335 (semantic_loss: 0.01588, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18738 
Train Epoch: 22 [71/1000 2272/32000 (7%)] Loss: 1.97123 (semantic_loss: 0.02473, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.25337 
Train Epoch: 22 [76/1000 2432/32000 (8%)] Loss: 1.96404 (semantic_loss: 0.01755, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18724 
Train Epoch: 22 [81/1000 2592/32000 (8%)] Loss: 1.96045 (semantic_loss: 0.01200, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18700 
Train Epoch: 22 [86/1000 2752/32000 (9%)] Loss: 1.96278 (semantic_loss: 0.01628, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18789 
Train Epoch: 22 [91/1000 2912/32000 (9%)] Loss: 1.96379 (semantic_loss: 0.01730, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20055 
Train Epoch: 22 [96/1000 3072/32000 (10%)] Loss: 1.96078 (semantic_loss: 0.01429, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18992 
Train Epoch: 22 [101/1000 3232/32000 (10%)] Loss: 1.95745 (semantic_loss: 0.01194, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18919 
Train Epoch: 22 [106/1000 3392/32000 (11%)] Loss: 1.96392 (semantic_loss: 0.01547, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18548 
Train Epoch: 22 [111/1000 3552/32000 (11%)] Loss: 1.96578 (semantic_loss: 0.01928, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19814 
Train Epoch: 22 [116/1000 3712/32000 (12%)] Loss: 1.96302 (semantic_loss: 0.01652, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18776 
Train Epoch: 22 [121/1000 3872/32000 (12%)] Loss: 1.95917 (semantic_loss: 0.01462, quant_loss: 1.94434, bit_balance_loss: 0.00022) batch_time=0.19182 
Train Epoch: 22 [126/1000 4032/32000 (13%)] Loss: 1.96003 (semantic_loss: 0.01353, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19768 
Train Epoch: 22 [131/1000 4192/32000 (13%)] Loss: 1.96441 (semantic_loss: 0.01596, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.24142 
Train Epoch: 22 [136/1000 4352/32000 (14%)] Loss: 1.96458 (semantic_loss: 0.01711, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20694 
Train Epoch: 22 [141/1000 4512/32000 (14%)] Loss: 1.96519 (semantic_loss: 0.01772, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24810 
Train Epoch: 22 [146/1000 4672/32000 (15%)] Loss: 1.96596 (semantic_loss: 0.01848, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.33266 
Train Epoch: 22 [151/1000 4832/32000 (15%)] Loss: 1.96339 (semantic_loss: 0.01494, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19650 
Train Epoch: 22 [156/1000 4992/32000 (16%)] Loss: 1.96734 (semantic_loss: 0.02084, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21145 
Train Epoch: 22 [161/1000 5152/32000 (16%)] Loss: 1.96226 (semantic_loss: 0.01478, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19570 
Train Epoch: 22 [166/1000 5312/32000 (17%)] Loss: 1.96811 (semantic_loss: 0.02064, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19450 
Train Epoch: 22 [171/1000 5472/32000 (17%)] Loss: 1.96081 (semantic_loss: 0.01432, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19664 
Train Epoch: 22 [176/1000 5632/32000 (18%)] Loss: 1.96479 (semantic_loss: 0.01829, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22335 
Train Epoch: 22 [181/1000 5792/32000 (18%)] Loss: 1.96767 (semantic_loss: 0.02117, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18807 
Train Epoch: 22 [186/1000 5952/32000 (19%)] Loss: 1.95983 (semantic_loss: 0.01236, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18698 
Train Epoch: 22 [191/1000 6112/32000 (19%)] Loss: 1.96455 (semantic_loss: 0.01708, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19038 
Train Epoch: 22 [196/1000 6272/32000 (20%)] Loss: 1.96241 (semantic_loss: 0.01397, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18953 
Train Epoch: 22 [201/1000 6432/32000 (20%)] Loss: 1.96449 (semantic_loss: 0.01799, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19101 
Train Epoch: 22 [206/1000 6592/32000 (21%)] Loss: 1.96350 (semantic_loss: 0.01603, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19017 
Train Epoch: 22 [211/1000 6752/32000 (21%)] Loss: 1.96272 (semantic_loss: 0.01622, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.50379 
Train Epoch: 22 [216/1000 6912/32000 (22%)] Loss: 1.96116 (semantic_loss: 0.01466, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19239 
Train Epoch: 22 [221/1000 7072/32000 (22%)] Loss: 1.96476 (semantic_loss: 0.01728, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18804 
Train Epoch: 22 [226/1000 7232/32000 (23%)] Loss: 1.96510 (semantic_loss: 0.01763, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18862 
Train Epoch: 22 [231/1000 7392/32000 (23%)] Loss: 1.96719 (semantic_loss: 0.01874, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18542 
Train Epoch: 22 [236/1000 7552/32000 (24%)] Loss: 1.95938 (semantic_loss: 0.01289, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18678 
Train Epoch: 22 [241/1000 7712/32000 (24%)] Loss: 1.96304 (semantic_loss: 0.01557, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19280 
Train Epoch: 22 [246/1000 7872/32000 (25%)] Loss: 1.96116 (semantic_loss: 0.01466, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18728 
Train Epoch: 22 [251/1000 8032/32000 (25%)] Loss: 1.96343 (semantic_loss: 0.01595, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18691 
Train Epoch: 22 [256/1000 8192/32000 (26%)] Loss: 1.96067 (semantic_loss: 0.01320, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18630 
Train Epoch: 22 [261/1000 8352/32000 (26%)] Loss: 1.96734 (semantic_loss: 0.01986, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18821 
Train Epoch: 22 [266/1000 8512/32000 (27%)] Loss: 1.96087 (semantic_loss: 0.01437, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23294 
Train Epoch: 22 [271/1000 8672/32000 (27%)] Loss: 1.96355 (semantic_loss: 0.01608, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18983 
Train Epoch: 22 [276/1000 8832/32000 (28%)] Loss: 1.96269 (semantic_loss: 0.01522, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18575 
Train Epoch: 22 [281/1000 8992/32000 (28%)] Loss: 1.96043 (semantic_loss: 0.01296, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.46373 
Train Epoch: 22 [286/1000 9152/32000 (29%)] Loss: 1.96232 (semantic_loss: 0.01583, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21850 
Train Epoch: 22 [291/1000 9312/32000 (29%)] Loss: 1.96480 (semantic_loss: 0.01732, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22175 
Train Epoch: 22 [296/1000 9472/32000 (30%)] Loss: 1.96547 (semantic_loss: 0.01798, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.20933 
Train Epoch: 22 [301/1000 9632/32000 (30%)] Loss: 1.96206 (semantic_loss: 0.01459, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21645 
Train Epoch: 22 [306/1000 9792/32000 (31%)] Loss: 1.96458 (semantic_loss: 0.01808, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19875 
Train Epoch: 22 [311/1000 9952/32000 (31%)] Loss: 1.96155 (semantic_loss: 0.01506, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21590 
Train Epoch: 22 [316/1000 10112/32000 (32%)] Loss: 1.96360 (semantic_loss: 0.01516, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23042 
Train Epoch: 22 [321/1000 10272/32000 (32%)] Loss: 1.96292 (semantic_loss: 0.01642, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18905 
Train Epoch: 22 [326/1000 10432/32000 (33%)] Loss: 1.96883 (semantic_loss: 0.02136, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20580 
Train Epoch: 22 [331/1000 10592/32000 (33%)] Loss: 1.96064 (semantic_loss: 0.01317, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19040 
Train Epoch: 22 [336/1000 10752/32000 (34%)] Loss: 1.96353 (semantic_loss: 0.01801, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.23846 
Train Epoch: 22 [341/1000 10912/32000 (34%)] Loss: 1.96258 (semantic_loss: 0.01510, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18486 
Train Epoch: 22 [346/1000 11072/32000 (35%)] Loss: 1.96080 (semantic_loss: 0.01430, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.61246 
Train Epoch: 22 [351/1000 11232/32000 (35%)] Loss: 1.96201 (semantic_loss: 0.01357, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19737 
Train Epoch: 22 [356/1000 11392/32000 (36%)] Loss: 1.96176 (semantic_loss: 0.01429, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20295 
Train Epoch: 22 [361/1000 11552/32000 (36%)] Loss: 1.96347 (semantic_loss: 0.01600, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19199 
Train Epoch: 22 [366/1000 11712/32000 (37%)] Loss: 1.96125 (semantic_loss: 0.01377, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18710 
Train Epoch: 22 [371/1000 11872/32000 (37%)] Loss: 1.96492 (semantic_loss: 0.01745, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18612 
Train Epoch: 22 [376/1000 12032/32000 (38%)] Loss: 1.96000 (semantic_loss: 0.01253, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19120 
Train Epoch: 22 [381/1000 12192/32000 (38%)] Loss: 1.96699 (semantic_loss: 0.01756, quant_loss: 1.94922, bit_balance_loss: 0.00022) batch_time=0.18752 
Train Epoch: 22 [386/1000 12352/32000 (39%)] Loss: 1.96208 (semantic_loss: 0.01558, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18989 
Train Epoch: 22 [391/1000 12512/32000 (39%)] Loss: 1.96224 (semantic_loss: 0.01574, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24884 
Train Epoch: 22 [396/1000 12672/32000 (40%)] Loss: 1.96253 (semantic_loss: 0.01506, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18762 
Train Epoch: 22 [401/1000 12832/32000 (40%)] Loss: 1.96531 (semantic_loss: 0.01784, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18697 
Train Epoch: 22 [406/1000 12992/32000 (41%)] Loss: 1.96371 (semantic_loss: 0.01722, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19221 
Train Epoch: 22 [411/1000 13152/32000 (41%)] Loss: 1.96243 (semantic_loss: 0.01691, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18404 
Train Epoch: 22 [416/1000 13312/32000 (42%)] Loss: 1.95989 (semantic_loss: 0.01339, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18948 
Train Epoch: 22 [421/1000 13472/32000 (42%)] Loss: 1.96346 (semantic_loss: 0.01696, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18775 
Train Epoch: 22 [426/1000 13632/32000 (43%)] Loss: 1.96225 (semantic_loss: 0.01478, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20904 
Train Epoch: 22 [431/1000 13792/32000 (43%)] Loss: 1.96392 (semantic_loss: 0.01645, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18828 
Train Epoch: 22 [436/1000 13952/32000 (44%)] Loss: 1.96371 (semantic_loss: 0.01624, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19022 
Train Epoch: 22 [441/1000 14112/32000 (44%)] Loss: 1.96665 (semantic_loss: 0.01820, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21936 
Train Epoch: 22 [446/1000 14272/32000 (45%)] Loss: 1.96132 (semantic_loss: 0.01384, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21843 
Train Epoch: 22 [451/1000 14432/32000 (45%)] Loss: 1.96466 (semantic_loss: 0.01816, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.26885 
Train Epoch: 22 [456/1000 14592/32000 (46%)] Loss: 1.96501 (semantic_loss: 0.01753, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20737 
Train Epoch: 22 [461/1000 14752/32000 (46%)] Loss: 1.96203 (semantic_loss: 0.01554, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.25168 
Train Epoch: 22 [466/1000 14912/32000 (47%)] Loss: 1.96226 (semantic_loss: 0.01576, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.30721 
Train Epoch: 22 [471/1000 15072/32000 (47%)] Loss: 1.96406 (semantic_loss: 0.01659, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18956 
Train Epoch: 22 [476/1000 15232/32000 (48%)] Loss: 1.96336 (semantic_loss: 0.01589, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19698 
Train Epoch: 22 [481/1000 15392/32000 (48%)] Loss: 1.96276 (semantic_loss: 0.01529, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18664 
Train Epoch: 22 [486/1000 15552/32000 (49%)] Loss: 1.96293 (semantic_loss: 0.01643, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18857 
Train Epoch: 22 [491/1000 15712/32000 (49%)] Loss: 1.96274 (semantic_loss: 0.01527, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18750 
Train Epoch: 22 [496/1000 15872/32000 (50%)] Loss: 1.96262 (semantic_loss: 0.01515, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21659 
Train Epoch: 22 [501/1000 16032/32000 (50%)] Loss: 1.95971 (semantic_loss: 0.01419, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18862 
Train Epoch: 22 [506/1000 16192/32000 (51%)] Loss: 1.95896 (semantic_loss: 0.01246, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19086 
Train Epoch: 22 [511/1000 16352/32000 (51%)] Loss: 1.96568 (semantic_loss: 0.01821, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18877 
Train Epoch: 22 [516/1000 16512/32000 (52%)] Loss: 1.96382 (semantic_loss: 0.01733, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19794 
Train Epoch: 22 [521/1000 16672/32000 (52%)] Loss: 1.95951 (semantic_loss: 0.01203, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18679 
Train Epoch: 22 [526/1000 16832/32000 (53%)] Loss: 1.96520 (semantic_loss: 0.01968, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18826 
Train Epoch: 22 [531/1000 16992/32000 (53%)] Loss: 1.96449 (semantic_loss: 0.01800, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.48427 
Train Epoch: 22 [536/1000 17152/32000 (54%)] Loss: 1.96720 (semantic_loss: 0.01972, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19129 
Train Epoch: 22 [541/1000 17312/32000 (54%)] Loss: 1.97102 (semantic_loss: 0.02257, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18925 
Train Epoch: 22 [546/1000 17472/32000 (55%)] Loss: 1.96449 (semantic_loss: 0.01702, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19345 
Train Epoch: 22 [551/1000 17632/32000 (55%)] Loss: 1.96070 (semantic_loss: 0.01421, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18715 
Train Epoch: 22 [556/1000 17792/32000 (56%)] Loss: 1.96236 (semantic_loss: 0.01489, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18808 
Train Epoch: 22 [561/1000 17952/32000 (56%)] Loss: 1.96394 (semantic_loss: 0.01549, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18638 
Train Epoch: 22 [566/1000 18112/32000 (57%)] Loss: 1.96318 (semantic_loss: 0.01669, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18645 
Train Epoch: 22 [571/1000 18272/32000 (57%)] Loss: 1.95959 (semantic_loss: 0.01310, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18675 
Train Epoch: 22 [576/1000 18432/32000 (58%)] Loss: 1.96160 (semantic_loss: 0.01412, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18664 
Train Epoch: 22 [581/1000 18592/32000 (58%)] Loss: 1.96533 (semantic_loss: 0.01688, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18627 
Train Epoch: 22 [586/1000 18752/32000 (59%)] Loss: 1.96536 (semantic_loss: 0.01691, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18712 
Train Epoch: 22 [591/1000 18912/32000 (59%)] Loss: 1.96271 (semantic_loss: 0.01622, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19363 
Train Epoch: 22 [596/1000 19072/32000 (60%)] Loss: 1.96279 (semantic_loss: 0.01532, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=1.22163 
Train Epoch: 22 [601/1000 19232/32000 (60%)] Loss: 1.96167 (semantic_loss: 0.01518, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22146 
Train Epoch: 22 [606/1000 19392/32000 (61%)] Loss: 1.96104 (semantic_loss: 0.01455, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21857 
Train Epoch: 22 [611/1000 19552/32000 (61%)] Loss: 1.96016 (semantic_loss: 0.01269, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19688 
Train Epoch: 22 [616/1000 19712/32000 (62%)] Loss: 1.96194 (semantic_loss: 0.01545, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20555 
Train Epoch: 22 [621/1000 19872/32000 (62%)] Loss: 1.96355 (semantic_loss: 0.01609, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19783 
Train Epoch: 22 [626/1000 20032/32000 (63%)] Loss: 1.96068 (semantic_loss: 0.01321, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20440 
Train Epoch: 22 [631/1000 20192/32000 (63%)] Loss: 1.96094 (semantic_loss: 0.01347, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19931 
Train Epoch: 22 [636/1000 20352/32000 (64%)] Loss: 1.96187 (semantic_loss: 0.01440, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18864 
Train Epoch: 22 [641/1000 20512/32000 (64%)] Loss: 1.96205 (semantic_loss: 0.01458, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19022 
Train Epoch: 22 [646/1000 20672/32000 (65%)] Loss: 1.96264 (semantic_loss: 0.01614, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18669 
Train Epoch: 22 [651/1000 20832/32000 (65%)] Loss: 1.96151 (semantic_loss: 0.01404, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18785 
Train Epoch: 22 [656/1000 20992/32000 (66%)] Loss: 1.96090 (semantic_loss: 0.01342, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18804 
Train Epoch: 22 [661/1000 21152/32000 (66%)] Loss: 1.96254 (semantic_loss: 0.01505, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19970 
Train Epoch: 22 [666/1000 21312/32000 (67%)] Loss: 1.96235 (semantic_loss: 0.01585, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19850 
Train Epoch: 22 [671/1000 21472/32000 (67%)] Loss: 1.96857 (semantic_loss: 0.02109, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19519 
Train Epoch: 22 [676/1000 21632/32000 (68%)] Loss: 1.96361 (semantic_loss: 0.01516, quant_loss: 1.94824, bit_balance_loss: 0.00022) batch_time=0.19161 
Train Epoch: 22 [681/1000 21792/32000 (68%)] Loss: 1.96163 (semantic_loss: 0.01416, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18900 
Train Epoch: 22 [686/1000 21952/32000 (69%)] Loss: 1.96122 (semantic_loss: 0.01375, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18687 
Train Epoch: 22 [691/1000 22112/32000 (69%)] Loss: 1.96435 (semantic_loss: 0.01688, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18493 
Train Epoch: 22 [696/1000 22272/32000 (70%)] Loss: 1.96272 (semantic_loss: 0.01427, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20898 
Train Epoch: 22 [701/1000 22432/32000 (70%)] Loss: 1.96845 (semantic_loss: 0.02097, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23184 
Train Epoch: 22 [706/1000 22592/32000 (71%)] Loss: 1.96220 (semantic_loss: 0.01473, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18574 
Train Epoch: 22 [711/1000 22752/32000 (71%)] Loss: 1.96714 (semantic_loss: 0.01869, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18765 
Train Epoch: 22 [716/1000 22912/32000 (72%)] Loss: 1.96219 (semantic_loss: 0.01569, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20778 
Train Epoch: 22 [721/1000 23072/32000 (72%)] Loss: 1.96253 (semantic_loss: 0.01604, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22019 
Train Epoch: 22 [726/1000 23232/32000 (73%)] Loss: 1.96189 (semantic_loss: 0.01344, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18480 
Train Epoch: 22 [731/1000 23392/32000 (73%)] Loss: 1.96409 (semantic_loss: 0.01760, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18549 
Train Epoch: 22 [736/1000 23552/32000 (74%)] Loss: 1.95831 (semantic_loss: 0.01182, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18426 
Train Epoch: 22 [741/1000 23712/32000 (74%)] Loss: 1.96342 (semantic_loss: 0.01497, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20192 
Train Epoch: 22 [746/1000 23872/32000 (75%)] Loss: 1.96233 (semantic_loss: 0.01485, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21870 
Train Epoch: 22 [751/1000 24032/32000 (75%)] Loss: 1.96408 (semantic_loss: 0.01662, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21077 
Train Epoch: 22 [756/1000 24192/32000 (76%)] Loss: 1.96049 (semantic_loss: 0.01400, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21079 
Train Epoch: 22 [761/1000 24352/32000 (76%)] Loss: 1.96194 (semantic_loss: 0.01545, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20226 
Train Epoch: 22 [766/1000 24512/32000 (77%)] Loss: 1.96470 (semantic_loss: 0.01723, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19338 
Train Epoch: 22 [771/1000 24672/32000 (77%)] Loss: 1.96079 (semantic_loss: 0.01429, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21178 
Train Epoch: 22 [776/1000 24832/32000 (78%)] Loss: 1.96387 (semantic_loss: 0.01639, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20808 
Train Epoch: 22 [781/1000 24992/32000 (78%)] Loss: 1.96219 (semantic_loss: 0.01569, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20205 
Train Epoch: 22 [786/1000 25152/32000 (79%)] Loss: 1.96373 (semantic_loss: 0.01625, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26973 
Train Epoch: 22 [791/1000 25312/32000 (79%)] Loss: 1.96607 (semantic_loss: 0.01859, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19301 
Train Epoch: 22 [796/1000 25472/32000 (80%)] Loss: 1.96285 (semantic_loss: 0.01537, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18971 
Train Epoch: 22 [801/1000 25632/32000 (80%)] Loss: 1.96399 (semantic_loss: 0.01750, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19395 
Train Epoch: 22 [806/1000 25792/32000 (81%)] Loss: 1.96464 (semantic_loss: 0.01717, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19263 
Train Epoch: 22 [811/1000 25952/32000 (81%)] Loss: 1.96267 (semantic_loss: 0.01520, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18966 
Train Epoch: 22 [816/1000 26112/32000 (82%)] Loss: 1.96132 (semantic_loss: 0.01482, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20185 
Train Epoch: 22 [821/1000 26272/32000 (82%)] Loss: 1.96482 (semantic_loss: 0.01735, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18941 
Train Epoch: 22 [826/1000 26432/32000 (83%)] Loss: 1.96308 (semantic_loss: 0.01559, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18694 
Train Epoch: 22 [831/1000 26592/32000 (83%)] Loss: 1.96325 (semantic_loss: 0.01480, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18712 
Train Epoch: 22 [836/1000 26752/32000 (84%)] Loss: 1.96112 (semantic_loss: 0.01462, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18741 
Train Epoch: 22 [841/1000 26912/32000 (84%)] Loss: 1.96438 (semantic_loss: 0.01789, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18793 
Train Epoch: 22 [846/1000 27072/32000 (85%)] Loss: 1.96229 (semantic_loss: 0.01482, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20114 
Train Epoch: 22 [851/1000 27232/32000 (85%)] Loss: 1.96461 (semantic_loss: 0.01714, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.54475 
Train Epoch: 22 [856/1000 27392/32000 (86%)] Loss: 1.96022 (semantic_loss: 0.01275, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18639 
Train Epoch: 22 [861/1000 27552/32000 (86%)] Loss: 1.96318 (semantic_loss: 0.01669, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19458 
Train Epoch: 22 [866/1000 27712/32000 (87%)] Loss: 1.96397 (semantic_loss: 0.01650, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18468 
Train Epoch: 22 [871/1000 27872/32000 (87%)] Loss: 1.96190 (semantic_loss: 0.01346, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18630 
Train Epoch: 22 [876/1000 28032/32000 (88%)] Loss: 1.96471 (semantic_loss: 0.01723, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19591 
Train Epoch: 22 [881/1000 28192/32000 (88%)] Loss: 1.96168 (semantic_loss: 0.01420, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18634 
Train Epoch: 22 [886/1000 28352/32000 (89%)] Loss: 1.96283 (semantic_loss: 0.01633, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18863 
Train Epoch: 22 [891/1000 28512/32000 (89%)] Loss: 1.96518 (semantic_loss: 0.01770, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18567 
Train Epoch: 22 [896/1000 28672/32000 (90%)] Loss: 1.96570 (semantic_loss: 0.02018, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18891 
Train Epoch: 22 [901/1000 28832/32000 (90%)] Loss: 1.96064 (semantic_loss: 0.01513, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.21703 
Train Epoch: 22 [906/1000 28992/32000 (91%)] Loss: 1.96132 (semantic_loss: 0.01385, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22452 
Train Epoch: 22 [911/1000 29152/32000 (91%)] Loss: 1.96193 (semantic_loss: 0.01446, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21700 
Train Epoch: 22 [916/1000 29312/32000 (92%)] Loss: 1.96117 (semantic_loss: 0.01468, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=1.39030 
Train Epoch: 22 [921/1000 29472/32000 (92%)] Loss: 1.96536 (semantic_loss: 0.01788, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19440 
Train Epoch: 22 [926/1000 29632/32000 (93%)] Loss: 1.96079 (semantic_loss: 0.01430, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19228 
Train Epoch: 22 [931/1000 29792/32000 (93%)] Loss: 1.95995 (semantic_loss: 0.01346, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21229 
Train Epoch: 22 [936/1000 29952/32000 (94%)] Loss: 1.96302 (semantic_loss: 0.01652, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19052 
Train Epoch: 22 [941/1000 30112/32000 (94%)] Loss: 1.96273 (semantic_loss: 0.01526, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18791 
Train Epoch: 22 [946/1000 30272/32000 (95%)] Loss: 1.96081 (semantic_loss: 0.01530, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19095 
Train Epoch: 22 [951/1000 30432/32000 (95%)] Loss: 1.96337 (semantic_loss: 0.01492, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18887 
Train Epoch: 22 [956/1000 30592/32000 (96%)] Loss: 1.96601 (semantic_loss: 0.01854, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18909 
Train Epoch: 22 [961/1000 30752/32000 (96%)] Loss: 1.96110 (semantic_loss: 0.01362, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19012 
Train Epoch: 22 [966/1000 30912/32000 (97%)] Loss: 1.96065 (semantic_loss: 0.01512, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18965 
Train Epoch: 22 [971/1000 31072/32000 (97%)] Loss: 1.96465 (semantic_loss: 0.01621, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19288 
Train Epoch: 22 [976/1000 31232/32000 (98%)] Loss: 1.96009 (semantic_loss: 0.01456, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20540 
Train Epoch: 22 [981/1000 31392/32000 (98%)] Loss: 1.96119 (semantic_loss: 0.01470, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18977 
Train Epoch: 22 [986/1000 31552/32000 (99%)] Loss: 1.96568 (semantic_loss: 0.01918, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19048 
Train Epoch: 22 [991/1000 31712/32000 (99%)] Loss: 1.96565 (semantic_loss: 0.01915, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18633 
Train Epoch: 22 [996/1000 31872/32000 (100%)] Loss: 1.96072 (semantic_loss: 0.01324, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19052 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch22.pth ...
Done in 5.615s
removing stale ckpt [epoch 21] [took 0.00s]
 epoch          : 22
 loss           : 1.9631079658269883
 learning_rate  : 5.470949456575622e-06
 n_samples      : 704000
 n_steps        : 22000
 ActivityNet_val1_test/t2v_metrics/R1: 11.673784828147244
 ActivityNet_val1_test/t2v_metrics/R5: 37.74659345129144
 ActivityNet_val1_test/t2v_metrics/R10: 54.40309131584299
 ActivityNet_val1_test/t2v_metrics/R50: 85.4586129753915
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 57.098739068537725
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 28.833963338122285
 ActivityNet_val1_test/v2t_metrics/R1: 12.34492576774456
 ActivityNet_val1_test/v2t_metrics/R5: 39.190563351637174
 ActivityNet_val1_test/v2t_metrics/R10: 55.92841163310962
 ActivityNet_val1_test/v2t_metrics/R50: 85.1738865161684
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 57.5594874923734
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 30.02162239715695
 mnt_best       : 28.911535532251342
 not_improved_count: 2
Train Epoch: 23 [1/1000 32/32000 (0%)] Loss: 1.96584 (semantic_loss: 0.01837, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=21.56788 
Train Epoch: 23 [6/1000 192/32000 (1%)] Loss: 1.96253 (semantic_loss: 0.01506, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18623 
Train Epoch: 23 [11/1000 352/32000 (1%)] Loss: 1.96182 (semantic_loss: 0.01435, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18660 
Train Epoch: 23 [16/1000 512/32000 (2%)] Loss: 1.96214 (semantic_loss: 0.01565, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.61678 
Train Epoch: 23 [21/1000 672/32000 (2%)] Loss: 1.96611 (semantic_loss: 0.01864, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.29279 
Train Epoch: 23 [26/1000 832/32000 (3%)] Loss: 1.95964 (semantic_loss: 0.01412, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20884 
Train Epoch: 23 [31/1000 992/32000 (3%)] Loss: 1.96469 (semantic_loss: 0.01721, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.26368 
Train Epoch: 23 [36/1000 1152/32000 (4%)] Loss: 1.96471 (semantic_loss: 0.01724, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20009 
Train Epoch: 23 [41/1000 1312/32000 (4%)] Loss: 1.96731 (semantic_loss: 0.01983, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19465 
Train Epoch: 23 [46/1000 1472/32000 (5%)] Loss: 1.96718 (semantic_loss: 0.01971, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19637 
Train Epoch: 23 [51/1000 1632/32000 (5%)] Loss: 1.96121 (semantic_loss: 0.01374, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18873 
Train Epoch: 23 [56/1000 1792/32000 (6%)] Loss: 1.95966 (semantic_loss: 0.01414, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19234 
Train Epoch: 23 [61/1000 1952/32000 (6%)] Loss: 1.96363 (semantic_loss: 0.01714, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18699 
Train Epoch: 23 [66/1000 2112/32000 (7%)] Loss: 1.96223 (semantic_loss: 0.01671, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18670 
Train Epoch: 23 [71/1000 2272/32000 (7%)] Loss: 1.96148 (semantic_loss: 0.01498, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18330 
Train Epoch: 23 [76/1000 2432/32000 (8%)] Loss: 1.96173 (semantic_loss: 0.01524, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18315 
Train Epoch: 23 [81/1000 2592/32000 (8%)] Loss: 1.96212 (semantic_loss: 0.01466, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18354 
Train Epoch: 23 [86/1000 2752/32000 (9%)] Loss: 1.96125 (semantic_loss: 0.01378, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19919 
Train Epoch: 23 [91/1000 2912/32000 (9%)] Loss: 1.96553 (semantic_loss: 0.01805, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19632 
Train Epoch: 23 [96/1000 3072/32000 (10%)] Loss: 1.96604 (semantic_loss: 0.01759, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.24536 
Train Epoch: 23 [101/1000 3232/32000 (10%)] Loss: 1.96088 (semantic_loss: 0.01341, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18554 
Train Epoch: 23 [106/1000 3392/32000 (11%)] Loss: 1.96102 (semantic_loss: 0.01354, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20446 
Train Epoch: 23 [111/1000 3552/32000 (11%)] Loss: 1.96249 (semantic_loss: 0.01503, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18917 
Train Epoch: 23 [116/1000 3712/32000 (12%)] Loss: 1.96057 (semantic_loss: 0.01310, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18473 
Train Epoch: 23 [121/1000 3872/32000 (12%)] Loss: 1.96416 (semantic_loss: 0.01474, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.18713 
Train Epoch: 23 [126/1000 4032/32000 (13%)] Loss: 1.96216 (semantic_loss: 0.01469, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19362 
Train Epoch: 23 [131/1000 4192/32000 (13%)] Loss: 1.96281 (semantic_loss: 0.01534, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.65919 
Train Epoch: 23 [136/1000 4352/32000 (14%)] Loss: 1.96337 (semantic_loss: 0.01589, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20727 
Train Epoch: 23 [141/1000 4512/32000 (14%)] Loss: 1.96414 (semantic_loss: 0.01765, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18607 
Train Epoch: 23 [146/1000 4672/32000 (15%)] Loss: 1.96574 (semantic_loss: 0.01923, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18580 
Train Epoch: 23 [151/1000 4832/32000 (15%)] Loss: 1.96197 (semantic_loss: 0.01352, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20746 
Train Epoch: 23 [156/1000 4992/32000 (16%)] Loss: 1.96365 (semantic_loss: 0.01617, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20648 
Train Epoch: 23 [161/1000 5152/32000 (16%)] Loss: 1.96016 (semantic_loss: 0.01269, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18685 
Train Epoch: 23 [166/1000 5312/32000 (17%)] Loss: 1.96713 (semantic_loss: 0.01966, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21191 
Train Epoch: 23 [171/1000 5472/32000 (17%)] Loss: 1.96418 (semantic_loss: 0.01572, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22442 
Train Epoch: 23 [176/1000 5632/32000 (18%)] Loss: 1.96201 (semantic_loss: 0.01552, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22823 
Train Epoch: 23 [181/1000 5792/32000 (18%)] Loss: 1.95911 (semantic_loss: 0.01261, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23120 
Train Epoch: 23 [186/1000 5952/32000 (19%)] Loss: 1.96344 (semantic_loss: 0.01694, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19985 
Train Epoch: 23 [191/1000 6112/32000 (19%)] Loss: 1.96573 (semantic_loss: 0.01826, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20881 
Train Epoch: 23 [196/1000 6272/32000 (20%)] Loss: 1.95963 (semantic_loss: 0.01411, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19792 
Train Epoch: 23 [201/1000 6432/32000 (20%)] Loss: 1.96496 (semantic_loss: 0.01847, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20908 
Train Epoch: 23 [206/1000 6592/32000 (21%)] Loss: 1.96806 (semantic_loss: 0.02058, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18973 
Train Epoch: 23 [211/1000 6752/32000 (21%)] Loss: 1.96303 (semantic_loss: 0.01458, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18446 
Train Epoch: 23 [216/1000 6912/32000 (22%)] Loss: 1.96295 (semantic_loss: 0.01548, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18449 
Train Epoch: 23 [221/1000 7072/32000 (22%)] Loss: 1.96624 (semantic_loss: 0.01780, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18794 
Train Epoch: 23 [226/1000 7232/32000 (23%)] Loss: 1.96079 (semantic_loss: 0.01429, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18432 
Train Epoch: 23 [231/1000 7392/32000 (23%)] Loss: 1.96257 (semantic_loss: 0.01509, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19905 
Train Epoch: 23 [236/1000 7552/32000 (24%)] Loss: 1.96332 (semantic_loss: 0.01488, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19635 
Train Epoch: 23 [241/1000 7712/32000 (24%)] Loss: 1.95806 (semantic_loss: 0.01157, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19291 
Train Epoch: 23 [246/1000 7872/32000 (25%)] Loss: 1.96233 (semantic_loss: 0.01388, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18905 
Train Epoch: 23 [251/1000 8032/32000 (25%)] Loss: 1.96832 (semantic_loss: 0.01987, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18626 
Train Epoch: 23 [256/1000 8192/32000 (26%)] Loss: 1.96385 (semantic_loss: 0.01540, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18711 
Train Epoch: 23 [261/1000 8352/32000 (26%)] Loss: 1.96535 (semantic_loss: 0.01788, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18792 
Train Epoch: 23 [266/1000 8512/32000 (27%)] Loss: 1.96205 (semantic_loss: 0.01459, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19232 
Train Epoch: 23 [271/1000 8672/32000 (27%)] Loss: 1.96828 (semantic_loss: 0.01983, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19032 
Train Epoch: 23 [276/1000 8832/32000 (28%)] Loss: 1.96582 (semantic_loss: 0.01834, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18732 
Train Epoch: 23 [281/1000 8992/32000 (28%)] Loss: 1.96552 (semantic_loss: 0.01707, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18545 
Train Epoch: 23 [286/1000 9152/32000 (29%)] Loss: 1.96321 (semantic_loss: 0.01574, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18822 
Train Epoch: 23 [291/1000 9312/32000 (29%)] Loss: 1.96602 (semantic_loss: 0.01854, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19198 
Train Epoch: 23 [296/1000 9472/32000 (30%)] Loss: 1.96479 (semantic_loss: 0.01635, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19954 
Train Epoch: 23 [301/1000 9632/32000 (30%)] Loss: 1.96097 (semantic_loss: 0.01350, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19902 
Train Epoch: 23 [306/1000 9792/32000 (31%)] Loss: 1.96393 (semantic_loss: 0.01646, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18361 
Train Epoch: 23 [311/1000 9952/32000 (31%)] Loss: 1.96054 (semantic_loss: 0.01307, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18604 
Train Epoch: 23 [316/1000 10112/32000 (32%)] Loss: 1.96222 (semantic_loss: 0.01476, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18357 
Train Epoch: 23 [321/1000 10272/32000 (32%)] Loss: 1.96314 (semantic_loss: 0.01567, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.27055 
Train Epoch: 23 [326/1000 10432/32000 (33%)] Loss: 1.96271 (semantic_loss: 0.01622, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22589 
Train Epoch: 23 [331/1000 10592/32000 (33%)] Loss: 1.96092 (semantic_loss: 0.01441, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21998 
Train Epoch: 23 [336/1000 10752/32000 (34%)] Loss: 1.96000 (semantic_loss: 0.01352, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.88706 
Train Epoch: 23 [341/1000 10912/32000 (34%)] Loss: 1.96046 (semantic_loss: 0.01299, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.30658 
Train Epoch: 23 [346/1000 11072/32000 (35%)] Loss: 1.96704 (semantic_loss: 0.01957, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20901 
Train Epoch: 23 [351/1000 11232/32000 (35%)] Loss: 1.96305 (semantic_loss: 0.01655, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18995 
Train Epoch: 23 [356/1000 11392/32000 (36%)] Loss: 1.96612 (semantic_loss: 0.02060, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18807 
Train Epoch: 23 [361/1000 11552/32000 (36%)] Loss: 1.96253 (semantic_loss: 0.01506, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18306 
Train Epoch: 23 [366/1000 11712/32000 (37%)] Loss: 1.96401 (semantic_loss: 0.01751, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18831 
Train Epoch: 23 [371/1000 11872/32000 (37%)] Loss: 1.96272 (semantic_loss: 0.01427, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18993 
Train Epoch: 23 [376/1000 12032/32000 (38%)] Loss: 1.96241 (semantic_loss: 0.01494, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18967 
Train Epoch: 23 [381/1000 12192/32000 (38%)] Loss: 1.95988 (semantic_loss: 0.01338, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19377 
Train Epoch: 23 [386/1000 12352/32000 (39%)] Loss: 1.96155 (semantic_loss: 0.01311, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18948 
Train Epoch: 23 [391/1000 12512/32000 (39%)] Loss: 1.96563 (semantic_loss: 0.01816, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21356 
Train Epoch: 23 [396/1000 12672/32000 (40%)] Loss: 1.96583 (semantic_loss: 0.01836, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19315 
Train Epoch: 23 [401/1000 12832/32000 (40%)] Loss: 1.96107 (semantic_loss: 0.01262, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18783 
Train Epoch: 23 [406/1000 12992/32000 (41%)] Loss: 1.96720 (semantic_loss: 0.01973, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19915 
Train Epoch: 23 [411/1000 13152/32000 (41%)] Loss: 1.96129 (semantic_loss: 0.01382, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18625 
Train Epoch: 23 [416/1000 13312/32000 (42%)] Loss: 1.96028 (semantic_loss: 0.01378, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21728 
Train Epoch: 23 [421/1000 13472/32000 (42%)] Loss: 1.96007 (semantic_loss: 0.01260, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18786 
Train Epoch: 23 [426/1000 13632/32000 (43%)] Loss: 1.96862 (semantic_loss: 0.02115, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18643 
Train Epoch: 23 [431/1000 13792/32000 (43%)] Loss: 1.96196 (semantic_loss: 0.01351, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18820 
Train Epoch: 23 [436/1000 13952/32000 (44%)] Loss: 1.96281 (semantic_loss: 0.01534, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18707 
Train Epoch: 23 [441/1000 14112/32000 (44%)] Loss: 1.96589 (semantic_loss: 0.01842, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18911 
Train Epoch: 23 [446/1000 14272/32000 (45%)] Loss: 1.96088 (semantic_loss: 0.01439, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18651 
Train Epoch: 23 [451/1000 14432/32000 (45%)] Loss: 1.96222 (semantic_loss: 0.01377, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.74621 
Train Epoch: 23 [456/1000 14592/32000 (46%)] Loss: 1.96252 (semantic_loss: 0.01603, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18856 
Train Epoch: 23 [461/1000 14752/32000 (46%)] Loss: 1.96238 (semantic_loss: 0.01588, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20028 
Train Epoch: 23 [466/1000 14912/32000 (47%)] Loss: 1.96120 (semantic_loss: 0.01568, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18829 
Train Epoch: 23 [471/1000 15072/32000 (47%)] Loss: 1.96214 (semantic_loss: 0.01564, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21876 
Train Epoch: 23 [476/1000 15232/32000 (48%)] Loss: 1.96293 (semantic_loss: 0.01741, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.23371 
Train Epoch: 23 [481/1000 15392/32000 (48%)] Loss: 1.96128 (semantic_loss: 0.01283, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22141 
Train Epoch: 23 [486/1000 15552/32000 (49%)] Loss: 1.96367 (semantic_loss: 0.01522, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20948 
Train Epoch: 23 [491/1000 15712/32000 (49%)] Loss: 1.96329 (semantic_loss: 0.01582, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21362 
Train Epoch: 23 [496/1000 15872/32000 (50%)] Loss: 1.96155 (semantic_loss: 0.01505, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20139 
Train Epoch: 23 [501/1000 16032/32000 (50%)] Loss: 1.96798 (semantic_loss: 0.02051, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19663 
Train Epoch: 23 [506/1000 16192/32000 (51%)] Loss: 1.96323 (semantic_loss: 0.01478, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20490 
Train Epoch: 23 [511/1000 16352/32000 (51%)] Loss: 1.96036 (semantic_loss: 0.01289, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18696 
Train Epoch: 23 [516/1000 16512/32000 (52%)] Loss: 1.96231 (semantic_loss: 0.01483, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21128 
Train Epoch: 23 [521/1000 16672/32000 (52%)] Loss: 1.96216 (semantic_loss: 0.01469, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18845 
Train Epoch: 23 [526/1000 16832/32000 (53%)] Loss: 1.96226 (semantic_loss: 0.01479, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18903 
Train Epoch: 23 [531/1000 16992/32000 (53%)] Loss: 1.96331 (semantic_loss: 0.01485, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18985 
Train Epoch: 23 [536/1000 17152/32000 (54%)] Loss: 1.96328 (semantic_loss: 0.01581, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19277 
Train Epoch: 23 [541/1000 17312/32000 (54%)] Loss: 1.96412 (semantic_loss: 0.01762, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19905 
Train Epoch: 23 [546/1000 17472/32000 (55%)] Loss: 1.96347 (semantic_loss: 0.01600, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19910 
Train Epoch: 23 [551/1000 17632/32000 (55%)] Loss: 1.96059 (semantic_loss: 0.01410, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18544 
Train Epoch: 23 [556/1000 17792/32000 (56%)] Loss: 1.95981 (semantic_loss: 0.01331, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21211 
Train Epoch: 23 [561/1000 17952/32000 (56%)] Loss: 1.96447 (semantic_loss: 0.01700, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18976 
Train Epoch: 23 [566/1000 18112/32000 (57%)] Loss: 1.96482 (semantic_loss: 0.01735, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19106 
Train Epoch: 23 [571/1000 18272/32000 (57%)] Loss: 1.95822 (semantic_loss: 0.01270, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18712 
Train Epoch: 23 [576/1000 18432/32000 (58%)] Loss: 1.96225 (semantic_loss: 0.01477, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18549 
Train Epoch: 23 [581/1000 18592/32000 (58%)] Loss: 1.96254 (semantic_loss: 0.01408, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18727 
Train Epoch: 23 [586/1000 18752/32000 (59%)] Loss: 1.96113 (semantic_loss: 0.01463, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19227 
Train Epoch: 23 [591/1000 18912/32000 (59%)] Loss: 1.96255 (semantic_loss: 0.01606, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18688 
Train Epoch: 23 [596/1000 19072/32000 (60%)] Loss: 1.97176 (semantic_loss: 0.02526, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18887 
Train Epoch: 23 [601/1000 19232/32000 (60%)] Loss: 1.96482 (semantic_loss: 0.01734, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18741 
Train Epoch: 23 [606/1000 19392/32000 (61%)] Loss: 1.96075 (semantic_loss: 0.01328, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19574 
Train Epoch: 23 [611/1000 19552/32000 (61%)] Loss: 1.96338 (semantic_loss: 0.01688, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18867 
Train Epoch: 23 [616/1000 19712/32000 (62%)] Loss: 1.96153 (semantic_loss: 0.01504, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19089 
Train Epoch: 23 [621/1000 19872/32000 (62%)] Loss: 1.96521 (semantic_loss: 0.01774, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19913 
Train Epoch: 23 [626/1000 20032/32000 (63%)] Loss: 1.96401 (semantic_loss: 0.01654, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22994 
Train Epoch: 23 [631/1000 20192/32000 (63%)] Loss: 1.96184 (semantic_loss: 0.01437, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21391 
Train Epoch: 23 [636/1000 20352/32000 (64%)] Loss: 1.96443 (semantic_loss: 0.01696, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19402 
Train Epoch: 23 [641/1000 20512/32000 (64%)] Loss: 1.96110 (semantic_loss: 0.01266, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21125 
Train Epoch: 23 [646/1000 20672/32000 (65%)] Loss: 1.96532 (semantic_loss: 0.01687, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19184 
Train Epoch: 23 [651/1000 20832/32000 (65%)] Loss: 1.95988 (semantic_loss: 0.01241, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19754 
Train Epoch: 23 [656/1000 20992/32000 (66%)] Loss: 1.96419 (semantic_loss: 0.01672, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.78332 
Train Epoch: 23 [661/1000 21152/32000 (66%)] Loss: 1.96106 (semantic_loss: 0.01358, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26083 
Train Epoch: 23 [666/1000 21312/32000 (67%)] Loss: 1.96073 (semantic_loss: 0.01326, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19001 
Train Epoch: 23 [671/1000 21472/32000 (67%)] Loss: 1.96260 (semantic_loss: 0.01610, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19574 
Train Epoch: 23 [676/1000 21632/32000 (68%)] Loss: 1.96161 (semantic_loss: 0.01414, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18860 
Train Epoch: 23 [681/1000 21792/32000 (68%)] Loss: 1.96424 (semantic_loss: 0.01579, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19392 
Train Epoch: 23 [686/1000 21952/32000 (69%)] Loss: 1.96351 (semantic_loss: 0.01604, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19890 
Train Epoch: 23 [691/1000 22112/32000 (69%)] Loss: 1.96039 (semantic_loss: 0.01390, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19377 
Train Epoch: 23 [696/1000 22272/32000 (70%)] Loss: 1.96036 (semantic_loss: 0.01387, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18969 
Train Epoch: 23 [701/1000 22432/32000 (70%)] Loss: 1.95999 (semantic_loss: 0.01349, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18482 
Train Epoch: 23 [706/1000 22592/32000 (71%)] Loss: 1.96334 (semantic_loss: 0.01684, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18980 
Train Epoch: 23 [711/1000 22752/32000 (71%)] Loss: 1.96650 (semantic_loss: 0.02001, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18954 
Train Epoch: 23 [716/1000 22912/32000 (72%)] Loss: 1.96127 (semantic_loss: 0.01380, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19193 
Train Epoch: 23 [721/1000 23072/32000 (72%)] Loss: 1.96041 (semantic_loss: 0.01294, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18412 
Train Epoch: 23 [726/1000 23232/32000 (73%)] Loss: 1.96204 (semantic_loss: 0.01457, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18297 
Train Epoch: 23 [731/1000 23392/32000 (73%)] Loss: 1.96220 (semantic_loss: 0.01376, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21141 
Train Epoch: 23 [736/1000 23552/32000 (74%)] Loss: 1.96091 (semantic_loss: 0.01344, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20997 
Train Epoch: 23 [741/1000 23712/32000 (74%)] Loss: 1.96114 (semantic_loss: 0.01464, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18676 
Train Epoch: 23 [746/1000 23872/32000 (75%)] Loss: 1.96427 (semantic_loss: 0.01582, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18623 
Train Epoch: 23 [751/1000 24032/32000 (75%)] Loss: 1.96073 (semantic_loss: 0.01424, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22194 
Train Epoch: 23 [756/1000 24192/32000 (76%)] Loss: 1.96236 (semantic_loss: 0.01391, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18727 
Train Epoch: 23 [761/1000 24352/32000 (76%)] Loss: 1.96217 (semantic_loss: 0.01568, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18908 
Train Epoch: 23 [766/1000 24512/32000 (77%)] Loss: 1.96309 (semantic_loss: 0.01562, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18723 
Train Epoch: 23 [771/1000 24672/32000 (77%)] Loss: 1.96162 (semantic_loss: 0.01415, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.67961 
Train Epoch: 23 [776/1000 24832/32000 (78%)] Loss: 1.96185 (semantic_loss: 0.01340, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20503 
Train Epoch: 23 [781/1000 24992/32000 (78%)] Loss: 1.96274 (semantic_loss: 0.01526, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20939 
Train Epoch: 23 [786/1000 25152/32000 (79%)] Loss: 1.96828 (semantic_loss: 0.02080, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20368 
Train Epoch: 23 [791/1000 25312/32000 (79%)] Loss: 1.96227 (semantic_loss: 0.01382, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21491 
Train Epoch: 23 [796/1000 25472/32000 (80%)] Loss: 1.96417 (semantic_loss: 0.01573, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21001 
Train Epoch: 23 [801/1000 25632/32000 (80%)] Loss: 1.96247 (semantic_loss: 0.01499, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19675 
Train Epoch: 23 [806/1000 25792/32000 (81%)] Loss: 1.96044 (semantic_loss: 0.01297, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21178 
Train Epoch: 23 [811/1000 25952/32000 (81%)] Loss: 1.96318 (semantic_loss: 0.01668, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19436 
Train Epoch: 23 [816/1000 26112/32000 (82%)] Loss: 1.96096 (semantic_loss: 0.01446, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18431 
Train Epoch: 23 [821/1000 26272/32000 (82%)] Loss: 1.96131 (semantic_loss: 0.01481, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18903 
Train Epoch: 23 [826/1000 26432/32000 (83%)] Loss: 1.96062 (semantic_loss: 0.01315, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18574 
Train Epoch: 23 [831/1000 26592/32000 (83%)] Loss: 1.96220 (semantic_loss: 0.01473, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18701 
Train Epoch: 23 [836/1000 26752/32000 (84%)] Loss: 1.96339 (semantic_loss: 0.01592, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18849 
Train Epoch: 23 [841/1000 26912/32000 (84%)] Loss: 1.96413 (semantic_loss: 0.01666, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19468 
Train Epoch: 23 [846/1000 27072/32000 (85%)] Loss: 1.96272 (semantic_loss: 0.01525, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19943 
Train Epoch: 23 [851/1000 27232/32000 (85%)] Loss: 1.95978 (semantic_loss: 0.01329, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19051 
Train Epoch: 23 [856/1000 27392/32000 (86%)] Loss: 1.96220 (semantic_loss: 0.01474, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18667 
Train Epoch: 23 [861/1000 27552/32000 (86%)] Loss: 1.96312 (semantic_loss: 0.01663, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19668 
Train Epoch: 23 [866/1000 27712/32000 (87%)] Loss: 1.95959 (semantic_loss: 0.01407, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18903 
Train Epoch: 23 [871/1000 27872/32000 (87%)] Loss: 1.95890 (semantic_loss: 0.01240, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18653 
Train Epoch: 23 [876/1000 28032/32000 (88%)] Loss: 1.96245 (semantic_loss: 0.01401, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19110 
Train Epoch: 23 [881/1000 28192/32000 (88%)] Loss: 1.96024 (semantic_loss: 0.01277, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18877 
Train Epoch: 23 [886/1000 28352/32000 (89%)] Loss: 1.96218 (semantic_loss: 0.01373, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18967 
Train Epoch: 23 [891/1000 28512/32000 (89%)] Loss: 1.96435 (semantic_loss: 0.01687, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18452 
Train Epoch: 23 [896/1000 28672/32000 (90%)] Loss: 1.95991 (semantic_loss: 0.01341, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18323 
Train Epoch: 23 [901/1000 28832/32000 (90%)] Loss: 1.96082 (semantic_loss: 0.01335, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18241 
Train Epoch: 23 [906/1000 28992/32000 (91%)] Loss: 1.95852 (semantic_loss: 0.01203, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19289 
Train Epoch: 23 [911/1000 29152/32000 (91%)] Loss: 1.96391 (semantic_loss: 0.01644, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18904 
Train Epoch: 23 [916/1000 29312/32000 (92%)] Loss: 1.96315 (semantic_loss: 0.01567, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18480 
Train Epoch: 23 [921/1000 29472/32000 (92%)] Loss: 1.96372 (semantic_loss: 0.01624, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18697 
Train Epoch: 23 [926/1000 29632/32000 (93%)] Loss: 1.96291 (semantic_loss: 0.01544, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18709 
Train Epoch: 23 [931/1000 29792/32000 (93%)] Loss: 1.96232 (semantic_loss: 0.01387, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18785 
Train Epoch: 23 [936/1000 29952/32000 (94%)] Loss: 1.96686 (semantic_loss: 0.01744, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.20716 
Train Epoch: 23 [941/1000 30112/32000 (94%)] Loss: 1.96162 (semantic_loss: 0.01415, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21262 
Train Epoch: 23 [946/1000 30272/32000 (95%)] Loss: 1.96494 (semantic_loss: 0.01649, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20390 
Train Epoch: 23 [951/1000 30432/32000 (95%)] Loss: 1.96278 (semantic_loss: 0.01433, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19689 
Train Epoch: 23 [956/1000 30592/32000 (96%)] Loss: 1.96213 (semantic_loss: 0.01467, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19456 
Train Epoch: 23 [961/1000 30752/32000 (96%)] Loss: 1.96454 (semantic_loss: 0.01610, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21455 
Train Epoch: 23 [966/1000 30912/32000 (97%)] Loss: 1.96434 (semantic_loss: 0.01687, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19273 
Train Epoch: 23 [971/1000 31072/32000 (97%)] Loss: 1.96480 (semantic_loss: 0.01830, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19718 
Train Epoch: 23 [976/1000 31232/32000 (98%)] Loss: 1.96471 (semantic_loss: 0.01626, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.81057 
Train Epoch: 23 [981/1000 31392/32000 (98%)] Loss: 1.95984 (semantic_loss: 0.01334, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.26059 
Train Epoch: 23 [986/1000 31552/32000 (99%)] Loss: 1.96592 (semantic_loss: 0.01748, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18595 
Train Epoch: 23 [991/1000 31712/32000 (99%)] Loss: 1.96101 (semantic_loss: 0.01451, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21154 
Train Epoch: 23 [996/1000 31872/32000 (100%)] Loss: 1.96344 (semantic_loss: 0.01694, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18756 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch23.pth ...
Done in 7.769s
removing stale ckpt [epoch 22] [took 0.04s]
 epoch          : 23
 loss           : 1.9627994252443313
 learning_rate  : 4.92385451091806e-06
 n_samples      : 736000
 n_steps        : 23000
 ActivityNet_val1_test/t2v_metrics/R1: 11.73479764083791
 ActivityNet_val1_test/t2v_metrics/R5: 37.604230221679884
 ActivityNet_val1_test/t2v_metrics/R10: 54.58612975391499
 ActivityNet_val1_test/t2v_metrics/R50: 85.58063860077283
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 56.74344112263575
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 28.880067479559816
 ActivityNet_val1_test/v2t_metrics/R1: 12.34492576774456
 ActivityNet_val1_test/v2t_metrics/R5: 38.13300793166565
 ActivityNet_val1_test/v2t_metrics/R10: 55.297945901972746
 ActivityNet_val1_test/v2t_metrics/R50: 85.29591214154972
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 58.88336383973968
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.63690503958444
 mnt_best       : 28.911535532251342
 not_improved_count: 3
Train Epoch: 24 [1/1000 32/32000 (0%)] Loss: 1.96584 (semantic_loss: 0.01837, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=24.17776 
Train Epoch: 24 [6/1000 192/32000 (1%)] Loss: 1.96260 (semantic_loss: 0.01513, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18398 
Train Epoch: 24 [11/1000 352/32000 (1%)] Loss: 1.96215 (semantic_loss: 0.01565, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20044 
Train Epoch: 24 [16/1000 512/32000 (2%)] Loss: 1.96380 (semantic_loss: 0.01536, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21519 
Train Epoch: 24 [21/1000 672/32000 (2%)] Loss: 1.96463 (semantic_loss: 0.01716, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18878 
Train Epoch: 24 [26/1000 832/32000 (3%)] Loss: 1.96373 (semantic_loss: 0.01723, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19017 
Train Epoch: 24 [31/1000 992/32000 (3%)] Loss: 1.96363 (semantic_loss: 0.01714, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18808 
Train Epoch: 24 [36/1000 1152/32000 (4%)] Loss: 1.96549 (semantic_loss: 0.01704, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21849 
Train Epoch: 24 [41/1000 1312/32000 (4%)] Loss: 1.96254 (semantic_loss: 0.01409, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20964 
Train Epoch: 24 [46/1000 1472/32000 (5%)] Loss: 1.96442 (semantic_loss: 0.01792, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21029 
Train Epoch: 24 [51/1000 1632/32000 (5%)] Loss: 1.96909 (semantic_loss: 0.02162, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21602 
Train Epoch: 24 [56/1000 1792/32000 (6%)] Loss: 1.96171 (semantic_loss: 0.01425, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19562 
Train Epoch: 24 [61/1000 1952/32000 (6%)] Loss: 1.96225 (semantic_loss: 0.01478, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19906 
Train Epoch: 24 [66/1000 2112/32000 (7%)] Loss: 1.96435 (semantic_loss: 0.01688, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25239 
Train Epoch: 24 [71/1000 2272/32000 (7%)] Loss: 1.96169 (semantic_loss: 0.01519, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20329 
Train Epoch: 24 [76/1000 2432/32000 (8%)] Loss: 1.96625 (semantic_loss: 0.01878, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19462 
Train Epoch: 24 [81/1000 2592/32000 (8%)] Loss: 1.96352 (semantic_loss: 0.01605, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.36777 
Train Epoch: 24 [86/1000 2752/32000 (9%)] Loss: 1.96168 (semantic_loss: 0.01421, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18642 
Train Epoch: 24 [91/1000 2912/32000 (9%)] Loss: 1.95990 (semantic_loss: 0.01341, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19137 
Train Epoch: 24 [96/1000 3072/32000 (10%)] Loss: 1.96325 (semantic_loss: 0.01676, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18695 
Train Epoch: 24 [101/1000 3232/32000 (10%)] Loss: 1.96171 (semantic_loss: 0.01424, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19058 
Train Epoch: 24 [106/1000 3392/32000 (11%)] Loss: 1.96261 (semantic_loss: 0.01513, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19590 
Train Epoch: 24 [111/1000 3552/32000 (11%)] Loss: 1.96123 (semantic_loss: 0.01473, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19742 
Train Epoch: 24 [116/1000 3712/32000 (12%)] Loss: 1.96353 (semantic_loss: 0.01702, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18871 
Train Epoch: 24 [121/1000 3872/32000 (12%)] Loss: 1.96590 (semantic_loss: 0.01940, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18746 
Train Epoch: 24 [126/1000 4032/32000 (13%)] Loss: 1.96645 (semantic_loss: 0.01800, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18722 
Train Epoch: 24 [131/1000 4192/32000 (13%)] Loss: 1.96235 (semantic_loss: 0.01488, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20501 
Train Epoch: 24 [136/1000 4352/32000 (14%)] Loss: 1.96216 (semantic_loss: 0.01371, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19040 
Train Epoch: 24 [141/1000 4512/32000 (14%)] Loss: 1.96501 (semantic_loss: 0.01754, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18569 
Train Epoch: 24 [146/1000 4672/32000 (15%)] Loss: 1.96632 (semantic_loss: 0.01788, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19042 
Train Epoch: 24 [151/1000 4832/32000 (15%)] Loss: 1.96681 (semantic_loss: 0.01935, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.45319 
Train Epoch: 24 [156/1000 4992/32000 (16%)] Loss: 1.96258 (semantic_loss: 0.01608, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18800 
Train Epoch: 24 [161/1000 5152/32000 (16%)] Loss: 1.96414 (semantic_loss: 0.01667, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18855 
Train Epoch: 24 [166/1000 5312/32000 (17%)] Loss: 1.96454 (semantic_loss: 0.01706, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18735 
Train Epoch: 24 [171/1000 5472/32000 (17%)] Loss: 1.96379 (semantic_loss: 0.01632, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18769 
Train Epoch: 24 [176/1000 5632/32000 (18%)] Loss: 1.96234 (semantic_loss: 0.01487, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19108 
Train Epoch: 24 [181/1000 5792/32000 (18%)] Loss: 1.96001 (semantic_loss: 0.01352, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19058 
Train Epoch: 24 [186/1000 5952/32000 (19%)] Loss: 1.96365 (semantic_loss: 0.01618, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18404 
Train Epoch: 24 [191/1000 6112/32000 (19%)] Loss: 1.96173 (semantic_loss: 0.01426, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20511 
Train Epoch: 24 [196/1000 6272/32000 (20%)] Loss: 1.96131 (semantic_loss: 0.01481, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22541 
Train Epoch: 24 [201/1000 6432/32000 (20%)] Loss: 1.96114 (semantic_loss: 0.01465, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.31804 
Train Epoch: 24 [206/1000 6592/32000 (21%)] Loss: 1.96224 (semantic_loss: 0.01477, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23010 
Train Epoch: 24 [211/1000 6752/32000 (21%)] Loss: 1.96264 (semantic_loss: 0.01614, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22731 
Train Epoch: 24 [216/1000 6912/32000 (22%)] Loss: 1.96354 (semantic_loss: 0.01607, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22256 
Train Epoch: 24 [221/1000 7072/32000 (22%)] Loss: 1.96686 (semantic_loss: 0.02036, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22904 
Train Epoch: 24 [226/1000 7232/32000 (23%)] Loss: 1.96638 (semantic_loss: 0.01891, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19725 
Train Epoch: 24 [231/1000 7392/32000 (23%)] Loss: 1.96086 (semantic_loss: 0.01339, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19140 
Train Epoch: 24 [236/1000 7552/32000 (24%)] Loss: 1.96403 (semantic_loss: 0.01558, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21710 
Train Epoch: 24 [241/1000 7712/32000 (24%)] Loss: 1.96345 (semantic_loss: 0.01501, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21013 
Train Epoch: 24 [246/1000 7872/32000 (25%)] Loss: 1.96940 (semantic_loss: 0.02095, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18881 
Train Epoch: 24 [251/1000 8032/32000 (25%)] Loss: 1.95953 (semantic_loss: 0.01304, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18943 
Train Epoch: 24 [256/1000 8192/32000 (26%)] Loss: 1.96025 (semantic_loss: 0.01278, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18842 
Train Epoch: 24 [261/1000 8352/32000 (26%)] Loss: 1.95774 (semantic_loss: 0.01222, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.21743 
Train Epoch: 24 [266/1000 8512/32000 (27%)] Loss: 1.96196 (semantic_loss: 0.01449, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21591 
Train Epoch: 24 [271/1000 8672/32000 (27%)] Loss: 1.96563 (semantic_loss: 0.01816, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20082 
Train Epoch: 24 [276/1000 8832/32000 (28%)] Loss: 1.96654 (semantic_loss: 0.01809, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.59435 
Train Epoch: 24 [281/1000 8992/32000 (28%)] Loss: 1.96776 (semantic_loss: 0.02029, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18847 
Train Epoch: 24 [286/1000 9152/32000 (29%)] Loss: 1.96029 (semantic_loss: 0.01379, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18827 
Train Epoch: 24 [291/1000 9312/32000 (29%)] Loss: 1.96142 (semantic_loss: 0.01492, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18776 
Train Epoch: 24 [296/1000 9472/32000 (30%)] Loss: 1.96148 (semantic_loss: 0.01499, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.28136 
Train Epoch: 24 [301/1000 9632/32000 (30%)] Loss: 1.96034 (semantic_loss: 0.01481, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18499 
Train Epoch: 24 [306/1000 9792/32000 (31%)] Loss: 1.96595 (semantic_loss: 0.01751, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18949 
Train Epoch: 24 [311/1000 9952/32000 (31%)] Loss: 1.96517 (semantic_loss: 0.01770, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19197 
Train Epoch: 24 [316/1000 10112/32000 (32%)] Loss: 1.96124 (semantic_loss: 0.01377, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20572 
Train Epoch: 24 [321/1000 10272/32000 (32%)] Loss: 1.96052 (semantic_loss: 0.01500, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.21023 
Train Epoch: 24 [326/1000 10432/32000 (33%)] Loss: 1.96134 (semantic_loss: 0.01485, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18702 
Train Epoch: 24 [331/1000 10592/32000 (33%)] Loss: 1.96327 (semantic_loss: 0.01580, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22769 
Train Epoch: 24 [336/1000 10752/32000 (34%)] Loss: 1.95905 (semantic_loss: 0.01256, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20615 
Train Epoch: 24 [341/1000 10912/32000 (34%)] Loss: 1.96459 (semantic_loss: 0.01810, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18839 
Train Epoch: 24 [346/1000 11072/32000 (35%)] Loss: 1.96007 (semantic_loss: 0.01259, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18640 
Train Epoch: 24 [351/1000 11232/32000 (35%)] Loss: 1.95976 (semantic_loss: 0.01326, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19189 
Train Epoch: 24 [356/1000 11392/32000 (36%)] Loss: 1.96219 (semantic_loss: 0.01471, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20903 
Train Epoch: 24 [361/1000 11552/32000 (36%)] Loss: 1.96299 (semantic_loss: 0.01455, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23137 
Train Epoch: 24 [366/1000 11712/32000 (37%)] Loss: 1.96607 (semantic_loss: 0.01957, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22442 
Train Epoch: 24 [371/1000 11872/32000 (37%)] Loss: 1.96573 (semantic_loss: 0.01826, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20342 
Train Epoch: 24 [376/1000 12032/32000 (38%)] Loss: 1.96179 (semantic_loss: 0.01627, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19537 
Train Epoch: 24 [381/1000 12192/32000 (38%)] Loss: 1.96511 (semantic_loss: 0.01764, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19782 
Train Epoch: 24 [386/1000 12352/32000 (39%)] Loss: 1.96402 (semantic_loss: 0.01752, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.26746 
Train Epoch: 24 [391/1000 12512/32000 (39%)] Loss: 1.96197 (semantic_loss: 0.01547, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19877 
Train Epoch: 24 [396/1000 12672/32000 (40%)] Loss: 1.96508 (semantic_loss: 0.01760, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19609 
Train Epoch: 24 [401/1000 12832/32000 (40%)] Loss: 1.96610 (semantic_loss: 0.01765, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.37559 
Train Epoch: 24 [406/1000 12992/32000 (41%)] Loss: 1.96490 (semantic_loss: 0.01840, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18709 
Train Epoch: 24 [411/1000 13152/32000 (41%)] Loss: 1.96108 (semantic_loss: 0.01556, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19163 
Train Epoch: 24 [416/1000 13312/32000 (42%)] Loss: 1.96067 (semantic_loss: 0.01320, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19694 
Train Epoch: 24 [421/1000 13472/32000 (42%)] Loss: 1.96116 (semantic_loss: 0.01467, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19005 
Train Epoch: 24 [426/1000 13632/32000 (43%)] Loss: 1.96132 (semantic_loss: 0.01384, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20041 
Train Epoch: 24 [431/1000 13792/32000 (43%)] Loss: 1.96462 (semantic_loss: 0.01715, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19323 
Train Epoch: 24 [436/1000 13952/32000 (44%)] Loss: 1.96252 (semantic_loss: 0.01602, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18907 
Train Epoch: 24 [441/1000 14112/32000 (44%)] Loss: 1.96218 (semantic_loss: 0.01471, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18848 
Train Epoch: 24 [446/1000 14272/32000 (45%)] Loss: 1.96102 (semantic_loss: 0.01452, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18910 
Train Epoch: 24 [451/1000 14432/32000 (45%)] Loss: 1.96571 (semantic_loss: 0.01921, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20476 
Train Epoch: 24 [456/1000 14592/32000 (46%)] Loss: 1.96084 (semantic_loss: 0.01434, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18872 
Train Epoch: 24 [461/1000 14752/32000 (46%)] Loss: 1.96171 (semantic_loss: 0.01521, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18596 
Train Epoch: 24 [466/1000 14912/32000 (47%)] Loss: 1.96520 (semantic_loss: 0.01871, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18797 
Train Epoch: 24 [471/1000 15072/32000 (47%)] Loss: 1.96514 (semantic_loss: 0.01864, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.46515 
Train Epoch: 24 [476/1000 15232/32000 (48%)] Loss: 1.96437 (semantic_loss: 0.01592, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19496 
Train Epoch: 24 [481/1000 15392/32000 (48%)] Loss: 1.96501 (semantic_loss: 0.01656, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20911 
Train Epoch: 24 [486/1000 15552/32000 (49%)] Loss: 1.96353 (semantic_loss: 0.01703, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18898 
Train Epoch: 24 [491/1000 15712/32000 (49%)] Loss: 1.96481 (semantic_loss: 0.01734, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19400 
Train Epoch: 24 [496/1000 15872/32000 (50%)] Loss: 1.96206 (semantic_loss: 0.01458, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18675 
Train Epoch: 24 [501/1000 16032/32000 (50%)] Loss: 1.96046 (semantic_loss: 0.01397, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18972 
Train Epoch: 24 [506/1000 16192/32000 (51%)] Loss: 1.96909 (semantic_loss: 0.02161, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21334 
Train Epoch: 24 [511/1000 16352/32000 (51%)] Loss: 1.96002 (semantic_loss: 0.01255, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.25183 
Train Epoch: 24 [516/1000 16512/32000 (52%)] Loss: 1.96219 (semantic_loss: 0.01472, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26387 
Train Epoch: 24 [521/1000 16672/32000 (52%)] Loss: 1.96110 (semantic_loss: 0.01559, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.34607 
Train Epoch: 24 [526/1000 16832/32000 (53%)] Loss: 1.96262 (semantic_loss: 0.01417, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21704 
Train Epoch: 24 [531/1000 16992/32000 (53%)] Loss: 1.96427 (semantic_loss: 0.01777, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20140 
Train Epoch: 24 [536/1000 17152/32000 (54%)] Loss: 1.96137 (semantic_loss: 0.01390, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20602 
Train Epoch: 24 [541/1000 17312/32000 (54%)] Loss: 1.96414 (semantic_loss: 0.01764, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20345 
Train Epoch: 24 [546/1000 17472/32000 (55%)] Loss: 1.96278 (semantic_loss: 0.01629, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18923 
Train Epoch: 24 [551/1000 17632/32000 (55%)] Loss: 1.95967 (semantic_loss: 0.01317, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18668 
Train Epoch: 24 [556/1000 17792/32000 (56%)] Loss: 1.96035 (semantic_loss: 0.01386, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21593 
Train Epoch: 24 [561/1000 17952/32000 (56%)] Loss: 1.96818 (semantic_loss: 0.01973, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18601 
Train Epoch: 24 [566/1000 18112/32000 (57%)] Loss: 1.96194 (semantic_loss: 0.01446, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18959 
Train Epoch: 24 [571/1000 18272/32000 (57%)] Loss: 1.96309 (semantic_loss: 0.01561, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19804 
Train Epoch: 24 [576/1000 18432/32000 (58%)] Loss: 1.96444 (semantic_loss: 0.01696, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19925 
Train Epoch: 24 [581/1000 18592/32000 (58%)] Loss: 1.96300 (semantic_loss: 0.01650, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21189 
Train Epoch: 24 [586/1000 18752/32000 (59%)] Loss: 1.96541 (semantic_loss: 0.01696, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20100 
Train Epoch: 24 [591/1000 18912/32000 (59%)] Loss: 1.96163 (semantic_loss: 0.01416, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18609 
Train Epoch: 24 [596/1000 19072/32000 (60%)] Loss: 1.96335 (semantic_loss: 0.01491, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.61012 
Train Epoch: 24 [601/1000 19232/32000 (60%)] Loss: 1.96482 (semantic_loss: 0.01735, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18715 
Train Epoch: 24 [606/1000 19392/32000 (61%)] Loss: 1.96093 (semantic_loss: 0.01443, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18689 
Train Epoch: 24 [611/1000 19552/32000 (61%)] Loss: 1.96266 (semantic_loss: 0.01617, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18844 
Train Epoch: 24 [616/1000 19712/32000 (62%)] Loss: 1.96188 (semantic_loss: 0.01441, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.27785 
Train Epoch: 24 [621/1000 19872/32000 (62%)] Loss: 1.96393 (semantic_loss: 0.01646, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18571 
Train Epoch: 24 [626/1000 20032/32000 (63%)] Loss: 1.96596 (semantic_loss: 0.01849, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18522 
Train Epoch: 24 [631/1000 20192/32000 (63%)] Loss: 1.96452 (semantic_loss: 0.01705, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19046 
Train Epoch: 24 [636/1000 20352/32000 (64%)] Loss: 1.96301 (semantic_loss: 0.01651, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18666 
Train Epoch: 24 [641/1000 20512/32000 (64%)] Loss: 1.96146 (semantic_loss: 0.01399, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18666 
Train Epoch: 24 [646/1000 20672/32000 (65%)] Loss: 1.96870 (semantic_loss: 0.02123, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18705 
Train Epoch: 24 [651/1000 20832/32000 (65%)] Loss: 1.96173 (semantic_loss: 0.01426, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20342 
Train Epoch: 24 [656/1000 20992/32000 (66%)] Loss: 1.95881 (semantic_loss: 0.01231, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20316 
Train Epoch: 24 [661/1000 21152/32000 (66%)] Loss: 1.95966 (semantic_loss: 0.01219, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21676 
Train Epoch: 24 [666/1000 21312/32000 (67%)] Loss: 1.96363 (semantic_loss: 0.01616, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22111 
Train Epoch: 24 [671/1000 21472/32000 (67%)] Loss: 1.96287 (semantic_loss: 0.01539, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22355 
Train Epoch: 24 [676/1000 21632/32000 (68%)] Loss: 1.96291 (semantic_loss: 0.01642, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21522 
Train Epoch: 24 [681/1000 21792/32000 (68%)] Loss: 1.96398 (semantic_loss: 0.01651, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20625 
Train Epoch: 24 [686/1000 21952/32000 (69%)] Loss: 1.96289 (semantic_loss: 0.01444, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20711 
Train Epoch: 24 [691/1000 22112/32000 (69%)] Loss: 1.95982 (semantic_loss: 0.01235, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20707 
Train Epoch: 24 [696/1000 22272/32000 (70%)] Loss: 1.96212 (semantic_loss: 0.01465, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18932 
Train Epoch: 24 [701/1000 22432/32000 (70%)] Loss: 1.96089 (semantic_loss: 0.01342, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18595 
Train Epoch: 24 [706/1000 22592/32000 (71%)] Loss: 1.96299 (semantic_loss: 0.01649, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23830 
Train Epoch: 24 [711/1000 22752/32000 (71%)] Loss: 1.95981 (semantic_loss: 0.01331, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18782 
Train Epoch: 24 [716/1000 22912/32000 (72%)] Loss: 1.96270 (semantic_loss: 0.01523, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18745 
Train Epoch: 24 [721/1000 23072/32000 (72%)] Loss: 1.96070 (semantic_loss: 0.01322, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.39106 
Train Epoch: 24 [726/1000 23232/32000 (73%)] Loss: 1.96174 (semantic_loss: 0.01524, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19242 
Train Epoch: 24 [731/1000 23392/32000 (73%)] Loss: 1.96119 (semantic_loss: 0.01372, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19098 
Train Epoch: 24 [736/1000 23552/32000 (74%)] Loss: 1.96179 (semantic_loss: 0.01530, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19125 
Train Epoch: 24 [741/1000 23712/32000 (74%)] Loss: 1.96380 (semantic_loss: 0.01633, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19797 
Train Epoch: 24 [746/1000 23872/32000 (75%)] Loss: 1.96870 (semantic_loss: 0.02123, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19049 
Train Epoch: 24 [751/1000 24032/32000 (75%)] Loss: 1.96821 (semantic_loss: 0.01977, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19669 
Train Epoch: 24 [756/1000 24192/32000 (76%)] Loss: 1.96529 (semantic_loss: 0.01684, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19097 
Train Epoch: 24 [761/1000 24352/32000 (76%)] Loss: 1.95981 (semantic_loss: 0.01332, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18940 
Train Epoch: 24 [766/1000 24512/32000 (77%)] Loss: 1.96218 (semantic_loss: 0.01471, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18729 
Train Epoch: 24 [771/1000 24672/32000 (77%)] Loss: 1.96424 (semantic_loss: 0.01775, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20657 
Train Epoch: 24 [776/1000 24832/32000 (78%)] Loss: 1.96354 (semantic_loss: 0.01704, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18789 
Train Epoch: 24 [781/1000 24992/32000 (78%)] Loss: 1.96322 (semantic_loss: 0.01575, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18712 
Train Epoch: 24 [786/1000 25152/32000 (79%)] Loss: 1.96257 (semantic_loss: 0.01608, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18799 
Train Epoch: 24 [791/1000 25312/32000 (79%)] Loss: 1.96211 (semantic_loss: 0.01464, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.46475 
Train Epoch: 24 [796/1000 25472/32000 (80%)] Loss: 1.96208 (semantic_loss: 0.01364, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19667 
Train Epoch: 24 [801/1000 25632/32000 (80%)] Loss: 1.96358 (semantic_loss: 0.01611, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18581 
Train Epoch: 24 [806/1000 25792/32000 (81%)] Loss: 1.96509 (semantic_loss: 0.01665, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18784 
Train Epoch: 24 [811/1000 25952/32000 (81%)] Loss: 1.96227 (semantic_loss: 0.01577, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18907 
Train Epoch: 24 [816/1000 26112/32000 (82%)] Loss: 1.96334 (semantic_loss: 0.01489, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18483 
Train Epoch: 24 [821/1000 26272/32000 (82%)] Loss: 1.96419 (semantic_loss: 0.01672, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21493 
Train Epoch: 24 [826/1000 26432/32000 (83%)] Loss: 1.96096 (semantic_loss: 0.01251, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21256 
Train Epoch: 24 [831/1000 26592/32000 (83%)] Loss: 1.96263 (semantic_loss: 0.01516, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24691 
Train Epoch: 24 [836/1000 26752/32000 (84%)] Loss: 1.95977 (semantic_loss: 0.01230, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.26095 
Train Epoch: 24 [841/1000 26912/32000 (84%)] Loss: 1.96144 (semantic_loss: 0.01299, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.30823 
Train Epoch: 24 [846/1000 27072/32000 (85%)] Loss: 1.96582 (semantic_loss: 0.01835, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21008 
Train Epoch: 24 [851/1000 27232/32000 (85%)] Loss: 1.96241 (semantic_loss: 0.01592, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19204 
Train Epoch: 24 [856/1000 27392/32000 (86%)] Loss: 1.96106 (semantic_loss: 0.01359, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21827 
Train Epoch: 24 [861/1000 27552/32000 (86%)] Loss: 1.96171 (semantic_loss: 0.01521, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19048 
Train Epoch: 24 [866/1000 27712/32000 (87%)] Loss: 1.96383 (semantic_loss: 0.01733, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18816 
Train Epoch: 24 [871/1000 27872/32000 (87%)] Loss: 1.96321 (semantic_loss: 0.01672, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19763 
Train Epoch: 24 [876/1000 28032/32000 (88%)] Loss: 1.96120 (semantic_loss: 0.01373, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23221 
Train Epoch: 24 [881/1000 28192/32000 (88%)] Loss: 1.96497 (semantic_loss: 0.01652, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18889 
Train Epoch: 24 [886/1000 28352/32000 (89%)] Loss: 1.96422 (semantic_loss: 0.01479, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.19910 
Train Epoch: 24 [891/1000 28512/32000 (89%)] Loss: 1.96073 (semantic_loss: 0.01521, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19110 
Train Epoch: 24 [896/1000 28672/32000 (90%)] Loss: 1.96684 (semantic_loss: 0.01840, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18854 
Train Epoch: 24 [901/1000 28832/32000 (90%)] Loss: 1.96234 (semantic_loss: 0.01584, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21149 
Train Epoch: 24 [906/1000 28992/32000 (91%)] Loss: 1.96070 (semantic_loss: 0.01518, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.20463 
Train Epoch: 24 [911/1000 29152/32000 (91%)] Loss: 1.96073 (semantic_loss: 0.01228, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18353 
Train Epoch: 24 [916/1000 29312/32000 (92%)] Loss: 1.96289 (semantic_loss: 0.01542, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.56607 
Train Epoch: 24 [921/1000 29472/32000 (92%)] Loss: 1.96231 (semantic_loss: 0.01485, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18401 
Train Epoch: 24 [926/1000 29632/32000 (93%)] Loss: 1.96139 (semantic_loss: 0.01392, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18661 
Train Epoch: 24 [931/1000 29792/32000 (93%)] Loss: 1.95807 (semantic_loss: 0.01158, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18565 
Train Epoch: 24 [936/1000 29952/32000 (94%)] Loss: 1.96131 (semantic_loss: 0.01384, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.28208 
Train Epoch: 24 [941/1000 30112/32000 (94%)] Loss: 1.96455 (semantic_loss: 0.01708, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18734 
Train Epoch: 24 [946/1000 30272/32000 (95%)] Loss: 1.95972 (semantic_loss: 0.01323, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18361 
Train Epoch: 24 [951/1000 30432/32000 (95%)] Loss: 1.96791 (semantic_loss: 0.02044, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18693 
Train Epoch: 24 [956/1000 30592/32000 (96%)] Loss: 1.96397 (semantic_loss: 0.01650, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18702 
Train Epoch: 24 [961/1000 30752/32000 (96%)] Loss: 1.96347 (semantic_loss: 0.01599, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19038 
Train Epoch: 24 [966/1000 30912/32000 (97%)] Loss: 1.96291 (semantic_loss: 0.01641, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20134 
Train Epoch: 24 [971/1000 31072/32000 (97%)] Loss: 1.96061 (semantic_loss: 0.01314, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20710 
Train Epoch: 24 [976/1000 31232/32000 (98%)] Loss: 1.96436 (semantic_loss: 0.01787, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20317 
Train Epoch: 24 [981/1000 31392/32000 (98%)] Loss: 1.96011 (semantic_loss: 0.01362, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20916 
Train Epoch: 24 [986/1000 31552/32000 (99%)] Loss: 1.96287 (semantic_loss: 0.01638, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21263 
Train Epoch: 24 [991/1000 31712/32000 (99%)] Loss: 1.96344 (semantic_loss: 0.01695, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22498 
Train Epoch: 24 [996/1000 31872/32000 (100%)] Loss: 1.96271 (semantic_loss: 0.01523, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20069 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch24.pth ...
Done in 3.819s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch24.pth ...
Done in 7.641s
removing stale ckpt [epoch 23] [took 0.00s]
 epoch          : 24
 loss           : 1.9627936695814132
 learning_rate  : 4.431469059826254e-06
 n_samples      : 768000
 n_steps        : 24000
 ActivityNet_val1_test/t2v_metrics/R1: 12.24323774659345
 ActivityNet_val1_test/t2v_metrics/R5: 37.848281472442544
 ActivityNet_val1_test/t2v_metrics/R10: 55.39963392312386
 ActivityNet_val1_test/t2v_metrics/R50: 85.41793776693106
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 57.07453731950376
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.49962176238898
 ActivityNet_val1_test/v2t_metrics/R1: 12.1008745169819
 ActivityNet_val1_test/v2t_metrics/R5: 38.88549928818385
 ActivityNet_val1_test/v2t_metrics/R10: 55.11490746390075
 ActivityNet_val1_test/v2t_metrics/R50: 84.84848484848484
 ActivityNet_val1_test/v2t_metrics/MedR: 8.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 59.64388854992882
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.59996334307817
 mnt_best       : 29.49962176238898
 not_improved_count: 0
Train Epoch: 25 [1/1000 32/32000 (0%)] Loss: 1.96155 (semantic_loss: 0.01505, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=23.06130 
Train Epoch: 25 [6/1000 192/32000 (1%)] Loss: 1.96358 (semantic_loss: 0.01610, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18917 
Train Epoch: 25 [11/1000 352/32000 (1%)] Loss: 1.96282 (semantic_loss: 0.01535, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20626 
Train Epoch: 25 [16/1000 512/32000 (2%)] Loss: 1.96353 (semantic_loss: 0.01606, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22641 
Train Epoch: 25 [21/1000 672/32000 (2%)] Loss: 1.96069 (semantic_loss: 0.01322, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18474 
Train Epoch: 25 [26/1000 832/32000 (3%)] Loss: 1.96170 (semantic_loss: 0.01423, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18829 
Train Epoch: 25 [31/1000 992/32000 (3%)] Loss: 1.96493 (semantic_loss: 0.01843, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.42670 
Train Epoch: 25 [36/1000 1152/32000 (4%)] Loss: 1.96452 (semantic_loss: 0.01705, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18740 
Train Epoch: 25 [41/1000 1312/32000 (4%)] Loss: 1.96327 (semantic_loss: 0.01580, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19927 
Train Epoch: 25 [46/1000 1472/32000 (5%)] Loss: 1.96016 (semantic_loss: 0.01269, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18738 
Train Epoch: 25 [51/1000 1632/32000 (5%)] Loss: 1.96160 (semantic_loss: 0.01510, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19694 
Train Epoch: 25 [56/1000 1792/32000 (6%)] Loss: 1.96429 (semantic_loss: 0.01584, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19928 
Train Epoch: 25 [61/1000 1952/32000 (6%)] Loss: 1.96144 (semantic_loss: 0.01494, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18765 
Train Epoch: 25 [66/1000 2112/32000 (7%)] Loss: 1.96110 (semantic_loss: 0.01363, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18925 
Train Epoch: 25 [71/1000 2272/32000 (7%)] Loss: 1.96264 (semantic_loss: 0.01516, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18791 
Train Epoch: 25 [76/1000 2432/32000 (8%)] Loss: 1.96176 (semantic_loss: 0.01430, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18664 
Train Epoch: 25 [81/1000 2592/32000 (8%)] Loss: 1.96396 (semantic_loss: 0.01650, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.42861 
Train Epoch: 25 [86/1000 2752/32000 (9%)] Loss: 1.96230 (semantic_loss: 0.01386, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18798 
Train Epoch: 25 [91/1000 2912/32000 (9%)] Loss: 1.96363 (semantic_loss: 0.01713, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18707 
Train Epoch: 25 [96/1000 3072/32000 (10%)] Loss: 1.96134 (semantic_loss: 0.01290, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20192 
Train Epoch: 25 [101/1000 3232/32000 (10%)] Loss: 1.96716 (semantic_loss: 0.01968, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.45112 
Train Epoch: 25 [106/1000 3392/32000 (11%)] Loss: 1.96339 (semantic_loss: 0.01690, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22044 
Train Epoch: 25 [111/1000 3552/32000 (11%)] Loss: 1.96031 (semantic_loss: 0.01284, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20846 
Train Epoch: 25 [116/1000 3712/32000 (12%)] Loss: 1.96353 (semantic_loss: 0.01508, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20687 
Train Epoch: 25 [121/1000 3872/32000 (12%)] Loss: 1.96311 (semantic_loss: 0.01662, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20658 
Train Epoch: 25 [126/1000 4032/32000 (13%)] Loss: 1.96201 (semantic_loss: 0.01551, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21096 
Train Epoch: 25 [131/1000 4192/32000 (13%)] Loss: 1.96413 (semantic_loss: 0.01763, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21347 
Train Epoch: 25 [136/1000 4352/32000 (14%)] Loss: 1.96470 (semantic_loss: 0.01625, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19665 
Train Epoch: 25 [141/1000 4512/32000 (14%)] Loss: 1.96728 (semantic_loss: 0.01786, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.20441 
Train Epoch: 25 [146/1000 4672/32000 (15%)] Loss: 1.96205 (semantic_loss: 0.01457, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18743 
Train Epoch: 25 [151/1000 4832/32000 (15%)] Loss: 1.96163 (semantic_loss: 0.01416, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18747 
Train Epoch: 25 [156/1000 4992/32000 (16%)] Loss: 1.96295 (semantic_loss: 0.01646, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18906 
Train Epoch: 25 [161/1000 5152/32000 (16%)] Loss: 1.96367 (semantic_loss: 0.01619, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22250 
Train Epoch: 25 [166/1000 5312/32000 (17%)] Loss: 1.96147 (semantic_loss: 0.01400, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19362 
Train Epoch: 25 [171/1000 5472/32000 (17%)] Loss: 1.96372 (semantic_loss: 0.01527, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19128 
Train Epoch: 25 [176/1000 5632/32000 (18%)] Loss: 1.96128 (semantic_loss: 0.01479, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18783 
Train Epoch: 25 [181/1000 5792/32000 (18%)] Loss: 1.96384 (semantic_loss: 0.01637, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18675 
Train Epoch: 25 [186/1000 5952/32000 (19%)] Loss: 1.96367 (semantic_loss: 0.01620, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19098 
Train Epoch: 25 [191/1000 6112/32000 (19%)] Loss: 1.96481 (semantic_loss: 0.01831, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20170 
Train Epoch: 25 [196/1000 6272/32000 (20%)] Loss: 1.96119 (semantic_loss: 0.01372, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18825 
Train Epoch: 25 [201/1000 6432/32000 (20%)] Loss: 1.96235 (semantic_loss: 0.01585, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18897 
Train Epoch: 25 [206/1000 6592/32000 (21%)] Loss: 1.96108 (semantic_loss: 0.01459, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18967 
Train Epoch: 25 [211/1000 6752/32000 (21%)] Loss: 1.96122 (semantic_loss: 0.01375, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19165 
Train Epoch: 25 [216/1000 6912/32000 (22%)] Loss: 1.96305 (semantic_loss: 0.01558, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18811 
Train Epoch: 25 [221/1000 7072/32000 (22%)] Loss: 1.96374 (semantic_loss: 0.01725, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18678 
Train Epoch: 25 [226/1000 7232/32000 (23%)] Loss: 1.96415 (semantic_loss: 0.01668, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25408 
Train Epoch: 25 [231/1000 7392/32000 (23%)] Loss: 1.96138 (semantic_loss: 0.01391, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18952 
Train Epoch: 25 [236/1000 7552/32000 (24%)] Loss: 1.96175 (semantic_loss: 0.01428, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19229 
Train Epoch: 25 [241/1000 7712/32000 (24%)] Loss: 1.96112 (semantic_loss: 0.01462, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18812 
Train Epoch: 25 [246/1000 7872/32000 (25%)] Loss: 1.96277 (semantic_loss: 0.01432, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21288 
Train Epoch: 25 [251/1000 8032/32000 (25%)] Loss: 1.96233 (semantic_loss: 0.01681, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.22267 
Train Epoch: 25 [256/1000 8192/32000 (26%)] Loss: 1.96216 (semantic_loss: 0.01469, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23247 
Train Epoch: 25 [261/1000 8352/32000 (26%)] Loss: 1.96102 (semantic_loss: 0.01356, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21547 
Train Epoch: 25 [266/1000 8512/32000 (27%)] Loss: 1.95913 (semantic_loss: 0.01263, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22174 
Train Epoch: 25 [271/1000 8672/32000 (27%)] Loss: 1.96389 (semantic_loss: 0.01740, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21657 
Train Epoch: 25 [276/1000 8832/32000 (28%)] Loss: 1.96485 (semantic_loss: 0.01835, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19395 
Train Epoch: 25 [281/1000 8992/32000 (28%)] Loss: 1.96197 (semantic_loss: 0.01450, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20040 
Train Epoch: 25 [286/1000 9152/32000 (29%)] Loss: 1.96159 (semantic_loss: 0.01510, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19783 
Train Epoch: 25 [291/1000 9312/32000 (29%)] Loss: 1.96160 (semantic_loss: 0.01413, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18755 
Train Epoch: 25 [296/1000 9472/32000 (30%)] Loss: 1.96114 (semantic_loss: 0.01464, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.26205 
Train Epoch: 25 [301/1000 9632/32000 (30%)] Loss: 1.96172 (semantic_loss: 0.01425, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18612 
Train Epoch: 25 [306/1000 9792/32000 (31%)] Loss: 1.96663 (semantic_loss: 0.01818, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.35087 
Train Epoch: 25 [311/1000 9952/32000 (31%)] Loss: 1.95990 (semantic_loss: 0.01341, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21535 
Train Epoch: 25 [316/1000 10112/32000 (32%)] Loss: 1.96256 (semantic_loss: 0.01508, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20671 
Train Epoch: 25 [321/1000 10272/32000 (32%)] Loss: 1.96462 (semantic_loss: 0.01715, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.26861 
Train Epoch: 25 [326/1000 10432/32000 (33%)] Loss: 1.96469 (semantic_loss: 0.01625, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20618 
Train Epoch: 25 [331/1000 10592/32000 (33%)] Loss: 1.96240 (semantic_loss: 0.01493, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18870 
Train Epoch: 25 [336/1000 10752/32000 (34%)] Loss: 1.95817 (semantic_loss: 0.01168, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22003 
Train Epoch: 25 [341/1000 10912/32000 (34%)] Loss: 1.95966 (semantic_loss: 0.01219, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19785 
Train Epoch: 25 [346/1000 11072/32000 (35%)] Loss: 1.96066 (semantic_loss: 0.01417, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18838 
Train Epoch: 25 [351/1000 11232/32000 (35%)] Loss: 1.96178 (semantic_loss: 0.01527, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.42242 
Train Epoch: 25 [356/1000 11392/32000 (36%)] Loss: 1.96353 (semantic_loss: 0.01508, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20023 
Train Epoch: 25 [361/1000 11552/32000 (36%)] Loss: 1.96451 (semantic_loss: 0.01803, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18817 
Train Epoch: 25 [366/1000 11712/32000 (37%)] Loss: 1.96452 (semantic_loss: 0.01510, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18701 
Train Epoch: 25 [371/1000 11872/32000 (37%)] Loss: 1.96271 (semantic_loss: 0.01621, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19541 
Train Epoch: 25 [376/1000 12032/32000 (38%)] Loss: 1.96338 (semantic_loss: 0.01590, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20283 
Train Epoch: 25 [381/1000 12192/32000 (38%)] Loss: 1.96193 (semantic_loss: 0.01544, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18825 
Train Epoch: 25 [386/1000 12352/32000 (39%)] Loss: 1.96419 (semantic_loss: 0.01769, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18957 
Train Epoch: 25 [391/1000 12512/32000 (39%)] Loss: 1.96243 (semantic_loss: 0.01594, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18468 
Train Epoch: 25 [396/1000 12672/32000 (40%)] Loss: 1.96373 (semantic_loss: 0.01528, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18790 
Train Epoch: 25 [401/1000 12832/32000 (40%)] Loss: 1.96180 (semantic_loss: 0.01531, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.50908 
Train Epoch: 25 [406/1000 12992/32000 (41%)] Loss: 1.96593 (semantic_loss: 0.01748, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22544 
Train Epoch: 25 [411/1000 13152/32000 (41%)] Loss: 1.95899 (semantic_loss: 0.01249, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22258 
Train Epoch: 25 [416/1000 13312/32000 (42%)] Loss: 1.96413 (semantic_loss: 0.01666, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19518 
Train Epoch: 25 [421/1000 13472/32000 (42%)] Loss: 1.96522 (semantic_loss: 0.01775, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.41927 
Train Epoch: 25 [426/1000 13632/32000 (43%)] Loss: 1.96441 (semantic_loss: 0.01694, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20105 
Train Epoch: 25 [431/1000 13792/32000 (43%)] Loss: 1.96247 (semantic_loss: 0.01500, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20887 
Train Epoch: 25 [436/1000 13952/32000 (44%)] Loss: 1.96080 (semantic_loss: 0.01334, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19257 
Train Epoch: 25 [441/1000 14112/32000 (44%)] Loss: 1.96815 (semantic_loss: 0.01971, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18760 
Train Epoch: 25 [446/1000 14272/32000 (45%)] Loss: 1.96831 (semantic_loss: 0.01987, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18829 
Train Epoch: 25 [451/1000 14432/32000 (45%)] Loss: 1.96090 (semantic_loss: 0.01441, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18774 
Train Epoch: 25 [456/1000 14592/32000 (46%)] Loss: 1.96448 (semantic_loss: 0.01700, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19059 
Train Epoch: 25 [461/1000 14752/32000 (46%)] Loss: 1.96406 (semantic_loss: 0.01659, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20069 
Train Epoch: 25 [466/1000 14912/32000 (47%)] Loss: 1.95864 (semantic_loss: 0.01312, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19803 
Train Epoch: 25 [471/1000 15072/32000 (47%)] Loss: 1.96182 (semantic_loss: 0.01434, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19239 
Train Epoch: 25 [476/1000 15232/32000 (48%)] Loss: 1.96126 (semantic_loss: 0.01573, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19065 
Train Epoch: 25 [481/1000 15392/32000 (48%)] Loss: 1.96035 (semantic_loss: 0.01385, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18917 
Train Epoch: 25 [486/1000 15552/32000 (49%)] Loss: 1.96261 (semantic_loss: 0.01514, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19726 
Train Epoch: 25 [491/1000 15712/32000 (49%)] Loss: 1.96316 (semantic_loss: 0.01666, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18762 
Train Epoch: 25 [496/1000 15872/32000 (50%)] Loss: 1.96499 (semantic_loss: 0.01850, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18750 
Train Epoch: 25 [501/1000 16032/32000 (50%)] Loss: 1.96028 (semantic_loss: 0.01280, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18751 
Train Epoch: 25 [506/1000 16192/32000 (51%)] Loss: 1.96634 (semantic_loss: 0.01886, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19261 
Train Epoch: 25 [511/1000 16352/32000 (51%)] Loss: 1.95998 (semantic_loss: 0.01446, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.21578 
Train Epoch: 25 [516/1000 16512/32000 (52%)] Loss: 1.96128 (semantic_loss: 0.01380, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19158 
Train Epoch: 25 [521/1000 16672/32000 (52%)] Loss: 1.96020 (semantic_loss: 0.01272, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18688 
Train Epoch: 25 [526/1000 16832/32000 (53%)] Loss: 1.96371 (semantic_loss: 0.01525, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18789 
Train Epoch: 25 [531/1000 16992/32000 (53%)] Loss: 1.96471 (semantic_loss: 0.01822, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18736 
Train Epoch: 25 [536/1000 17152/32000 (54%)] Loss: 1.96953 (semantic_loss: 0.02109, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19500 
Train Epoch: 25 [541/1000 17312/32000 (54%)] Loss: 1.96867 (semantic_loss: 0.02218, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18683 
Train Epoch: 25 [546/1000 17472/32000 (55%)] Loss: 1.96490 (semantic_loss: 0.01743, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.27170 
Train Epoch: 25 [551/1000 17632/32000 (55%)] Loss: 1.96248 (semantic_loss: 0.01598, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19152 
Train Epoch: 25 [556/1000 17792/32000 (56%)] Loss: 1.96401 (semantic_loss: 0.01653, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18748 
Train Epoch: 25 [561/1000 17952/32000 (56%)] Loss: 1.96702 (semantic_loss: 0.01955, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20967 
Train Epoch: 25 [566/1000 18112/32000 (57%)] Loss: 1.96375 (semantic_loss: 0.01725, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22865 
Train Epoch: 25 [571/1000 18272/32000 (57%)] Loss: 1.96153 (semantic_loss: 0.01406, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22306 
Train Epoch: 25 [576/1000 18432/32000 (58%)] Loss: 1.96297 (semantic_loss: 0.01550, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20312 
Train Epoch: 25 [581/1000 18592/32000 (58%)] Loss: 1.96478 (semantic_loss: 0.01633, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20072 
Train Epoch: 25 [586/1000 18752/32000 (59%)] Loss: 1.96017 (semantic_loss: 0.01465, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.23129 
Train Epoch: 25 [591/1000 18912/32000 (59%)] Loss: 1.96062 (semantic_loss: 0.01412, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19080 
Train Epoch: 25 [596/1000 19072/32000 (60%)] Loss: 1.96269 (semantic_loss: 0.01522, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19217 
Train Epoch: 25 [601/1000 19232/32000 (60%)] Loss: 1.96044 (semantic_loss: 0.01394, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18755 
Train Epoch: 25 [606/1000 19392/32000 (61%)] Loss: 1.95952 (semantic_loss: 0.01205, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18928 
Train Epoch: 25 [611/1000 19552/32000 (61%)] Loss: 1.96314 (semantic_loss: 0.01567, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18719 
Train Epoch: 25 [616/1000 19712/32000 (62%)] Loss: 1.96290 (semantic_loss: 0.01542, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.27071 
Train Epoch: 25 [621/1000 19872/32000 (62%)] Loss: 1.96259 (semantic_loss: 0.01512, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19721 
Train Epoch: 25 [626/1000 20032/32000 (63%)] Loss: 1.96466 (semantic_loss: 0.01622, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.36318 
Train Epoch: 25 [631/1000 20192/32000 (63%)] Loss: 1.96127 (semantic_loss: 0.01282, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19894 
Train Epoch: 25 [636/1000 20352/32000 (64%)] Loss: 1.96390 (semantic_loss: 0.01546, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19986 
Train Epoch: 25 [641/1000 20512/32000 (64%)] Loss: 1.96273 (semantic_loss: 0.01624, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.25590 
Train Epoch: 25 [646/1000 20672/32000 (65%)] Loss: 1.96049 (semantic_loss: 0.01400, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18673 
Train Epoch: 25 [651/1000 20832/32000 (65%)] Loss: 1.97300 (semantic_loss: 0.02455, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18613 
Train Epoch: 25 [656/1000 20992/32000 (66%)] Loss: 1.95973 (semantic_loss: 0.01323, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22123 
Train Epoch: 25 [661/1000 21152/32000 (66%)] Loss: 1.96404 (semantic_loss: 0.01560, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18504 
Train Epoch: 25 [666/1000 21312/32000 (67%)] Loss: 1.96172 (semantic_loss: 0.01229, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.20080 
Train Epoch: 25 [671/1000 21472/32000 (67%)] Loss: 1.96194 (semantic_loss: 0.01349, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.41003 
Train Epoch: 25 [676/1000 21632/32000 (68%)] Loss: 1.96727 (semantic_loss: 0.01980, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18250 
Train Epoch: 25 [681/1000 21792/32000 (68%)] Loss: 1.96443 (semantic_loss: 0.01794, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18467 
Train Epoch: 25 [686/1000 21952/32000 (69%)] Loss: 1.96232 (semantic_loss: 0.01583, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18559 
Train Epoch: 25 [691/1000 22112/32000 (69%)] Loss: 1.96352 (semantic_loss: 0.01605, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18703 
Train Epoch: 25 [696/1000 22272/32000 (70%)] Loss: 1.96457 (semantic_loss: 0.01613, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18930 
Train Epoch: 25 [701/1000 22432/32000 (70%)] Loss: 1.96187 (semantic_loss: 0.01440, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18693 
Train Epoch: 25 [706/1000 22592/32000 (71%)] Loss: 1.96329 (semantic_loss: 0.01484, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18647 
Train Epoch: 25 [711/1000 22752/32000 (71%)] Loss: 1.96148 (semantic_loss: 0.01499, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18612 
Train Epoch: 25 [716/1000 22912/32000 (72%)] Loss: 1.96026 (semantic_loss: 0.01376, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18490 
Train Epoch: 25 [721/1000 23072/32000 (72%)] Loss: 1.96433 (semantic_loss: 0.01685, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.53214 
Train Epoch: 25 [726/1000 23232/32000 (73%)] Loss: 1.96013 (semantic_loss: 0.01363, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20668 
Train Epoch: 25 [731/1000 23392/32000 (73%)] Loss: 1.96228 (semantic_loss: 0.01482, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22237 
Train Epoch: 25 [736/1000 23552/32000 (74%)] Loss: 1.96417 (semantic_loss: 0.01670, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20278 
Train Epoch: 25 [741/1000 23712/32000 (74%)] Loss: 1.96336 (semantic_loss: 0.01492, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.39028 
Train Epoch: 25 [746/1000 23872/32000 (75%)] Loss: 1.96219 (semantic_loss: 0.01472, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19470 
Train Epoch: 25 [751/1000 24032/32000 (75%)] Loss: 1.96557 (semantic_loss: 0.01810, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20936 
Train Epoch: 25 [756/1000 24192/32000 (76%)] Loss: 1.96203 (semantic_loss: 0.01456, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20565 
Train Epoch: 25 [761/1000 24352/32000 (76%)] Loss: 1.96200 (semantic_loss: 0.01356, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19134 
Train Epoch: 25 [766/1000 24512/32000 (77%)] Loss: 1.96338 (semantic_loss: 0.01493, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18961 
Train Epoch: 25 [771/1000 24672/32000 (77%)] Loss: 1.96186 (semantic_loss: 0.01536, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18811 
Train Epoch: 25 [776/1000 24832/32000 (78%)] Loss: 1.96294 (semantic_loss: 0.01449, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19068 
Train Epoch: 25 [781/1000 24992/32000 (78%)] Loss: 1.95949 (semantic_loss: 0.01300, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20410 
Train Epoch: 25 [786/1000 25152/32000 (79%)] Loss: 1.96066 (semantic_loss: 0.01222, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19886 
Train Epoch: 25 [791/1000 25312/32000 (79%)] Loss: 1.95849 (semantic_loss: 0.01200, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19195 
Train Epoch: 25 [796/1000 25472/32000 (80%)] Loss: 1.96075 (semantic_loss: 0.01426, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22099 
Train Epoch: 25 [801/1000 25632/32000 (80%)] Loss: 1.96823 (semantic_loss: 0.02076, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18726 
Train Epoch: 25 [806/1000 25792/32000 (81%)] Loss: 1.96368 (semantic_loss: 0.01622, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18838 
Train Epoch: 25 [811/1000 25952/32000 (81%)] Loss: 1.96268 (semantic_loss: 0.01521, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18839 
Train Epoch: 25 [816/1000 26112/32000 (82%)] Loss: 1.96011 (semantic_loss: 0.01361, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18687 
Train Epoch: 25 [821/1000 26272/32000 (82%)] Loss: 1.96411 (semantic_loss: 0.01664, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18845 
Train Epoch: 25 [826/1000 26432/32000 (83%)] Loss: 1.96271 (semantic_loss: 0.01719, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18816 
Train Epoch: 25 [831/1000 26592/32000 (83%)] Loss: 1.96062 (semantic_loss: 0.01315, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20731 
Train Epoch: 25 [836/1000 26752/32000 (84%)] Loss: 1.96095 (semantic_loss: 0.01347, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19499 
Train Epoch: 25 [841/1000 26912/32000 (84%)] Loss: 1.95983 (semantic_loss: 0.01237, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18762 
Train Epoch: 25 [846/1000 27072/32000 (85%)] Loss: 1.96309 (semantic_loss: 0.01562, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18760 
Train Epoch: 25 [851/1000 27232/32000 (85%)] Loss: 1.96007 (semantic_loss: 0.01358, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18804 
Train Epoch: 25 [856/1000 27392/32000 (86%)] Loss: 1.96390 (semantic_loss: 0.01740, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18701 
Train Epoch: 25 [861/1000 27552/32000 (86%)] Loss: 1.95902 (semantic_loss: 0.01253, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18662 
Train Epoch: 25 [866/1000 27712/32000 (87%)] Loss: 1.96196 (semantic_loss: 0.01546, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.26574 
Train Epoch: 25 [871/1000 27872/32000 (87%)] Loss: 1.96207 (semantic_loss: 0.01558, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21902 
Train Epoch: 25 [876/1000 28032/32000 (88%)] Loss: 1.96088 (semantic_loss: 0.01341, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20597 
Train Epoch: 25 [881/1000 28192/32000 (88%)] Loss: 1.96041 (semantic_loss: 0.01392, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22153 
Train Epoch: 25 [886/1000 28352/32000 (89%)] Loss: 1.95998 (semantic_loss: 0.01349, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19800 
Train Epoch: 25 [891/1000 28512/32000 (89%)] Loss: 1.96000 (semantic_loss: 0.01254, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19353 
Train Epoch: 25 [896/1000 28672/32000 (90%)] Loss: 1.96257 (semantic_loss: 0.01509, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20040 
Train Epoch: 25 [901/1000 28832/32000 (90%)] Loss: 1.96078 (semantic_loss: 0.01331, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20264 
Train Epoch: 25 [906/1000 28992/32000 (91%)] Loss: 1.96076 (semantic_loss: 0.01328, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21776 
Train Epoch: 25 [911/1000 29152/32000 (91%)] Loss: 1.96162 (semantic_loss: 0.01513, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19860 
Train Epoch: 25 [916/1000 29312/32000 (92%)] Loss: 1.96223 (semantic_loss: 0.01476, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18789 
Train Epoch: 25 [921/1000 29472/32000 (92%)] Loss: 1.96183 (semantic_loss: 0.01534, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18739 
Train Epoch: 25 [926/1000 29632/32000 (93%)] Loss: 1.96257 (semantic_loss: 0.01509, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19322 
Train Epoch: 25 [931/1000 29792/32000 (93%)] Loss: 1.96416 (semantic_loss: 0.01571, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18759 
Train Epoch: 25 [936/1000 29952/32000 (94%)] Loss: 1.96045 (semantic_loss: 0.01396, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.25927 
Train Epoch: 25 [941/1000 30112/32000 (94%)] Loss: 1.96416 (semantic_loss: 0.01669, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19324 
Train Epoch: 25 [946/1000 30272/32000 (95%)] Loss: 1.96286 (semantic_loss: 0.01637, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.36233 
Train Epoch: 25 [951/1000 30432/32000 (95%)] Loss: 1.96590 (semantic_loss: 0.01843, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19743 
Train Epoch: 25 [956/1000 30592/32000 (96%)] Loss: 1.96069 (semantic_loss: 0.01322, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18749 
Train Epoch: 25 [961/1000 30752/32000 (96%)] Loss: 1.96023 (semantic_loss: 0.01275, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26131 
Train Epoch: 25 [966/1000 30912/32000 (97%)] Loss: 1.96355 (semantic_loss: 0.01510, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18572 
Train Epoch: 25 [971/1000 31072/32000 (97%)] Loss: 1.96473 (semantic_loss: 0.01726, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19372 
Train Epoch: 25 [976/1000 31232/32000 (98%)] Loss: 1.96412 (semantic_loss: 0.01664, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21850 
Train Epoch: 25 [981/1000 31392/32000 (98%)] Loss: 1.96097 (semantic_loss: 0.01447, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18546 
Train Epoch: 25 [986/1000 31552/32000 (99%)] Loss: 1.96548 (semantic_loss: 0.01703, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18629 
Train Epoch: 25 [991/1000 31712/32000 (99%)] Loss: 1.96271 (semantic_loss: 0.01427, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.42019 
Train Epoch: 25 [996/1000 31872/32000 (100%)] Loss: 1.96476 (semantic_loss: 0.01826, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18705 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch25.pth ...
Done in 4.074s
removing stale ckpt [epoch 24] [took 0.00s]
 epoch          : 25
 loss           : 1.962619880080223
 learning_rate  : 3.9883221538436285e-06
 n_samples      : 800000
 n_steps        : 25000
 ActivityNet_val1_test/t2v_metrics/R1: 11.836485661989018
 ActivityNet_val1_test/t2v_metrics/R5: 37.9906447020541
 ActivityNet_val1_test/t2v_metrics/R10: 55.17592027659142
 ActivityNet_val1_test/t2v_metrics/R50: 85.29591214154972
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 58.238865161683954
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.166412147983635
 ActivityNet_val1_test/v2t_metrics/R1: 12.405938580435224
 ActivityNet_val1_test/v2t_metrics/R5: 38.82448647549319
 ActivityNet_val1_test/v2t_metrics/R10: 55.541997152735405
 ActivityNet_val1_test/v2t_metrics/R50: 84.82814724425462
 ActivityNet_val1_test/v2t_metrics/MedR: 8.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 61.18039454952206
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.907878859708322
 mnt_best       : 29.49962176238898
 not_improved_count: 1
Train Epoch: 26 [1/1000 32/32000 (0%)] Loss: 1.96276 (semantic_loss: 0.01627, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=22.37606 
Train Epoch: 26 [6/1000 192/32000 (1%)] Loss: 1.95985 (semantic_loss: 0.01434, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.24206 
Train Epoch: 26 [11/1000 352/32000 (1%)] Loss: 1.95994 (semantic_loss: 0.01247, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22471 
Train Epoch: 26 [16/1000 512/32000 (2%)] Loss: 1.96471 (semantic_loss: 0.01724, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19221 
Train Epoch: 26 [21/1000 672/32000 (2%)] Loss: 1.96149 (semantic_loss: 0.01402, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20423 
Train Epoch: 26 [26/1000 832/32000 (3%)] Loss: 1.96188 (semantic_loss: 0.01636, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20635 
Train Epoch: 26 [31/1000 992/32000 (3%)] Loss: 1.96298 (semantic_loss: 0.01551, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19371 
Train Epoch: 26 [36/1000 1152/32000 (4%)] Loss: 1.96047 (semantic_loss: 0.01495, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19367 
Train Epoch: 26 [41/1000 1312/32000 (4%)] Loss: 1.96195 (semantic_loss: 0.01448, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18608 
Train Epoch: 26 [46/1000 1472/32000 (5%)] Loss: 1.96291 (semantic_loss: 0.01641, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18558 
Train Epoch: 26 [51/1000 1632/32000 (5%)] Loss: 1.96154 (semantic_loss: 0.01309, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19188 
Train Epoch: 26 [56/1000 1792/32000 (6%)] Loss: 1.96477 (semantic_loss: 0.01632, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18682 
Train Epoch: 26 [61/1000 1952/32000 (6%)] Loss: 1.95937 (semantic_loss: 0.01287, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18746 
Train Epoch: 26 [66/1000 2112/32000 (7%)] Loss: 1.96212 (semantic_loss: 0.01465, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21698 
Train Epoch: 26 [71/1000 2272/32000 (7%)] Loss: 1.96535 (semantic_loss: 0.01788, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19360 
Train Epoch: 26 [76/1000 2432/32000 (8%)] Loss: 1.96136 (semantic_loss: 0.01487, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19974 
Train Epoch: 26 [81/1000 2592/32000 (8%)] Loss: 1.96277 (semantic_loss: 0.01725, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.54667 
Train Epoch: 26 [86/1000 2752/32000 (9%)] Loss: 1.96181 (semantic_loss: 0.01531, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19501 
Train Epoch: 26 [91/1000 2912/32000 (9%)] Loss: 1.96025 (semantic_loss: 0.01278, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18702 
Train Epoch: 26 [96/1000 3072/32000 (10%)] Loss: 1.96369 (semantic_loss: 0.01622, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19362 
Train Epoch: 26 [101/1000 3232/32000 (10%)] Loss: 1.96126 (semantic_loss: 0.01380, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18629 
Train Epoch: 26 [106/1000 3392/32000 (11%)] Loss: 1.96405 (semantic_loss: 0.01658, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18671 
Train Epoch: 26 [111/1000 3552/32000 (11%)] Loss: 1.96313 (semantic_loss: 0.01566, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18536 
Train Epoch: 26 [116/1000 3712/32000 (12%)] Loss: 1.96184 (semantic_loss: 0.01437, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18533 
Train Epoch: 26 [121/1000 3872/32000 (12%)] Loss: 1.96274 (semantic_loss: 0.01527, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18801 
Train Epoch: 26 [126/1000 4032/32000 (13%)] Loss: 1.96117 (semantic_loss: 0.01370, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22582 
Train Epoch: 26 [131/1000 4192/32000 (13%)] Loss: 1.96216 (semantic_loss: 0.01566, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18598 
Train Epoch: 26 [136/1000 4352/32000 (14%)] Loss: 1.96166 (semantic_loss: 0.01419, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18719 
Train Epoch: 26 [141/1000 4512/32000 (14%)] Loss: 1.96217 (semantic_loss: 0.01568, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19271 
Train Epoch: 26 [146/1000 4672/32000 (15%)] Loss: 1.96187 (semantic_loss: 0.01440, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20020 
Train Epoch: 26 [151/1000 4832/32000 (15%)] Loss: 1.96642 (semantic_loss: 0.01797, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21813 
Train Epoch: 26 [156/1000 4992/32000 (16%)] Loss: 1.96118 (semantic_loss: 0.01273, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.23063 
Train Epoch: 26 [161/1000 5152/32000 (16%)] Loss: 1.96036 (semantic_loss: 0.01289, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22639 
Train Epoch: 26 [166/1000 5312/32000 (17%)] Loss: 1.96190 (semantic_loss: 0.01345, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.61455 
Train Epoch: 26 [171/1000 5472/32000 (17%)] Loss: 1.96084 (semantic_loss: 0.01338, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20414 
Train Epoch: 26 [176/1000 5632/32000 (18%)] Loss: 1.96002 (semantic_loss: 0.01352, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19569 
Train Epoch: 26 [181/1000 5792/32000 (18%)] Loss: 1.96006 (semantic_loss: 0.01258, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19876 
Train Epoch: 26 [186/1000 5952/32000 (19%)] Loss: 1.96365 (semantic_loss: 0.01618, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19737 
Train Epoch: 26 [191/1000 6112/32000 (19%)] Loss: 1.96293 (semantic_loss: 0.01644, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18861 
Train Epoch: 26 [196/1000 6272/32000 (20%)] Loss: 1.96433 (semantic_loss: 0.01686, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24573 
Train Epoch: 26 [201/1000 6432/32000 (20%)] Loss: 1.96142 (semantic_loss: 0.01396, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.43069 
Train Epoch: 26 [206/1000 6592/32000 (21%)] Loss: 1.96041 (semantic_loss: 0.01294, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18703 
Train Epoch: 26 [211/1000 6752/32000 (21%)] Loss: 1.97169 (semantic_loss: 0.02520, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.27640 
Train Epoch: 26 [216/1000 6912/32000 (22%)] Loss: 1.96214 (semantic_loss: 0.01466, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20526 
Train Epoch: 26 [221/1000 7072/32000 (22%)] Loss: 1.96753 (semantic_loss: 0.02006, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20163 
Train Epoch: 26 [226/1000 7232/32000 (23%)] Loss: 1.96116 (semantic_loss: 0.01467, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19101 
Train Epoch: 26 [231/1000 7392/32000 (23%)] Loss: 1.96260 (semantic_loss: 0.01513, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18819 
Train Epoch: 26 [236/1000 7552/32000 (24%)] Loss: 1.96490 (semantic_loss: 0.01939, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19843 
Train Epoch: 26 [241/1000 7712/32000 (24%)] Loss: 1.96267 (semantic_loss: 0.01423, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19613 
Train Epoch: 26 [246/1000 7872/32000 (25%)] Loss: 1.96415 (semantic_loss: 0.01668, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18609 
Train Epoch: 26 [251/1000 8032/32000 (25%)] Loss: 1.96298 (semantic_loss: 0.01453, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19893 
Train Epoch: 26 [256/1000 8192/32000 (26%)] Loss: 1.96234 (semantic_loss: 0.01389, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19301 
Train Epoch: 26 [261/1000 8352/32000 (26%)] Loss: 1.96378 (semantic_loss: 0.01631, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18830 
Train Epoch: 26 [266/1000 8512/32000 (27%)] Loss: 1.96267 (semantic_loss: 0.01520, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18885 
Train Epoch: 26 [271/1000 8672/32000 (27%)] Loss: 1.96130 (semantic_loss: 0.01383, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19625 
Train Epoch: 26 [276/1000 8832/32000 (28%)] Loss: 1.96273 (semantic_loss: 0.01526, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19411 
Train Epoch: 26 [281/1000 8992/32000 (28%)] Loss: 1.95968 (semantic_loss: 0.01319, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19567 
Train Epoch: 26 [286/1000 9152/32000 (29%)] Loss: 1.96214 (semantic_loss: 0.01565, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19525 
Train Epoch: 26 [291/1000 9312/32000 (29%)] Loss: 1.96141 (semantic_loss: 0.01492, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20013 
Train Epoch: 26 [296/1000 9472/32000 (30%)] Loss: 1.96372 (semantic_loss: 0.01625, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19554 
Train Epoch: 26 [301/1000 9632/32000 (30%)] Loss: 1.96424 (semantic_loss: 0.01677, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18807 
Train Epoch: 26 [306/1000 9792/32000 (31%)] Loss: 1.95978 (semantic_loss: 0.01231, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18868 
Train Epoch: 26 [311/1000 9952/32000 (31%)] Loss: 1.95970 (semantic_loss: 0.01321, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21209 
Train Epoch: 26 [316/1000 10112/32000 (32%)] Loss: 1.96119 (semantic_loss: 0.01274, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21539 
Train Epoch: 26 [321/1000 10272/32000 (32%)] Loss: 1.96673 (semantic_loss: 0.02024, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22143 
Train Epoch: 26 [326/1000 10432/32000 (33%)] Loss: 1.96318 (semantic_loss: 0.01668, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21770 
Train Epoch: 26 [331/1000 10592/32000 (33%)] Loss: 1.96137 (semantic_loss: 0.01293, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20902 
Train Epoch: 26 [336/1000 10752/32000 (34%)] Loss: 1.96348 (semantic_loss: 0.01601, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19751 
Train Epoch: 26 [341/1000 10912/32000 (34%)] Loss: 1.96409 (semantic_loss: 0.01760, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20423 
Train Epoch: 26 [346/1000 11072/32000 (35%)] Loss: 1.96084 (semantic_loss: 0.01239, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19504 
Train Epoch: 26 [351/1000 11232/32000 (35%)] Loss: 1.96155 (semantic_loss: 0.01407, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18959 
Train Epoch: 26 [356/1000 11392/32000 (36%)] Loss: 1.96367 (semantic_loss: 0.01620, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19266 
Train Epoch: 26 [361/1000 11552/32000 (36%)] Loss: 1.96393 (semantic_loss: 0.01645, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18875 
Train Epoch: 26 [366/1000 11712/32000 (37%)] Loss: 1.96111 (semantic_loss: 0.01461, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18683 
Train Epoch: 26 [371/1000 11872/32000 (37%)] Loss: 1.95932 (semantic_loss: 0.01381, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18975 
Train Epoch: 26 [376/1000 12032/32000 (38%)] Loss: 1.96149 (semantic_loss: 0.01499, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19511 
Train Epoch: 26 [381/1000 12192/32000 (38%)] Loss: 1.96228 (semantic_loss: 0.01480, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19835 
Train Epoch: 26 [386/1000 12352/32000 (39%)] Loss: 1.96026 (semantic_loss: 0.01279, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19882 
Train Epoch: 26 [391/1000 12512/32000 (39%)] Loss: 1.96073 (semantic_loss: 0.01423, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18960 
Train Epoch: 26 [396/1000 12672/32000 (40%)] Loss: 1.96154 (semantic_loss: 0.01309, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18656 
Train Epoch: 26 [401/1000 12832/32000 (40%)] Loss: 1.95986 (semantic_loss: 0.01239, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.55042 
Train Epoch: 26 [406/1000 12992/32000 (41%)] Loss: 1.95934 (semantic_loss: 0.01187, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18721 
Train Epoch: 26 [411/1000 13152/32000 (41%)] Loss: 1.96323 (semantic_loss: 0.01576, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18788 
Train Epoch: 26 [416/1000 13312/32000 (42%)] Loss: 1.96171 (semantic_loss: 0.01522, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18826 
Train Epoch: 26 [421/1000 13472/32000 (42%)] Loss: 1.96511 (semantic_loss: 0.01861, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19266 
Train Epoch: 26 [426/1000 13632/32000 (43%)] Loss: 1.96340 (semantic_loss: 0.01593, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18942 
Train Epoch: 26 [431/1000 13792/32000 (43%)] Loss: 1.96038 (semantic_loss: 0.01291, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18698 
Train Epoch: 26 [436/1000 13952/32000 (44%)] Loss: 1.96087 (semantic_loss: 0.01438, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18512 
Train Epoch: 26 [441/1000 14112/32000 (44%)] Loss: 1.96980 (semantic_loss: 0.02233, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18843 
Train Epoch: 26 [446/1000 14272/32000 (45%)] Loss: 1.96047 (semantic_loss: 0.01300, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18668 
Train Epoch: 26 [451/1000 14432/32000 (45%)] Loss: 1.96316 (semantic_loss: 0.01666, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18915 
Train Epoch: 26 [456/1000 14592/32000 (46%)] Loss: 1.96042 (semantic_loss: 0.01393, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19211 
Train Epoch: 26 [461/1000 14752/32000 (46%)] Loss: 1.96296 (semantic_loss: 0.01647, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19433 
Train Epoch: 26 [466/1000 14912/32000 (47%)] Loss: 1.96137 (semantic_loss: 0.01292, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20984 
Train Epoch: 26 [471/1000 15072/32000 (47%)] Loss: 1.96039 (semantic_loss: 0.01487, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20655 
Train Epoch: 26 [476/1000 15232/32000 (48%)] Loss: 1.96117 (semantic_loss: 0.01371, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27343 
Train Epoch: 26 [481/1000 15392/32000 (48%)] Loss: 1.96099 (semantic_loss: 0.01449, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22009 
Train Epoch: 26 [486/1000 15552/32000 (49%)] Loss: 1.96536 (semantic_loss: 0.01887, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.57214 
Train Epoch: 26 [491/1000 15712/32000 (49%)] Loss: 1.96047 (semantic_loss: 0.01399, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21821 
Train Epoch: 26 [496/1000 15872/32000 (50%)] Loss: 1.96283 (semantic_loss: 0.01536, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20873 
Train Epoch: 26 [501/1000 16032/32000 (50%)] Loss: 1.96073 (semantic_loss: 0.01327, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20531 
Train Epoch: 26 [506/1000 16192/32000 (51%)] Loss: 1.96222 (semantic_loss: 0.01476, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21867 
Train Epoch: 26 [511/1000 16352/32000 (51%)] Loss: 1.96211 (semantic_loss: 0.01464, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19103 
Train Epoch: 26 [516/1000 16512/32000 (52%)] Loss: 1.96208 (semantic_loss: 0.01461, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23630 
Train Epoch: 26 [521/1000 16672/32000 (52%)] Loss: 1.96050 (semantic_loss: 0.01303, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.41136 
Train Epoch: 26 [526/1000 16832/32000 (53%)] Loss: 1.96804 (semantic_loss: 0.02058, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18735 
Train Epoch: 26 [531/1000 16992/32000 (53%)] Loss: 1.96253 (semantic_loss: 0.01506, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.29211 
Train Epoch: 26 [536/1000 17152/32000 (54%)] Loss: 1.95980 (semantic_loss: 0.01330, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19094 
Train Epoch: 26 [541/1000 17312/32000 (54%)] Loss: 1.96084 (semantic_loss: 0.01337, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18855 
Train Epoch: 26 [546/1000 17472/32000 (55%)] Loss: 1.96233 (semantic_loss: 0.01584, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20743 
Train Epoch: 26 [551/1000 17632/32000 (55%)] Loss: 1.96268 (semantic_loss: 0.01521, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19314 
Train Epoch: 26 [556/1000 17792/32000 (56%)] Loss: 1.96093 (semantic_loss: 0.01346, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18523 
Train Epoch: 26 [561/1000 17952/32000 (56%)] Loss: 1.96066 (semantic_loss: 0.01417, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18792 
Train Epoch: 26 [566/1000 18112/32000 (57%)] Loss: 1.96278 (semantic_loss: 0.01531, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18548 
Train Epoch: 26 [571/1000 18272/32000 (57%)] Loss: 1.96545 (semantic_loss: 0.01895, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18493 
Train Epoch: 26 [576/1000 18432/32000 (58%)] Loss: 1.96212 (semantic_loss: 0.01465, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18783 
Train Epoch: 26 [581/1000 18592/32000 (58%)] Loss: 1.96132 (semantic_loss: 0.01482, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18955 
Train Epoch: 26 [586/1000 18752/32000 (59%)] Loss: 1.96379 (semantic_loss: 0.01632, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18825 
Train Epoch: 26 [591/1000 18912/32000 (59%)] Loss: 1.96684 (semantic_loss: 0.01936, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18558 
Train Epoch: 26 [596/1000 19072/32000 (60%)] Loss: 1.96093 (semantic_loss: 0.01444, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18592 
Train Epoch: 26 [601/1000 19232/32000 (60%)] Loss: 1.96375 (semantic_loss: 0.01628, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20883 
Train Epoch: 26 [606/1000 19392/32000 (61%)] Loss: 1.96344 (semantic_loss: 0.01695, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18823 
Train Epoch: 26 [611/1000 19552/32000 (61%)] Loss: 1.96403 (semantic_loss: 0.01656, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18731 
Train Epoch: 26 [616/1000 19712/32000 (62%)] Loss: 1.96053 (semantic_loss: 0.01404, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19242 
Train Epoch: 26 [621/1000 19872/32000 (62%)] Loss: 1.96445 (semantic_loss: 0.01698, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19992 
Train Epoch: 26 [626/1000 20032/32000 (63%)] Loss: 1.96291 (semantic_loss: 0.01447, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21809 
Train Epoch: 26 [631/1000 20192/32000 (63%)] Loss: 1.95996 (semantic_loss: 0.01249, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23809 
Train Epoch: 26 [636/1000 20352/32000 (64%)] Loss: 1.95879 (semantic_loss: 0.01132, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22044 
Train Epoch: 26 [641/1000 20512/32000 (64%)] Loss: 1.96129 (semantic_loss: 0.01383, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20765 
Train Epoch: 26 [646/1000 20672/32000 (65%)] Loss: 1.96159 (semantic_loss: 0.01314, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19929 
Train Epoch: 26 [651/1000 20832/32000 (65%)] Loss: 1.96061 (semantic_loss: 0.01314, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19587 
Train Epoch: 26 [656/1000 20992/32000 (66%)] Loss: 1.96024 (semantic_loss: 0.01375, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20586 
Train Epoch: 26 [661/1000 21152/32000 (66%)] Loss: 1.96741 (semantic_loss: 0.01992, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19583 
Train Epoch: 26 [666/1000 21312/32000 (67%)] Loss: 1.96372 (semantic_loss: 0.01625, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18707 
Train Epoch: 26 [671/1000 21472/32000 (67%)] Loss: 1.96240 (semantic_loss: 0.01590, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18657 
Train Epoch: 26 [676/1000 21632/32000 (68%)] Loss: 1.96658 (semantic_loss: 0.01910, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18618 
Train Epoch: 26 [681/1000 21792/32000 (68%)] Loss: 1.96162 (semantic_loss: 0.01513, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18808 
Train Epoch: 26 [686/1000 21952/32000 (69%)] Loss: 1.96035 (semantic_loss: 0.01191, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19044 
Train Epoch: 26 [691/1000 22112/32000 (69%)] Loss: 1.96370 (semantic_loss: 0.01622, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19336 
Train Epoch: 26 [696/1000 22272/32000 (70%)] Loss: 1.96314 (semantic_loss: 0.01567, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19768 
Train Epoch: 26 [701/1000 22432/32000 (70%)] Loss: 1.96346 (semantic_loss: 0.01501, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19714 
Train Epoch: 26 [706/1000 22592/32000 (71%)] Loss: 1.96142 (semantic_loss: 0.01493, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18571 
Train Epoch: 26 [711/1000 22752/32000 (71%)] Loss: 1.96129 (semantic_loss: 0.01480, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19206 
Train Epoch: 26 [716/1000 22912/32000 (72%)] Loss: 1.96821 (semantic_loss: 0.02074, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18722 
Train Epoch: 26 [721/1000 23072/32000 (72%)] Loss: 1.96118 (semantic_loss: 0.01469, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.55207 
Train Epoch: 26 [726/1000 23232/32000 (73%)] Loss: 1.96181 (semantic_loss: 0.01532, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18554 
Train Epoch: 26 [731/1000 23392/32000 (73%)] Loss: 1.96285 (semantic_loss: 0.01538, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18892 
Train Epoch: 26 [736/1000 23552/32000 (74%)] Loss: 1.96235 (semantic_loss: 0.01488, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18809 
Train Epoch: 26 [741/1000 23712/32000 (74%)] Loss: 1.96782 (semantic_loss: 0.01937, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18582 
Train Epoch: 26 [746/1000 23872/32000 (75%)] Loss: 1.96212 (semantic_loss: 0.01270, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18538 
Train Epoch: 26 [751/1000 24032/32000 (75%)] Loss: 1.96563 (semantic_loss: 0.01816, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18518 
Train Epoch: 26 [756/1000 24192/32000 (76%)] Loss: 1.96270 (semantic_loss: 0.01426, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19788 
Train Epoch: 26 [761/1000 24352/32000 (76%)] Loss: 1.96495 (semantic_loss: 0.01748, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19225 
Train Epoch: 26 [766/1000 24512/32000 (77%)] Loss: 1.96111 (semantic_loss: 0.01365, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18988 
Train Epoch: 26 [771/1000 24672/32000 (77%)] Loss: 1.96284 (semantic_loss: 0.01635, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20483 
Train Epoch: 26 [776/1000 24832/32000 (78%)] Loss: 1.96571 (semantic_loss: 0.01726, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19035 
Train Epoch: 26 [781/1000 24992/32000 (78%)] Loss: 1.96167 (semantic_loss: 0.01420, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19675 
Train Epoch: 26 [786/1000 25152/32000 (79%)] Loss: 1.96000 (semantic_loss: 0.01350, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22606 
Train Epoch: 26 [791/1000 25312/32000 (79%)] Loss: 1.96270 (semantic_loss: 0.01523, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23538 
Train Epoch: 26 [796/1000 25472/32000 (80%)] Loss: 1.96230 (semantic_loss: 0.01386, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.24693 
Train Epoch: 26 [801/1000 25632/32000 (80%)] Loss: 1.96132 (semantic_loss: 0.01288, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21302 
Train Epoch: 26 [806/1000 25792/32000 (81%)] Loss: 1.96340 (semantic_loss: 0.01496, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.63806 
Train Epoch: 26 [811/1000 25952/32000 (81%)] Loss: 1.96423 (semantic_loss: 0.01579, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19398 
Train Epoch: 26 [816/1000 26112/32000 (82%)] Loss: 1.96270 (semantic_loss: 0.01621, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20958 
Train Epoch: 26 [821/1000 26272/32000 (82%)] Loss: 1.96127 (semantic_loss: 0.01379, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21585 
Train Epoch: 26 [826/1000 26432/32000 (83%)] Loss: 1.96253 (semantic_loss: 0.01603, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19480 
Train Epoch: 26 [831/1000 26592/32000 (83%)] Loss: 1.96260 (semantic_loss: 0.01513, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18843 
Train Epoch: 26 [836/1000 26752/32000 (84%)] Loss: 1.96448 (semantic_loss: 0.01701, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23383 
Train Epoch: 26 [841/1000 26912/32000 (84%)] Loss: 1.96191 (semantic_loss: 0.01347, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.40610 
Train Epoch: 26 [846/1000 27072/32000 (85%)] Loss: 1.96310 (semantic_loss: 0.01562, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19093 
Train Epoch: 26 [851/1000 27232/32000 (85%)] Loss: 1.95978 (semantic_loss: 0.01329, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.29683 
Train Epoch: 26 [856/1000 27392/32000 (86%)] Loss: 1.96388 (semantic_loss: 0.01543, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19292 
Train Epoch: 26 [861/1000 27552/32000 (86%)] Loss: 1.96434 (semantic_loss: 0.01686, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18572 
Train Epoch: 26 [866/1000 27712/32000 (87%)] Loss: 1.95990 (semantic_loss: 0.01438, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18857 
Train Epoch: 26 [871/1000 27872/32000 (87%)] Loss: 1.96369 (semantic_loss: 0.01622, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18710 
Train Epoch: 26 [876/1000 28032/32000 (88%)] Loss: 1.96402 (semantic_loss: 0.01556, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18899 
Train Epoch: 26 [881/1000 28192/32000 (88%)] Loss: 1.96366 (semantic_loss: 0.01618, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19967 
Train Epoch: 26 [886/1000 28352/32000 (89%)] Loss: 1.96324 (semantic_loss: 0.01675, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19000 
Train Epoch: 26 [891/1000 28512/32000 (89%)] Loss: 1.96273 (semantic_loss: 0.01428, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19070 
Train Epoch: 26 [896/1000 28672/32000 (90%)] Loss: 1.95914 (semantic_loss: 0.01265, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19287 
Train Epoch: 26 [901/1000 28832/32000 (90%)] Loss: 1.96213 (semantic_loss: 0.01564, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18879 
Train Epoch: 26 [906/1000 28992/32000 (91%)] Loss: 1.96141 (semantic_loss: 0.01395, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18671 
Train Epoch: 26 [911/1000 29152/32000 (91%)] Loss: 1.96236 (semantic_loss: 0.01488, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18718 
Train Epoch: 26 [916/1000 29312/32000 (92%)] Loss: 1.96334 (semantic_loss: 0.01587, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19022 
Train Epoch: 26 [921/1000 29472/32000 (92%)] Loss: 1.96137 (semantic_loss: 0.01585, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18924 
Train Epoch: 26 [926/1000 29632/32000 (93%)] Loss: 1.96097 (semantic_loss: 0.01350, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19067 
Train Epoch: 26 [931/1000 29792/32000 (93%)] Loss: 1.96214 (semantic_loss: 0.01369, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19214 
Train Epoch: 26 [936/1000 29952/32000 (94%)] Loss: 1.96146 (semantic_loss: 0.01497, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20556 
Train Epoch: 26 [941/1000 30112/32000 (94%)] Loss: 1.96287 (semantic_loss: 0.01541, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22079 
Train Epoch: 26 [946/1000 30272/32000 (95%)] Loss: 1.96640 (semantic_loss: 0.01990, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21836 
Train Epoch: 26 [951/1000 30432/32000 (95%)] Loss: 1.96239 (semantic_loss: 0.01394, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19183 
Train Epoch: 26 [956/1000 30592/32000 (96%)] Loss: 1.96484 (semantic_loss: 0.01834, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19495 
Train Epoch: 26 [961/1000 30752/32000 (96%)] Loss: 1.96149 (semantic_loss: 0.01304, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19508 
Train Epoch: 26 [966/1000 30912/32000 (97%)] Loss: 1.96134 (semantic_loss: 0.01485, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20698 
Train Epoch: 26 [971/1000 31072/32000 (97%)] Loss: 1.96600 (semantic_loss: 0.01853, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20593 
Train Epoch: 26 [976/1000 31232/32000 (98%)] Loss: 1.96535 (semantic_loss: 0.01691, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20016 
Train Epoch: 26 [981/1000 31392/32000 (98%)] Loss: 1.96140 (semantic_loss: 0.01393, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18645 
Train Epoch: 26 [986/1000 31552/32000 (99%)] Loss: 1.96327 (semantic_loss: 0.01580, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18969 
Train Epoch: 26 [991/1000 31712/32000 (99%)] Loss: 1.95863 (semantic_loss: 0.01312, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18849 
Train Epoch: 26 [996/1000 31872/32000 (100%)] Loss: 1.96246 (semantic_loss: 0.01401, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19486 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch26.pth ...
Done in 4.066s
removing stale ckpt [epoch 25] [took 0.00s]
 epoch          : 26
 loss           : 1.9624108568429948
 learning_rate  : 3.5894899384592657e-06
 n_samples      : 832000
 n_steps        : 26000
 ActivityNet_val1_test/t2v_metrics/R1: 11.856823266219239
 ActivityNet_val1_test/t2v_metrics/R5: 39.08887533048607
 ActivityNet_val1_test/t2v_metrics/R10: 55.07423225544031
 ActivityNet_val1_test/t2v_metrics/R50: 85.25523693308928
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 58.657616432784216
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.443537762782434
 ActivityNet_val1_test/v2t_metrics/R1: 12.324588163514338
 ActivityNet_val1_test/v2t_metrics/R5: 38.946512100874514
 ActivityNet_val1_test/v2t_metrics/R10: 56.619890176937155
 ActivityNet_val1_test/v2t_metrics/R50: 85.64165141346349
 ActivityNet_val1_test/v2t_metrics/MedR: 8.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 60.216900549115316
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 30.065608836367073
 mnt_best       : 29.49962176238898
 not_improved_count: 2
Train Epoch: 27 [1/1000 32/32000 (0%)] Loss: 1.96205 (semantic_loss: 0.01458, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=23.86374 
Train Epoch: 27 [6/1000 192/32000 (1%)] Loss: 1.95909 (semantic_loss: 0.01358, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18619 
Train Epoch: 27 [11/1000 352/32000 (1%)] Loss: 1.96543 (semantic_loss: 0.01796, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20141 
Train Epoch: 27 [16/1000 512/32000 (2%)] Loss: 1.95988 (semantic_loss: 0.01338, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22831 
Train Epoch: 27 [21/1000 672/32000 (2%)] Loss: 1.95978 (semantic_loss: 0.01231, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18356 
Train Epoch: 27 [26/1000 832/32000 (3%)] Loss: 1.96305 (semantic_loss: 0.01461, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18400 
Train Epoch: 27 [31/1000 992/32000 (3%)] Loss: 1.96197 (semantic_loss: 0.01450, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18396 
Train Epoch: 27 [36/1000 1152/32000 (4%)] Loss: 1.96179 (semantic_loss: 0.01528, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18427 
Train Epoch: 27 [41/1000 1312/32000 (4%)] Loss: 1.96126 (semantic_loss: 0.01379, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18851 
Train Epoch: 27 [46/1000 1472/32000 (5%)] Loss: 1.96326 (semantic_loss: 0.01677, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18497 
Train Epoch: 27 [51/1000 1632/32000 (5%)] Loss: 1.96086 (semantic_loss: 0.01437, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18613 
Train Epoch: 27 [56/1000 1792/32000 (6%)] Loss: 1.96170 (semantic_loss: 0.01423, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23879 
Train Epoch: 27 [61/1000 1952/32000 (6%)] Loss: 1.96180 (semantic_loss: 0.01433, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19793 
Train Epoch: 27 [66/1000 2112/32000 (7%)] Loss: 1.96213 (semantic_loss: 0.01661, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.22967 
Train Epoch: 27 [71/1000 2272/32000 (7%)] Loss: 1.96273 (semantic_loss: 0.01526, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21704 
Train Epoch: 27 [76/1000 2432/32000 (8%)] Loss: 1.96647 (semantic_loss: 0.01899, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22601 
Train Epoch: 27 [81/1000 2592/32000 (8%)] Loss: 1.96604 (semantic_loss: 0.01758, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.96137 
Train Epoch: 27 [86/1000 2752/32000 (9%)] Loss: 1.96034 (semantic_loss: 0.01385, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20241 
Train Epoch: 27 [91/1000 2912/32000 (9%)] Loss: 1.96302 (semantic_loss: 0.01555, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19220 
Train Epoch: 27 [96/1000 3072/32000 (10%)] Loss: 1.95904 (semantic_loss: 0.01157, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21971 
Train Epoch: 27 [101/1000 3232/32000 (10%)] Loss: 1.96473 (semantic_loss: 0.01531, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.19489 
Train Epoch: 27 [106/1000 3392/32000 (11%)] Loss: 1.96082 (semantic_loss: 0.01433, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18472 
Train Epoch: 27 [111/1000 3552/32000 (11%)] Loss: 1.96024 (semantic_loss: 0.01376, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21465 
Train Epoch: 27 [116/1000 3712/32000 (12%)] Loss: 1.96077 (semantic_loss: 0.01427, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18565 
Train Epoch: 27 [121/1000 3872/32000 (12%)] Loss: 1.96592 (semantic_loss: 0.01748, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21026 
Train Epoch: 27 [126/1000 4032/32000 (13%)] Loss: 1.96462 (semantic_loss: 0.01812, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21094 
Train Epoch: 27 [131/1000 4192/32000 (13%)] Loss: 1.96328 (semantic_loss: 0.01581, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19428 
Train Epoch: 27 [136/1000 4352/32000 (14%)] Loss: 1.96088 (semantic_loss: 0.01341, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19128 
Train Epoch: 27 [141/1000 4512/32000 (14%)] Loss: 1.96231 (semantic_loss: 0.01484, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19998 
Train Epoch: 27 [146/1000 4672/32000 (15%)] Loss: 1.96553 (semantic_loss: 0.01805, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18885 
Train Epoch: 27 [151/1000 4832/32000 (15%)] Loss: 1.95920 (semantic_loss: 0.01271, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.25779 
Train Epoch: 27 [156/1000 4992/32000 (16%)] Loss: 1.96469 (semantic_loss: 0.01722, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18604 
Train Epoch: 27 [161/1000 5152/32000 (16%)] Loss: 1.96193 (semantic_loss: 0.01446, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20452 
Train Epoch: 27 [166/1000 5312/32000 (17%)] Loss: 1.96249 (semantic_loss: 0.01501, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22014 
Train Epoch: 27 [171/1000 5472/32000 (17%)] Loss: 1.96114 (semantic_loss: 0.01367, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18938 
Train Epoch: 27 [176/1000 5632/32000 (18%)] Loss: 1.96432 (semantic_loss: 0.01782, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19026 
Train Epoch: 27 [181/1000 5792/32000 (18%)] Loss: 1.96831 (semantic_loss: 0.01986, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18855 
Train Epoch: 27 [186/1000 5952/32000 (19%)] Loss: 1.96198 (semantic_loss: 0.01451, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18843 
Train Epoch: 27 [191/1000 6112/32000 (19%)] Loss: 1.95914 (semantic_loss: 0.01363, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19026 
Train Epoch: 27 [196/1000 6272/32000 (20%)] Loss: 1.96454 (semantic_loss: 0.01706, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18867 
Train Epoch: 27 [201/1000 6432/32000 (20%)] Loss: 1.96497 (semantic_loss: 0.01749, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18898 
Train Epoch: 27 [206/1000 6592/32000 (21%)] Loss: 1.96256 (semantic_loss: 0.01606, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19814 
Train Epoch: 27 [211/1000 6752/32000 (21%)] Loss: 1.96235 (semantic_loss: 0.01390, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18505 
Train Epoch: 27 [216/1000 6912/32000 (22%)] Loss: 1.96395 (semantic_loss: 0.01648, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20110 
Train Epoch: 27 [221/1000 7072/32000 (22%)] Loss: 1.96239 (semantic_loss: 0.01590, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21932 
Train Epoch: 27 [226/1000 7232/32000 (23%)] Loss: 1.96139 (semantic_loss: 0.01295, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22628 
Train Epoch: 27 [231/1000 7392/32000 (23%)] Loss: 1.96324 (semantic_loss: 0.01576, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.48194 
Train Epoch: 27 [236/1000 7552/32000 (24%)] Loss: 1.96117 (semantic_loss: 0.01468, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21236 
Train Epoch: 27 [241/1000 7712/32000 (24%)] Loss: 1.96276 (semantic_loss: 0.01725, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19129 
Train Epoch: 27 [246/1000 7872/32000 (25%)] Loss: 1.96073 (semantic_loss: 0.01326, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19753 
Train Epoch: 27 [251/1000 8032/32000 (25%)] Loss: 1.95981 (semantic_loss: 0.01234, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20854 
Train Epoch: 27 [256/1000 8192/32000 (26%)] Loss: 1.95993 (semantic_loss: 0.01344, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21088 
Train Epoch: 27 [261/1000 8352/32000 (26%)] Loss: 1.97135 (semantic_loss: 0.02388, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18857 
Train Epoch: 27 [266/1000 8512/32000 (27%)] Loss: 1.97106 (semantic_loss: 0.02456, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19022 
Train Epoch: 27 [271/1000 8672/32000 (27%)] Loss: 1.96502 (semantic_loss: 0.01658, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18816 
Train Epoch: 27 [276/1000 8832/32000 (28%)] Loss: 1.95991 (semantic_loss: 0.01341, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18852 
Train Epoch: 27 [281/1000 8992/32000 (28%)] Loss: 1.96408 (semantic_loss: 0.01564, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19505 
Train Epoch: 27 [286/1000 9152/32000 (29%)] Loss: 1.96196 (semantic_loss: 0.01449, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22099 
Train Epoch: 27 [291/1000 9312/32000 (29%)] Loss: 1.96132 (semantic_loss: 0.01483, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23164 
Train Epoch: 27 [296/1000 9472/32000 (30%)] Loss: 1.96320 (semantic_loss: 0.01573, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20841 
Train Epoch: 27 [301/1000 9632/32000 (30%)] Loss: 1.96910 (semantic_loss: 0.02163, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18750 
Train Epoch: 27 [306/1000 9792/32000 (31%)] Loss: 1.96302 (semantic_loss: 0.01555, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18783 
Train Epoch: 27 [311/1000 9952/32000 (31%)] Loss: 1.96430 (semantic_loss: 0.01683, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19925 
Train Epoch: 27 [316/1000 10112/32000 (32%)] Loss: 1.96576 (semantic_loss: 0.01828, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19262 
Train Epoch: 27 [321/1000 10272/32000 (32%)] Loss: 1.96471 (semantic_loss: 0.01626, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.27987 
Train Epoch: 27 [326/1000 10432/32000 (33%)] Loss: 1.96008 (semantic_loss: 0.01359, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18698 
Train Epoch: 27 [331/1000 10592/32000 (33%)] Loss: 1.96141 (semantic_loss: 0.01492, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21086 
Train Epoch: 27 [336/1000 10752/32000 (34%)] Loss: 1.96100 (semantic_loss: 0.01352, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20687 
Train Epoch: 27 [341/1000 10912/32000 (34%)] Loss: 1.96264 (semantic_loss: 0.01712, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18716 
Train Epoch: 27 [346/1000 11072/32000 (35%)] Loss: 1.96044 (semantic_loss: 0.01492, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18663 
Train Epoch: 27 [351/1000 11232/32000 (35%)] Loss: 1.95969 (semantic_loss: 0.01222, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18432 
Train Epoch: 27 [356/1000 11392/32000 (36%)] Loss: 1.96068 (semantic_loss: 0.01321, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19776 
Train Epoch: 27 [361/1000 11552/32000 (36%)] Loss: 1.96728 (semantic_loss: 0.01980, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20680 
Train Epoch: 27 [366/1000 11712/32000 (37%)] Loss: 1.96248 (semantic_loss: 0.01696, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18949 
Train Epoch: 27 [371/1000 11872/32000 (37%)] Loss: 1.96665 (semantic_loss: 0.02113, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18421 
Train Epoch: 27 [376/1000 12032/32000 (38%)] Loss: 1.95932 (semantic_loss: 0.01185, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24876 
Train Epoch: 27 [381/1000 12192/32000 (38%)] Loss: 1.96727 (semantic_loss: 0.02077, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20281 
Train Epoch: 27 [386/1000 12352/32000 (39%)] Loss: 1.96060 (semantic_loss: 0.01215, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.25776 
Train Epoch: 27 [391/1000 12512/32000 (39%)] Loss: 1.95954 (semantic_loss: 0.01207, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21715 
Train Epoch: 27 [396/1000 12672/32000 (40%)] Loss: 1.96489 (semantic_loss: 0.01840, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20056 
Train Epoch: 27 [401/1000 12832/32000 (40%)] Loss: 1.96438 (semantic_loss: 0.01691, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=1.02017 
Train Epoch: 27 [406/1000 12992/32000 (41%)] Loss: 1.96102 (semantic_loss: 0.01550, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19792 
Train Epoch: 27 [411/1000 13152/32000 (41%)] Loss: 1.96297 (semantic_loss: 0.01354, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.19874 
Train Epoch: 27 [416/1000 13312/32000 (42%)] Loss: 1.96422 (semantic_loss: 0.01675, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19108 
Train Epoch: 27 [421/1000 13472/32000 (42%)] Loss: 1.96401 (semantic_loss: 0.01556, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18928 
Train Epoch: 27 [426/1000 13632/32000 (43%)] Loss: 1.96437 (semantic_loss: 0.01593, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18611 
Train Epoch: 27 [431/1000 13792/32000 (43%)] Loss: 1.96194 (semantic_loss: 0.01544, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19016 
Train Epoch: 27 [436/1000 13952/32000 (44%)] Loss: 1.96167 (semantic_loss: 0.01517, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18325 
Train Epoch: 27 [441/1000 14112/32000 (44%)] Loss: 1.96142 (semantic_loss: 0.01395, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20067 
Train Epoch: 27 [446/1000 14272/32000 (45%)] Loss: 1.96463 (semantic_loss: 0.01715, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19372 
Train Epoch: 27 [451/1000 14432/32000 (45%)] Loss: 1.95945 (semantic_loss: 0.01295, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19542 
Train Epoch: 27 [456/1000 14592/32000 (46%)] Loss: 1.96277 (semantic_loss: 0.01530, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19741 
Train Epoch: 27 [461/1000 14752/32000 (46%)] Loss: 1.96210 (semantic_loss: 0.01365, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19599 
Train Epoch: 27 [466/1000 14912/32000 (47%)] Loss: 1.96166 (semantic_loss: 0.01517, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18481 
Train Epoch: 27 [471/1000 15072/32000 (47%)] Loss: 1.96264 (semantic_loss: 0.01615, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23307 
Train Epoch: 27 [476/1000 15232/32000 (48%)] Loss: 1.96254 (semantic_loss: 0.01605, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18734 
Train Epoch: 27 [481/1000 15392/32000 (48%)] Loss: 1.96491 (semantic_loss: 0.01647, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18237 
Train Epoch: 27 [486/1000 15552/32000 (49%)] Loss: 1.96294 (semantic_loss: 0.01448, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22549 
Train Epoch: 27 [491/1000 15712/32000 (49%)] Loss: 1.96070 (semantic_loss: 0.01323, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18294 
Train Epoch: 27 [496/1000 15872/32000 (50%)] Loss: 1.96118 (semantic_loss: 0.01371, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18812 
Train Epoch: 27 [501/1000 16032/32000 (50%)] Loss: 1.96462 (semantic_loss: 0.01812, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18486 
Train Epoch: 27 [506/1000 16192/32000 (51%)] Loss: 1.96131 (semantic_loss: 0.01384, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18388 
Train Epoch: 27 [511/1000 16352/32000 (51%)] Loss: 1.96038 (semantic_loss: 0.01486, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18271 
Train Epoch: 27 [516/1000 16512/32000 (52%)] Loss: 1.96147 (semantic_loss: 0.01596, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18944 
Train Epoch: 27 [521/1000 16672/32000 (52%)] Loss: 1.96593 (semantic_loss: 0.01747, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18256 
Train Epoch: 27 [526/1000 16832/32000 (53%)] Loss: 1.96078 (semantic_loss: 0.01429, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20891 
Train Epoch: 27 [531/1000 16992/32000 (53%)] Loss: 1.96254 (semantic_loss: 0.01507, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18960 
Train Epoch: 27 [536/1000 17152/32000 (54%)] Loss: 1.96253 (semantic_loss: 0.01507, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22928 
Train Epoch: 27 [541/1000 17312/32000 (54%)] Loss: 1.96314 (semantic_loss: 0.01567, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21635 
Train Epoch: 27 [546/1000 17472/32000 (55%)] Loss: 1.96385 (semantic_loss: 0.01639, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24700 
Train Epoch: 27 [551/1000 17632/32000 (55%)] Loss: 1.96307 (semantic_loss: 0.01463, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.48657 
Train Epoch: 27 [556/1000 17792/32000 (56%)] Loss: 1.95882 (semantic_loss: 0.01135, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19491 
Train Epoch: 27 [561/1000 17952/32000 (56%)] Loss: 1.96198 (semantic_loss: 0.01451, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19588 
Train Epoch: 27 [566/1000 18112/32000 (57%)] Loss: 1.96030 (semantic_loss: 0.01380, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19079 
Train Epoch: 27 [571/1000 18272/32000 (57%)] Loss: 1.95991 (semantic_loss: 0.01342, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19352 
Train Epoch: 27 [576/1000 18432/32000 (58%)] Loss: 1.96326 (semantic_loss: 0.01579, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18990 
Train Epoch: 27 [581/1000 18592/32000 (58%)] Loss: 1.96282 (semantic_loss: 0.01535, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18668 
Train Epoch: 27 [586/1000 18752/32000 (59%)] Loss: 1.95997 (semantic_loss: 0.01250, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18494 
Train Epoch: 27 [591/1000 18912/32000 (59%)] Loss: 1.96786 (semantic_loss: 0.01941, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19699 
Train Epoch: 27 [596/1000 19072/32000 (60%)] Loss: 1.95970 (semantic_loss: 0.01321, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19687 
Train Epoch: 27 [601/1000 19232/32000 (60%)] Loss: 1.96337 (semantic_loss: 0.01688, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20127 
Train Epoch: 27 [606/1000 19392/32000 (61%)] Loss: 1.96185 (semantic_loss: 0.01341, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19522 
Train Epoch: 27 [611/1000 19552/32000 (61%)] Loss: 1.96042 (semantic_loss: 0.01295, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19250 
Train Epoch: 27 [616/1000 19712/32000 (62%)] Loss: 1.96361 (semantic_loss: 0.01516, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.78579 
Train Epoch: 27 [621/1000 19872/32000 (62%)] Loss: 1.96407 (semantic_loss: 0.01660, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18616 
Train Epoch: 27 [626/1000 20032/32000 (63%)] Loss: 1.96546 (semantic_loss: 0.01896, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19486 
Train Epoch: 27 [631/1000 20192/32000 (63%)] Loss: 1.96202 (semantic_loss: 0.01455, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20754 
Train Epoch: 27 [636/1000 20352/32000 (64%)] Loss: 1.96470 (semantic_loss: 0.01625, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18666 
Train Epoch: 27 [641/1000 20512/32000 (64%)] Loss: 1.95872 (semantic_loss: 0.01223, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19526 
Train Epoch: 27 [646/1000 20672/32000 (65%)] Loss: 1.96198 (semantic_loss: 0.01549, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18763 
Train Epoch: 27 [651/1000 20832/32000 (65%)] Loss: 1.96194 (semantic_loss: 0.01544, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18709 
Train Epoch: 27 [656/1000 20992/32000 (66%)] Loss: 1.96143 (semantic_loss: 0.01395, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18919 
Train Epoch: 27 [661/1000 21152/32000 (66%)] Loss: 1.96161 (semantic_loss: 0.01511, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18666 
Train Epoch: 27 [666/1000 21312/32000 (67%)] Loss: 1.96512 (semantic_loss: 0.01764, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21172 
Train Epoch: 27 [671/1000 21472/32000 (67%)] Loss: 1.96328 (semantic_loss: 0.01679, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18571 
Train Epoch: 27 [676/1000 21632/32000 (68%)] Loss: 1.96135 (semantic_loss: 0.01388, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18546 
Train Epoch: 27 [681/1000 21792/32000 (68%)] Loss: 1.96273 (semantic_loss: 0.01526, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18705 
Train Epoch: 27 [686/1000 21952/32000 (69%)] Loss: 1.96047 (semantic_loss: 0.01397, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19129 
Train Epoch: 27 [691/1000 22112/32000 (69%)] Loss: 1.96493 (semantic_loss: 0.01843, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20127 
Train Epoch: 27 [696/1000 22272/32000 (70%)] Loss: 1.95855 (semantic_loss: 0.01108, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20951 
Train Epoch: 27 [701/1000 22432/32000 (70%)] Loss: 1.95990 (semantic_loss: 0.01243, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23493 
Train Epoch: 27 [706/1000 22592/32000 (71%)] Loss: 1.95964 (semantic_loss: 0.01217, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23487 
Train Epoch: 27 [711/1000 22752/32000 (71%)] Loss: 1.96037 (semantic_loss: 0.01290, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23055 
Train Epoch: 27 [716/1000 22912/32000 (72%)] Loss: 1.95995 (semantic_loss: 0.01248, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21806 
Train Epoch: 27 [721/1000 23072/32000 (72%)] Loss: 1.95914 (semantic_loss: 0.01362, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.21129 
Train Epoch: 27 [726/1000 23232/32000 (73%)] Loss: 1.96348 (semantic_loss: 0.01504, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22216 
Train Epoch: 27 [731/1000 23392/32000 (73%)] Loss: 1.95893 (semantic_loss: 0.01243, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19098 
Train Epoch: 27 [736/1000 23552/32000 (74%)] Loss: 1.96416 (semantic_loss: 0.01669, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19392 
Train Epoch: 27 [741/1000 23712/32000 (74%)] Loss: 1.96069 (semantic_loss: 0.01420, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20560 
Train Epoch: 27 [746/1000 23872/32000 (75%)] Loss: 1.96460 (semantic_loss: 0.01713, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18955 
Train Epoch: 27 [751/1000 24032/32000 (75%)] Loss: 1.96148 (semantic_loss: 0.01401, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19076 
Train Epoch: 27 [756/1000 24192/32000 (76%)] Loss: 1.96062 (semantic_loss: 0.01413, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19819 
Train Epoch: 27 [761/1000 24352/32000 (76%)] Loss: 1.96207 (semantic_loss: 0.01460, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20081 
Train Epoch: 27 [766/1000 24512/32000 (77%)] Loss: 1.95961 (semantic_loss: 0.01312, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19475 
Train Epoch: 27 [771/1000 24672/32000 (77%)] Loss: 1.95902 (semantic_loss: 0.01253, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19191 
Train Epoch: 27 [776/1000 24832/32000 (78%)] Loss: 1.96143 (semantic_loss: 0.01494, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18896 
Train Epoch: 27 [781/1000 24992/32000 (78%)] Loss: 1.96193 (semantic_loss: 0.01348, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18625 
Train Epoch: 27 [786/1000 25152/32000 (79%)] Loss: 1.96175 (semantic_loss: 0.01526, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18742 
Train Epoch: 27 [791/1000 25312/32000 (79%)] Loss: 1.96210 (semantic_loss: 0.01561, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18425 
Train Epoch: 27 [796/1000 25472/32000 (80%)] Loss: 1.96133 (semantic_loss: 0.01582, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18424 
Train Epoch: 27 [801/1000 25632/32000 (80%)] Loss: 1.96626 (semantic_loss: 0.01879, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19470 
Train Epoch: 27 [806/1000 25792/32000 (81%)] Loss: 1.96266 (semantic_loss: 0.01519, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24409 
Train Epoch: 27 [811/1000 25952/32000 (81%)] Loss: 1.96066 (semantic_loss: 0.01319, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18847 
Train Epoch: 27 [816/1000 26112/32000 (82%)] Loss: 1.96341 (semantic_loss: 0.01691, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18688 
Train Epoch: 27 [821/1000 26272/32000 (82%)] Loss: 1.96304 (semantic_loss: 0.01557, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20772 
Train Epoch: 27 [826/1000 26432/32000 (83%)] Loss: 1.96106 (semantic_loss: 0.01457, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18986 
Train Epoch: 27 [831/1000 26592/32000 (83%)] Loss: 1.96517 (semantic_loss: 0.01575, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.19011 
Train Epoch: 27 [836/1000 26752/32000 (84%)] Loss: 1.96315 (semantic_loss: 0.01666, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19228 
Train Epoch: 27 [841/1000 26912/32000 (84%)] Loss: 1.96457 (semantic_loss: 0.01612, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19914 
Train Epoch: 27 [846/1000 27072/32000 (85%)] Loss: 1.96293 (semantic_loss: 0.01546, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21595 
Train Epoch: 27 [851/1000 27232/32000 (85%)] Loss: 1.96210 (semantic_loss: 0.01463, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21631 
Train Epoch: 27 [856/1000 27392/32000 (86%)] Loss: 1.96591 (semantic_loss: 0.01746, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21801 
Train Epoch: 27 [861/1000 27552/32000 (86%)] Loss: 1.96223 (semantic_loss: 0.01475, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21214 
Train Epoch: 27 [866/1000 27712/32000 (87%)] Loss: 1.96311 (semantic_loss: 0.01564, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21232 
Train Epoch: 27 [871/1000 27872/32000 (87%)] Loss: 1.96361 (semantic_loss: 0.01516, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.95931 
Train Epoch: 27 [876/1000 28032/32000 (88%)] Loss: 1.95956 (semantic_loss: 0.01306, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22651 
Train Epoch: 27 [881/1000 28192/32000 (88%)] Loss: 1.96059 (semantic_loss: 0.01312, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19326 
Train Epoch: 27 [886/1000 28352/32000 (89%)] Loss: 1.96093 (semantic_loss: 0.01541, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19117 
Train Epoch: 27 [891/1000 28512/32000 (89%)] Loss: 1.96266 (semantic_loss: 0.01519, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18951 
Train Epoch: 27 [896/1000 28672/32000 (90%)] Loss: 1.96270 (semantic_loss: 0.01523, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18944 
Train Epoch: 27 [901/1000 28832/32000 (90%)] Loss: 1.96525 (semantic_loss: 0.01679, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19007 
Train Epoch: 27 [906/1000 28992/32000 (91%)] Loss: 1.96188 (semantic_loss: 0.01539, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18625 
Train Epoch: 27 [911/1000 29152/32000 (91%)] Loss: 1.96172 (semantic_loss: 0.01328, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20026 
Train Epoch: 27 [916/1000 29312/32000 (92%)] Loss: 1.96071 (semantic_loss: 0.01324, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19611 
Train Epoch: 27 [921/1000 29472/32000 (92%)] Loss: 1.96601 (semantic_loss: 0.01854, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19280 
Train Epoch: 27 [926/1000 29632/32000 (93%)] Loss: 1.96204 (semantic_loss: 0.01457, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19085 
Train Epoch: 27 [931/1000 29792/32000 (93%)] Loss: 1.96420 (semantic_loss: 0.01576, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18719 
Train Epoch: 27 [936/1000 29952/32000 (94%)] Loss: 1.96139 (semantic_loss: 0.01489, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.98496 
Train Epoch: 27 [941/1000 30112/32000 (94%)] Loss: 1.96260 (semantic_loss: 0.01513, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18928 
Train Epoch: 27 [946/1000 30272/32000 (95%)] Loss: 1.96223 (semantic_loss: 0.01573, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19444 
Train Epoch: 27 [951/1000 30432/32000 (95%)] Loss: 1.96166 (semantic_loss: 0.01419, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19004 
Train Epoch: 27 [956/1000 30592/32000 (96%)] Loss: 1.96705 (semantic_loss: 0.01959, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19040 
Train Epoch: 27 [961/1000 30752/32000 (96%)] Loss: 1.96412 (semantic_loss: 0.01665, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18914 
Train Epoch: 27 [966/1000 30912/32000 (97%)] Loss: 1.96396 (semantic_loss: 0.01649, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18723 
Train Epoch: 27 [971/1000 31072/32000 (97%)] Loss: 1.96512 (semantic_loss: 0.01765, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18684 
Train Epoch: 27 [976/1000 31232/32000 (98%)] Loss: 1.96350 (semantic_loss: 0.01603, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18777 
Train Epoch: 27 [981/1000 31392/32000 (98%)] Loss: 1.96126 (semantic_loss: 0.01476, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19005 
Train Epoch: 27 [986/1000 31552/32000 (99%)] Loss: 1.96120 (semantic_loss: 0.01276, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18630 
Train Epoch: 27 [991/1000 31712/32000 (99%)] Loss: 1.96165 (semantic_loss: 0.01417, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18384 
Train Epoch: 27 [996/1000 31872/32000 (100%)] Loss: 1.95912 (semantic_loss: 0.01262, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18753 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch27.pth ...
Done in 4.418s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch27.pth ...
Done in 8.384s
removing stale ckpt [epoch 26] [took 0.00s]
 epoch          : 27
 loss           : 1.9625407308340073
 learning_rate  : 3.230540944613339e-06
 n_samples      : 864000
 n_steps        : 27000
 ActivityNet_val1_test/t2v_metrics/R1: 11.978848891600569
 ActivityNet_val1_test/t2v_metrics/R5: 38.53976001627009
 ActivityNet_val1_test/t2v_metrics/R10: 55.847061216188735
 ActivityNet_val1_test/t2v_metrics/R50: 85.49928818385195
 ActivityNet_val1_test/t2v_metrics/MedR: 8.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 59.270896888346556
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.542107379154057
 ActivityNet_val1_test/v2t_metrics/R1: 12.446613788895668
 ActivityNet_val1_test/v2t_metrics/R5: 38.92617449664429
 ActivityNet_val1_test/v2t_metrics/R10: 56.05043725849095
 ActivityNet_val1_test/v2t_metrics/R50: 85.01118568232663
 ActivityNet_val1_test/v2t_metrics/MedR: 8.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 61.414582062233066
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 30.05780859979709
 mnt_best       : 29.542107379154057
 not_improved_count: 0
Train Epoch: 28 [1/1000 32/32000 (0%)] Loss: 1.96125 (semantic_loss: 0.01379, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=24.26282 
Train Epoch: 28 [6/1000 192/32000 (1%)] Loss: 1.96230 (semantic_loss: 0.01484, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18871 
Train Epoch: 28 [11/1000 352/32000 (1%)] Loss: 1.96110 (semantic_loss: 0.01363, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18824 
Train Epoch: 28 [16/1000 512/32000 (2%)] Loss: 1.96840 (semantic_loss: 0.02094, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.26326 
Train Epoch: 28 [21/1000 672/32000 (2%)] Loss: 1.96114 (semantic_loss: 0.01367, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18727 
Train Epoch: 28 [26/1000 832/32000 (3%)] Loss: 1.96381 (semantic_loss: 0.01537, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19170 
Train Epoch: 28 [31/1000 992/32000 (3%)] Loss: 1.95852 (semantic_loss: 0.01202, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.58165 
Train Epoch: 28 [36/1000 1152/32000 (4%)] Loss: 1.96016 (semantic_loss: 0.01269, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18901 
Train Epoch: 28 [41/1000 1312/32000 (4%)] Loss: 1.96333 (semantic_loss: 0.01586, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18697 
Train Epoch: 28 [46/1000 1472/32000 (5%)] Loss: 1.96189 (semantic_loss: 0.01442, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19577 
Train Epoch: 28 [51/1000 1632/32000 (5%)] Loss: 1.96642 (semantic_loss: 0.01895, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19659 
Train Epoch: 28 [56/1000 1792/32000 (6%)] Loss: 1.96031 (semantic_loss: 0.01284, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18562 
Train Epoch: 28 [61/1000 1952/32000 (6%)] Loss: 1.96136 (semantic_loss: 0.01389, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18726 
Train Epoch: 28 [66/1000 2112/32000 (7%)] Loss: 1.96148 (semantic_loss: 0.01303, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18957 
Train Epoch: 28 [71/1000 2272/32000 (7%)] Loss: 1.95956 (semantic_loss: 0.01209, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18853 
Train Epoch: 28 [76/1000 2432/32000 (8%)] Loss: 1.96118 (semantic_loss: 0.01273, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18576 
Train Epoch: 28 [81/1000 2592/32000 (8%)] Loss: 1.96452 (semantic_loss: 0.01705, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.39789 
Train Epoch: 28 [86/1000 2752/32000 (9%)] Loss: 1.96117 (semantic_loss: 0.01272, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19521 
Train Epoch: 28 [91/1000 2912/32000 (9%)] Loss: 1.96274 (semantic_loss: 0.01526, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.43701 
Train Epoch: 28 [96/1000 3072/32000 (10%)] Loss: 1.96085 (semantic_loss: 0.01338, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19543 
Train Epoch: 28 [101/1000 3232/32000 (10%)] Loss: 1.96022 (semantic_loss: 0.01373, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.25051 
Train Epoch: 28 [106/1000 3392/32000 (11%)] Loss: 1.96003 (semantic_loss: 0.01256, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22905 
Train Epoch: 28 [111/1000 3552/32000 (11%)] Loss: 1.96592 (semantic_loss: 0.01846, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23184 
Train Epoch: 28 [116/1000 3712/32000 (12%)] Loss: 1.96375 (semantic_loss: 0.01725, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21437 
Train Epoch: 28 [121/1000 3872/32000 (12%)] Loss: 1.95938 (semantic_loss: 0.01288, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19710 
Train Epoch: 28 [126/1000 4032/32000 (13%)] Loss: 1.96053 (semantic_loss: 0.01404, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22145 
Train Epoch: 28 [131/1000 4192/32000 (13%)] Loss: 1.96065 (semantic_loss: 0.01317, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.30832 
Train Epoch: 28 [136/1000 4352/32000 (14%)] Loss: 1.96024 (semantic_loss: 0.01277, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.27523 
Train Epoch: 28 [141/1000 4512/32000 (14%)] Loss: 1.96244 (semantic_loss: 0.01399, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19220 
Train Epoch: 28 [146/1000 4672/32000 (15%)] Loss: 1.95970 (semantic_loss: 0.01320, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18788 
Train Epoch: 28 [151/1000 4832/32000 (15%)] Loss: 1.96545 (semantic_loss: 0.01798, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18704 
Train Epoch: 28 [156/1000 4992/32000 (16%)] Loss: 1.96209 (semantic_loss: 0.01559, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18797 
Train Epoch: 28 [161/1000 5152/32000 (16%)] Loss: 1.95938 (semantic_loss: 0.01289, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18911 
Train Epoch: 28 [166/1000 5312/32000 (17%)] Loss: 1.96245 (semantic_loss: 0.01595, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19411 
Train Epoch: 28 [171/1000 5472/32000 (17%)] Loss: 1.96248 (semantic_loss: 0.01599, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20810 
Train Epoch: 28 [176/1000 5632/32000 (18%)] Loss: 1.96172 (semantic_loss: 0.01425, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19865 
Train Epoch: 28 [181/1000 5792/32000 (18%)] Loss: 1.96149 (semantic_loss: 0.01402, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18757 
Train Epoch: 28 [186/1000 5952/32000 (19%)] Loss: 1.96126 (semantic_loss: 0.01476, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19437 
Train Epoch: 28 [191/1000 6112/32000 (19%)] Loss: 1.96541 (semantic_loss: 0.01696, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18778 
Train Epoch: 28 [196/1000 6272/32000 (20%)] Loss: 1.96090 (semantic_loss: 0.01440, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19029 
Train Epoch: 28 [201/1000 6432/32000 (20%)] Loss: 1.96277 (semantic_loss: 0.01529, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24062 
Train Epoch: 28 [206/1000 6592/32000 (21%)] Loss: 1.96047 (semantic_loss: 0.01300, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20187 
Train Epoch: 28 [211/1000 6752/32000 (21%)] Loss: 1.96154 (semantic_loss: 0.01407, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18565 
Train Epoch: 28 [216/1000 6912/32000 (22%)] Loss: 1.96201 (semantic_loss: 0.01551, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18479 
Train Epoch: 28 [221/1000 7072/32000 (22%)] Loss: 1.96214 (semantic_loss: 0.01467, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19730 
Train Epoch: 28 [226/1000 7232/32000 (23%)] Loss: 1.96606 (semantic_loss: 0.01858, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18561 
Train Epoch: 28 [231/1000 7392/32000 (23%)] Loss: 1.96294 (semantic_loss: 0.01644, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19645 
Train Epoch: 28 [236/1000 7552/32000 (24%)] Loss: 1.96107 (semantic_loss: 0.01361, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18796 
Train Epoch: 28 [241/1000 7712/32000 (24%)] Loss: 1.96168 (semantic_loss: 0.01324, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21194 
Train Epoch: 28 [246/1000 7872/32000 (25%)] Loss: 1.96165 (semantic_loss: 0.01418, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18626 
Train Epoch: 28 [251/1000 8032/32000 (25%)] Loss: 1.96728 (semantic_loss: 0.01981, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21597 
Train Epoch: 28 [256/1000 8192/32000 (26%)] Loss: 1.96496 (semantic_loss: 0.01749, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22099 
Train Epoch: 28 [261/1000 8352/32000 (26%)] Loss: 1.96220 (semantic_loss: 0.01473, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24373 
Train Epoch: 28 [266/1000 8512/32000 (27%)] Loss: 1.96120 (semantic_loss: 0.01373, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22244 
Train Epoch: 28 [271/1000 8672/32000 (27%)] Loss: 1.96354 (semantic_loss: 0.01704, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22440 
Train Epoch: 28 [276/1000 8832/32000 (28%)] Loss: 1.96091 (semantic_loss: 0.01442, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20580 
Train Epoch: 28 [281/1000 8992/32000 (28%)] Loss: 1.96295 (semantic_loss: 0.01547, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20481 
Train Epoch: 28 [286/1000 9152/32000 (29%)] Loss: 1.96038 (semantic_loss: 0.01291, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19208 
Train Epoch: 28 [291/1000 9312/32000 (29%)] Loss: 1.95992 (semantic_loss: 0.01342, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19066 
Train Epoch: 28 [296/1000 9472/32000 (30%)] Loss: 1.96191 (semantic_loss: 0.01346, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.46936 
Train Epoch: 28 [301/1000 9632/32000 (30%)] Loss: 1.96168 (semantic_loss: 0.01420, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19001 
Train Epoch: 28 [306/1000 9792/32000 (31%)] Loss: 1.96561 (semantic_loss: 0.01814, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20880 
Train Epoch: 28 [311/1000 9952/32000 (31%)] Loss: 1.95972 (semantic_loss: 0.01323, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18946 
Train Epoch: 28 [316/1000 10112/32000 (32%)] Loss: 1.96270 (semantic_loss: 0.01523, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20296 
Train Epoch: 28 [321/1000 10272/32000 (32%)] Loss: 1.95877 (semantic_loss: 0.01130, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19118 
Train Epoch: 28 [326/1000 10432/32000 (33%)] Loss: 1.96063 (semantic_loss: 0.01413, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19868 
Train Epoch: 28 [331/1000 10592/32000 (33%)] Loss: 1.96799 (semantic_loss: 0.02051, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19136 
Train Epoch: 28 [336/1000 10752/32000 (34%)] Loss: 1.96014 (semantic_loss: 0.01266, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25219 
Train Epoch: 28 [341/1000 10912/32000 (34%)] Loss: 1.96331 (semantic_loss: 0.01584, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18817 
Train Epoch: 28 [346/1000 11072/32000 (35%)] Loss: 1.97123 (semantic_loss: 0.02181, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.18914 
Train Epoch: 28 [351/1000 11232/32000 (35%)] Loss: 1.96629 (semantic_loss: 0.01784, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.45589 
Train Epoch: 28 [356/1000 11392/32000 (36%)] Loss: 1.96322 (semantic_loss: 0.01673, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18858 
Train Epoch: 28 [361/1000 11552/32000 (36%)] Loss: 1.96387 (semantic_loss: 0.01640, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18713 
Train Epoch: 28 [366/1000 11712/32000 (37%)] Loss: 1.96104 (semantic_loss: 0.01455, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18611 
Train Epoch: 28 [371/1000 11872/32000 (37%)] Loss: 1.96259 (semantic_loss: 0.01512, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21107 
Train Epoch: 28 [376/1000 12032/32000 (38%)] Loss: 1.96107 (semantic_loss: 0.01458, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18797 
Train Epoch: 28 [381/1000 12192/32000 (38%)] Loss: 1.96329 (semantic_loss: 0.01484, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18979 
Train Epoch: 28 [386/1000 12352/32000 (39%)] Loss: 1.96215 (semantic_loss: 0.01468, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21699 
Train Epoch: 28 [391/1000 12512/32000 (39%)] Loss: 1.96173 (semantic_loss: 0.01329, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18886 
Train Epoch: 28 [396/1000 12672/32000 (40%)] Loss: 1.96231 (semantic_loss: 0.01581, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19124 
Train Epoch: 28 [401/1000 12832/32000 (40%)] Loss: 1.96716 (semantic_loss: 0.01968, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.38900 
Train Epoch: 28 [406/1000 12992/32000 (41%)] Loss: 1.95982 (semantic_loss: 0.01333, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19615 
Train Epoch: 28 [411/1000 13152/32000 (41%)] Loss: 1.95951 (semantic_loss: 0.01205, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.59121 
Train Epoch: 28 [416/1000 13312/32000 (42%)] Loss: 1.96365 (semantic_loss: 0.01618, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23676 
Train Epoch: 28 [421/1000 13472/32000 (42%)] Loss: 1.96117 (semantic_loss: 0.01369, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22894 
Train Epoch: 28 [426/1000 13632/32000 (43%)] Loss: 1.96207 (semantic_loss: 0.01558, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20472 
Train Epoch: 28 [431/1000 13792/32000 (43%)] Loss: 1.96066 (semantic_loss: 0.01417, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20663 
Train Epoch: 28 [436/1000 13952/32000 (44%)] Loss: 1.96147 (semantic_loss: 0.01400, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20443 
Train Epoch: 28 [441/1000 14112/32000 (44%)] Loss: 1.96094 (semantic_loss: 0.01543, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.20820 
Train Epoch: 28 [446/1000 14272/32000 (45%)] Loss: 1.96088 (semantic_loss: 0.01439, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21571 
Train Epoch: 28 [451/1000 14432/32000 (45%)] Loss: 1.96565 (semantic_loss: 0.01720, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.27800 
Train Epoch: 28 [456/1000 14592/32000 (46%)] Loss: 1.96143 (semantic_loss: 0.01397, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27197 
Train Epoch: 28 [461/1000 14752/32000 (46%)] Loss: 1.95970 (semantic_loss: 0.01419, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.20139 
Train Epoch: 28 [466/1000 14912/32000 (47%)] Loss: 1.96087 (semantic_loss: 0.01437, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18682 
Train Epoch: 28 [471/1000 15072/32000 (47%)] Loss: 1.96013 (semantic_loss: 0.01364, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19019 
Train Epoch: 28 [476/1000 15232/32000 (48%)] Loss: 1.95900 (semantic_loss: 0.01153, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19184 
Train Epoch: 28 [481/1000 15392/32000 (48%)] Loss: 1.96126 (semantic_loss: 0.01282, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19565 
Train Epoch: 28 [486/1000 15552/32000 (49%)] Loss: 1.96009 (semantic_loss: 0.01360, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18864 
Train Epoch: 28 [491/1000 15712/32000 (49%)] Loss: 1.96281 (semantic_loss: 0.01533, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18657 
Train Epoch: 28 [496/1000 15872/32000 (50%)] Loss: 1.96271 (semantic_loss: 0.01622, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20599 
Train Epoch: 28 [501/1000 16032/32000 (50%)] Loss: 1.95938 (semantic_loss: 0.01387, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18671 
Train Epoch: 28 [506/1000 16192/32000 (51%)] Loss: 1.96342 (semantic_loss: 0.01497, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20692 
Train Epoch: 28 [511/1000 16352/32000 (51%)] Loss: 1.95883 (semantic_loss: 0.01234, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18676 
Train Epoch: 28 [516/1000 16512/32000 (52%)] Loss: 1.95899 (semantic_loss: 0.01250, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19171 
Train Epoch: 28 [521/1000 16672/32000 (52%)] Loss: 1.96581 (semantic_loss: 0.01737, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23779 
Train Epoch: 28 [526/1000 16832/32000 (53%)] Loss: 1.96229 (semantic_loss: 0.01482, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20291 
Train Epoch: 28 [531/1000 16992/32000 (53%)] Loss: 1.96112 (semantic_loss: 0.01267, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19689 
Train Epoch: 28 [536/1000 17152/32000 (54%)] Loss: 1.96330 (semantic_loss: 0.01583, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19116 
Train Epoch: 28 [541/1000 17312/32000 (54%)] Loss: 1.96249 (semantic_loss: 0.01405, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18778 
Train Epoch: 28 [546/1000 17472/32000 (55%)] Loss: 1.96541 (semantic_loss: 0.01696, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18939 
Train Epoch: 28 [551/1000 17632/32000 (55%)] Loss: 1.95893 (semantic_loss: 0.01146, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18988 
Train Epoch: 28 [556/1000 17792/32000 (56%)] Loss: 1.96076 (semantic_loss: 0.01427, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20775 
Train Epoch: 28 [561/1000 17952/32000 (56%)] Loss: 1.96178 (semantic_loss: 0.01333, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20766 
Train Epoch: 28 [566/1000 18112/32000 (57%)] Loss: 1.96303 (semantic_loss: 0.01556, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19775 
Train Epoch: 28 [571/1000 18272/32000 (57%)] Loss: 1.95782 (semantic_loss: 0.01230, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.21081 
Train Epoch: 28 [576/1000 18432/32000 (58%)] Loss: 1.96149 (semantic_loss: 0.01499, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20613 
Train Epoch: 28 [581/1000 18592/32000 (58%)] Loss: 1.96721 (semantic_loss: 0.01876, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22074 
Train Epoch: 28 [586/1000 18752/32000 (59%)] Loss: 1.96159 (semantic_loss: 0.01510, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22892 
Train Epoch: 28 [591/1000 18912/32000 (59%)] Loss: 1.96021 (semantic_loss: 0.01275, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21445 
Train Epoch: 28 [596/1000 19072/32000 (60%)] Loss: 1.96113 (semantic_loss: 0.01464, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20882 
Train Epoch: 28 [601/1000 19232/32000 (60%)] Loss: 1.96287 (semantic_loss: 0.01540, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19580 
Train Epoch: 28 [606/1000 19392/32000 (61%)] Loss: 1.96311 (semantic_loss: 0.01466, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18876 
Train Epoch: 28 [611/1000 19552/32000 (61%)] Loss: 1.95935 (semantic_loss: 0.01285, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18695 
Train Epoch: 28 [616/1000 19712/32000 (62%)] Loss: 1.96332 (semantic_loss: 0.01585, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19582 
Train Epoch: 28 [621/1000 19872/32000 (62%)] Loss: 1.96088 (semantic_loss: 0.01439, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18717 
Train Epoch: 28 [626/1000 20032/32000 (63%)] Loss: 1.96468 (semantic_loss: 0.01623, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19281 
Train Epoch: 28 [631/1000 20192/32000 (63%)] Loss: 1.96483 (semantic_loss: 0.01638, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20076 
Train Epoch: 28 [636/1000 20352/32000 (64%)] Loss: 1.96193 (semantic_loss: 0.01446, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19996 
Train Epoch: 28 [641/1000 20512/32000 (64%)] Loss: 1.96136 (semantic_loss: 0.01389, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19320 
Train Epoch: 28 [646/1000 20672/32000 (65%)] Loss: 1.95894 (semantic_loss: 0.01245, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18908 
Train Epoch: 28 [651/1000 20832/32000 (65%)] Loss: 1.96261 (semantic_loss: 0.01612, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20244 
Train Epoch: 28 [656/1000 20992/32000 (66%)] Loss: 1.96033 (semantic_loss: 0.01286, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27353 
Train Epoch: 28 [661/1000 21152/32000 (66%)] Loss: 1.96462 (semantic_loss: 0.01715, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19406 
Train Epoch: 28 [666/1000 21312/32000 (67%)] Loss: 1.96065 (semantic_loss: 0.01416, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18889 
Train Epoch: 28 [671/1000 21472/32000 (67%)] Loss: 1.96128 (semantic_loss: 0.01381, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.44976 
Train Epoch: 28 [676/1000 21632/32000 (68%)] Loss: 1.96192 (semantic_loss: 0.01347, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20741 
Train Epoch: 28 [681/1000 21792/32000 (68%)] Loss: 1.96276 (semantic_loss: 0.01528, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18877 
Train Epoch: 28 [686/1000 21952/32000 (69%)] Loss: 1.95814 (semantic_loss: 0.01165, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18720 
Train Epoch: 28 [691/1000 22112/32000 (69%)] Loss: 1.96132 (semantic_loss: 0.01581, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18748 
Train Epoch: 28 [696/1000 22272/32000 (70%)] Loss: 1.96185 (semantic_loss: 0.01535, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20210 
Train Epoch: 28 [701/1000 22432/32000 (70%)] Loss: 1.96254 (semantic_loss: 0.01604, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19356 
Train Epoch: 28 [706/1000 22592/32000 (71%)] Loss: 1.95940 (semantic_loss: 0.01291, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18843 
Train Epoch: 28 [711/1000 22752/32000 (71%)] Loss: 1.96060 (semantic_loss: 0.01312, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18815 
Train Epoch: 28 [716/1000 22912/32000 (72%)] Loss: 1.96346 (semantic_loss: 0.01599, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19171 
Train Epoch: 28 [721/1000 23072/32000 (72%)] Loss: 1.96260 (semantic_loss: 0.01513, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.46717 
Train Epoch: 28 [726/1000 23232/32000 (73%)] Loss: 1.96411 (semantic_loss: 0.01664, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23104 
Train Epoch: 28 [731/1000 23392/32000 (73%)] Loss: 1.96159 (semantic_loss: 0.01412, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.59181 
Train Epoch: 28 [736/1000 23552/32000 (74%)] Loss: 1.96282 (semantic_loss: 0.01535, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19948 
Train Epoch: 28 [741/1000 23712/32000 (74%)] Loss: 1.95930 (semantic_loss: 0.01281, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21602 
Train Epoch: 28 [746/1000 23872/32000 (75%)] Loss: 1.96032 (semantic_loss: 0.01383, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20525 
Train Epoch: 28 [751/1000 24032/32000 (75%)] Loss: 1.96096 (semantic_loss: 0.01349, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20154 
Train Epoch: 28 [756/1000 24192/32000 (76%)] Loss: 1.96506 (semantic_loss: 0.01759, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20862 
Train Epoch: 28 [761/1000 24352/32000 (76%)] Loss: 1.96216 (semantic_loss: 0.01567, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19478 
Train Epoch: 28 [766/1000 24512/32000 (77%)] Loss: 1.96543 (semantic_loss: 0.01796, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22567 
Train Epoch: 28 [771/1000 24672/32000 (77%)] Loss: 1.96360 (semantic_loss: 0.01612, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26777 
Train Epoch: 28 [776/1000 24832/32000 (78%)] Loss: 1.96514 (semantic_loss: 0.01865, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.25456 
Train Epoch: 28 [781/1000 24992/32000 (78%)] Loss: 1.96328 (semantic_loss: 0.01581, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20931 
Train Epoch: 28 [786/1000 25152/32000 (79%)] Loss: 1.96052 (semantic_loss: 0.01403, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19183 
Train Epoch: 28 [791/1000 25312/32000 (79%)] Loss: 1.96310 (semantic_loss: 0.01563, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19445 
Train Epoch: 28 [796/1000 25472/32000 (80%)] Loss: 1.96484 (semantic_loss: 0.01639, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19302 
Train Epoch: 28 [801/1000 25632/32000 (80%)] Loss: 1.96530 (semantic_loss: 0.01783, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18942 
Train Epoch: 28 [806/1000 25792/32000 (81%)] Loss: 1.96076 (semantic_loss: 0.01329, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19331 
Train Epoch: 28 [811/1000 25952/32000 (81%)] Loss: 1.96160 (semantic_loss: 0.01413, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19237 
Train Epoch: 28 [816/1000 26112/32000 (82%)] Loss: 1.96481 (semantic_loss: 0.01636, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18913 
Train Epoch: 28 [821/1000 26272/32000 (82%)] Loss: 1.96242 (semantic_loss: 0.01397, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18601 
Train Epoch: 28 [826/1000 26432/32000 (83%)] Loss: 1.95920 (semantic_loss: 0.01270, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20629 
Train Epoch: 28 [831/1000 26592/32000 (83%)] Loss: 1.96092 (semantic_loss: 0.01345, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23657 
Train Epoch: 28 [836/1000 26752/32000 (84%)] Loss: 1.96225 (semantic_loss: 0.01478, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18856 
Train Epoch: 28 [841/1000 26912/32000 (84%)] Loss: 1.96322 (semantic_loss: 0.01575, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.26329 
Train Epoch: 28 [846/1000 27072/32000 (85%)] Loss: 1.96420 (semantic_loss: 0.01673, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20394 
Train Epoch: 28 [851/1000 27232/32000 (85%)] Loss: 1.96325 (semantic_loss: 0.01578, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18723 
Train Epoch: 28 [856/1000 27392/32000 (86%)] Loss: 1.96178 (semantic_loss: 0.01528, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20076 
Train Epoch: 28 [861/1000 27552/32000 (86%)] Loss: 1.96104 (semantic_loss: 0.01357, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19987 
Train Epoch: 28 [866/1000 27712/32000 (87%)] Loss: 1.96117 (semantic_loss: 0.01467, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19018 
Train Epoch: 28 [871/1000 27872/32000 (87%)] Loss: 1.96608 (semantic_loss: 0.01861, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18912 
Train Epoch: 28 [876/1000 28032/32000 (88%)] Loss: 1.96086 (semantic_loss: 0.01436, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18642 
Train Epoch: 28 [881/1000 28192/32000 (88%)] Loss: 1.96247 (semantic_loss: 0.01500, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19351 
Train Epoch: 28 [886/1000 28352/32000 (89%)] Loss: 1.95995 (semantic_loss: 0.01345, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20942 
Train Epoch: 28 [891/1000 28512/32000 (89%)] Loss: 1.96273 (semantic_loss: 0.01526, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20623 
Train Epoch: 28 [896/1000 28672/32000 (90%)] Loss: 1.96510 (semantic_loss: 0.01568, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.20659 
Train Epoch: 28 [901/1000 28832/32000 (90%)] Loss: 1.95973 (semantic_loss: 0.01421, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.21428 
Train Epoch: 28 [906/1000 28992/32000 (91%)] Loss: 1.96178 (semantic_loss: 0.01528, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19714 
Train Epoch: 28 [911/1000 29152/32000 (91%)] Loss: 1.96090 (semantic_loss: 0.01343, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19934 
Train Epoch: 28 [916/1000 29312/32000 (92%)] Loss: 1.97301 (semantic_loss: 0.02457, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20647 
Train Epoch: 28 [921/1000 29472/32000 (92%)] Loss: 1.96165 (semantic_loss: 0.01418, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20716 
Train Epoch: 28 [926/1000 29632/32000 (93%)] Loss: 1.96330 (semantic_loss: 0.01486, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18825 
Train Epoch: 28 [931/1000 29792/32000 (93%)] Loss: 1.96161 (semantic_loss: 0.01414, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18429 
Train Epoch: 28 [936/1000 29952/32000 (94%)] Loss: 1.96065 (semantic_loss: 0.01318, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18863 
Train Epoch: 28 [941/1000 30112/32000 (94%)] Loss: 1.96188 (semantic_loss: 0.01441, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18661 
Train Epoch: 28 [946/1000 30272/32000 (95%)] Loss: 1.96490 (semantic_loss: 0.01744, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19027 
Train Epoch: 28 [951/1000 30432/32000 (95%)] Loss: 1.96145 (semantic_loss: 0.01495, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19385 
Train Epoch: 28 [956/1000 30592/32000 (96%)] Loss: 1.96254 (semantic_loss: 0.01507, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19196 
Train Epoch: 28 [961/1000 30752/32000 (96%)] Loss: 1.96136 (semantic_loss: 0.01389, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19957 
Train Epoch: 28 [966/1000 30912/32000 (97%)] Loss: 1.96090 (semantic_loss: 0.01344, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18854 
Train Epoch: 28 [971/1000 31072/32000 (97%)] Loss: 1.96054 (semantic_loss: 0.01405, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19126 
Train Epoch: 28 [976/1000 31232/32000 (98%)] Loss: 1.96349 (semantic_loss: 0.01700, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.25221 
Train Epoch: 28 [981/1000 31392/32000 (98%)] Loss: 1.96393 (semantic_loss: 0.01645, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19855 
Train Epoch: 28 [986/1000 31552/32000 (99%)] Loss: 1.96395 (semantic_loss: 0.01550, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18601 
Train Epoch: 28 [991/1000 31712/32000 (99%)] Loss: 1.96420 (semantic_loss: 0.01575, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.47758 
Train Epoch: 28 [996/1000 31872/32000 (100%)] Loss: 1.96292 (semantic_loss: 0.01544, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19557 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch28.pth ...
Done in 9.970s
removing stale ckpt [epoch 27] [took 0.01s]
 epoch          : 28
 loss           : 1.9622690861225127
 learning_rate  : 2.907486850152005e-06
 n_samples      : 896000
 n_steps        : 28000
 ActivityNet_val1_test/t2v_metrics/R1: 11.917836078909904
 ActivityNet_val1_test/t2v_metrics/R5: 38.01098230628432
 ActivityNet_val1_test/t2v_metrics/R10: 55.257270693512304
 ActivityNet_val1_test/t2v_metrics/R50: 85.07219849501729
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 60.83851942241204
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.252656421867325
 ActivityNet_val1_test/v2t_metrics/R1: 12.914378686190767
 ActivityNet_val1_test/v2t_metrics/R5: 39.41427699816962
 ActivityNet_val1_test/v2t_metrics/R10: 55.623347569656296
 ActivityNet_val1_test/v2t_metrics/R50: 84.74679682733374
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 62.19371568029286
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 30.47858037090283
 mnt_best       : 29.542107379154057
 not_improved_count: 1
Train Epoch: 29 [1/1000 32/32000 (0%)] Loss: 1.96075 (semantic_loss: 0.01328, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=23.63121 
Train Epoch: 29 [6/1000 192/32000 (1%)] Loss: 1.96485 (semantic_loss: 0.01932, quant_loss: 1.94531, bit_balance_loss: 0.00022) batch_time=0.20616 
Train Epoch: 29 [11/1000 352/32000 (1%)] Loss: 1.96295 (semantic_loss: 0.01450, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21371 
Train Epoch: 29 [16/1000 512/32000 (2%)] Loss: 1.96198 (semantic_loss: 0.01450, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18956 
Train Epoch: 29 [21/1000 672/32000 (2%)] Loss: 1.96233 (semantic_loss: 0.01486, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.45294 
Train Epoch: 29 [26/1000 832/32000 (3%)] Loss: 1.96270 (semantic_loss: 0.01523, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19203 
Train Epoch: 29 [31/1000 992/32000 (3%)] Loss: 1.96226 (semantic_loss: 0.01382, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19311 
Train Epoch: 29 [36/1000 1152/32000 (4%)] Loss: 1.95883 (semantic_loss: 0.01234, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.48929 
Train Epoch: 29 [41/1000 1312/32000 (4%)] Loss: 1.96372 (semantic_loss: 0.01625, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20613 
Train Epoch: 29 [46/1000 1472/32000 (5%)] Loss: 1.96828 (semantic_loss: 0.02178, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18865 
Train Epoch: 29 [51/1000 1632/32000 (5%)] Loss: 1.95963 (semantic_loss: 0.01216, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18970 
Train Epoch: 29 [56/1000 1792/32000 (6%)] Loss: 1.95922 (semantic_loss: 0.01273, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18625 
Train Epoch: 29 [61/1000 1952/32000 (6%)] Loss: 1.95980 (semantic_loss: 0.01136, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19070 
Train Epoch: 29 [66/1000 2112/32000 (7%)] Loss: 1.96780 (semantic_loss: 0.02033, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18792 
Train Epoch: 29 [71/1000 2272/32000 (7%)] Loss: 1.96543 (semantic_loss: 0.01796, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24386 
Train Epoch: 29 [76/1000 2432/32000 (8%)] Loss: 1.96384 (semantic_loss: 0.01734, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18413 
Train Epoch: 29 [81/1000 2592/32000 (8%)] Loss: 1.96280 (semantic_loss: 0.01436, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19610 
Train Epoch: 29 [86/1000 2752/32000 (9%)] Loss: 1.96230 (semantic_loss: 0.01482, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19852 
Train Epoch: 29 [91/1000 2912/32000 (9%)] Loss: 1.96321 (semantic_loss: 0.01574, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18874 
Train Epoch: 29 [96/1000 3072/32000 (10%)] Loss: 1.96441 (semantic_loss: 0.01792, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19294 
Train Epoch: 29 [101/1000 3232/32000 (10%)] Loss: 1.96307 (semantic_loss: 0.01560, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21676 
Train Epoch: 29 [106/1000 3392/32000 (11%)] Loss: 1.96404 (semantic_loss: 0.01657, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19555 
Train Epoch: 29 [111/1000 3552/32000 (11%)] Loss: 1.96495 (semantic_loss: 0.01748, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23655 
Train Epoch: 29 [116/1000 3712/32000 (12%)] Loss: 1.96365 (semantic_loss: 0.01618, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21368 
Train Epoch: 29 [121/1000 3872/32000 (12%)] Loss: 1.96490 (semantic_loss: 0.01743, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21619 
Train Epoch: 29 [126/1000 4032/32000 (13%)] Loss: 1.95995 (semantic_loss: 0.01249, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20411 
Train Epoch: 29 [131/1000 4192/32000 (13%)] Loss: 1.96069 (semantic_loss: 0.01420, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.32387 
Train Epoch: 29 [136/1000 4352/32000 (14%)] Loss: 1.96296 (semantic_loss: 0.01549, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19758 
Train Epoch: 29 [141/1000 4512/32000 (14%)] Loss: 1.96349 (semantic_loss: 0.01602, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19281 
Train Epoch: 29 [146/1000 4672/32000 (15%)] Loss: 1.95825 (semantic_loss: 0.01273, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.24304 
Train Epoch: 29 [151/1000 4832/32000 (15%)] Loss: 1.96395 (semantic_loss: 0.01550, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23492 
Train Epoch: 29 [156/1000 4992/32000 (16%)] Loss: 1.96191 (semantic_loss: 0.01444, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18800 
Train Epoch: 29 [161/1000 5152/32000 (16%)] Loss: 1.96502 (semantic_loss: 0.01755, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.34050 
Train Epoch: 29 [166/1000 5312/32000 (17%)] Loss: 1.96039 (semantic_loss: 0.01292, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.32661 
Train Epoch: 29 [171/1000 5472/32000 (17%)] Loss: 1.96005 (semantic_loss: 0.01356, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19072 
Train Epoch: 29 [176/1000 5632/32000 (18%)] Loss: 1.96290 (semantic_loss: 0.01640, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19443 
Train Epoch: 29 [181/1000 5792/32000 (18%)] Loss: 1.96145 (semantic_loss: 0.01495, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18918 
Train Epoch: 29 [186/1000 5952/32000 (19%)] Loss: 1.96038 (semantic_loss: 0.01291, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18737 
Train Epoch: 29 [191/1000 6112/32000 (19%)] Loss: 1.95882 (semantic_loss: 0.01135, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19298 
Train Epoch: 29 [196/1000 6272/32000 (20%)] Loss: 1.95925 (semantic_loss: 0.01276, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18601 
Train Epoch: 29 [201/1000 6432/32000 (20%)] Loss: 1.96200 (semantic_loss: 0.01453, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19171 
Train Epoch: 29 [206/1000 6592/32000 (21%)] Loss: 1.96113 (semantic_loss: 0.01367, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18653 
Train Epoch: 29 [211/1000 6752/32000 (21%)] Loss: 1.96221 (semantic_loss: 0.01474, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18667 
Train Epoch: 29 [216/1000 6912/32000 (22%)] Loss: 1.96133 (semantic_loss: 0.01289, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18137 
Train Epoch: 29 [221/1000 7072/32000 (22%)] Loss: 1.95946 (semantic_loss: 0.01200, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.36483 
Train Epoch: 29 [226/1000 7232/32000 (23%)] Loss: 1.95803 (semantic_loss: 0.01056, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18815 
Train Epoch: 29 [231/1000 7392/32000 (23%)] Loss: 1.96329 (semantic_loss: 0.01680, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18682 
Train Epoch: 29 [236/1000 7552/32000 (24%)] Loss: 1.96036 (semantic_loss: 0.01387, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18455 
Train Epoch: 29 [241/1000 7712/32000 (24%)] Loss: 1.96222 (semantic_loss: 0.01573, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20230 
Train Epoch: 29 [246/1000 7872/32000 (25%)] Loss: 1.96598 (semantic_loss: 0.01851, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19284 
Train Epoch: 29 [251/1000 8032/32000 (25%)] Loss: 1.96194 (semantic_loss: 0.01447, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19153 
Train Epoch: 29 [256/1000 8192/32000 (26%)] Loss: 1.96176 (semantic_loss: 0.01526, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20130 
Train Epoch: 29 [261/1000 8352/32000 (26%)] Loss: 1.95818 (semantic_loss: 0.01168, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21220 
Train Epoch: 29 [266/1000 8512/32000 (27%)] Loss: 1.96138 (semantic_loss: 0.01489, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20507 
Train Epoch: 29 [271/1000 8672/32000 (27%)] Loss: 1.96168 (semantic_loss: 0.01421, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21954 
Train Epoch: 29 [276/1000 8832/32000 (28%)] Loss: 1.95903 (semantic_loss: 0.01254, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22973 
Train Epoch: 29 [281/1000 8992/32000 (28%)] Loss: 1.96277 (semantic_loss: 0.01433, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23007 
Train Epoch: 29 [286/1000 9152/32000 (29%)] Loss: 1.96230 (semantic_loss: 0.01288, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.19150 
Train Epoch: 29 [291/1000 9312/32000 (29%)] Loss: 1.95984 (semantic_loss: 0.01335, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20375 
Train Epoch: 29 [296/1000 9472/32000 (30%)] Loss: 1.96100 (semantic_loss: 0.01451, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19800 
Train Epoch: 29 [301/1000 9632/32000 (30%)] Loss: 1.96549 (semantic_loss: 0.01704, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19740 
Train Epoch: 29 [306/1000 9792/32000 (31%)] Loss: 1.96043 (semantic_loss: 0.01393, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18649 
Train Epoch: 29 [311/1000 9952/32000 (31%)] Loss: 1.96122 (semantic_loss: 0.01472, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18775 
Train Epoch: 29 [316/1000 10112/32000 (32%)] Loss: 1.96292 (semantic_loss: 0.01545, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18602 
Train Epoch: 29 [321/1000 10272/32000 (32%)] Loss: 1.96118 (semantic_loss: 0.01468, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18683 
Train Epoch: 29 [326/1000 10432/32000 (33%)] Loss: 1.95998 (semantic_loss: 0.01251, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23051 
Train Epoch: 29 [331/1000 10592/32000 (33%)] Loss: 1.96569 (semantic_loss: 0.01822, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18896 
Train Epoch: 29 [336/1000 10752/32000 (34%)] Loss: 1.96530 (semantic_loss: 0.01880, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21408 
Train Epoch: 29 [341/1000 10912/32000 (34%)] Loss: 1.96720 (semantic_loss: 0.01973, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.44035 
Train Epoch: 29 [346/1000 11072/32000 (35%)] Loss: 1.96541 (semantic_loss: 0.01794, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18610 
Train Epoch: 29 [351/1000 11232/32000 (35%)] Loss: 1.95884 (semantic_loss: 0.01234, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19092 
Train Epoch: 29 [356/1000 11392/32000 (36%)] Loss: 1.95986 (semantic_loss: 0.01239, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.43928 
Train Epoch: 29 [361/1000 11552/32000 (36%)] Loss: 1.96750 (semantic_loss: 0.01905, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18814 
Train Epoch: 29 [366/1000 11712/32000 (37%)] Loss: 1.95793 (semantic_loss: 0.01143, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18824 
Train Epoch: 29 [371/1000 11872/32000 (37%)] Loss: 1.96211 (semantic_loss: 0.01366, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18728 
Train Epoch: 29 [376/1000 12032/32000 (38%)] Loss: 1.96096 (semantic_loss: 0.01251, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19679 
Train Epoch: 29 [381/1000 12192/32000 (38%)] Loss: 1.96562 (semantic_loss: 0.01717, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18535 
Train Epoch: 29 [386/1000 12352/32000 (39%)] Loss: 1.96033 (semantic_loss: 0.01384, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18666 
Train Epoch: 29 [391/1000 12512/32000 (39%)] Loss: 1.96234 (semantic_loss: 0.01487, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23972 
Train Epoch: 29 [396/1000 12672/32000 (40%)] Loss: 1.96446 (semantic_loss: 0.01797, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18946 
Train Epoch: 29 [401/1000 12832/32000 (40%)] Loss: 1.96540 (semantic_loss: 0.01793, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20823 
Train Epoch: 29 [406/1000 12992/32000 (41%)] Loss: 1.96349 (semantic_loss: 0.01699, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18992 
Train Epoch: 29 [411/1000 13152/32000 (41%)] Loss: 1.96207 (semantic_loss: 0.01362, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18773 
Train Epoch: 29 [416/1000 13312/32000 (42%)] Loss: 1.96420 (semantic_loss: 0.01673, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18963 
Train Epoch: 29 [421/1000 13472/32000 (42%)] Loss: 1.96141 (semantic_loss: 0.01296, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18861 
Train Epoch: 29 [426/1000 13632/32000 (43%)] Loss: 1.96201 (semantic_loss: 0.01454, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21739 
Train Epoch: 29 [431/1000 13792/32000 (43%)] Loss: 1.96416 (semantic_loss: 0.01669, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22761 
Train Epoch: 29 [436/1000 13952/32000 (44%)] Loss: 1.96399 (semantic_loss: 0.01652, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23165 
Train Epoch: 29 [441/1000 14112/32000 (44%)] Loss: 1.96362 (semantic_loss: 0.01615, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19698 
Train Epoch: 29 [446/1000 14272/32000 (45%)] Loss: 1.95999 (semantic_loss: 0.01350, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23138 
Train Epoch: 29 [451/1000 14432/32000 (45%)] Loss: 1.96415 (semantic_loss: 0.01570, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.32164 
Train Epoch: 29 [456/1000 14592/32000 (46%)] Loss: 1.96993 (semantic_loss: 0.02343, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20796 
Train Epoch: 29 [461/1000 14752/32000 (46%)] Loss: 1.96372 (semantic_loss: 0.01625, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19856 
Train Epoch: 29 [466/1000 14912/32000 (47%)] Loss: 1.96172 (semantic_loss: 0.01424, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24359 
Train Epoch: 29 [471/1000 15072/32000 (47%)] Loss: 1.96334 (semantic_loss: 0.01490, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20726 
Train Epoch: 29 [476/1000 15232/32000 (48%)] Loss: 1.96423 (semantic_loss: 0.01676, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18923 
Train Epoch: 29 [481/1000 15392/32000 (48%)] Loss: 1.96345 (semantic_loss: 0.01597, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.33945 
Train Epoch: 29 [486/1000 15552/32000 (49%)] Loss: 1.95922 (semantic_loss: 0.01273, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.30638 
Train Epoch: 29 [491/1000 15712/32000 (49%)] Loss: 1.96126 (semantic_loss: 0.01379, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19386 
Train Epoch: 29 [496/1000 15872/32000 (50%)] Loss: 1.95942 (semantic_loss: 0.01293, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19380 
Train Epoch: 29 [501/1000 16032/32000 (50%)] Loss: 1.96345 (semantic_loss: 0.01598, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20021 
Train Epoch: 29 [506/1000 16192/32000 (51%)] Loss: 1.96294 (semantic_loss: 0.01547, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18941 
Train Epoch: 29 [511/1000 16352/32000 (51%)] Loss: 1.96222 (semantic_loss: 0.01573, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19002 
Train Epoch: 29 [516/1000 16512/32000 (52%)] Loss: 1.96128 (semantic_loss: 0.01381, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19892 
Train Epoch: 29 [521/1000 16672/32000 (52%)] Loss: 1.96720 (semantic_loss: 0.01876, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20413 
Train Epoch: 29 [526/1000 16832/32000 (53%)] Loss: 1.96549 (semantic_loss: 0.01704, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19668 
Train Epoch: 29 [531/1000 16992/32000 (53%)] Loss: 1.96188 (semantic_loss: 0.01441, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20917 
Train Epoch: 29 [536/1000 17152/32000 (54%)] Loss: 1.96016 (semantic_loss: 0.01270, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18761 
Train Epoch: 29 [541/1000 17312/32000 (54%)] Loss: 1.96541 (semantic_loss: 0.01794, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.36689 
Train Epoch: 29 [546/1000 17472/32000 (55%)] Loss: 1.96249 (semantic_loss: 0.01405, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19304 
Train Epoch: 29 [551/1000 17632/32000 (55%)] Loss: 1.96527 (semantic_loss: 0.01682, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18873 
Train Epoch: 29 [556/1000 17792/32000 (56%)] Loss: 1.95883 (semantic_loss: 0.01330, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20261 
Train Epoch: 29 [561/1000 17952/32000 (56%)] Loss: 1.96145 (semantic_loss: 0.01399, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18898 
Train Epoch: 29 [566/1000 18112/32000 (57%)] Loss: 1.96733 (semantic_loss: 0.01888, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18723 
Train Epoch: 29 [571/1000 18272/32000 (57%)] Loss: 1.95986 (semantic_loss: 0.01337, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18797 
Train Epoch: 29 [576/1000 18432/32000 (58%)] Loss: 1.96208 (semantic_loss: 0.01558, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20338 
Train Epoch: 29 [581/1000 18592/32000 (58%)] Loss: 1.95991 (semantic_loss: 0.01340, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22662 
Train Epoch: 29 [586/1000 18752/32000 (59%)] Loss: 1.96136 (semantic_loss: 0.01291, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22948 
Train Epoch: 29 [591/1000 18912/32000 (59%)] Loss: 1.96029 (semantic_loss: 0.01282, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22087 
Train Epoch: 29 [596/1000 19072/32000 (60%)] Loss: 1.96068 (semantic_loss: 0.01419, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21357 
Train Epoch: 29 [601/1000 19232/32000 (60%)] Loss: 1.96033 (semantic_loss: 0.01285, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21280 
Train Epoch: 29 [606/1000 19392/32000 (61%)] Loss: 1.96420 (semantic_loss: 0.01575, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19164 
Train Epoch: 29 [611/1000 19552/32000 (61%)] Loss: 1.95974 (semantic_loss: 0.01227, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21017 
Train Epoch: 29 [616/1000 19712/32000 (62%)] Loss: 1.96432 (semantic_loss: 0.01587, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19093 
Train Epoch: 29 [621/1000 19872/32000 (62%)] Loss: 1.96005 (semantic_loss: 0.01258, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19696 
Train Epoch: 29 [626/1000 20032/32000 (63%)] Loss: 1.96063 (semantic_loss: 0.01316, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18840 
Train Epoch: 29 [631/1000 20192/32000 (63%)] Loss: 1.96143 (semantic_loss: 0.01494, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18861 
Train Epoch: 29 [636/1000 20352/32000 (64%)] Loss: 1.96078 (semantic_loss: 0.01428, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18706 
Train Epoch: 29 [641/1000 20512/32000 (64%)] Loss: 1.96410 (semantic_loss: 0.01760, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19071 
Train Epoch: 29 [646/1000 20672/32000 (65%)] Loss: 1.96010 (semantic_loss: 0.01360, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22264 
Train Epoch: 29 [651/1000 20832/32000 (65%)] Loss: 1.96775 (semantic_loss: 0.02223, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19333 
Train Epoch: 29 [656/1000 20992/32000 (66%)] Loss: 1.96316 (semantic_loss: 0.01569, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19173 
Train Epoch: 29 [661/1000 21152/32000 (66%)] Loss: 1.96032 (semantic_loss: 0.01285, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.43167 
Train Epoch: 29 [666/1000 21312/32000 (67%)] Loss: 1.96323 (semantic_loss: 0.01576, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18968 
Train Epoch: 29 [671/1000 21472/32000 (67%)] Loss: 1.96257 (semantic_loss: 0.01511, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18999 
Train Epoch: 29 [676/1000 21632/32000 (68%)] Loss: 1.96266 (semantic_loss: 0.01519, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.44309 
Train Epoch: 29 [681/1000 21792/32000 (68%)] Loss: 1.96051 (semantic_loss: 0.01402, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18850 
Train Epoch: 29 [686/1000 21952/32000 (69%)] Loss: 1.96199 (semantic_loss: 0.01550, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18563 
Train Epoch: 29 [691/1000 22112/32000 (69%)] Loss: 1.96824 (semantic_loss: 0.02077, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18460 
Train Epoch: 29 [696/1000 22272/32000 (70%)] Loss: 1.96302 (semantic_loss: 0.01555, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20468 
Train Epoch: 29 [701/1000 22432/32000 (70%)] Loss: 1.95899 (semantic_loss: 0.01152, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19090 
Train Epoch: 29 [706/1000 22592/32000 (71%)] Loss: 1.96199 (semantic_loss: 0.01452, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18862 
Train Epoch: 29 [711/1000 22752/32000 (71%)] Loss: 1.95894 (semantic_loss: 0.01147, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24038 
Train Epoch: 29 [716/1000 22912/32000 (72%)] Loss: 1.96369 (semantic_loss: 0.01720, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18985 
Train Epoch: 29 [721/1000 23072/32000 (72%)] Loss: 1.96007 (semantic_loss: 0.01163, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19809 
Train Epoch: 29 [726/1000 23232/32000 (73%)] Loss: 1.96271 (semantic_loss: 0.01426, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18555 
Train Epoch: 29 [731/1000 23392/32000 (73%)] Loss: 1.96108 (semantic_loss: 0.01459, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21694 
Train Epoch: 29 [736/1000 23552/32000 (74%)] Loss: 1.96285 (semantic_loss: 0.01440, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21925 
Train Epoch: 29 [741/1000 23712/32000 (74%)] Loss: 1.95831 (semantic_loss: 0.01280, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.21113 
Train Epoch: 29 [746/1000 23872/32000 (75%)] Loss: 1.96527 (semantic_loss: 0.01780, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19898 
Train Epoch: 29 [751/1000 24032/32000 (75%)] Loss: 1.96156 (semantic_loss: 0.01409, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19343 
Train Epoch: 29 [756/1000 24192/32000 (76%)] Loss: 1.96213 (semantic_loss: 0.01466, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=1.49121 
Train Epoch: 29 [761/1000 24352/32000 (76%)] Loss: 1.96037 (semantic_loss: 0.01291, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19843 
Train Epoch: 29 [766/1000 24512/32000 (77%)] Loss: 1.96345 (semantic_loss: 0.01597, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18943 
Train Epoch: 29 [771/1000 24672/32000 (77%)] Loss: 1.96658 (semantic_loss: 0.01911, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18655 
Train Epoch: 29 [776/1000 24832/32000 (78%)] Loss: 1.96163 (semantic_loss: 0.01416, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19470 
Train Epoch: 29 [781/1000 24992/32000 (78%)] Loss: 1.96203 (semantic_loss: 0.01456, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19656 
Train Epoch: 29 [786/1000 25152/32000 (79%)] Loss: 1.96041 (semantic_loss: 0.01294, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19528 
Train Epoch: 29 [791/1000 25312/32000 (79%)] Loss: 1.96302 (semantic_loss: 0.01555, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19921 
Train Epoch: 29 [796/1000 25472/32000 (80%)] Loss: 1.95821 (semantic_loss: 0.01172, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19659 
Train Epoch: 29 [801/1000 25632/32000 (80%)] Loss: 1.96155 (semantic_loss: 0.01408, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20859 
Train Epoch: 29 [806/1000 25792/32000 (81%)] Loss: 1.96440 (semantic_loss: 0.01693, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.29126 
Train Epoch: 29 [811/1000 25952/32000 (81%)] Loss: 1.96221 (semantic_loss: 0.01474, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18734 
Train Epoch: 29 [816/1000 26112/32000 (82%)] Loss: 1.96294 (semantic_loss: 0.01547, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20533 
Train Epoch: 29 [821/1000 26272/32000 (82%)] Loss: 1.95927 (semantic_loss: 0.01278, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20030 
Train Epoch: 29 [826/1000 26432/32000 (83%)] Loss: 1.96277 (semantic_loss: 0.01530, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19910 
Train Epoch: 29 [831/1000 26592/32000 (83%)] Loss: 1.95984 (semantic_loss: 0.01335, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21632 
Train Epoch: 29 [836/1000 26752/32000 (84%)] Loss: 1.96320 (semantic_loss: 0.01671, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21326 
Train Epoch: 29 [841/1000 26912/32000 (84%)] Loss: 1.96087 (semantic_loss: 0.01438, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20858 
Train Epoch: 29 [846/1000 27072/32000 (85%)] Loss: 1.96204 (semantic_loss: 0.01457, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18905 
Train Epoch: 29 [851/1000 27232/32000 (85%)] Loss: 1.96272 (semantic_loss: 0.01524, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18718 
Train Epoch: 29 [856/1000 27392/32000 (86%)] Loss: 1.95854 (semantic_loss: 0.01205, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18878 
Train Epoch: 29 [861/1000 27552/32000 (86%)] Loss: 1.96088 (semantic_loss: 0.01341, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19937 
Train Epoch: 29 [866/1000 27712/32000 (87%)] Loss: 1.96130 (semantic_loss: 0.01480, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19692 
Train Epoch: 29 [871/1000 27872/32000 (87%)] Loss: 1.96259 (semantic_loss: 0.01708, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18535 
Train Epoch: 29 [876/1000 28032/32000 (88%)] Loss: 1.96222 (semantic_loss: 0.01474, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18544 
Train Epoch: 29 [881/1000 28192/32000 (88%)] Loss: 1.96012 (semantic_loss: 0.01363, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19018 
Train Epoch: 29 [886/1000 28352/32000 (89%)] Loss: 1.95975 (semantic_loss: 0.01228, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22019 
Train Epoch: 29 [891/1000 28512/32000 (89%)] Loss: 1.96164 (semantic_loss: 0.01515, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23368 
Train Epoch: 29 [896/1000 28672/32000 (90%)] Loss: 1.96101 (semantic_loss: 0.01355, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22503 
Train Epoch: 29 [901/1000 28832/32000 (90%)] Loss: 1.96086 (semantic_loss: 0.01241, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21423 
Train Epoch: 29 [906/1000 28992/32000 (91%)] Loss: 1.96392 (semantic_loss: 0.01646, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20442 
Train Epoch: 29 [911/1000 29152/32000 (91%)] Loss: 1.96268 (semantic_loss: 0.01619, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20599 
Train Epoch: 29 [916/1000 29312/32000 (92%)] Loss: 1.96478 (semantic_loss: 0.01828, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20334 
Train Epoch: 29 [921/1000 29472/32000 (92%)] Loss: 1.96398 (semantic_loss: 0.01749, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20904 
Train Epoch: 29 [926/1000 29632/32000 (93%)] Loss: 1.96153 (semantic_loss: 0.01601, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20905 
Train Epoch: 29 [931/1000 29792/32000 (93%)] Loss: 1.96576 (semantic_loss: 0.01731, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19924 
Train Epoch: 29 [936/1000 29952/32000 (94%)] Loss: 1.96171 (semantic_loss: 0.01325, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18974 
Train Epoch: 29 [941/1000 30112/32000 (94%)] Loss: 1.96532 (semantic_loss: 0.01785, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20751 
Train Epoch: 29 [946/1000 30272/32000 (95%)] Loss: 1.96564 (semantic_loss: 0.01915, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18849 
Train Epoch: 29 [951/1000 30432/32000 (95%)] Loss: 1.96282 (semantic_loss: 0.01535, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20805 
Train Epoch: 29 [956/1000 30592/32000 (96%)] Loss: 1.96712 (semantic_loss: 0.01965, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19700 
Train Epoch: 29 [961/1000 30752/32000 (96%)] Loss: 1.96193 (semantic_loss: 0.01445, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19759 
Train Epoch: 29 [966/1000 30912/32000 (97%)] Loss: 1.96169 (semantic_loss: 0.01520, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18597 
Train Epoch: 29 [971/1000 31072/32000 (97%)] Loss: 1.96828 (semantic_loss: 0.01984, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21228 
Train Epoch: 29 [976/1000 31232/32000 (98%)] Loss: 1.95661 (semantic_loss: 0.01207, quant_loss: 1.94434, bit_balance_loss: 0.00021) batch_time=0.18922 
Train Epoch: 29 [981/1000 31392/32000 (98%)] Loss: 1.96296 (semantic_loss: 0.01647, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18607 
Train Epoch: 29 [986/1000 31552/32000 (99%)] Loss: 1.96062 (semantic_loss: 0.01315, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18701 
Train Epoch: 29 [991/1000 31712/32000 (99%)] Loss: 1.95834 (semantic_loss: 0.01185, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18711 
Train Epoch: 29 [996/1000 31872/32000 (100%)] Loss: 1.96031 (semantic_loss: 0.01382, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.43577 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch29.pth ...
Done in 4.638s
removing stale ckpt [epoch 28] [took 0.00s]
 epoch          : 29
 loss           : 1.9622291955947877
 learning_rate  : 2.616738165136805e-06
 n_samples      : 928000
 n_steps        : 29000
 ActivityNet_val1_test/t2v_metrics/R1: 12.039861704291235
 ActivityNet_val1_test/t2v_metrics/R5: 37.949969493593656
 ActivityNet_val1_test/t2v_metrics/R10: 54.54545454545455
 ActivityNet_val1_test/t2v_metrics/R50: 85.29591214154972
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 60.228899735611144
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.20992879255661
 ActivityNet_val1_test/v2t_metrics/R1: 12.487288997356112
 ActivityNet_val1_test/v2t_metrics/R5: 39.10921293471629
 ActivityNet_val1_test/v2t_metrics/R10: 55.78604840349807
 ActivityNet_val1_test/v2t_metrics/R50: 84.72645922310352
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 62.20286760219646
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 30.090144918564448
 mnt_best       : 29.542107379154057
 not_improved_count: 2
Train Epoch: 30 [1/1000 32/32000 (0%)] Loss: 1.96237 (semantic_loss: 0.01588, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=21.76186 
Train Epoch: 30 [6/1000 192/32000 (1%)] Loss: 1.96279 (semantic_loss: 0.01532, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22648 
Train Epoch: 30 [11/1000 352/32000 (1%)] Loss: 1.96308 (semantic_loss: 0.01463, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21596 
Train Epoch: 30 [16/1000 512/32000 (2%)] Loss: 1.96232 (semantic_loss: 0.01485, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=1.29480 
Train Epoch: 30 [21/1000 672/32000 (2%)] Loss: 1.96321 (semantic_loss: 0.01574, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20592 
Train Epoch: 30 [26/1000 832/32000 (3%)] Loss: 1.96136 (semantic_loss: 0.01389, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21694 
Train Epoch: 30 [31/1000 992/32000 (3%)] Loss: 1.96088 (semantic_loss: 0.01536, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19606 
Train Epoch: 30 [36/1000 1152/32000 (4%)] Loss: 1.96232 (semantic_loss: 0.01484, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18882 
Train Epoch: 30 [41/1000 1312/32000 (4%)] Loss: 1.95836 (semantic_loss: 0.01186, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20907 
Train Epoch: 30 [46/1000 1472/32000 (5%)] Loss: 1.96247 (semantic_loss: 0.01500, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18930 
Train Epoch: 30 [51/1000 1632/32000 (5%)] Loss: 1.96314 (semantic_loss: 0.01665, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20604 
Train Epoch: 30 [56/1000 1792/32000 (6%)] Loss: 1.96050 (semantic_loss: 0.01303, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18545 
Train Epoch: 30 [61/1000 1952/32000 (6%)] Loss: 1.96555 (semantic_loss: 0.01613, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.19596 
Train Epoch: 30 [66/1000 2112/32000 (7%)] Loss: 1.96431 (semantic_loss: 0.01586, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22716 
Train Epoch: 30 [71/1000 2272/32000 (7%)] Loss: 1.96102 (semantic_loss: 0.01355, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20437 
Train Epoch: 30 [76/1000 2432/32000 (8%)] Loss: 1.96744 (semantic_loss: 0.01997, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19474 
Train Epoch: 30 [81/1000 2592/32000 (8%)] Loss: 1.95949 (semantic_loss: 0.01202, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18386 
Train Epoch: 30 [86/1000 2752/32000 (9%)] Loss: 1.96364 (semantic_loss: 0.01519, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18622 
Train Epoch: 30 [91/1000 2912/32000 (9%)] Loss: 1.96105 (semantic_loss: 0.01358, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18916 
Train Epoch: 30 [96/1000 3072/32000 (10%)] Loss: 1.96259 (semantic_loss: 0.01610, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18619 
Train Epoch: 30 [101/1000 3232/32000 (10%)] Loss: 1.96514 (semantic_loss: 0.01767, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18519 
Train Epoch: 30 [106/1000 3392/32000 (11%)] Loss: 1.96384 (semantic_loss: 0.01638, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27330 
Train Epoch: 30 [111/1000 3552/32000 (11%)] Loss: 1.96333 (semantic_loss: 0.01585, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19172 
Train Epoch: 30 [116/1000 3712/32000 (12%)] Loss: 1.96231 (semantic_loss: 0.01386, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21260 
Train Epoch: 30 [121/1000 3872/32000 (12%)] Loss: 1.96402 (semantic_loss: 0.01656, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20195 
Train Epoch: 30 [126/1000 4032/32000 (13%)] Loss: 1.95994 (semantic_loss: 0.01345, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18606 
Train Epoch: 30 [131/1000 4192/32000 (13%)] Loss: 1.96085 (semantic_loss: 0.01435, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18576 
Train Epoch: 30 [136/1000 4352/32000 (14%)] Loss: 1.95896 (semantic_loss: 0.01247, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20591 
Train Epoch: 30 [141/1000 4512/32000 (14%)] Loss: 1.96085 (semantic_loss: 0.01338, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18834 
Train Epoch: 30 [146/1000 4672/32000 (15%)] Loss: 1.95984 (semantic_loss: 0.01236, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18453 
Train Epoch: 30 [151/1000 4832/32000 (15%)] Loss: 1.96085 (semantic_loss: 0.01339, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22171 
Train Epoch: 30 [156/1000 4992/32000 (16%)] Loss: 1.96263 (semantic_loss: 0.01418, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21600 
Train Epoch: 30 [161/1000 5152/32000 (16%)] Loss: 1.96115 (semantic_loss: 0.01466, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22351 
Train Epoch: 30 [166/1000 5312/32000 (17%)] Loss: 1.96223 (semantic_loss: 0.01476, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21279 
Train Epoch: 30 [171/1000 5472/32000 (17%)] Loss: 1.96164 (semantic_loss: 0.01514, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20339 
Train Epoch: 30 [176/1000 5632/32000 (18%)] Loss: 1.95963 (semantic_loss: 0.01216, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19496 
Train Epoch: 30 [181/1000 5792/32000 (18%)] Loss: 1.96297 (semantic_loss: 0.01549, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22313 
Train Epoch: 30 [186/1000 5952/32000 (19%)] Loss: 1.96180 (semantic_loss: 0.01335, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21720 
Train Epoch: 30 [191/1000 6112/32000 (19%)] Loss: 1.96126 (semantic_loss: 0.01282, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23510 
Train Epoch: 30 [196/1000 6272/32000 (20%)] Loss: 1.95932 (semantic_loss: 0.01186, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20137 
Train Epoch: 30 [201/1000 6432/32000 (20%)] Loss: 1.96495 (semantic_loss: 0.01748, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19993 
Train Epoch: 30 [206/1000 6592/32000 (21%)] Loss: 1.96238 (semantic_loss: 0.01589, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20387 
Train Epoch: 30 [211/1000 6752/32000 (21%)] Loss: 1.95907 (semantic_loss: 0.01258, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18326 
Train Epoch: 30 [216/1000 6912/32000 (22%)] Loss: 1.96595 (semantic_loss: 0.01653, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.19265 
Train Epoch: 30 [221/1000 7072/32000 (22%)] Loss: 1.96638 (semantic_loss: 0.01891, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19348 
Train Epoch: 30 [226/1000 7232/32000 (23%)] Loss: 1.96034 (semantic_loss: 0.01287, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19523 
Train Epoch: 30 [231/1000 7392/32000 (23%)] Loss: 1.96351 (semantic_loss: 0.01604, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18192 
Train Epoch: 30 [236/1000 7552/32000 (24%)] Loss: 1.96171 (semantic_loss: 0.01424, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18154 
Train Epoch: 30 [241/1000 7712/32000 (24%)] Loss: 1.96764 (semantic_loss: 0.02017, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18163 
Train Epoch: 30 [246/1000 7872/32000 (25%)] Loss: 1.96150 (semantic_loss: 0.01305, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18825 
Train Epoch: 30 [251/1000 8032/32000 (25%)] Loss: 1.96500 (semantic_loss: 0.01655, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18704 
Train Epoch: 30 [256/1000 8192/32000 (26%)] Loss: 1.96366 (semantic_loss: 0.01619, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19458 
Train Epoch: 30 [261/1000 8352/32000 (26%)] Loss: 1.96004 (semantic_loss: 0.01355, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21544 
Train Epoch: 30 [266/1000 8512/32000 (27%)] Loss: 1.96268 (semantic_loss: 0.01619, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18951 
Train Epoch: 30 [271/1000 8672/32000 (27%)] Loss: 1.96570 (semantic_loss: 0.01725, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19045 
Train Epoch: 30 [276/1000 8832/32000 (28%)] Loss: 1.96258 (semantic_loss: 0.01413, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18697 
Train Epoch: 30 [281/1000 8992/32000 (28%)] Loss: 1.96282 (semantic_loss: 0.01633, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.47069 
Train Epoch: 30 [286/1000 9152/32000 (29%)] Loss: 1.96074 (semantic_loss: 0.01327, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21429 
Train Epoch: 30 [291/1000 9312/32000 (29%)] Loss: 1.95902 (semantic_loss: 0.01253, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18423 
Train Epoch: 30 [296/1000 9472/32000 (30%)] Loss: 1.96109 (semantic_loss: 0.01460, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18408 
Train Epoch: 30 [301/1000 9632/32000 (30%)] Loss: 1.96116 (semantic_loss: 0.01369, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19110 
Train Epoch: 30 [306/1000 9792/32000 (31%)] Loss: 1.96348 (semantic_loss: 0.01601, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20829 
Train Epoch: 30 [311/1000 9952/32000 (31%)] Loss: 1.96115 (semantic_loss: 0.01466, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21290 
Train Epoch: 30 [316/1000 10112/32000 (32%)] Loss: 1.96442 (semantic_loss: 0.01694, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23815 
Train Epoch: 30 [321/1000 10272/32000 (32%)] Loss: 1.95900 (semantic_loss: 0.01349, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.27694 
Train Epoch: 30 [326/1000 10432/32000 (33%)] Loss: 1.96496 (semantic_loss: 0.01845, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.20582 
Train Epoch: 30 [331/1000 10592/32000 (33%)] Loss: 1.96551 (semantic_loss: 0.01803, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20140 
Train Epoch: 30 [336/1000 10752/32000 (34%)] Loss: 1.96116 (semantic_loss: 0.01467, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=1.07626 
Train Epoch: 30 [341/1000 10912/32000 (34%)] Loss: 1.95960 (semantic_loss: 0.01213, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19543 
Train Epoch: 30 [346/1000 11072/32000 (35%)] Loss: 1.96151 (semantic_loss: 0.01404, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19560 
Train Epoch: 30 [351/1000 11232/32000 (35%)] Loss: 1.96326 (semantic_loss: 0.01579, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18843 
Train Epoch: 30 [356/1000 11392/32000 (36%)] Loss: 1.96002 (semantic_loss: 0.01353, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18751 
Train Epoch: 30 [361/1000 11552/32000 (36%)] Loss: 1.96699 (semantic_loss: 0.02048, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19038 
Train Epoch: 30 [366/1000 11712/32000 (37%)] Loss: 1.96198 (semantic_loss: 0.01451, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19155 
Train Epoch: 30 [371/1000 11872/32000 (37%)] Loss: 1.96039 (semantic_loss: 0.01390, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19558 
Train Epoch: 30 [376/1000 12032/32000 (38%)] Loss: 1.96280 (semantic_loss: 0.01534, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19786 
Train Epoch: 30 [381/1000 12192/32000 (38%)] Loss: 1.96315 (semantic_loss: 0.01568, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19628 
Train Epoch: 30 [386/1000 12352/32000 (39%)] Loss: 1.96380 (semantic_loss: 0.01536, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20277 
Train Epoch: 30 [391/1000 12512/32000 (39%)] Loss: 1.96314 (semantic_loss: 0.01567, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19789 
Train Epoch: 30 [396/1000 12672/32000 (40%)] Loss: 1.96181 (semantic_loss: 0.01336, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18552 
Train Epoch: 30 [401/1000 12832/32000 (40%)] Loss: 1.96351 (semantic_loss: 0.01702, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18859 
Train Epoch: 30 [406/1000 12992/32000 (41%)] Loss: 1.96106 (semantic_loss: 0.01359, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18900 
Train Epoch: 30 [411/1000 13152/32000 (41%)] Loss: 1.96148 (semantic_loss: 0.01401, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18925 
Train Epoch: 30 [416/1000 13312/32000 (42%)] Loss: 1.96350 (semantic_loss: 0.01603, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19186 
Train Epoch: 30 [421/1000 13472/32000 (42%)] Loss: 1.95996 (semantic_loss: 0.01249, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18786 
Train Epoch: 30 [426/1000 13632/32000 (43%)] Loss: 1.96190 (semantic_loss: 0.01443, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.28165 
Train Epoch: 30 [431/1000 13792/32000 (43%)] Loss: 1.96619 (semantic_loss: 0.01969, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18989 
Train Epoch: 30 [436/1000 13952/32000 (44%)] Loss: 1.96124 (semantic_loss: 0.01378, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18415 
Train Epoch: 30 [441/1000 14112/32000 (44%)] Loss: 1.96097 (semantic_loss: 0.01448, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19913 
Train Epoch: 30 [446/1000 14272/32000 (45%)] Loss: 1.95989 (semantic_loss: 0.01242, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18766 
Train Epoch: 30 [451/1000 14432/32000 (45%)] Loss: 1.96499 (semantic_loss: 0.01751, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.18824 
Train Epoch: 30 [456/1000 14592/32000 (46%)] Loss: 1.96722 (semantic_loss: 0.01878, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20221 
Train Epoch: 30 [461/1000 14752/32000 (46%)] Loss: 1.96052 (semantic_loss: 0.01305, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19041 
Train Epoch: 30 [466/1000 14912/32000 (47%)] Loss: 1.96173 (semantic_loss: 0.01427, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21550 
Train Epoch: 30 [471/1000 15072/32000 (47%)] Loss: 1.95966 (semantic_loss: 0.01219, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23084 
Train Epoch: 30 [476/1000 15232/32000 (48%)] Loss: 1.95983 (semantic_loss: 0.01236, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20044 
Train Epoch: 30 [481/1000 15392/32000 (48%)] Loss: 1.96689 (semantic_loss: 0.02040, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20642 
Train Epoch: 30 [486/1000 15552/32000 (49%)] Loss: 1.96173 (semantic_loss: 0.01426, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20846 
Train Epoch: 30 [491/1000 15712/32000 (49%)] Loss: 1.95870 (semantic_loss: 0.01221, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19931 
Train Epoch: 30 [496/1000 15872/32000 (50%)] Loss: 1.96327 (semantic_loss: 0.01483, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20344 
Train Epoch: 30 [501/1000 16032/32000 (50%)] Loss: 1.96060 (semantic_loss: 0.01313, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20590 
Train Epoch: 30 [506/1000 16192/32000 (51%)] Loss: 1.96341 (semantic_loss: 0.01594, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18379 
Train Epoch: 30 [511/1000 16352/32000 (51%)] Loss: 1.96158 (semantic_loss: 0.01313, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18646 
Train Epoch: 30 [516/1000 16512/32000 (52%)] Loss: 1.96034 (semantic_loss: 0.01385, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19823 
Train Epoch: 30 [521/1000 16672/32000 (52%)] Loss: 1.96001 (semantic_loss: 0.01351, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18859 
Train Epoch: 30 [526/1000 16832/32000 (53%)] Loss: 1.95929 (semantic_loss: 0.01182, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18482 
Train Epoch: 30 [531/1000 16992/32000 (53%)] Loss: 1.96517 (semantic_loss: 0.01770, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19493 
Train Epoch: 30 [536/1000 17152/32000 (54%)] Loss: 1.96705 (semantic_loss: 0.01958, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19407 
Train Epoch: 30 [541/1000 17312/32000 (54%)] Loss: 1.96593 (semantic_loss: 0.01749, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19028 
Train Epoch: 30 [546/1000 17472/32000 (55%)] Loss: 1.96396 (semantic_loss: 0.01746, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18439 
Train Epoch: 30 [551/1000 17632/32000 (55%)] Loss: 1.96765 (semantic_loss: 0.02018, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18447 
Train Epoch: 30 [556/1000 17792/32000 (56%)] Loss: 1.96134 (semantic_loss: 0.01386, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18401 
Train Epoch: 30 [561/1000 17952/32000 (56%)] Loss: 1.96227 (semantic_loss: 0.01578, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19295 
Train Epoch: 30 [566/1000 18112/32000 (57%)] Loss: 1.95935 (semantic_loss: 0.01384, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19101 
Train Epoch: 30 [571/1000 18272/32000 (57%)] Loss: 1.96062 (semantic_loss: 0.01315, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19056 
Train Epoch: 30 [576/1000 18432/32000 (58%)] Loss: 1.96169 (semantic_loss: 0.01422, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20606 
Train Epoch: 30 [581/1000 18592/32000 (58%)] Loss: 1.96109 (semantic_loss: 0.01460, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21593 
Train Epoch: 30 [586/1000 18752/32000 (59%)] Loss: 1.96337 (semantic_loss: 0.01493, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18645 
Train Epoch: 30 [591/1000 18912/32000 (59%)] Loss: 1.96119 (semantic_loss: 0.01275, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18965 
Train Epoch: 30 [596/1000 19072/32000 (60%)] Loss: 1.96136 (semantic_loss: 0.01390, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18844 
Train Epoch: 30 [601/1000 19232/32000 (60%)] Loss: 1.96155 (semantic_loss: 0.01505, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.49134 
Train Epoch: 30 [606/1000 19392/32000 (61%)] Loss: 1.96319 (semantic_loss: 0.01572, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18899 
Train Epoch: 30 [611/1000 19552/32000 (61%)] Loss: 1.96579 (semantic_loss: 0.01831, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18826 
Train Epoch: 30 [616/1000 19712/32000 (62%)] Loss: 1.96041 (semantic_loss: 0.01392, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18692 
Train Epoch: 30 [621/1000 19872/32000 (62%)] Loss: 1.96022 (semantic_loss: 0.01177, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20781 
Train Epoch: 30 [626/1000 20032/32000 (63%)] Loss: 1.96440 (semantic_loss: 0.01595, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.23830 
Train Epoch: 30 [631/1000 20192/32000 (63%)] Loss: 1.96005 (semantic_loss: 0.01258, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21822 
Train Epoch: 30 [636/1000 20352/32000 (64%)] Loss: 1.96022 (semantic_loss: 0.01372, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21612 
Train Epoch: 30 [641/1000 20512/32000 (64%)] Loss: 1.95975 (semantic_loss: 0.01326, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23798 
Train Epoch: 30 [646/1000 20672/32000 (65%)] Loss: 1.96235 (semantic_loss: 0.01488, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19658 
Train Epoch: 30 [651/1000 20832/32000 (65%)] Loss: 1.96095 (semantic_loss: 0.01348, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22986 
Train Epoch: 30 [656/1000 20992/32000 (66%)] Loss: 1.96048 (semantic_loss: 0.01301, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=1.01155 
Train Epoch: 30 [661/1000 21152/32000 (66%)] Loss: 1.96208 (semantic_loss: 0.01364, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19507 
Train Epoch: 30 [666/1000 21312/32000 (67%)] Loss: 1.96186 (semantic_loss: 0.01439, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18735 
Train Epoch: 30 [671/1000 21472/32000 (67%)] Loss: 1.96311 (semantic_loss: 0.01467, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18811 
Train Epoch: 30 [676/1000 21632/32000 (68%)] Loss: 1.96042 (semantic_loss: 0.01393, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18672 
Train Epoch: 30 [681/1000 21792/32000 (68%)] Loss: 1.96270 (semantic_loss: 0.01621, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19355 
Train Epoch: 30 [686/1000 21952/32000 (69%)] Loss: 1.96275 (semantic_loss: 0.01723, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19067 
Train Epoch: 30 [691/1000 22112/32000 (69%)] Loss: 1.96182 (semantic_loss: 0.01337, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20488 
Train Epoch: 30 [696/1000 22272/32000 (70%)] Loss: 1.96267 (semantic_loss: 0.01521, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19250 
Train Epoch: 30 [701/1000 22432/32000 (70%)] Loss: 1.96077 (semantic_loss: 0.01329, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18689 
Train Epoch: 30 [706/1000 22592/32000 (71%)] Loss: 1.96216 (semantic_loss: 0.01371, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20466 
Train Epoch: 30 [711/1000 22752/32000 (71%)] Loss: 1.96103 (semantic_loss: 0.01453, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19390 
Train Epoch: 30 [716/1000 22912/32000 (72%)] Loss: 1.96217 (semantic_loss: 0.01471, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18478 
Train Epoch: 30 [721/1000 23072/32000 (72%)] Loss: 1.96276 (semantic_loss: 0.01529, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18678 
Train Epoch: 30 [726/1000 23232/32000 (73%)] Loss: 1.96383 (semantic_loss: 0.01734, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18746 
Train Epoch: 30 [731/1000 23392/32000 (73%)] Loss: 1.95887 (semantic_loss: 0.01238, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18884 
Train Epoch: 30 [736/1000 23552/32000 (74%)] Loss: 1.96025 (semantic_loss: 0.01278, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20653 
Train Epoch: 30 [741/1000 23712/32000 (74%)] Loss: 1.97049 (semantic_loss: 0.02400, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18351 
Train Epoch: 30 [746/1000 23872/32000 (75%)] Loss: 1.95946 (semantic_loss: 0.01199, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27597 
Train Epoch: 30 [751/1000 24032/32000 (75%)] Loss: 1.96169 (semantic_loss: 0.01422, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18378 
Train Epoch: 30 [756/1000 24192/32000 (76%)] Loss: 1.96363 (semantic_loss: 0.01616, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18950 
Train Epoch: 30 [761/1000 24352/32000 (76%)] Loss: 1.96733 (semantic_loss: 0.01986, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22795 
Train Epoch: 30 [766/1000 24512/32000 (77%)] Loss: 1.96384 (semantic_loss: 0.01832, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19138 
Train Epoch: 30 [771/1000 24672/32000 (77%)] Loss: 1.96327 (semantic_loss: 0.01482, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18560 
Train Epoch: 30 [776/1000 24832/32000 (78%)] Loss: 1.96117 (semantic_loss: 0.01370, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19778 
Train Epoch: 30 [781/1000 24992/32000 (78%)] Loss: 1.96201 (semantic_loss: 0.01454, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19139 
Train Epoch: 30 [786/1000 25152/32000 (79%)] Loss: 1.96008 (semantic_loss: 0.01261, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22314 
Train Epoch: 30 [791/1000 25312/32000 (79%)] Loss: 1.96164 (semantic_loss: 0.01319, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22657 
Train Epoch: 30 [796/1000 25472/32000 (80%)] Loss: 1.95916 (semantic_loss: 0.01267, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19392 
Train Epoch: 30 [801/1000 25632/32000 (80%)] Loss: 1.96159 (semantic_loss: 0.01413, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19089 
Train Epoch: 30 [806/1000 25792/32000 (81%)] Loss: 1.96537 (semantic_loss: 0.01888, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19193 
Train Epoch: 30 [811/1000 25952/32000 (81%)] Loss: 1.96052 (semantic_loss: 0.01305, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20991 
Train Epoch: 30 [816/1000 26112/32000 (82%)] Loss: 1.96239 (semantic_loss: 0.01394, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20808 
Train Epoch: 30 [821/1000 26272/32000 (82%)] Loss: 1.96151 (semantic_loss: 0.01403, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21615 
Train Epoch: 30 [826/1000 26432/32000 (83%)] Loss: 1.96452 (semantic_loss: 0.01607, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19068 
Train Epoch: 30 [831/1000 26592/32000 (83%)] Loss: 1.96534 (semantic_loss: 0.01885, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19012 
Train Epoch: 30 [836/1000 26752/32000 (84%)] Loss: 1.96064 (semantic_loss: 0.01317, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21368 
Train Epoch: 30 [841/1000 26912/32000 (84%)] Loss: 1.95905 (semantic_loss: 0.01158, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20624 
Train Epoch: 30 [846/1000 27072/32000 (85%)] Loss: 1.96282 (semantic_loss: 0.01535, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19636 
Train Epoch: 30 [851/1000 27232/32000 (85%)] Loss: 1.96067 (semantic_loss: 0.01320, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20505 
Train Epoch: 30 [856/1000 27392/32000 (86%)] Loss: 1.96290 (semantic_loss: 0.01445, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20256 
Train Epoch: 30 [861/1000 27552/32000 (86%)] Loss: 1.96194 (semantic_loss: 0.01446, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19180 
Train Epoch: 30 [866/1000 27712/32000 (87%)] Loss: 1.95910 (semantic_loss: 0.01260, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18729 
Train Epoch: 30 [871/1000 27872/32000 (87%)] Loss: 1.95896 (semantic_loss: 0.01246, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19240 
Train Epoch: 30 [876/1000 28032/32000 (88%)] Loss: 1.96420 (semantic_loss: 0.01673, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18786 
Train Epoch: 30 [881/1000 28192/32000 (88%)] Loss: 1.96834 (semantic_loss: 0.01990, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19256 
Train Epoch: 30 [886/1000 28352/32000 (89%)] Loss: 1.96497 (semantic_loss: 0.01554, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.19075 
Train Epoch: 30 [891/1000 28512/32000 (89%)] Loss: 1.96292 (semantic_loss: 0.01544, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18836 
Train Epoch: 30 [896/1000 28672/32000 (90%)] Loss: 1.96013 (semantic_loss: 0.01169, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18677 
Train Epoch: 30 [901/1000 28832/32000 (90%)] Loss: 1.95827 (semantic_loss: 0.01178, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22604 
Train Epoch: 30 [906/1000 28992/32000 (91%)] Loss: 1.96342 (semantic_loss: 0.01595, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18600 
Train Epoch: 30 [911/1000 29152/32000 (91%)] Loss: 1.96090 (semantic_loss: 0.01343, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19056 
Train Epoch: 30 [916/1000 29312/32000 (92%)] Loss: 1.96173 (semantic_loss: 0.01621, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18998 
Train Epoch: 30 [921/1000 29472/32000 (92%)] Loss: 1.95882 (semantic_loss: 0.01135, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.49631 
Train Epoch: 30 [926/1000 29632/32000 (93%)] Loss: 1.96323 (semantic_loss: 0.01478, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19093 
Train Epoch: 30 [931/1000 29792/32000 (93%)] Loss: 1.96308 (semantic_loss: 0.01463, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19465 
Train Epoch: 30 [936/1000 29952/32000 (94%)] Loss: 1.96173 (semantic_loss: 0.01426, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19291 
Train Epoch: 30 [941/1000 30112/32000 (94%)] Loss: 1.96556 (semantic_loss: 0.01810, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21297 
Train Epoch: 30 [946/1000 30272/32000 (95%)] Loss: 1.96384 (semantic_loss: 0.01637, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22934 
Train Epoch: 30 [951/1000 30432/32000 (95%)] Loss: 1.96207 (semantic_loss: 0.01460, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22172 
Train Epoch: 30 [956/1000 30592/32000 (96%)] Loss: 1.96327 (semantic_loss: 0.01580, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20873 
Train Epoch: 30 [961/1000 30752/32000 (96%)] Loss: 1.96094 (semantic_loss: 0.01347, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24016 
Train Epoch: 30 [966/1000 30912/32000 (97%)] Loss: 1.96031 (semantic_loss: 0.01285, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20920 
Train Epoch: 30 [971/1000 31072/32000 (97%)] Loss: 1.96185 (semantic_loss: 0.01341, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19621 
Train Epoch: 30 [976/1000 31232/32000 (98%)] Loss: 1.95892 (semantic_loss: 0.01145, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=1.11101 
Train Epoch: 30 [981/1000 31392/32000 (98%)] Loss: 1.96083 (semantic_loss: 0.01336, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21338 
Train Epoch: 30 [986/1000 31552/32000 (99%)] Loss: 1.95834 (semantic_loss: 0.01186, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20565 
Train Epoch: 30 [991/1000 31712/32000 (99%)] Loss: 1.96451 (semantic_loss: 0.01704, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18944 
Train Epoch: 30 [996/1000 31872/32000 (100%)] Loss: 1.96012 (semantic_loss: 0.01363, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18766 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch30.pth ...
Done in 4.220s
removing stale ckpt [epoch 29] [took 0.00s]
 epoch          : 30
 loss           : 1.9622624627351761
 learning_rate  : 2.3550643486231244e-06
 n_samples      : 960000
 n_steps        : 30000
 ActivityNet_val1_test/t2v_metrics/R1: 11.61277201545658
 ActivityNet_val1_test/t2v_metrics/R5: 37.909294285133214
 ActivityNet_val1_test/t2v_metrics/R10: 54.36241610738255
 ActivityNet_val1_test/t2v_metrics/R50: 85.1738865161684
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 60.35204392922514
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 28.817753136815853
 ActivityNet_val1_test/v2t_metrics/R1: 11.816148057758795
 ActivityNet_val1_test/v2t_metrics/R5: 38.25503355704698
 ActivityNet_val1_test/v2t_metrics/R10: 55.66402277811674
 ActivityNet_val1_test/v2t_metrics/R50: 84.95017286963596
 ActivityNet_val1_test/v2t_metrics/MedR: 8.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 62.823774659345126
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.303063991392097
 mnt_best       : 29.542107379154057
 not_improved_count: 3
Train Epoch: 31 [1/1000 32/32000 (0%)] Loss: 1.96156 (semantic_loss: 0.01507, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=23.44775 
Train Epoch: 31 [6/1000 192/32000 (1%)] Loss: 1.96313 (semantic_loss: 0.01663, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18715 
Train Epoch: 31 [11/1000 352/32000 (1%)] Loss: 1.95867 (semantic_loss: 0.01217, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18570 
Train Epoch: 31 [16/1000 512/32000 (2%)] Loss: 1.95930 (semantic_loss: 0.01183, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.65209 
Train Epoch: 31 [21/1000 672/32000 (2%)] Loss: 1.96070 (semantic_loss: 0.01323, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.46240 
Train Epoch: 31 [26/1000 832/32000 (3%)] Loss: 1.96163 (semantic_loss: 0.01513, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18563 
Train Epoch: 31 [31/1000 992/32000 (3%)] Loss: 1.96197 (semantic_loss: 0.01450, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18616 
Train Epoch: 31 [36/1000 1152/32000 (4%)] Loss: 1.95718 (semantic_loss: 0.01166, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18871 
Train Epoch: 31 [41/1000 1312/32000 (4%)] Loss: 1.96235 (semantic_loss: 0.01586, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18739 
Train Epoch: 31 [46/1000 1472/32000 (5%)] Loss: 1.96367 (semantic_loss: 0.01718, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19026 
Train Epoch: 31 [51/1000 1632/32000 (5%)] Loss: 1.96065 (semantic_loss: 0.01318, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18889 
Train Epoch: 31 [56/1000 1792/32000 (6%)] Loss: 1.96296 (semantic_loss: 0.01550, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18839 
Train Epoch: 31 [61/1000 1952/32000 (6%)] Loss: 1.96018 (semantic_loss: 0.01271, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18476 
Train Epoch: 31 [66/1000 2112/32000 (7%)] Loss: 1.96053 (semantic_loss: 0.01306, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18557 
Train Epoch: 31 [71/1000 2272/32000 (7%)] Loss: 1.96161 (semantic_loss: 0.01610, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.20866 
Train Epoch: 31 [76/1000 2432/32000 (8%)] Loss: 1.96191 (semantic_loss: 0.01542, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20375 
Train Epoch: 31 [81/1000 2592/32000 (8%)] Loss: 1.96467 (semantic_loss: 0.01623, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21335 
Train Epoch: 31 [86/1000 2752/32000 (9%)] Loss: 1.96102 (semantic_loss: 0.01452, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19181 
Train Epoch: 31 [91/1000 2912/32000 (9%)] Loss: 1.96894 (semantic_loss: 0.02050, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19917 
Train Epoch: 31 [96/1000 3072/32000 (10%)] Loss: 1.96380 (semantic_loss: 0.01633, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20753 
Train Epoch: 31 [101/1000 3232/32000 (10%)] Loss: 1.96329 (semantic_loss: 0.01581, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20300 
Train Epoch: 31 [106/1000 3392/32000 (11%)] Loss: 1.96307 (semantic_loss: 0.01560, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19026 
Train Epoch: 31 [111/1000 3552/32000 (11%)] Loss: 1.95977 (semantic_loss: 0.01133, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18909 
Train Epoch: 31 [116/1000 3712/32000 (12%)] Loss: 1.96325 (semantic_loss: 0.01578, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18628 
Train Epoch: 31 [121/1000 3872/32000 (12%)] Loss: 1.96393 (semantic_loss: 0.01647, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19237 
Train Epoch: 31 [126/1000 4032/32000 (13%)] Loss: 1.96264 (semantic_loss: 0.01615, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19686 
Train Epoch: 31 [131/1000 4192/32000 (13%)] Loss: 1.96015 (semantic_loss: 0.01267, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.36197 
Train Epoch: 31 [136/1000 4352/32000 (14%)] Loss: 1.96178 (semantic_loss: 0.01431, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19533 
Train Epoch: 31 [141/1000 4512/32000 (14%)] Loss: 1.96310 (semantic_loss: 0.01466, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21060 
Train Epoch: 31 [146/1000 4672/32000 (15%)] Loss: 1.96056 (semantic_loss: 0.01406, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18653 
Train Epoch: 31 [151/1000 4832/32000 (15%)] Loss: 1.96379 (semantic_loss: 0.01730, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18624 
Train Epoch: 31 [156/1000 4992/32000 (16%)] Loss: 1.96151 (semantic_loss: 0.01502, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.44836 
Train Epoch: 31 [161/1000 5152/32000 (16%)] Loss: 1.96385 (semantic_loss: 0.01638, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18692 
Train Epoch: 31 [166/1000 5312/32000 (17%)] Loss: 1.96073 (semantic_loss: 0.01423, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18775 
Train Epoch: 31 [171/1000 5472/32000 (17%)] Loss: 1.97005 (semantic_loss: 0.02355, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18662 
Train Epoch: 31 [176/1000 5632/32000 (18%)] Loss: 1.96337 (semantic_loss: 0.01589, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18606 
Train Epoch: 31 [181/1000 5792/32000 (18%)] Loss: 1.96314 (semantic_loss: 0.01469, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18478 
Train Epoch: 31 [186/1000 5952/32000 (19%)] Loss: 1.96417 (semantic_loss: 0.01670, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19430 
Train Epoch: 31 [191/1000 6112/32000 (19%)] Loss: 1.95957 (semantic_loss: 0.01308, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18721 
Train Epoch: 31 [196/1000 6272/32000 (20%)] Loss: 1.96320 (semantic_loss: 0.01573, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18958 
Train Epoch: 31 [201/1000 6432/32000 (20%)] Loss: 1.95925 (semantic_loss: 0.01276, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18771 
Train Epoch: 31 [206/1000 6592/32000 (21%)] Loss: 1.96605 (semantic_loss: 0.01858, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19534 
Train Epoch: 31 [211/1000 6752/32000 (21%)] Loss: 1.96157 (semantic_loss: 0.01410, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18725 
Train Epoch: 31 [216/1000 6912/32000 (22%)] Loss: 1.96266 (semantic_loss: 0.01324, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.19177 
Train Epoch: 31 [221/1000 7072/32000 (22%)] Loss: 1.96232 (semantic_loss: 0.01485, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19981 
Train Epoch: 31 [226/1000 7232/32000 (23%)] Loss: 1.96062 (semantic_loss: 0.01511, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.20567 
Train Epoch: 31 [231/1000 7392/32000 (23%)] Loss: 1.96350 (semantic_loss: 0.01604, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21910 
Train Epoch: 31 [236/1000 7552/32000 (24%)] Loss: 1.95980 (semantic_loss: 0.01428, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.20589 
Train Epoch: 31 [241/1000 7712/32000 (24%)] Loss: 1.96387 (semantic_loss: 0.01738, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20815 
Train Epoch: 31 [246/1000 7872/32000 (25%)] Loss: 1.96371 (semantic_loss: 0.01624, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20718 
Train Epoch: 31 [251/1000 8032/32000 (25%)] Loss: 1.96330 (semantic_loss: 0.01583, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19915 
Train Epoch: 31 [256/1000 8192/32000 (26%)] Loss: 1.96075 (semantic_loss: 0.01328, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21398 
Train Epoch: 31 [261/1000 8352/32000 (26%)] Loss: 1.96203 (semantic_loss: 0.01455, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=1.59282 
Train Epoch: 31 [266/1000 8512/32000 (27%)] Loss: 1.96520 (semantic_loss: 0.01773, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18751 
Train Epoch: 31 [271/1000 8672/32000 (27%)] Loss: 1.96124 (semantic_loss: 0.01475, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18730 
Train Epoch: 31 [276/1000 8832/32000 (28%)] Loss: 1.96041 (semantic_loss: 0.01392, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18693 
Train Epoch: 31 [281/1000 8992/32000 (28%)] Loss: 1.95945 (semantic_loss: 0.01199, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18923 
Train Epoch: 31 [286/1000 9152/32000 (29%)] Loss: 1.96205 (semantic_loss: 0.01459, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19443 
Train Epoch: 31 [291/1000 9312/32000 (29%)] Loss: 1.96161 (semantic_loss: 0.01414, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20225 
Train Epoch: 31 [296/1000 9472/32000 (30%)] Loss: 1.96156 (semantic_loss: 0.01409, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19662 
Train Epoch: 31 [301/1000 9632/32000 (30%)] Loss: 1.95920 (semantic_loss: 0.01173, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19584 
Train Epoch: 31 [306/1000 9792/32000 (31%)] Loss: 1.96422 (semantic_loss: 0.01675, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18639 
Train Epoch: 31 [311/1000 9952/32000 (31%)] Loss: 1.96144 (semantic_loss: 0.01397, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18804 
Train Epoch: 31 [316/1000 10112/32000 (32%)] Loss: 1.96057 (semantic_loss: 0.01311, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19046 
Train Epoch: 31 [321/1000 10272/32000 (32%)] Loss: 1.96017 (semantic_loss: 0.01270, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19527 
Train Epoch: 31 [326/1000 10432/32000 (33%)] Loss: 1.95873 (semantic_loss: 0.01224, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18545 
Train Epoch: 31 [331/1000 10592/32000 (33%)] Loss: 1.96655 (semantic_loss: 0.01908, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18854 
Train Epoch: 31 [336/1000 10752/32000 (34%)] Loss: 1.95934 (semantic_loss: 0.01285, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18683 
Train Epoch: 31 [341/1000 10912/32000 (34%)] Loss: 1.96495 (semantic_loss: 0.01748, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20969 
Train Epoch: 31 [346/1000 11072/32000 (35%)] Loss: 1.96274 (semantic_loss: 0.01430, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18753 
Train Epoch: 31 [351/1000 11232/32000 (35%)] Loss: 1.95976 (semantic_loss: 0.01229, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18702 
Train Epoch: 31 [356/1000 11392/32000 (36%)] Loss: 1.96136 (semantic_loss: 0.01389, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19086 
Train Epoch: 31 [361/1000 11552/32000 (36%)] Loss: 1.96285 (semantic_loss: 0.01538, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18685 
Train Epoch: 31 [366/1000 11712/32000 (37%)] Loss: 1.95981 (semantic_loss: 0.01331, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18701 
Train Epoch: 31 [371/1000 11872/32000 (37%)] Loss: 1.95871 (semantic_loss: 0.01125, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18623 
Train Epoch: 31 [376/1000 12032/32000 (38%)] Loss: 1.96110 (semantic_loss: 0.01363, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18856 
Train Epoch: 31 [381/1000 12192/32000 (38%)] Loss: 1.96260 (semantic_loss: 0.01513, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19993 
Train Epoch: 31 [386/1000 12352/32000 (39%)] Loss: 1.96083 (semantic_loss: 0.01336, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20496 
Train Epoch: 31 [391/1000 12512/32000 (39%)] Loss: 1.96633 (semantic_loss: 0.01886, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19134 
Train Epoch: 31 [396/1000 12672/32000 (40%)] Loss: 1.96059 (semantic_loss: 0.01215, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21063 
Train Epoch: 31 [401/1000 12832/32000 (40%)] Loss: 1.96872 (semantic_loss: 0.02125, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19364 
Train Epoch: 31 [406/1000 12992/32000 (41%)] Loss: 1.96555 (semantic_loss: 0.01808, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19532 
Train Epoch: 31 [411/1000 13152/32000 (41%)] Loss: 1.96043 (semantic_loss: 0.01394, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19883 
Train Epoch: 31 [416/1000 13312/32000 (42%)] Loss: 1.96456 (semantic_loss: 0.01807, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19336 
Train Epoch: 31 [421/1000 13472/32000 (42%)] Loss: 1.96426 (semantic_loss: 0.01678, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18874 
Train Epoch: 31 [426/1000 13632/32000 (43%)] Loss: 1.95915 (semantic_loss: 0.01265, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18777 
Train Epoch: 31 [431/1000 13792/32000 (43%)] Loss: 1.96354 (semantic_loss: 0.01607, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18707 
Train Epoch: 31 [436/1000 13952/32000 (44%)] Loss: 1.96199 (semantic_loss: 0.01550, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18778 
Train Epoch: 31 [441/1000 14112/32000 (44%)] Loss: 1.96377 (semantic_loss: 0.01532, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19458 
Train Epoch: 31 [446/1000 14272/32000 (45%)] Loss: 1.96041 (semantic_loss: 0.01392, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20466 
Train Epoch: 31 [451/1000 14432/32000 (45%)] Loss: 1.96488 (semantic_loss: 0.01740, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.37589 
Train Epoch: 31 [456/1000 14592/32000 (46%)] Loss: 1.96430 (semantic_loss: 0.01684, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.46808 
Train Epoch: 31 [461/1000 14752/32000 (46%)] Loss: 1.96176 (semantic_loss: 0.01429, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18764 
Train Epoch: 31 [466/1000 14912/32000 (47%)] Loss: 1.96318 (semantic_loss: 0.01571, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20300 
Train Epoch: 31 [471/1000 15072/32000 (47%)] Loss: 1.96355 (semantic_loss: 0.01510, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19115 
Train Epoch: 31 [476/1000 15232/32000 (48%)] Loss: 1.96145 (semantic_loss: 0.01398, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19680 
Train Epoch: 31 [481/1000 15392/32000 (48%)] Loss: 1.96405 (semantic_loss: 0.01658, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18605 
Train Epoch: 31 [486/1000 15552/32000 (49%)] Loss: 1.96078 (semantic_loss: 0.01331, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18796 
Train Epoch: 31 [491/1000 15712/32000 (49%)] Loss: 1.96222 (semantic_loss: 0.01475, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18729 
Train Epoch: 31 [496/1000 15872/32000 (50%)] Loss: 1.96017 (semantic_loss: 0.01368, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18729 
Train Epoch: 31 [501/1000 16032/32000 (50%)] Loss: 1.95821 (semantic_loss: 0.01074, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18771 
Train Epoch: 31 [506/1000 16192/32000 (51%)] Loss: 1.96014 (semantic_loss: 0.01365, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20703 
Train Epoch: 31 [511/1000 16352/32000 (51%)] Loss: 1.96056 (semantic_loss: 0.01407, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21045 
Train Epoch: 31 [516/1000 16512/32000 (52%)] Loss: 1.96173 (semantic_loss: 0.01426, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18838 
Train Epoch: 31 [521/1000 16672/32000 (52%)] Loss: 1.96175 (semantic_loss: 0.01428, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21941 
Train Epoch: 31 [526/1000 16832/32000 (53%)] Loss: 1.95944 (semantic_loss: 0.01197, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22465 
Train Epoch: 31 [531/1000 16992/32000 (53%)] Loss: 1.96121 (semantic_loss: 0.01276, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21370 
Train Epoch: 31 [536/1000 17152/32000 (54%)] Loss: 1.96174 (semantic_loss: 0.01427, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20703 
Train Epoch: 31 [541/1000 17312/32000 (54%)] Loss: 1.96251 (semantic_loss: 0.01504, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19784 
Train Epoch: 31 [546/1000 17472/32000 (55%)] Loss: 1.96303 (semantic_loss: 0.01555, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19791 
Train Epoch: 31 [551/1000 17632/32000 (55%)] Loss: 1.96391 (semantic_loss: 0.01644, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19920 
Train Epoch: 31 [556/1000 17792/32000 (56%)] Loss: 1.96336 (semantic_loss: 0.01590, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19003 
Train Epoch: 31 [561/1000 17952/32000 (56%)] Loss: 1.95917 (semantic_loss: 0.01268, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19248 
Train Epoch: 31 [566/1000 18112/32000 (57%)] Loss: 1.95950 (semantic_loss: 0.01203, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18577 
Train Epoch: 31 [571/1000 18272/32000 (57%)] Loss: 1.96304 (semantic_loss: 0.01557, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19377 
Train Epoch: 31 [576/1000 18432/32000 (58%)] Loss: 1.95921 (semantic_loss: 0.01272, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21653 
Train Epoch: 31 [581/1000 18592/32000 (58%)] Loss: 1.96168 (semantic_loss: 0.01519, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=1.47736 
Train Epoch: 31 [586/1000 18752/32000 (59%)] Loss: 1.96143 (semantic_loss: 0.01494, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19732 
Train Epoch: 31 [591/1000 18912/32000 (59%)] Loss: 1.96014 (semantic_loss: 0.01365, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19974 
Train Epoch: 31 [596/1000 19072/32000 (60%)] Loss: 1.96218 (semantic_loss: 0.01373, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19602 
Train Epoch: 31 [601/1000 19232/32000 (60%)] Loss: 1.96133 (semantic_loss: 0.01386, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18758 
Train Epoch: 31 [606/1000 19392/32000 (61%)] Loss: 1.96444 (semantic_loss: 0.01697, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18854 
Train Epoch: 31 [611/1000 19552/32000 (61%)] Loss: 1.96716 (semantic_loss: 0.01969, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18888 
Train Epoch: 31 [616/1000 19712/32000 (62%)] Loss: 1.96058 (semantic_loss: 0.01311, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18570 
Train Epoch: 31 [621/1000 19872/32000 (62%)] Loss: 1.96243 (semantic_loss: 0.01593, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18648 
Train Epoch: 31 [626/1000 20032/32000 (63%)] Loss: 1.96635 (semantic_loss: 0.01888, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19119 
Train Epoch: 31 [631/1000 20192/32000 (63%)] Loss: 1.96415 (semantic_loss: 0.01668, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19438 
Train Epoch: 31 [636/1000 20352/32000 (64%)] Loss: 1.95919 (semantic_loss: 0.01269, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18798 
Train Epoch: 31 [641/1000 20512/32000 (64%)] Loss: 1.96314 (semantic_loss: 0.01469, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19574 
Train Epoch: 31 [646/1000 20672/32000 (65%)] Loss: 1.96000 (semantic_loss: 0.01350, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18697 
Train Epoch: 31 [651/1000 20832/32000 (65%)] Loss: 1.96226 (semantic_loss: 0.01478, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20950 
Train Epoch: 31 [656/1000 20992/32000 (66%)] Loss: 1.96441 (semantic_loss: 0.01596, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18744 
Train Epoch: 31 [661/1000 21152/32000 (66%)] Loss: 1.96165 (semantic_loss: 0.01515, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19624 
Train Epoch: 31 [666/1000 21312/32000 (67%)] Loss: 1.96085 (semantic_loss: 0.01435, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19001 
Train Epoch: 31 [671/1000 21472/32000 (67%)] Loss: 1.96301 (semantic_loss: 0.01554, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18964 
Train Epoch: 31 [676/1000 21632/32000 (68%)] Loss: 1.96318 (semantic_loss: 0.01571, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20422 
Train Epoch: 31 [681/1000 21792/32000 (68%)] Loss: 1.96123 (semantic_loss: 0.01376, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21164 
Train Epoch: 31 [686/1000 21952/32000 (69%)] Loss: 1.96497 (semantic_loss: 0.01750, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23109 
Train Epoch: 31 [691/1000 22112/32000 (69%)] Loss: 1.96330 (semantic_loss: 0.01681, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21854 
Train Epoch: 31 [696/1000 22272/32000 (70%)] Loss: 1.96208 (semantic_loss: 0.01558, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20916 
Train Epoch: 31 [701/1000 22432/32000 (70%)] Loss: 1.95945 (semantic_loss: 0.01198, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24106 
Train Epoch: 31 [706/1000 22592/32000 (71%)] Loss: 1.96112 (semantic_loss: 0.01561, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19698 
Train Epoch: 31 [711/1000 22752/32000 (71%)] Loss: 1.96263 (semantic_loss: 0.01613, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18944 
Train Epoch: 31 [716/1000 22912/32000 (72%)] Loss: 1.95957 (semantic_loss: 0.01307, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18901 
Train Epoch: 31 [721/1000 23072/32000 (72%)] Loss: 1.96220 (semantic_loss: 0.01376, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20022 
Train Epoch: 31 [726/1000 23232/32000 (73%)] Loss: 1.96057 (semantic_loss: 0.01310, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21537 
Train Epoch: 31 [731/1000 23392/32000 (73%)] Loss: 1.96323 (semantic_loss: 0.01576, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18820 
Train Epoch: 31 [736/1000 23552/32000 (74%)] Loss: 1.96260 (semantic_loss: 0.01415, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19004 
Train Epoch: 31 [741/1000 23712/32000 (74%)] Loss: 1.96156 (semantic_loss: 0.01507, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19064 
Train Epoch: 31 [746/1000 23872/32000 (75%)] Loss: 1.96285 (semantic_loss: 0.01636, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19191 
Train Epoch: 31 [751/1000 24032/32000 (75%)] Loss: 1.95929 (semantic_loss: 0.01182, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19298 
Train Epoch: 31 [756/1000 24192/32000 (76%)] Loss: 1.96242 (semantic_loss: 0.01495, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18625 
Train Epoch: 31 [761/1000 24352/32000 (76%)] Loss: 1.96439 (semantic_loss: 0.01595, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19311 
Train Epoch: 31 [766/1000 24512/32000 (77%)] Loss: 1.96858 (semantic_loss: 0.02209, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19519 
Train Epoch: 31 [771/1000 24672/32000 (77%)] Loss: 1.96593 (semantic_loss: 0.01651, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.35385 
Train Epoch: 31 [776/1000 24832/32000 (78%)] Loss: 1.96342 (semantic_loss: 0.01595, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18787 
Train Epoch: 31 [781/1000 24992/32000 (78%)] Loss: 1.96199 (semantic_loss: 0.01355, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18430 
Train Epoch: 31 [786/1000 25152/32000 (79%)] Loss: 1.96306 (semantic_loss: 0.01560, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18825 
Train Epoch: 31 [791/1000 25312/32000 (79%)] Loss: 1.96131 (semantic_loss: 0.01482, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18623 
Train Epoch: 31 [796/1000 25472/32000 (80%)] Loss: 1.96399 (semantic_loss: 0.01652, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18663 
Train Epoch: 31 [801/1000 25632/32000 (80%)] Loss: 1.96174 (semantic_loss: 0.01427, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18693 
Train Epoch: 31 [806/1000 25792/32000 (81%)] Loss: 1.95907 (semantic_loss: 0.01356, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19080 
Train Epoch: 31 [811/1000 25952/32000 (81%)] Loss: 1.96436 (semantic_loss: 0.01690, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20920 
Train Epoch: 31 [816/1000 26112/32000 (82%)] Loss: 1.96336 (semantic_loss: 0.01589, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18728 
Train Epoch: 31 [821/1000 26272/32000 (82%)] Loss: 1.96063 (semantic_loss: 0.01316, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19233 
Train Epoch: 31 [826/1000 26432/32000 (83%)] Loss: 1.96087 (semantic_loss: 0.01340, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19625 
Train Epoch: 31 [831/1000 26592/32000 (83%)] Loss: 1.96020 (semantic_loss: 0.01177, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19045 
Train Epoch: 31 [836/1000 26752/32000 (84%)] Loss: 1.96124 (semantic_loss: 0.01377, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20760 
Train Epoch: 31 [841/1000 26912/32000 (84%)] Loss: 1.96082 (semantic_loss: 0.01432, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23439 
Train Epoch: 31 [846/1000 27072/32000 (85%)] Loss: 1.96245 (semantic_loss: 0.01498, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21418 
Train Epoch: 31 [851/1000 27232/32000 (85%)] Loss: 1.96005 (semantic_loss: 0.01259, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22449 
Train Epoch: 31 [856/1000 27392/32000 (86%)] Loss: 1.96598 (semantic_loss: 0.01851, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19410 
Train Epoch: 31 [861/1000 27552/32000 (86%)] Loss: 1.96049 (semantic_loss: 0.01302, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19626 
Train Epoch: 31 [866/1000 27712/32000 (87%)] Loss: 1.96397 (semantic_loss: 0.01552, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19319 
Train Epoch: 31 [871/1000 27872/32000 (87%)] Loss: 1.95881 (semantic_loss: 0.01232, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18886 
Train Epoch: 31 [876/1000 28032/32000 (88%)] Loss: 1.96064 (semantic_loss: 0.01415, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18335 
Train Epoch: 31 [881/1000 28192/32000 (88%)] Loss: 1.95894 (semantic_loss: 0.01148, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18849 
Train Epoch: 31 [886/1000 28352/32000 (89%)] Loss: 1.96297 (semantic_loss: 0.01551, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18670 
Train Epoch: 31 [891/1000 28512/32000 (89%)] Loss: 1.96533 (semantic_loss: 0.01786, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18660 
Train Epoch: 31 [896/1000 28672/32000 (90%)] Loss: 1.96462 (semantic_loss: 0.01715, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22325 
Train Epoch: 31 [901/1000 28832/32000 (90%)] Loss: 1.96097 (semantic_loss: 0.01351, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=1.50581 
Train Epoch: 31 [906/1000 28992/32000 (91%)] Loss: 1.96823 (semantic_loss: 0.02075, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21225 
Train Epoch: 31 [911/1000 29152/32000 (91%)] Loss: 1.96310 (semantic_loss: 0.01563, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18865 
Train Epoch: 31 [916/1000 29312/32000 (92%)] Loss: 1.96009 (semantic_loss: 0.01360, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18626 
Train Epoch: 31 [921/1000 29472/32000 (92%)] Loss: 1.96480 (semantic_loss: 0.01733, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18704 
Train Epoch: 31 [926/1000 29632/32000 (93%)] Loss: 1.96475 (semantic_loss: 0.01826, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18736 
Train Epoch: 31 [931/1000 29792/32000 (93%)] Loss: 1.96221 (semantic_loss: 0.01474, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18870 
Train Epoch: 31 [936/1000 29952/32000 (94%)] Loss: 1.96322 (semantic_loss: 0.01673, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18572 
Train Epoch: 31 [941/1000 30112/32000 (94%)] Loss: 1.96088 (semantic_loss: 0.01341, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23706 
Train Epoch: 31 [946/1000 30272/32000 (95%)] Loss: 1.96290 (semantic_loss: 0.01641, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18954 
Train Epoch: 31 [951/1000 30432/32000 (95%)] Loss: 1.96077 (semantic_loss: 0.01427, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18527 
Train Epoch: 31 [956/1000 30592/32000 (96%)] Loss: 1.96140 (semantic_loss: 0.01394, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18473 
Train Epoch: 31 [961/1000 30752/32000 (96%)] Loss: 1.96170 (semantic_loss: 0.01520, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19689 
Train Epoch: 31 [966/1000 30912/32000 (97%)] Loss: 1.96607 (semantic_loss: 0.01861, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18944 
Train Epoch: 31 [971/1000 31072/32000 (97%)] Loss: 1.96220 (semantic_loss: 0.01473, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18699 
Train Epoch: 31 [976/1000 31232/32000 (98%)] Loss: 1.96378 (semantic_loss: 0.01534, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18745 
Train Epoch: 31 [981/1000 31392/32000 (98%)] Loss: 1.96089 (semantic_loss: 0.01342, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18595 
Train Epoch: 31 [986/1000 31552/32000 (99%)] Loss: 1.96140 (semantic_loss: 0.01490, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21003 
Train Epoch: 31 [991/1000 31712/32000 (99%)] Loss: 1.96009 (semantic_loss: 0.01360, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22166 
Train Epoch: 31 [996/1000 31872/32000 (100%)] Loss: 1.96389 (semantic_loss: 0.01544, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22042 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch31.pth ...
Done in 4.419s
removing stale ckpt [epoch 30] [took 0.00s]
 epoch          : 31
 loss           : 1.9622343287467956
 learning_rate  : 2.119557913760812e-06
 n_samples      : 992000
 n_steps        : 31000
 ActivityNet_val1_test/t2v_metrics/R1: 11.958511287370348
 ActivityNet_val1_test/t2v_metrics/R5: 38.173683140126094
 ActivityNet_val1_test/t2v_metrics/R10: 55.4403091315843
 ActivityNet_val1_test/t2v_metrics/R50: 85.15354891193817
 ActivityNet_val1_test/t2v_metrics/MedR: 8.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 60.808928208257065
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.35997006486386
 ActivityNet_val1_test/v2t_metrics/R1: 12.1008745169819
 ActivityNet_val1_test/v2t_metrics/R5: 39.271913768558065
 ActivityNet_val1_test/v2t_metrics/R10: 55.9080740288794
 ActivityNet_val1_test/v2t_metrics/R50: 84.52308318080131
 ActivityNet_val1_test/v2t_metrics/MedR: 8.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 63.54972544234289
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.839470900323242
 mnt_best       : 29.542107379154057
 not_improved_count: 4
Train Epoch: 32 [1/1000 32/32000 (0%)] Loss: 1.96187 (semantic_loss: 0.01440, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=23.03928 
Train Epoch: 32 [6/1000 192/32000 (1%)] Loss: 1.95974 (semantic_loss: 0.01325, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18916 
Train Epoch: 32 [11/1000 352/32000 (1%)] Loss: 1.96045 (semantic_loss: 0.01298, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18405 
Train Epoch: 32 [16/1000 512/32000 (2%)] Loss: 1.96202 (semantic_loss: 0.01358, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.50002 
Train Epoch: 32 [21/1000 672/32000 (2%)] Loss: 1.96349 (semantic_loss: 0.01601, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19028 
Train Epoch: 32 [26/1000 832/32000 (3%)] Loss: 1.96466 (semantic_loss: 0.01720, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19726 
Train Epoch: 32 [31/1000 992/32000 (3%)] Loss: 1.96427 (semantic_loss: 0.01681, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20690 
Train Epoch: 32 [36/1000 1152/32000 (4%)] Loss: 1.96058 (semantic_loss: 0.01311, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19673 
Train Epoch: 32 [41/1000 1312/32000 (4%)] Loss: 1.96081 (semantic_loss: 0.01334, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18487 
Train Epoch: 32 [46/1000 1472/32000 (5%)] Loss: 1.96432 (semantic_loss: 0.01685, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18867 
Train Epoch: 32 [51/1000 1632/32000 (5%)] Loss: 1.95980 (semantic_loss: 0.01330, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18659 
Train Epoch: 32 [56/1000 1792/32000 (6%)] Loss: 1.96553 (semantic_loss: 0.01806, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19844 
Train Epoch: 32 [61/1000 1952/32000 (6%)] Loss: 1.96736 (semantic_loss: 0.01989, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19015 
Train Epoch: 32 [66/1000 2112/32000 (7%)] Loss: 1.96283 (semantic_loss: 0.01439, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18823 
Train Epoch: 32 [71/1000 2272/32000 (7%)] Loss: 1.96298 (semantic_loss: 0.01550, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21026 
Train Epoch: 32 [76/1000 2432/32000 (8%)] Loss: 1.96451 (semantic_loss: 0.01802, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18615 
Train Epoch: 32 [81/1000 2592/32000 (8%)] Loss: 1.96445 (semantic_loss: 0.01698, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.29701 
Train Epoch: 32 [86/1000 2752/32000 (9%)] Loss: 1.96037 (semantic_loss: 0.01290, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18884 
Train Epoch: 32 [91/1000 2912/32000 (9%)] Loss: 1.96189 (semantic_loss: 0.01441, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21305 
Train Epoch: 32 [96/1000 3072/32000 (10%)] Loss: 1.95899 (semantic_loss: 0.01250, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20116 
Train Epoch: 32 [101/1000 3232/32000 (10%)] Loss: 1.96320 (semantic_loss: 0.01573, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19021 
Train Epoch: 32 [106/1000 3392/32000 (11%)] Loss: 1.96301 (semantic_loss: 0.01554, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26985 
Train Epoch: 32 [111/1000 3552/32000 (11%)] Loss: 1.96229 (semantic_loss: 0.01385, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18792 
Train Epoch: 32 [116/1000 3712/32000 (12%)] Loss: 1.95977 (semantic_loss: 0.01327, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21166 
Train Epoch: 32 [121/1000 3872/32000 (12%)] Loss: 1.96461 (semantic_loss: 0.01714, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22101 
Train Epoch: 32 [126/1000 4032/32000 (13%)] Loss: 1.96037 (semantic_loss: 0.01290, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21641 
Train Epoch: 32 [131/1000 4192/32000 (13%)] Loss: 1.96118 (semantic_loss: 0.01372, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27513 
Train Epoch: 32 [136/1000 4352/32000 (14%)] Loss: 1.95966 (semantic_loss: 0.01220, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19254 
Train Epoch: 32 [141/1000 4512/32000 (14%)] Loss: 1.96236 (semantic_loss: 0.01587, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19221 
Train Epoch: 32 [146/1000 4672/32000 (15%)] Loss: 1.96512 (semantic_loss: 0.01764, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19438 
Train Epoch: 32 [151/1000 4832/32000 (15%)] Loss: 1.96215 (semantic_loss: 0.01566, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.80787 
Train Epoch: 32 [156/1000 4992/32000 (16%)] Loss: 1.96065 (semantic_loss: 0.01318, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19007 
Train Epoch: 32 [161/1000 5152/32000 (16%)] Loss: 1.96014 (semantic_loss: 0.01365, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18993 
Train Epoch: 32 [166/1000 5312/32000 (17%)] Loss: 1.96147 (semantic_loss: 0.01303, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19757 
Train Epoch: 32 [171/1000 5472/32000 (17%)] Loss: 1.96256 (semantic_loss: 0.01412, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18888 
Train Epoch: 32 [176/1000 5632/32000 (18%)] Loss: 1.96023 (semantic_loss: 0.01471, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18901 
Train Epoch: 32 [181/1000 5792/32000 (18%)] Loss: 1.96151 (semantic_loss: 0.01403, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18947 
Train Epoch: 32 [186/1000 5952/32000 (19%)] Loss: 1.96089 (semantic_loss: 0.01342, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19171 
Train Epoch: 32 [191/1000 6112/32000 (19%)] Loss: 1.96293 (semantic_loss: 0.01644, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20433 
Train Epoch: 32 [196/1000 6272/32000 (20%)] Loss: 1.96061 (semantic_loss: 0.01412, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.28353 
Train Epoch: 32 [201/1000 6432/32000 (20%)] Loss: 1.96330 (semantic_loss: 0.01387, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.19622 
Train Epoch: 32 [206/1000 6592/32000 (21%)] Loss: 1.96235 (semantic_loss: 0.01487, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20890 
Train Epoch: 32 [211/1000 6752/32000 (21%)] Loss: 1.96360 (semantic_loss: 0.01613, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18615 
Train Epoch: 32 [216/1000 6912/32000 (22%)] Loss: 1.96926 (semantic_loss: 0.02082, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18845 
Train Epoch: 32 [221/1000 7072/32000 (22%)] Loss: 1.96202 (semantic_loss: 0.01456, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18616 
Train Epoch: 32 [226/1000 7232/32000 (23%)] Loss: 1.96321 (semantic_loss: 0.01574, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19753 
Train Epoch: 32 [231/1000 7392/32000 (23%)] Loss: 1.95852 (semantic_loss: 0.01202, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19400 
Train Epoch: 32 [236/1000 7552/32000 (24%)] Loss: 1.95999 (semantic_loss: 0.01448, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20199 
Train Epoch: 32 [241/1000 7712/32000 (24%)] Loss: 1.96254 (semantic_loss: 0.01410, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19619 
Train Epoch: 32 [246/1000 7872/32000 (25%)] Loss: 1.96069 (semantic_loss: 0.01323, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19615 
Train Epoch: 32 [251/1000 8032/32000 (25%)] Loss: 1.96361 (semantic_loss: 0.01614, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19837 
Train Epoch: 32 [256/1000 8192/32000 (26%)] Loss: 1.96213 (semantic_loss: 0.01368, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19444 
Train Epoch: 32 [261/1000 8352/32000 (26%)] Loss: 1.96132 (semantic_loss: 0.01580, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18763 
Train Epoch: 32 [266/1000 8512/32000 (27%)] Loss: 1.96443 (semantic_loss: 0.01697, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18900 
Train Epoch: 32 [271/1000 8672/32000 (27%)] Loss: 1.96208 (semantic_loss: 0.01461, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20337 
Train Epoch: 32 [276/1000 8832/32000 (28%)] Loss: 1.96207 (semantic_loss: 0.01461, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27944 
Train Epoch: 32 [281/1000 8992/32000 (28%)] Loss: 1.96020 (semantic_loss: 0.01370, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22037 
Train Epoch: 32 [286/1000 9152/32000 (29%)] Loss: 1.96075 (semantic_loss: 0.01328, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20167 
Train Epoch: 32 [291/1000 9312/32000 (29%)] Loss: 1.96166 (semantic_loss: 0.01516, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20553 
Train Epoch: 32 [296/1000 9472/32000 (30%)] Loss: 1.96250 (semantic_loss: 0.01504, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19314 
Train Epoch: 32 [301/1000 9632/32000 (30%)] Loss: 1.96701 (semantic_loss: 0.01856, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20874 
Train Epoch: 32 [306/1000 9792/32000 (31%)] Loss: 1.96209 (semantic_loss: 0.01463, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19143 
Train Epoch: 32 [311/1000 9952/32000 (31%)] Loss: 1.96403 (semantic_loss: 0.01656, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22252 
Train Epoch: 32 [316/1000 10112/32000 (32%)] Loss: 1.96441 (semantic_loss: 0.01694, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18748 
Train Epoch: 32 [321/1000 10272/32000 (32%)] Loss: 1.96416 (semantic_loss: 0.01669, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20592 
Train Epoch: 32 [326/1000 10432/32000 (33%)] Loss: 1.96316 (semantic_loss: 0.01471, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19076 
Train Epoch: 32 [331/1000 10592/32000 (33%)] Loss: 1.96296 (semantic_loss: 0.01549, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18727 
Train Epoch: 32 [336/1000 10752/32000 (34%)] Loss: 1.96482 (semantic_loss: 0.01735, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.54378 
Train Epoch: 32 [341/1000 10912/32000 (34%)] Loss: 1.96146 (semantic_loss: 0.01497, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19988 
Train Epoch: 32 [346/1000 11072/32000 (35%)] Loss: 1.96179 (semantic_loss: 0.01627, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20263 
Train Epoch: 32 [351/1000 11232/32000 (35%)] Loss: 1.96171 (semantic_loss: 0.01423, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19325 
Train Epoch: 32 [356/1000 11392/32000 (36%)] Loss: 1.96348 (semantic_loss: 0.01699, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19329 
Train Epoch: 32 [361/1000 11552/32000 (36%)] Loss: 1.96111 (semantic_loss: 0.01462, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21772 
Train Epoch: 32 [366/1000 11712/32000 (37%)] Loss: 1.95998 (semantic_loss: 0.01348, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18757 
Train Epoch: 32 [371/1000 11872/32000 (37%)] Loss: 1.96408 (semantic_loss: 0.01661, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18592 
Train Epoch: 32 [376/1000 12032/32000 (38%)] Loss: 1.96115 (semantic_loss: 0.01466, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18722 
Train Epoch: 32 [381/1000 12192/32000 (38%)] Loss: 1.96043 (semantic_loss: 0.01394, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18289 
Train Epoch: 32 [386/1000 12352/32000 (39%)] Loss: 1.96659 (semantic_loss: 0.01912, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18835 
Train Epoch: 32 [391/1000 12512/32000 (39%)] Loss: 1.95993 (semantic_loss: 0.01246, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20181 
Train Epoch: 32 [396/1000 12672/32000 (40%)] Loss: 1.95935 (semantic_loss: 0.01286, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18731 
Train Epoch: 32 [401/1000 12832/32000 (40%)] Loss: 1.96337 (semantic_loss: 0.01590, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.31720 
Train Epoch: 32 [406/1000 12992/32000 (41%)] Loss: 1.95939 (semantic_loss: 0.01191, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19486 
Train Epoch: 32 [411/1000 13152/32000 (41%)] Loss: 1.96259 (semantic_loss: 0.01609, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18742 
Train Epoch: 32 [416/1000 13312/32000 (42%)] Loss: 1.95919 (semantic_loss: 0.01173, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21179 
Train Epoch: 32 [421/1000 13472/32000 (42%)] Loss: 1.96043 (semantic_loss: 0.01394, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21241 
Train Epoch: 32 [426/1000 13632/32000 (43%)] Loss: 1.96443 (semantic_loss: 0.01794, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.28865 
Train Epoch: 32 [431/1000 13792/32000 (43%)] Loss: 1.96720 (semantic_loss: 0.02070, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21120 
Train Epoch: 32 [436/1000 13952/32000 (44%)] Loss: 1.96304 (semantic_loss: 0.01459, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20906 
Train Epoch: 32 [441/1000 14112/32000 (44%)] Loss: 1.96506 (semantic_loss: 0.01661, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23113 
Train Epoch: 32 [446/1000 14272/32000 (45%)] Loss: 1.96130 (semantic_loss: 0.01383, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20723 
Train Epoch: 32 [451/1000 14432/32000 (45%)] Loss: 1.96452 (semantic_loss: 0.01608, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.25492 
Train Epoch: 32 [456/1000 14592/32000 (46%)] Loss: 1.96328 (semantic_loss: 0.01581, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20179 
Train Epoch: 32 [461/1000 14752/32000 (46%)] Loss: 1.96338 (semantic_loss: 0.01493, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18646 
Train Epoch: 32 [466/1000 14912/32000 (47%)] Loss: 1.96121 (semantic_loss: 0.01374, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18929 
Train Epoch: 32 [471/1000 15072/32000 (47%)] Loss: 1.96240 (semantic_loss: 0.01396, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.76696 
Train Epoch: 32 [476/1000 15232/32000 (48%)] Loss: 1.95956 (semantic_loss: 0.01404, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.20864 
Train Epoch: 32 [481/1000 15392/32000 (48%)] Loss: 1.95883 (semantic_loss: 0.01136, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19212 
Train Epoch: 32 [486/1000 15552/32000 (49%)] Loss: 1.95999 (semantic_loss: 0.01350, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19283 
Train Epoch: 32 [491/1000 15712/32000 (49%)] Loss: 1.96413 (semantic_loss: 0.01569, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19033 
Train Epoch: 32 [496/1000 15872/32000 (50%)] Loss: 1.96041 (semantic_loss: 0.01294, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18605 
Train Epoch: 32 [501/1000 16032/32000 (50%)] Loss: 1.95987 (semantic_loss: 0.01436, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18827 
Train Epoch: 32 [506/1000 16192/32000 (51%)] Loss: 1.96024 (semantic_loss: 0.01375, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18686 
Train Epoch: 32 [511/1000 16352/32000 (51%)] Loss: 1.96443 (semantic_loss: 0.01793, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20027 
Train Epoch: 32 [516/1000 16512/32000 (52%)] Loss: 1.96017 (semantic_loss: 0.01368, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.28078 
Train Epoch: 32 [521/1000 16672/32000 (52%)] Loss: 1.96406 (semantic_loss: 0.01757, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19496 
Train Epoch: 32 [526/1000 16832/32000 (53%)] Loss: 1.96106 (semantic_loss: 0.01456, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20297 
Train Epoch: 32 [531/1000 16992/32000 (53%)] Loss: 1.96736 (semantic_loss: 0.02086, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18654 
Train Epoch: 32 [536/1000 17152/32000 (54%)] Loss: 1.96501 (semantic_loss: 0.01656, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18622 
Train Epoch: 32 [541/1000 17312/32000 (54%)] Loss: 1.96081 (semantic_loss: 0.01432, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18842 
Train Epoch: 32 [546/1000 17472/32000 (55%)] Loss: 1.96332 (semantic_loss: 0.01584, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18895 
Train Epoch: 32 [551/1000 17632/32000 (55%)] Loss: 1.96225 (semantic_loss: 0.01381, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19128 
Train Epoch: 32 [556/1000 17792/32000 (56%)] Loss: 1.96197 (semantic_loss: 0.01548, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18742 
Train Epoch: 32 [561/1000 17952/32000 (56%)] Loss: 1.96033 (semantic_loss: 0.01383, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18682 
Train Epoch: 32 [566/1000 18112/32000 (57%)] Loss: 1.96001 (semantic_loss: 0.01255, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18575 
Train Epoch: 32 [571/1000 18272/32000 (57%)] Loss: 1.95926 (semantic_loss: 0.01180, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18852 
Train Epoch: 32 [576/1000 18432/32000 (58%)] Loss: 1.96072 (semantic_loss: 0.01423, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21258 
Train Epoch: 32 [581/1000 18592/32000 (58%)] Loss: 1.96116 (semantic_loss: 0.01370, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21922 
Train Epoch: 32 [586/1000 18752/32000 (59%)] Loss: 1.96020 (semantic_loss: 0.01273, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21803 
Train Epoch: 32 [591/1000 18912/32000 (59%)] Loss: 1.95796 (semantic_loss: 0.01147, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24172 
Train Epoch: 32 [596/1000 19072/32000 (60%)] Loss: 1.96090 (semantic_loss: 0.01343, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.26683 
Train Epoch: 32 [601/1000 19232/32000 (60%)] Loss: 1.96348 (semantic_loss: 0.01504, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21935 
Train Epoch: 32 [606/1000 19392/32000 (61%)] Loss: 1.96311 (semantic_loss: 0.01662, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20807 
Train Epoch: 32 [611/1000 19552/32000 (61%)] Loss: 1.96168 (semantic_loss: 0.01323, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20064 
Train Epoch: 32 [616/1000 19712/32000 (62%)] Loss: 1.96238 (semantic_loss: 0.01589, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19119 
Train Epoch: 32 [621/1000 19872/32000 (62%)] Loss: 1.95871 (semantic_loss: 0.01320, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19101 
Train Epoch: 32 [626/1000 20032/32000 (63%)] Loss: 1.96133 (semantic_loss: 0.01484, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18606 
Train Epoch: 32 [631/1000 20192/32000 (63%)] Loss: 1.96204 (semantic_loss: 0.01555, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22449 
Train Epoch: 32 [636/1000 20352/32000 (64%)] Loss: 1.96030 (semantic_loss: 0.01381, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19180 
Train Epoch: 32 [641/1000 20512/32000 (64%)] Loss: 1.96011 (semantic_loss: 0.01166, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21822 
Train Epoch: 32 [646/1000 20672/32000 (65%)] Loss: 1.96229 (semantic_loss: 0.01481, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19969 
Train Epoch: 32 [651/1000 20832/32000 (65%)] Loss: 1.96174 (semantic_loss: 0.01426, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19572 
Train Epoch: 32 [656/1000 20992/32000 (66%)] Loss: 1.96374 (semantic_loss: 0.01627, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.49405 
Train Epoch: 32 [661/1000 21152/32000 (66%)] Loss: 1.96359 (semantic_loss: 0.01515, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18830 
Train Epoch: 32 [666/1000 21312/32000 (67%)] Loss: 1.95987 (semantic_loss: 0.01435, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18677 
Train Epoch: 32 [671/1000 21472/32000 (67%)] Loss: 1.96266 (semantic_loss: 0.01422, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18997 
Train Epoch: 32 [676/1000 21632/32000 (68%)] Loss: 1.96402 (semantic_loss: 0.01655, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19026 
Train Epoch: 32 [681/1000 21792/32000 (68%)] Loss: 1.96060 (semantic_loss: 0.01313, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18552 
Train Epoch: 32 [686/1000 21952/32000 (69%)] Loss: 1.96310 (semantic_loss: 0.01466, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19017 
Train Epoch: 32 [691/1000 22112/32000 (69%)] Loss: 1.96499 (semantic_loss: 0.01655, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18731 
Train Epoch: 32 [696/1000 22272/32000 (70%)] Loss: 1.96004 (semantic_loss: 0.01355, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18561 
Train Epoch: 32 [701/1000 22432/32000 (70%)] Loss: 1.96207 (semantic_loss: 0.01460, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19012 
Train Epoch: 32 [706/1000 22592/32000 (71%)] Loss: 1.96594 (semantic_loss: 0.01749, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18837 
Train Epoch: 32 [711/1000 22752/32000 (71%)] Loss: 1.96145 (semantic_loss: 0.01398, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18740 
Train Epoch: 32 [716/1000 22912/32000 (72%)] Loss: 1.96372 (semantic_loss: 0.01723, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18895 
Train Epoch: 32 [721/1000 23072/32000 (72%)] Loss: 1.96076 (semantic_loss: 0.01427, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.28959 
Train Epoch: 32 [726/1000 23232/32000 (73%)] Loss: 1.96486 (semantic_loss: 0.01739, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18758 
Train Epoch: 32 [731/1000 23392/32000 (73%)] Loss: 1.96082 (semantic_loss: 0.01335, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22015 
Train Epoch: 32 [736/1000 23552/32000 (74%)] Loss: 1.96613 (semantic_loss: 0.01964, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20337 
Train Epoch: 32 [741/1000 23712/32000 (74%)] Loss: 1.96339 (semantic_loss: 0.01690, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21126 
Train Epoch: 32 [746/1000 23872/32000 (75%)] Loss: 1.96331 (semantic_loss: 0.01487, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.25874 
Train Epoch: 32 [751/1000 24032/32000 (75%)] Loss: 1.96205 (semantic_loss: 0.01458, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20043 
Train Epoch: 32 [756/1000 24192/32000 (76%)] Loss: 1.96225 (semantic_loss: 0.01478, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19260 
Train Epoch: 32 [761/1000 24352/32000 (76%)] Loss: 1.96472 (semantic_loss: 0.01725, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19950 
Train Epoch: 32 [766/1000 24512/32000 (77%)] Loss: 1.96051 (semantic_loss: 0.01402, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19208 
Train Epoch: 32 [771/1000 24672/32000 (77%)] Loss: 1.96207 (semantic_loss: 0.01459, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24568 
Train Epoch: 32 [776/1000 24832/32000 (78%)] Loss: 1.96257 (semantic_loss: 0.01510, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18942 
Train Epoch: 32 [781/1000 24992/32000 (78%)] Loss: 1.96084 (semantic_loss: 0.01338, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19705 
Train Epoch: 32 [786/1000 25152/32000 (79%)] Loss: 1.96298 (semantic_loss: 0.01551, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22250 
Train Epoch: 32 [791/1000 25312/32000 (79%)] Loss: 1.96089 (semantic_loss: 0.01440, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.77208 
Train Epoch: 32 [796/1000 25472/32000 (80%)] Loss: 1.96311 (semantic_loss: 0.01466, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19424 
Train Epoch: 32 [801/1000 25632/32000 (80%)] Loss: 1.95895 (semantic_loss: 0.01343, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19349 
Train Epoch: 32 [806/1000 25792/32000 (81%)] Loss: 1.96407 (semantic_loss: 0.01660, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19275 
Train Epoch: 32 [811/1000 25952/32000 (81%)] Loss: 1.96102 (semantic_loss: 0.01355, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18779 
Train Epoch: 32 [816/1000 26112/32000 (82%)] Loss: 1.96150 (semantic_loss: 0.01306, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19400 
Train Epoch: 32 [821/1000 26272/32000 (82%)] Loss: 1.96159 (semantic_loss: 0.01315, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19137 
Train Epoch: 32 [826/1000 26432/32000 (83%)] Loss: 1.95942 (semantic_loss: 0.01391, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18770 
Train Epoch: 32 [831/1000 26592/32000 (83%)] Loss: 1.96070 (semantic_loss: 0.01324, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20118 
Train Epoch: 32 [836/1000 26752/32000 (84%)] Loss: 1.96517 (semantic_loss: 0.01770, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27779 
Train Epoch: 32 [841/1000 26912/32000 (84%)] Loss: 1.96508 (semantic_loss: 0.01761, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19397 
Train Epoch: 32 [846/1000 27072/32000 (85%)] Loss: 1.96206 (semantic_loss: 0.01459, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20191 
Train Epoch: 32 [851/1000 27232/32000 (85%)] Loss: 1.96208 (semantic_loss: 0.01363, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18636 
Train Epoch: 32 [856/1000 27392/32000 (86%)] Loss: 1.96343 (semantic_loss: 0.01498, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18666 
Train Epoch: 32 [861/1000 27552/32000 (86%)] Loss: 1.96467 (semantic_loss: 0.01720, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18639 
Train Epoch: 32 [866/1000 27712/32000 (87%)] Loss: 1.96400 (semantic_loss: 0.01653, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18823 
Train Epoch: 32 [871/1000 27872/32000 (87%)] Loss: 1.96140 (semantic_loss: 0.01394, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18874 
Train Epoch: 32 [876/1000 28032/32000 (88%)] Loss: 1.97445 (semantic_loss: 0.02601, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18443 
Train Epoch: 32 [881/1000 28192/32000 (88%)] Loss: 1.96608 (semantic_loss: 0.01959, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20541 
Train Epoch: 32 [886/1000 28352/32000 (89%)] Loss: 1.96056 (semantic_loss: 0.01310, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21646 
Train Epoch: 32 [891/1000 28512/32000 (89%)] Loss: 1.96012 (semantic_loss: 0.01265, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23572 
Train Epoch: 32 [896/1000 28672/32000 (90%)] Loss: 1.96157 (semantic_loss: 0.01508, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21717 
Train Epoch: 32 [901/1000 28832/32000 (90%)] Loss: 1.96224 (semantic_loss: 0.01476, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21684 
Train Epoch: 32 [906/1000 28992/32000 (91%)] Loss: 1.95738 (semantic_loss: 0.01186, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.21101 
Train Epoch: 32 [911/1000 29152/32000 (91%)] Loss: 1.96004 (semantic_loss: 0.01354, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23035 
Train Epoch: 32 [916/1000 29312/32000 (92%)] Loss: 1.96002 (semantic_loss: 0.01255, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26096 
Train Epoch: 32 [921/1000 29472/32000 (92%)] Loss: 1.96303 (semantic_loss: 0.01751, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18758 
Train Epoch: 32 [926/1000 29632/32000 (93%)] Loss: 1.96536 (semantic_loss: 0.01691, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18748 
Train Epoch: 32 [931/1000 29792/32000 (93%)] Loss: 1.95934 (semantic_loss: 0.01187, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18828 
Train Epoch: 32 [936/1000 29952/32000 (94%)] Loss: 1.96419 (semantic_loss: 0.01574, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19406 
Train Epoch: 32 [941/1000 30112/32000 (94%)] Loss: 1.96337 (semantic_loss: 0.01493, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18736 
Train Epoch: 32 [946/1000 30272/32000 (95%)] Loss: 1.96325 (semantic_loss: 0.01676, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19406 
Train Epoch: 32 [951/1000 30432/32000 (95%)] Loss: 1.96096 (semantic_loss: 0.01446, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23210 
Train Epoch: 32 [956/1000 30592/32000 (96%)] Loss: 1.96359 (semantic_loss: 0.01514, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19069 
Train Epoch: 32 [961/1000 30752/32000 (96%)] Loss: 1.96228 (semantic_loss: 0.01286, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.20817 
Train Epoch: 32 [966/1000 30912/32000 (97%)] Loss: 1.96480 (semantic_loss: 0.01733, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18851 
Train Epoch: 32 [971/1000 31072/32000 (97%)] Loss: 1.96080 (semantic_loss: 0.01333, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18660 
Train Epoch: 32 [976/1000 31232/32000 (98%)] Loss: 1.96303 (semantic_loss: 0.01556, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.48824 
Train Epoch: 32 [981/1000 31392/32000 (98%)] Loss: 1.96310 (semantic_loss: 0.01563, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18624 
Train Epoch: 32 [986/1000 31552/32000 (99%)] Loss: 1.96404 (semantic_loss: 0.01657, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18923 
Train Epoch: 32 [991/1000 31712/32000 (99%)] Loss: 1.96180 (semantic_loss: 0.01433, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18778 
Train Epoch: 32 [996/1000 31872/32000 (100%)] Loss: 1.96149 (semantic_loss: 0.01500, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18591 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch32.pth ...
Done in 16.641s
removing stale ckpt [epoch 31] [took 0.00s]
 epoch          : 32
 loss           : 1.9621003580093384
 learning_rate  : 1.907602122384731e-06
 n_samples      : 1024000
 n_steps        : 32000
 ActivityNet_val1_test/t2v_metrics/R1: 12.141549725442342
 ActivityNet_val1_test/t2v_metrics/R5: 38.194020744356315
 ActivityNet_val1_test/t2v_metrics/R10: 55.09456985967053
 ActivityNet_val1_test/t2v_metrics/R50: 85.25523693308928
 ActivityNet_val1_test/t2v_metrics/MedR: 8.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 61.11846654464104
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.452766865991318
 ActivityNet_val1_test/v2t_metrics/R1: 12.283912955053895
 ActivityNet_val1_test/v2t_metrics/R5: 39.2109009558674
 ActivityNet_val1_test/v2t_metrics/R10: 55.74537319503762
 ActivityNet_val1_test/v2t_metrics/R50: 85.1738865161684
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 63.847467968273335
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.944527152092913
 mnt_best       : 29.542107379154057
 not_improved_count: 5
Train Epoch: 33 [1/1000 32/32000 (0%)] Loss: 1.96410 (semantic_loss: 0.01761, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=21.62144 
Train Epoch: 33 [6/1000 192/32000 (1%)] Loss: 1.96103 (semantic_loss: 0.01357, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23520 
Train Epoch: 33 [11/1000 352/32000 (1%)] Loss: 1.96105 (semantic_loss: 0.01358, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18525 
Train Epoch: 33 [16/1000 512/32000 (2%)] Loss: 1.96028 (semantic_loss: 0.01281, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.36385 
Train Epoch: 33 [21/1000 672/32000 (2%)] Loss: 1.96066 (semantic_loss: 0.01319, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19252 
Train Epoch: 33 [26/1000 832/32000 (3%)] Loss: 1.96288 (semantic_loss: 0.01443, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18710 
Train Epoch: 33 [31/1000 992/32000 (3%)] Loss: 1.96352 (semantic_loss: 0.01605, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20087 
Train Epoch: 33 [36/1000 1152/32000 (4%)] Loss: 1.96201 (semantic_loss: 0.01455, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19596 
Train Epoch: 33 [41/1000 1312/32000 (4%)] Loss: 1.96264 (semantic_loss: 0.01614, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18843 
Train Epoch: 33 [46/1000 1472/32000 (5%)] Loss: 1.96759 (semantic_loss: 0.02011, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18768 
Train Epoch: 33 [51/1000 1632/32000 (5%)] Loss: 1.96106 (semantic_loss: 0.01359, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18679 
Train Epoch: 33 [56/1000 1792/32000 (6%)] Loss: 1.96225 (semantic_loss: 0.01381, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18875 
Train Epoch: 33 [61/1000 1952/32000 (6%)] Loss: 1.96087 (semantic_loss: 0.01340, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19436 
Train Epoch: 33 [66/1000 2112/32000 (7%)] Loss: 1.96207 (semantic_loss: 0.01363, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18689 
Train Epoch: 33 [71/1000 2272/32000 (7%)] Loss: 1.96513 (semantic_loss: 0.01668, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18719 
Train Epoch: 33 [76/1000 2432/32000 (8%)] Loss: 1.96228 (semantic_loss: 0.01481, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21026 
Train Epoch: 33 [81/1000 2592/32000 (8%)] Loss: 1.95837 (semantic_loss: 0.01189, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18882 
Train Epoch: 33 [86/1000 2752/32000 (9%)] Loss: 1.95988 (semantic_loss: 0.01242, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.88676 
Train Epoch: 33 [91/1000 2912/32000 (9%)] Loss: 1.96229 (semantic_loss: 0.01579, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18894 
Train Epoch: 33 [96/1000 3072/32000 (10%)] Loss: 1.96607 (semantic_loss: 0.01860, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20366 
Train Epoch: 33 [101/1000 3232/32000 (10%)] Loss: 1.96582 (semantic_loss: 0.01836, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.26678 
Train Epoch: 33 [106/1000 3392/32000 (11%)] Loss: 1.96570 (semantic_loss: 0.01921, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21363 
Train Epoch: 33 [111/1000 3552/32000 (11%)] Loss: 1.96705 (semantic_loss: 0.01958, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21490 
Train Epoch: 33 [116/1000 3712/32000 (12%)] Loss: 1.96025 (semantic_loss: 0.01376, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20305 
Train Epoch: 33 [121/1000 3872/32000 (12%)] Loss: 1.96125 (semantic_loss: 0.01378, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20290 
Train Epoch: 33 [126/1000 4032/32000 (13%)] Loss: 1.96065 (semantic_loss: 0.01416, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19327 
Train Epoch: 33 [131/1000 4192/32000 (13%)] Loss: 1.96054 (semantic_loss: 0.01307, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.35958 
Train Epoch: 33 [136/1000 4352/32000 (14%)] Loss: 1.96383 (semantic_loss: 0.01636, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19311 
Train Epoch: 33 [141/1000 4512/32000 (14%)] Loss: 1.96008 (semantic_loss: 0.01260, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18933 
Train Epoch: 33 [146/1000 4672/32000 (15%)] Loss: 1.96060 (semantic_loss: 0.01313, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18586 
Train Epoch: 33 [151/1000 4832/32000 (15%)] Loss: 1.96088 (semantic_loss: 0.01440, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18548 
Train Epoch: 33 [156/1000 4992/32000 (16%)] Loss: 1.96250 (semantic_loss: 0.01503, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18986 
Train Epoch: 33 [161/1000 5152/32000 (16%)] Loss: 1.96063 (semantic_loss: 0.01413, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20063 
Train Epoch: 33 [166/1000 5312/32000 (17%)] Loss: 1.96067 (semantic_loss: 0.01320, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19306 
Train Epoch: 33 [171/1000 5472/32000 (17%)] Loss: 1.96089 (semantic_loss: 0.01342, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19305 
Train Epoch: 33 [176/1000 5632/32000 (18%)] Loss: 1.96007 (semantic_loss: 0.01162, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18816 
Train Epoch: 33 [181/1000 5792/32000 (18%)] Loss: 1.96108 (semantic_loss: 0.01361, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18907 
Train Epoch: 33 [186/1000 5952/32000 (19%)] Loss: 1.95975 (semantic_loss: 0.01228, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19393 
Train Epoch: 33 [191/1000 6112/32000 (19%)] Loss: 1.96174 (semantic_loss: 0.01427, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19008 
Train Epoch: 33 [196/1000 6272/32000 (20%)] Loss: 1.96574 (semantic_loss: 0.01827, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18858 
Train Epoch: 33 [201/1000 6432/32000 (20%)] Loss: 1.96331 (semantic_loss: 0.01780, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18980 
Train Epoch: 33 [206/1000 6592/32000 (21%)] Loss: 1.96193 (semantic_loss: 0.01349, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18926 
Train Epoch: 33 [211/1000 6752/32000 (21%)] Loss: 1.96228 (semantic_loss: 0.01481, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.30547 
Train Epoch: 33 [216/1000 6912/32000 (22%)] Loss: 1.96338 (semantic_loss: 0.01591, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18902 
Train Epoch: 33 [221/1000 7072/32000 (22%)] Loss: 1.96162 (semantic_loss: 0.01513, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19760 
Train Epoch: 33 [226/1000 7232/32000 (23%)] Loss: 1.96155 (semantic_loss: 0.01408, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18692 
Train Epoch: 33 [231/1000 7392/32000 (23%)] Loss: 1.96243 (semantic_loss: 0.01594, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18833 
Train Epoch: 33 [236/1000 7552/32000 (24%)] Loss: 1.96295 (semantic_loss: 0.01645, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18797 
Train Epoch: 33 [241/1000 7712/32000 (24%)] Loss: 1.96054 (semantic_loss: 0.01308, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18838 
Train Epoch: 33 [246/1000 7872/32000 (25%)] Loss: 1.96072 (semantic_loss: 0.01325, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19171 
Train Epoch: 33 [251/1000 8032/32000 (25%)] Loss: 1.96451 (semantic_loss: 0.01704, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22900 
Train Epoch: 33 [256/1000 8192/32000 (26%)] Loss: 1.96061 (semantic_loss: 0.01412, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.28912 
Train Epoch: 33 [261/1000 8352/32000 (26%)] Loss: 1.96281 (semantic_loss: 0.01632, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22680 
Train Epoch: 33 [266/1000 8512/32000 (27%)] Loss: 1.96237 (semantic_loss: 0.01393, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.26089 
Train Epoch: 33 [271/1000 8672/32000 (27%)] Loss: 1.95876 (semantic_loss: 0.01227, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20438 
Train Epoch: 33 [276/1000 8832/32000 (28%)] Loss: 1.95996 (semantic_loss: 0.01249, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20255 
Train Epoch: 33 [281/1000 8992/32000 (28%)] Loss: 1.96089 (semantic_loss: 0.01440, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20110 
Train Epoch: 33 [286/1000 9152/32000 (29%)] Loss: 1.96200 (semantic_loss: 0.01551, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19656 
Train Epoch: 33 [291/1000 9312/32000 (29%)] Loss: 1.96094 (semantic_loss: 0.01347, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18862 
Train Epoch: 33 [296/1000 9472/32000 (30%)] Loss: 1.96602 (semantic_loss: 0.01952, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18733 
Train Epoch: 33 [301/1000 9632/32000 (30%)] Loss: 1.96087 (semantic_loss: 0.01341, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18910 
Train Epoch: 33 [306/1000 9792/32000 (31%)] Loss: 1.96205 (semantic_loss: 0.01458, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19274 
Train Epoch: 33 [311/1000 9952/32000 (31%)] Loss: 1.96210 (semantic_loss: 0.01366, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21007 
Train Epoch: 33 [316/1000 10112/32000 (32%)] Loss: 1.96116 (semantic_loss: 0.01467, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18964 
Train Epoch: 33 [321/1000 10272/32000 (32%)] Loss: 1.96229 (semantic_loss: 0.01482, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19874 
Train Epoch: 33 [326/1000 10432/32000 (33%)] Loss: 1.95809 (semantic_loss: 0.01160, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.26042 
Train Epoch: 33 [331/1000 10592/32000 (33%)] Loss: 1.96620 (semantic_loss: 0.01873, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19033 
Train Epoch: 33 [336/1000 10752/32000 (34%)] Loss: 1.96144 (semantic_loss: 0.01494, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.34108 
Train Epoch: 33 [341/1000 10912/32000 (34%)] Loss: 1.96526 (semantic_loss: 0.01681, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18739 
Train Epoch: 33 [346/1000 11072/32000 (35%)] Loss: 1.96056 (semantic_loss: 0.01308, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18826 
Train Epoch: 33 [351/1000 11232/32000 (35%)] Loss: 1.96654 (semantic_loss: 0.01907, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19009 
Train Epoch: 33 [356/1000 11392/32000 (36%)] Loss: 1.96181 (semantic_loss: 0.01434, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19801 
Train Epoch: 33 [361/1000 11552/32000 (36%)] Loss: 1.96289 (semantic_loss: 0.01348, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.19125 
Train Epoch: 33 [366/1000 11712/32000 (37%)] Loss: 1.96099 (semantic_loss: 0.01352, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18738 
Train Epoch: 33 [371/1000 11872/32000 (37%)] Loss: 1.95880 (semantic_loss: 0.01231, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18660 
Train Epoch: 33 [376/1000 12032/32000 (38%)] Loss: 1.96661 (semantic_loss: 0.01914, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18781 
Train Epoch: 33 [381/1000 12192/32000 (38%)] Loss: 1.95985 (semantic_loss: 0.01238, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19577 
Train Epoch: 33 [386/1000 12352/32000 (39%)] Loss: 1.96471 (semantic_loss: 0.01724, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18330 
Train Epoch: 33 [391/1000 12512/32000 (39%)] Loss: 1.96383 (semantic_loss: 0.01538, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19687 
Train Epoch: 33 [396/1000 12672/32000 (40%)] Loss: 1.96143 (semantic_loss: 0.01298, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20693 
Train Epoch: 33 [401/1000 12832/32000 (40%)] Loss: 1.96108 (semantic_loss: 0.01361, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19151 
Train Epoch: 33 [406/1000 12992/32000 (41%)] Loss: 1.96243 (semantic_loss: 0.01496, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=1.02227 
Train Epoch: 33 [411/1000 13152/32000 (41%)] Loss: 1.96233 (semantic_loss: 0.01485, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21917 
Train Epoch: 33 [416/1000 13312/32000 (42%)] Loss: 1.96387 (semantic_loss: 0.01640, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20821 
Train Epoch: 33 [421/1000 13472/32000 (42%)] Loss: 1.96199 (semantic_loss: 0.01549, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.25300 
Train Epoch: 33 [426/1000 13632/32000 (43%)] Loss: 1.96519 (semantic_loss: 0.01674, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19196 
Train Epoch: 33 [431/1000 13792/32000 (43%)] Loss: 1.96679 (semantic_loss: 0.01737, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.20278 
Train Epoch: 33 [436/1000 13952/32000 (44%)] Loss: 1.95825 (semantic_loss: 0.01078, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19789 
Train Epoch: 33 [441/1000 14112/32000 (44%)] Loss: 1.96157 (semantic_loss: 0.01312, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19391 
Train Epoch: 33 [446/1000 14272/32000 (45%)] Loss: 1.96256 (semantic_loss: 0.01412, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19019 
Train Epoch: 33 [451/1000 14432/32000 (45%)] Loss: 1.96284 (semantic_loss: 0.01537, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.31184 
Train Epoch: 33 [456/1000 14592/32000 (46%)] Loss: 1.96161 (semantic_loss: 0.01414, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19515 
Train Epoch: 33 [461/1000 14752/32000 (46%)] Loss: 1.96655 (semantic_loss: 0.01810, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18792 
Train Epoch: 33 [466/1000 14912/32000 (47%)] Loss: 1.96070 (semantic_loss: 0.01324, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19203 
Train Epoch: 33 [471/1000 15072/32000 (47%)] Loss: 1.96422 (semantic_loss: 0.01675, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19879 
Train Epoch: 33 [476/1000 15232/32000 (48%)] Loss: 1.96250 (semantic_loss: 0.01503, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19997 
Train Epoch: 33 [481/1000 15392/32000 (48%)] Loss: 1.96095 (semantic_loss: 0.01250, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19111 
Train Epoch: 33 [486/1000 15552/32000 (49%)] Loss: 1.95995 (semantic_loss: 0.01346, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18968 
Train Epoch: 33 [491/1000 15712/32000 (49%)] Loss: 1.96081 (semantic_loss: 0.01432, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18499 
Train Epoch: 33 [496/1000 15872/32000 (50%)] Loss: 1.95972 (semantic_loss: 0.01323, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18418 
Train Epoch: 33 [501/1000 16032/32000 (50%)] Loss: 1.96044 (semantic_loss: 0.01297, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19372 
Train Epoch: 33 [506/1000 16192/32000 (51%)] Loss: 1.96048 (semantic_loss: 0.01301, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18800 
Train Epoch: 33 [511/1000 16352/32000 (51%)] Loss: 1.95917 (semantic_loss: 0.01171, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18876 
Train Epoch: 33 [516/1000 16512/32000 (52%)] Loss: 1.96151 (semantic_loss: 0.01502, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18617 
Train Epoch: 33 [521/1000 16672/32000 (52%)] Loss: 1.95971 (semantic_loss: 0.01127, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19456 
Train Epoch: 33 [526/1000 16832/32000 (53%)] Loss: 1.96238 (semantic_loss: 0.01491, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20706 
Train Epoch: 33 [531/1000 16992/32000 (53%)] Loss: 1.96016 (semantic_loss: 0.01269, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.30344 
Train Epoch: 33 [536/1000 17152/32000 (54%)] Loss: 1.96506 (semantic_loss: 0.01758, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18872 
Train Epoch: 33 [541/1000 17312/32000 (54%)] Loss: 1.96271 (semantic_loss: 0.01622, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19073 
Train Epoch: 33 [546/1000 17472/32000 (55%)] Loss: 1.96072 (semantic_loss: 0.01325, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18728 
Train Epoch: 33 [551/1000 17632/32000 (55%)] Loss: 1.96598 (semantic_loss: 0.01753, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18470 
Train Epoch: 33 [556/1000 17792/32000 (56%)] Loss: 1.96170 (semantic_loss: 0.01424, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21180 
Train Epoch: 33 [561/1000 17952/32000 (56%)] Loss: 1.96250 (semantic_loss: 0.01504, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22204 
Train Epoch: 33 [566/1000 18112/32000 (57%)] Loss: 1.96047 (semantic_loss: 0.01398, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22308 
Train Epoch: 33 [571/1000 18272/32000 (57%)] Loss: 1.96174 (semantic_loss: 0.01427, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.26325 
Train Epoch: 33 [576/1000 18432/32000 (58%)] Loss: 1.95933 (semantic_loss: 0.01284, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22430 
Train Epoch: 33 [581/1000 18592/32000 (58%)] Loss: 1.96225 (semantic_loss: 0.01381, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20982 
Train Epoch: 33 [586/1000 18752/32000 (59%)] Loss: 1.96820 (semantic_loss: 0.02073, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24671 
Train Epoch: 33 [591/1000 18912/32000 (59%)] Loss: 1.95975 (semantic_loss: 0.01228, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22400 
Train Epoch: 33 [596/1000 19072/32000 (60%)] Loss: 1.95932 (semantic_loss: 0.01185, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18640 
Train Epoch: 33 [601/1000 19232/32000 (60%)] Loss: 1.96034 (semantic_loss: 0.01482, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18922 
Train Epoch: 33 [606/1000 19392/32000 (61%)] Loss: 1.95900 (semantic_loss: 0.01153, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18949 
Train Epoch: 33 [611/1000 19552/32000 (61%)] Loss: 1.96798 (semantic_loss: 0.02051, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18647 
Train Epoch: 33 [616/1000 19712/32000 (62%)] Loss: 1.96132 (semantic_loss: 0.01484, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19833 
Train Epoch: 33 [621/1000 19872/32000 (62%)] Loss: 1.96306 (semantic_loss: 0.01363, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.21323 
Train Epoch: 33 [626/1000 20032/32000 (63%)] Loss: 1.96016 (semantic_loss: 0.01269, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21082 
Train Epoch: 33 [631/1000 20192/32000 (63%)] Loss: 1.96083 (semantic_loss: 0.01337, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21455 
Train Epoch: 33 [636/1000 20352/32000 (64%)] Loss: 1.95968 (semantic_loss: 0.01222, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19637 
Train Epoch: 33 [641/1000 20512/32000 (64%)] Loss: 1.96355 (semantic_loss: 0.01510, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19669 
Train Epoch: 33 [646/1000 20672/32000 (65%)] Loss: 1.96176 (semantic_loss: 0.01332, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.25410 
Train Epoch: 33 [651/1000 20832/32000 (65%)] Loss: 1.96068 (semantic_loss: 0.01419, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18631 
Train Epoch: 33 [656/1000 20992/32000 (66%)] Loss: 1.96058 (semantic_loss: 0.01408, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.34313 
Train Epoch: 33 [661/1000 21152/32000 (66%)] Loss: 1.96129 (semantic_loss: 0.01382, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18236 
Train Epoch: 33 [666/1000 21312/32000 (67%)] Loss: 1.96028 (semantic_loss: 0.01379, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18876 
Train Epoch: 33 [671/1000 21472/32000 (67%)] Loss: 1.96604 (semantic_loss: 0.01858, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18985 
Train Epoch: 33 [676/1000 21632/32000 (68%)] Loss: 1.96488 (semantic_loss: 0.01644, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18417 
Train Epoch: 33 [681/1000 21792/32000 (68%)] Loss: 1.96010 (semantic_loss: 0.01263, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18525 
Train Epoch: 33 [686/1000 21952/32000 (69%)] Loss: 1.95999 (semantic_loss: 0.01350, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18369 
Train Epoch: 33 [691/1000 22112/32000 (69%)] Loss: 1.96229 (semantic_loss: 0.01578, quant_loss: 1.94629, bit_balance_loss: 0.00022) batch_time=0.18342 
Train Epoch: 33 [696/1000 22272/32000 (70%)] Loss: 1.96083 (semantic_loss: 0.01336, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19511 
Train Epoch: 33 [701/1000 22432/32000 (70%)] Loss: 1.96264 (semantic_loss: 0.01517, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19407 
Train Epoch: 33 [706/1000 22592/32000 (71%)] Loss: 1.96144 (semantic_loss: 0.01396, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20121 
Train Epoch: 33 [711/1000 22752/32000 (71%)] Loss: 1.96242 (semantic_loss: 0.01592, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21096 
Train Epoch: 33 [716/1000 22912/32000 (72%)] Loss: 1.96355 (semantic_loss: 0.01510, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22895 
Train Epoch: 33 [721/1000 23072/32000 (72%)] Loss: 1.97167 (semantic_loss: 0.02421, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22321 
Train Epoch: 33 [726/1000 23232/32000 (73%)] Loss: 1.96212 (semantic_loss: 0.01465, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=1.02069 
Train Epoch: 33 [731/1000 23392/32000 (73%)] Loss: 1.96403 (semantic_loss: 0.01558, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19751 
Train Epoch: 33 [736/1000 23552/32000 (74%)] Loss: 1.96313 (semantic_loss: 0.01664, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20620 
Train Epoch: 33 [741/1000 23712/32000 (74%)] Loss: 1.96013 (semantic_loss: 0.01364, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.26748 
Train Epoch: 33 [746/1000 23872/32000 (75%)] Loss: 1.96322 (semantic_loss: 0.01575, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18588 
Train Epoch: 33 [751/1000 24032/32000 (75%)] Loss: 1.96493 (semantic_loss: 0.01746, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18534 
Train Epoch: 33 [756/1000 24192/32000 (76%)] Loss: 1.96463 (semantic_loss: 0.01716, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18827 
Train Epoch: 33 [761/1000 24352/32000 (76%)] Loss: 1.96197 (semantic_loss: 0.01548, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18524 
Train Epoch: 33 [766/1000 24512/32000 (77%)] Loss: 1.96472 (semantic_loss: 0.01627, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18436 
Train Epoch: 33 [771/1000 24672/32000 (77%)] Loss: 1.96252 (semantic_loss: 0.01408, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20225 
Train Epoch: 33 [776/1000 24832/32000 (78%)] Loss: 1.96319 (semantic_loss: 0.01475, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22052 
Train Epoch: 33 [781/1000 24992/32000 (78%)] Loss: 1.96232 (semantic_loss: 0.01485, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19736 
Train Epoch: 33 [786/1000 25152/32000 (79%)] Loss: 1.95990 (semantic_loss: 0.01243, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18500 
Train Epoch: 33 [791/1000 25312/32000 (79%)] Loss: 1.95854 (semantic_loss: 0.01205, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19181 
Train Epoch: 33 [796/1000 25472/32000 (80%)] Loss: 1.96336 (semantic_loss: 0.01491, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18839 
Train Epoch: 33 [801/1000 25632/32000 (80%)] Loss: 1.96359 (semantic_loss: 0.01514, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18443 
Train Epoch: 33 [806/1000 25792/32000 (81%)] Loss: 1.96033 (semantic_loss: 0.01286, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18613 
Train Epoch: 33 [811/1000 25952/32000 (81%)] Loss: 1.96240 (semantic_loss: 0.01493, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19336 
Train Epoch: 33 [816/1000 26112/32000 (82%)] Loss: 1.96030 (semantic_loss: 0.01283, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18389 
Train Epoch: 33 [821/1000 26272/32000 (82%)] Loss: 1.96262 (semantic_loss: 0.01320, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18375 
Train Epoch: 33 [826/1000 26432/32000 (83%)] Loss: 1.96207 (semantic_loss: 0.01461, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18287 
Train Epoch: 33 [831/1000 26592/32000 (83%)] Loss: 1.96057 (semantic_loss: 0.01311, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18281 
Train Epoch: 33 [836/1000 26752/32000 (84%)] Loss: 1.96133 (semantic_loss: 0.01386, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18608 
Train Epoch: 33 [841/1000 26912/32000 (84%)] Loss: 1.96190 (semantic_loss: 0.01443, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18588 
Train Epoch: 33 [846/1000 27072/32000 (85%)] Loss: 1.96950 (semantic_loss: 0.02105, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18869 
Train Epoch: 33 [851/1000 27232/32000 (85%)] Loss: 1.96911 (semantic_loss: 0.02164, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.28476 
Train Epoch: 33 [856/1000 27392/32000 (86%)] Loss: 1.96200 (semantic_loss: 0.01550, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21589 
Train Epoch: 33 [861/1000 27552/32000 (86%)] Loss: 1.96114 (semantic_loss: 0.01367, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22788 
Train Epoch: 33 [866/1000 27712/32000 (87%)] Loss: 1.95866 (semantic_loss: 0.01217, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21315 
Train Epoch: 33 [871/1000 27872/32000 (87%)] Loss: 1.95796 (semantic_loss: 0.01147, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21038 
Train Epoch: 33 [876/1000 28032/32000 (88%)] Loss: 1.96085 (semantic_loss: 0.01338, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21032 
Train Epoch: 33 [881/1000 28192/32000 (88%)] Loss: 1.96091 (semantic_loss: 0.01345, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20515 
Train Epoch: 33 [886/1000 28352/32000 (89%)] Loss: 1.96412 (semantic_loss: 0.01568, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20689 
Train Epoch: 33 [891/1000 28512/32000 (89%)] Loss: 1.96088 (semantic_loss: 0.01244, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21661 
Train Epoch: 33 [896/1000 28672/32000 (90%)] Loss: 1.96208 (semantic_loss: 0.01363, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22067 
Train Epoch: 33 [901/1000 28832/32000 (90%)] Loss: 1.96056 (semantic_loss: 0.01309, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18617 
Train Epoch: 33 [906/1000 28992/32000 (91%)] Loss: 1.95876 (semantic_loss: 0.01227, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18630 
Train Epoch: 33 [911/1000 29152/32000 (91%)] Loss: 1.96317 (semantic_loss: 0.01667, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18353 
Train Epoch: 33 [916/1000 29312/32000 (92%)] Loss: 1.96882 (semantic_loss: 0.02037, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18986 
Train Epoch: 33 [921/1000 29472/32000 (92%)] Loss: 1.96563 (semantic_loss: 0.01719, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19949 
Train Epoch: 33 [926/1000 29632/32000 (93%)] Loss: 1.96408 (semantic_loss: 0.01563, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19808 
Train Epoch: 33 [931/1000 29792/32000 (93%)] Loss: 1.96260 (semantic_loss: 0.01513, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19260 
Train Epoch: 33 [936/1000 29952/32000 (94%)] Loss: 1.96589 (semantic_loss: 0.01841, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19601 
Train Epoch: 33 [941/1000 30112/32000 (94%)] Loss: 1.96551 (semantic_loss: 0.01804, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18654 
Train Epoch: 33 [946/1000 30272/32000 (95%)] Loss: 1.96639 (semantic_loss: 0.01892, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18386 
Train Epoch: 33 [951/1000 30432/32000 (95%)] Loss: 1.96461 (semantic_loss: 0.01714, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20199 
Train Epoch: 33 [956/1000 30592/32000 (96%)] Loss: 1.96311 (semantic_loss: 0.01466, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18818 
Train Epoch: 33 [961/1000 30752/32000 (96%)] Loss: 1.96111 (semantic_loss: 0.01267, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.52127 
Train Epoch: 33 [966/1000 30912/32000 (97%)] Loss: 1.96124 (semantic_loss: 0.01280, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18585 
Train Epoch: 33 [971/1000 31072/32000 (97%)] Loss: 1.96032 (semantic_loss: 0.01284, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19173 
Train Epoch: 33 [976/1000 31232/32000 (98%)] Loss: 1.96061 (semantic_loss: 0.01314, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.26890 
Train Epoch: 33 [981/1000 31392/32000 (98%)] Loss: 1.96077 (semantic_loss: 0.01428, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18742 
Train Epoch: 33 [986/1000 31552/32000 (99%)] Loss: 1.96178 (semantic_loss: 0.01334, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18964 
Train Epoch: 33 [991/1000 31712/32000 (99%)] Loss: 1.96117 (semantic_loss: 0.01370, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19858 
Train Epoch: 33 [996/1000 31872/32000 (100%)] Loss: 1.95987 (semantic_loss: 0.01239, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18817 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch33.pth ...
Done in 4.049s
removing stale ckpt [epoch 32] [took 0.00s]
 epoch          : 33
 loss           : 1.962006866455078
 learning_rate  : 1.7168419101462578e-06
 n_samples      : 1056000
 n_steps        : 33000
 ActivityNet_val1_test/t2v_metrics/R1: 11.958511287370348
 ActivityNet_val1_test/t2v_metrics/R5: 38.51942241203986
 ActivityNet_val1_test/t2v_metrics/R10: 55.033557046979865
 ActivityNet_val1_test/t2v_metrics/R50: 85.11287370347773
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 61.2115110839943
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.37614632016016
 ActivityNet_val1_test/v2t_metrics/R1: 12.58897701850722
 ActivityNet_val1_test/v2t_metrics/R5: 38.66178564165141
 ActivityNet_val1_test/v2t_metrics/R10: 55.66402277811674
 ActivityNet_val1_test/v2t_metrics/R50: 84.82814724425462
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 63.493797030709786
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 30.03417079551315
 mnt_best       : 29.542107379154057
 not_improved_count: 6
Train Epoch: 34 [1/1000 32/32000 (0%)] Loss: 1.96438 (semantic_loss: 0.01594, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=21.19427 
Train Epoch: 34 [6/1000 192/32000 (1%)] Loss: 1.96066 (semantic_loss: 0.01319, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20224 
Train Epoch: 34 [11/1000 352/32000 (1%)] Loss: 1.96402 (semantic_loss: 0.01654, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22498 
Train Epoch: 34 [16/1000 512/32000 (2%)] Loss: 1.95987 (semantic_loss: 0.01240, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=1.33210 
Train Epoch: 34 [21/1000 672/32000 (2%)] Loss: 1.96320 (semantic_loss: 0.01476, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20836 
Train Epoch: 34 [26/1000 832/32000 (3%)] Loss: 1.95886 (semantic_loss: 0.01236, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21397 
Train Epoch: 34 [31/1000 992/32000 (3%)] Loss: 1.95993 (semantic_loss: 0.01344, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20419 
Train Epoch: 34 [36/1000 1152/32000 (4%)] Loss: 1.96229 (semantic_loss: 0.01287, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.21888 
Train Epoch: 34 [41/1000 1312/32000 (4%)] Loss: 1.96502 (semantic_loss: 0.01755, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19002 
Train Epoch: 34 [46/1000 1472/32000 (5%)] Loss: 1.96044 (semantic_loss: 0.01199, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18942 
Train Epoch: 34 [51/1000 1632/32000 (5%)] Loss: 1.96075 (semantic_loss: 0.01328, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18644 
Train Epoch: 34 [56/1000 1792/32000 (6%)] Loss: 1.96191 (semantic_loss: 0.01444, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.48089 
Train Epoch: 34 [61/1000 1952/32000 (6%)] Loss: 1.95844 (semantic_loss: 0.01195, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20337 
Train Epoch: 34 [66/1000 2112/32000 (7%)] Loss: 1.96312 (semantic_loss: 0.01565, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20936 
Train Epoch: 34 [71/1000 2272/32000 (7%)] Loss: 1.96095 (semantic_loss: 0.01349, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20735 
Train Epoch: 34 [76/1000 2432/32000 (8%)] Loss: 1.96074 (semantic_loss: 0.01424, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20996 
Train Epoch: 34 [81/1000 2592/32000 (8%)] Loss: 1.96025 (semantic_loss: 0.01375, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20884 
Train Epoch: 34 [86/1000 2752/32000 (9%)] Loss: 1.96231 (semantic_loss: 0.01386, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18761 
Train Epoch: 34 [91/1000 2912/32000 (9%)] Loss: 1.96017 (semantic_loss: 0.01368, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18854 
Train Epoch: 34 [96/1000 3072/32000 (10%)] Loss: 1.96255 (semantic_loss: 0.01509, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18755 
Train Epoch: 34 [101/1000 3232/32000 (10%)] Loss: 1.96317 (semantic_loss: 0.01570, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18589 
Train Epoch: 34 [106/1000 3392/32000 (11%)] Loss: 1.96143 (semantic_loss: 0.01299, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18467 
Train Epoch: 34 [111/1000 3552/32000 (11%)] Loss: 1.95950 (semantic_loss: 0.01203, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18884 
Train Epoch: 34 [116/1000 3712/32000 (12%)] Loss: 1.96384 (semantic_loss: 0.01636, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19954 
Train Epoch: 34 [121/1000 3872/32000 (12%)] Loss: 1.96224 (semantic_loss: 0.01575, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22362 
Train Epoch: 34 [126/1000 4032/32000 (13%)] Loss: 1.96351 (semantic_loss: 0.01507, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19740 
Train Epoch: 34 [131/1000 4192/32000 (13%)] Loss: 1.96427 (semantic_loss: 0.01583, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.34901 
Train Epoch: 34 [136/1000 4352/32000 (14%)] Loss: 1.96174 (semantic_loss: 0.01427, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20164 
Train Epoch: 34 [141/1000 4512/32000 (14%)] Loss: 1.95889 (semantic_loss: 0.01240, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18559 
Train Epoch: 34 [146/1000 4672/32000 (15%)] Loss: 1.96431 (semantic_loss: 0.01586, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.29587 
Train Epoch: 34 [151/1000 4832/32000 (15%)] Loss: 1.96031 (semantic_loss: 0.01381, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21896 
Train Epoch: 34 [156/1000 4992/32000 (16%)] Loss: 1.96021 (semantic_loss: 0.01274, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21119 
Train Epoch: 34 [161/1000 5152/32000 (16%)] Loss: 1.96448 (semantic_loss: 0.01604, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20330 
Train Epoch: 34 [166/1000 5312/32000 (17%)] Loss: 1.96237 (semantic_loss: 0.01587, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23188 
Train Epoch: 34 [171/1000 5472/32000 (17%)] Loss: 1.96141 (semantic_loss: 0.01492, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19533 
Train Epoch: 34 [176/1000 5632/32000 (18%)] Loss: 1.96292 (semantic_loss: 0.01448, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21170 
Train Epoch: 34 [181/1000 5792/32000 (18%)] Loss: 1.95998 (semantic_loss: 0.01349, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20095 
Train Epoch: 34 [186/1000 5952/32000 (19%)] Loss: 1.96382 (semantic_loss: 0.01636, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19159 
Train Epoch: 34 [191/1000 6112/32000 (19%)] Loss: 1.96522 (semantic_loss: 0.01677, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19524 
Train Epoch: 34 [196/1000 6272/32000 (20%)] Loss: 1.95904 (semantic_loss: 0.01256, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18932 
Train Epoch: 34 [201/1000 6432/32000 (20%)] Loss: 1.96473 (semantic_loss: 0.01726, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18931 
Train Epoch: 34 [206/1000 6592/32000 (21%)] Loss: 1.95883 (semantic_loss: 0.01233, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21963 
Train Epoch: 34 [211/1000 6752/32000 (21%)] Loss: 1.96168 (semantic_loss: 0.01422, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18571 
Train Epoch: 34 [216/1000 6912/32000 (22%)] Loss: 1.96150 (semantic_loss: 0.01306, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20182 
Train Epoch: 34 [221/1000 7072/32000 (22%)] Loss: 1.96157 (semantic_loss: 0.01312, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19349 
Train Epoch: 34 [226/1000 7232/32000 (23%)] Loss: 1.96742 (semantic_loss: 0.01996, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19296 
Train Epoch: 34 [231/1000 7392/32000 (23%)] Loss: 1.96301 (semantic_loss: 0.01554, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19422 
Train Epoch: 34 [236/1000 7552/32000 (24%)] Loss: 1.96475 (semantic_loss: 0.01729, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18296 
Train Epoch: 34 [241/1000 7712/32000 (24%)] Loss: 1.95929 (semantic_loss: 0.01280, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18612 
Train Epoch: 34 [246/1000 7872/32000 (25%)] Loss: 1.96541 (semantic_loss: 0.01697, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18845 
Train Epoch: 34 [251/1000 8032/32000 (25%)] Loss: 1.95890 (semantic_loss: 0.01240, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18731 
Train Epoch: 34 [256/1000 8192/32000 (26%)] Loss: 1.95877 (semantic_loss: 0.01229, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18919 
Train Epoch: 34 [261/1000 8352/32000 (26%)] Loss: 1.96475 (semantic_loss: 0.01630, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18784 
Train Epoch: 34 [266/1000 8512/32000 (27%)] Loss: 1.96303 (semantic_loss: 0.01653, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18598 
Train Epoch: 34 [271/1000 8672/32000 (27%)] Loss: 1.96041 (semantic_loss: 0.01197, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18756 
Train Epoch: 34 [276/1000 8832/32000 (28%)] Loss: 1.96353 (semantic_loss: 0.01606, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19607 
Train Epoch: 34 [281/1000 8992/32000 (28%)] Loss: 1.96147 (semantic_loss: 0.01400, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21412 
Train Epoch: 34 [286/1000 9152/32000 (29%)] Loss: 1.96016 (semantic_loss: 0.01367, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19833 
Train Epoch: 34 [291/1000 9312/32000 (29%)] Loss: 1.96068 (semantic_loss: 0.01321, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18974 
Train Epoch: 34 [296/1000 9472/32000 (30%)] Loss: 1.95996 (semantic_loss: 0.01347, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18894 
Train Epoch: 34 [301/1000 9632/32000 (30%)] Loss: 1.95858 (semantic_loss: 0.01111, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18833 
Train Epoch: 34 [306/1000 9792/32000 (31%)] Loss: 1.96355 (semantic_loss: 0.01706, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.29539 
Train Epoch: 34 [311/1000 9952/32000 (31%)] Loss: 1.96237 (semantic_loss: 0.01490, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21711 
Train Epoch: 34 [316/1000 10112/32000 (32%)] Loss: 1.96542 (semantic_loss: 0.01698, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21040 
Train Epoch: 34 [321/1000 10272/32000 (32%)] Loss: 1.96315 (semantic_loss: 0.01567, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22470 
Train Epoch: 34 [326/1000 10432/32000 (33%)] Loss: 1.95991 (semantic_loss: 0.01342, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21674 
Train Epoch: 34 [331/1000 10592/32000 (33%)] Loss: 1.96479 (semantic_loss: 0.01634, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22994 
Train Epoch: 34 [336/1000 10752/32000 (34%)] Loss: 1.96272 (semantic_loss: 0.01623, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=1.14476 
Train Epoch: 34 [341/1000 10912/32000 (34%)] Loss: 1.96135 (semantic_loss: 0.01388, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21287 
Train Epoch: 34 [346/1000 11072/32000 (35%)] Loss: 1.95943 (semantic_loss: 0.01294, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20288 
Train Epoch: 34 [351/1000 11232/32000 (35%)] Loss: 1.96116 (semantic_loss: 0.01369, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19501 
Train Epoch: 34 [356/1000 11392/32000 (36%)] Loss: 1.96019 (semantic_loss: 0.01370, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19093 
Train Epoch: 34 [361/1000 11552/32000 (36%)] Loss: 1.96310 (semantic_loss: 0.01563, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18615 
Train Epoch: 34 [366/1000 11712/32000 (37%)] Loss: 1.96137 (semantic_loss: 0.01390, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19216 
Train Epoch: 34 [371/1000 11872/32000 (37%)] Loss: 1.96099 (semantic_loss: 0.01353, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18921 
Train Epoch: 34 [376/1000 12032/32000 (38%)] Loss: 1.96125 (semantic_loss: 0.01475, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20242 
Train Epoch: 34 [381/1000 12192/32000 (38%)] Loss: 1.96509 (semantic_loss: 0.01762, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21067 
Train Epoch: 34 [386/1000 12352/32000 (39%)] Loss: 1.96594 (semantic_loss: 0.01944, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20870 
Train Epoch: 34 [391/1000 12512/32000 (39%)] Loss: 1.96139 (semantic_loss: 0.01295, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18654 
Train Epoch: 34 [396/1000 12672/32000 (40%)] Loss: 1.96103 (semantic_loss: 0.01454, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20317 
Train Epoch: 34 [401/1000 12832/32000 (40%)] Loss: 1.96373 (semantic_loss: 0.01529, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18530 
Train Epoch: 34 [406/1000 12992/32000 (41%)] Loss: 1.96378 (semantic_loss: 0.01631, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18920 
Train Epoch: 34 [411/1000 13152/32000 (41%)] Loss: 1.96676 (semantic_loss: 0.01831, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18916 
Train Epoch: 34 [416/1000 13312/32000 (42%)] Loss: 1.96360 (semantic_loss: 0.01613, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18390 
Train Epoch: 34 [421/1000 13472/32000 (42%)] Loss: 1.96109 (semantic_loss: 0.01460, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18742 
Train Epoch: 34 [426/1000 13632/32000 (43%)] Loss: 1.96039 (semantic_loss: 0.01195, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18391 
Train Epoch: 34 [431/1000 13792/32000 (43%)] Loss: 1.96098 (semantic_loss: 0.01351, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18817 
Train Epoch: 34 [436/1000 13952/32000 (44%)] Loss: 1.96010 (semantic_loss: 0.01263, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18853 
Train Epoch: 34 [441/1000 14112/32000 (44%)] Loss: 1.96001 (semantic_loss: 0.01352, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22260 
Train Epoch: 34 [446/1000 14272/32000 (45%)] Loss: 1.96031 (semantic_loss: 0.01382, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19755 
Train Epoch: 34 [451/1000 14432/32000 (45%)] Loss: 1.96105 (semantic_loss: 0.01358, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.35730 
Train Epoch: 34 [456/1000 14592/32000 (46%)] Loss: 1.96130 (semantic_loss: 0.01383, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19086 
Train Epoch: 34 [461/1000 14752/32000 (46%)] Loss: 1.96136 (semantic_loss: 0.01389, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18520 
Train Epoch: 34 [466/1000 14912/32000 (47%)] Loss: 1.96302 (semantic_loss: 0.01458, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.33191 
Train Epoch: 34 [471/1000 15072/32000 (47%)] Loss: 1.95997 (semantic_loss: 0.01348, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19908 
Train Epoch: 34 [476/1000 15232/32000 (48%)] Loss: 1.95967 (semantic_loss: 0.01220, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21952 
Train Epoch: 34 [481/1000 15392/32000 (48%)] Loss: 1.96017 (semantic_loss: 0.01270, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19577 
Train Epoch: 34 [486/1000 15552/32000 (49%)] Loss: 1.96308 (semantic_loss: 0.01464, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21209 
Train Epoch: 34 [491/1000 15712/32000 (49%)] Loss: 1.96459 (semantic_loss: 0.01615, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20090 
Train Epoch: 34 [496/1000 15872/32000 (50%)] Loss: 1.96237 (semantic_loss: 0.01490, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19317 
Train Epoch: 34 [501/1000 16032/32000 (50%)] Loss: 1.95912 (semantic_loss: 0.01262, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19124 
Train Epoch: 34 [506/1000 16192/32000 (51%)] Loss: 1.96145 (semantic_loss: 0.01495, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19008 
Train Epoch: 34 [511/1000 16352/32000 (51%)] Loss: 1.96301 (semantic_loss: 0.01554, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18819 
Train Epoch: 34 [516/1000 16512/32000 (52%)] Loss: 1.96075 (semantic_loss: 0.01329, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18894 
Train Epoch: 34 [521/1000 16672/32000 (52%)] Loss: 1.96022 (semantic_loss: 0.01373, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18760 
Train Epoch: 34 [526/1000 16832/32000 (53%)] Loss: 1.96362 (semantic_loss: 0.01615, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20859 
Train Epoch: 34 [531/1000 16992/32000 (53%)] Loss: 1.96096 (semantic_loss: 0.01349, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19692 
Train Epoch: 34 [536/1000 17152/32000 (54%)] Loss: 1.96225 (semantic_loss: 0.01576, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21673 
Train Epoch: 34 [541/1000 17312/32000 (54%)] Loss: 1.96188 (semantic_loss: 0.01246, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18660 
Train Epoch: 34 [546/1000 17472/32000 (55%)] Loss: 1.96143 (semantic_loss: 0.01494, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18605 
Train Epoch: 34 [551/1000 17632/32000 (55%)] Loss: 1.95954 (semantic_loss: 0.01304, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18726 
Train Epoch: 34 [556/1000 17792/32000 (56%)] Loss: 1.96539 (semantic_loss: 0.01793, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19398 
Train Epoch: 34 [561/1000 17952/32000 (56%)] Loss: 1.96540 (semantic_loss: 0.01794, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19956 
Train Epoch: 34 [566/1000 18112/32000 (57%)] Loss: 1.96064 (semantic_loss: 0.01317, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18677 
Train Epoch: 34 [571/1000 18272/32000 (57%)] Loss: 1.96357 (semantic_loss: 0.01610, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18744 
Train Epoch: 34 [576/1000 18432/32000 (58%)] Loss: 1.95896 (semantic_loss: 0.01247, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18656 
Train Epoch: 34 [581/1000 18592/32000 (58%)] Loss: 1.96123 (semantic_loss: 0.01376, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18408 
Train Epoch: 34 [586/1000 18752/32000 (59%)] Loss: 1.96254 (semantic_loss: 0.01409, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18635 
Train Epoch: 34 [591/1000 18912/32000 (59%)] Loss: 1.96330 (semantic_loss: 0.01583, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20949 
Train Epoch: 34 [596/1000 19072/32000 (60%)] Loss: 1.95966 (semantic_loss: 0.01220, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18553 
Train Epoch: 34 [601/1000 19232/32000 (60%)] Loss: 1.96105 (semantic_loss: 0.01359, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18629 
Train Epoch: 34 [606/1000 19392/32000 (61%)] Loss: 1.96361 (semantic_loss: 0.01613, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21106 
Train Epoch: 34 [611/1000 19552/32000 (61%)] Loss: 1.96093 (semantic_loss: 0.01346, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21390 
Train Epoch: 34 [616/1000 19712/32000 (62%)] Loss: 1.96174 (semantic_loss: 0.01428, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23762 
Train Epoch: 34 [621/1000 19872/32000 (62%)] Loss: 1.96055 (semantic_loss: 0.01308, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22825 
Train Epoch: 34 [626/1000 20032/32000 (63%)] Loss: 1.96078 (semantic_loss: 0.01331, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19432 
Train Epoch: 34 [631/1000 20192/32000 (63%)] Loss: 1.96209 (semantic_loss: 0.01560, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21935 
Train Epoch: 34 [636/1000 20352/32000 (64%)] Loss: 1.96106 (semantic_loss: 0.01457, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20896 
Train Epoch: 34 [641/1000 20512/32000 (64%)] Loss: 1.96461 (semantic_loss: 0.01714, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19272 
Train Epoch: 34 [646/1000 20672/32000 (65%)] Loss: 1.96229 (semantic_loss: 0.01385, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19441 
Train Epoch: 34 [651/1000 20832/32000 (65%)] Loss: 1.96146 (semantic_loss: 0.01399, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18934 
Train Epoch: 34 [656/1000 20992/32000 (66%)] Loss: 1.96042 (semantic_loss: 0.01393, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.58852 
Train Epoch: 34 [661/1000 21152/32000 (66%)] Loss: 1.96399 (semantic_loss: 0.01653, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18644 
Train Epoch: 34 [666/1000 21312/32000 (67%)] Loss: 1.96089 (semantic_loss: 0.01342, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21022 
Train Epoch: 34 [671/1000 21472/32000 (67%)] Loss: 1.96556 (semantic_loss: 0.01711, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19039 
Train Epoch: 34 [676/1000 21632/32000 (68%)] Loss: 1.96565 (semantic_loss: 0.01916, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19184 
Train Epoch: 34 [681/1000 21792/32000 (68%)] Loss: 1.95905 (semantic_loss: 0.01158, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19356 
Train Epoch: 34 [686/1000 21952/32000 (69%)] Loss: 1.96444 (semantic_loss: 0.01502, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18884 
Train Epoch: 34 [691/1000 22112/32000 (69%)] Loss: 1.96262 (semantic_loss: 0.01516, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18512 
Train Epoch: 34 [696/1000 22272/32000 (70%)] Loss: 1.95990 (semantic_loss: 0.01243, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20954 
Train Epoch: 34 [701/1000 22432/32000 (70%)] Loss: 1.96263 (semantic_loss: 0.01517, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18876 
Train Epoch: 34 [706/1000 22592/32000 (71%)] Loss: 1.96234 (semantic_loss: 0.01487, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19703 
Train Epoch: 34 [711/1000 22752/32000 (71%)] Loss: 1.96276 (semantic_loss: 0.01627, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19641 
Train Epoch: 34 [716/1000 22912/32000 (72%)] Loss: 1.96184 (semantic_loss: 0.01340, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18803 
Train Epoch: 34 [721/1000 23072/32000 (72%)] Loss: 1.96199 (semantic_loss: 0.01355, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19725 
Train Epoch: 34 [726/1000 23232/32000 (73%)] Loss: 1.96454 (semantic_loss: 0.01707, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19549 
Train Epoch: 34 [731/1000 23392/32000 (73%)] Loss: 1.96080 (semantic_loss: 0.01334, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18564 
Train Epoch: 34 [736/1000 23552/32000 (74%)] Loss: 1.96197 (semantic_loss: 0.01547, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18648 
Train Epoch: 34 [741/1000 23712/32000 (74%)] Loss: 1.95935 (semantic_loss: 0.01286, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18818 
Train Epoch: 34 [746/1000 23872/32000 (75%)] Loss: 1.96117 (semantic_loss: 0.01565, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19039 
Train Epoch: 34 [751/1000 24032/32000 (75%)] Loss: 1.96282 (semantic_loss: 0.01535, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19242 
Train Epoch: 34 [756/1000 24192/32000 (76%)] Loss: 1.96344 (semantic_loss: 0.01597, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18855 
Train Epoch: 34 [761/1000 24352/32000 (76%)] Loss: 1.96092 (semantic_loss: 0.01345, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18994 
Train Epoch: 34 [766/1000 24512/32000 (77%)] Loss: 1.96022 (semantic_loss: 0.01373, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19274 
Train Epoch: 34 [771/1000 24672/32000 (77%)] Loss: 1.95969 (semantic_loss: 0.01320, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20293 
Train Epoch: 34 [776/1000 24832/32000 (78%)] Loss: 1.96702 (semantic_loss: 0.01955, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22546 
Train Epoch: 34 [781/1000 24992/32000 (78%)] Loss: 1.96091 (semantic_loss: 0.01441, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22132 
Train Epoch: 34 [786/1000 25152/32000 (79%)] Loss: 1.95965 (semantic_loss: 0.01218, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.30916 
Train Epoch: 34 [791/1000 25312/32000 (79%)] Loss: 1.96235 (semantic_loss: 0.01488, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.78616 
Train Epoch: 34 [796/1000 25472/32000 (80%)] Loss: 1.96455 (semantic_loss: 0.01806, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20291 
Train Epoch: 34 [801/1000 25632/32000 (80%)] Loss: 1.96074 (semantic_loss: 0.01230, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19103 
Train Epoch: 34 [806/1000 25792/32000 (81%)] Loss: 1.96556 (semantic_loss: 0.01810, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18914 
Train Epoch: 34 [811/1000 25952/32000 (81%)] Loss: 1.95891 (semantic_loss: 0.01242, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18572 
Train Epoch: 34 [816/1000 26112/32000 (82%)] Loss: 1.96127 (semantic_loss: 0.01379, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19057 
Train Epoch: 34 [821/1000 26272/32000 (82%)] Loss: 1.96068 (semantic_loss: 0.01419, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18582 
Train Epoch: 34 [826/1000 26432/32000 (83%)] Loss: 1.95973 (semantic_loss: 0.01324, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18956 
Train Epoch: 34 [831/1000 26592/32000 (83%)] Loss: 1.96122 (semantic_loss: 0.01375, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20393 
Train Epoch: 34 [836/1000 26752/32000 (84%)] Loss: 1.96186 (semantic_loss: 0.01635, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19165 
Train Epoch: 34 [841/1000 26912/32000 (84%)] Loss: 1.96095 (semantic_loss: 0.01348, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18920 
Train Epoch: 34 [846/1000 27072/32000 (85%)] Loss: 1.95997 (semantic_loss: 0.01446, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18567 
Train Epoch: 34 [851/1000 27232/32000 (85%)] Loss: 1.96798 (semantic_loss: 0.02051, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18405 
Train Epoch: 34 [856/1000 27392/32000 (86%)] Loss: 1.96016 (semantic_loss: 0.01367, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21137 
Train Epoch: 34 [861/1000 27552/32000 (86%)] Loss: 1.96411 (semantic_loss: 0.01566, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18830 
Train Epoch: 34 [866/1000 27712/32000 (87%)] Loss: 1.96145 (semantic_loss: 0.01398, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18616 
Train Epoch: 34 [871/1000 27872/32000 (87%)] Loss: 1.96015 (semantic_loss: 0.01269, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18568 
Train Epoch: 34 [876/1000 28032/32000 (88%)] Loss: 1.96124 (semantic_loss: 0.01279, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18426 
Train Epoch: 34 [881/1000 28192/32000 (88%)] Loss: 1.95815 (semantic_loss: 0.01166, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18901 
Train Epoch: 34 [886/1000 28352/32000 (89%)] Loss: 1.96681 (semantic_loss: 0.01836, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18762 
Train Epoch: 34 [891/1000 28512/32000 (89%)] Loss: 1.96046 (semantic_loss: 0.01299, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18673 
Train Epoch: 34 [896/1000 28672/32000 (90%)] Loss: 1.96177 (semantic_loss: 0.01430, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19759 
Train Epoch: 34 [901/1000 28832/32000 (90%)] Loss: 1.96082 (semantic_loss: 0.01335, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20321 
Train Epoch: 34 [906/1000 28992/32000 (91%)] Loss: 1.96061 (semantic_loss: 0.01314, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18971 
Train Epoch: 34 [911/1000 29152/32000 (91%)] Loss: 1.96157 (semantic_loss: 0.01410, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18826 
Train Epoch: 34 [916/1000 29312/32000 (92%)] Loss: 1.96409 (semantic_loss: 0.01662, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19907 
Train Epoch: 34 [921/1000 29472/32000 (92%)] Loss: 1.96167 (semantic_loss: 0.01518, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21554 
Train Epoch: 34 [926/1000 29632/32000 (93%)] Loss: 1.96108 (semantic_loss: 0.01361, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21134 
Train Epoch: 34 [931/1000 29792/32000 (93%)] Loss: 1.96132 (semantic_loss: 0.01483, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22374 
Train Epoch: 34 [936/1000 29952/32000 (94%)] Loss: 1.96525 (semantic_loss: 0.01681, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22233 
Train Epoch: 34 [941/1000 30112/32000 (94%)] Loss: 1.96100 (semantic_loss: 0.01353, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20883 
Train Epoch: 34 [946/1000 30272/32000 (95%)] Loss: 1.96441 (semantic_loss: 0.01597, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20578 
Train Epoch: 34 [951/1000 30432/32000 (95%)] Loss: 1.96043 (semantic_loss: 0.01198, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20065 
Train Epoch: 34 [956/1000 30592/32000 (96%)] Loss: 1.96390 (semantic_loss: 0.01740, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19484 
Train Epoch: 34 [961/1000 30752/32000 (96%)] Loss: 1.95817 (semantic_loss: 0.01168, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20520 
Train Epoch: 34 [966/1000 30912/32000 (97%)] Loss: 1.96180 (semantic_loss: 0.01336, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18912 
Train Epoch: 34 [971/1000 31072/32000 (97%)] Loss: 1.96216 (semantic_loss: 0.01469, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18616 
Train Epoch: 34 [976/1000 31232/32000 (98%)] Loss: 1.96036 (semantic_loss: 0.01290, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.78054 
Train Epoch: 34 [981/1000 31392/32000 (98%)] Loss: 1.96586 (semantic_loss: 0.01741, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19856 
Train Epoch: 34 [986/1000 31552/32000 (99%)] Loss: 1.96065 (semantic_loss: 0.01319, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19031 
Train Epoch: 34 [991/1000 31712/32000 (99%)] Loss: 1.96294 (semantic_loss: 0.01450, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19226 
Train Epoch: 34 [996/1000 31872/32000 (100%)] Loss: 1.96310 (semantic_loss: 0.01563, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19976 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch34.pth ...
Done in 3.986s
removing stale ckpt [epoch 33] [took 0.00s]
 epoch          : 34
 loss           : 1.9620637383461
 learning_rate  : 1.545157719131632e-06
 n_samples      : 1088000
 n_steps        : 34000
 ActivityNet_val1_test/t2v_metrics/R1: 11.775472849298353
 ActivityNet_val1_test/t2v_metrics/R5: 38.35672157819809
 ActivityNet_val1_test/t2v_metrics/R10: 55.4403091315843
 ActivityNet_val1_test/t2v_metrics/R50: 85.09253609924751
 ActivityNet_val1_test/t2v_metrics/MedR: 8.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 61.75757575757576
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.256014298484324
 ActivityNet_val1_test/v2t_metrics/R1: 12.182224933902786
 ActivityNet_val1_test/v2t_metrics/R5: 38.84482407972341
 ActivityNet_val1_test/v2t_metrics/R10: 55.867398820418956
 ActivityNet_val1_test/v2t_metrics/R50: 84.7061216188733
 ActivityNet_val1_test/v2t_metrics/MedR: 8.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 63.613178767541186
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.79015352618212
 mnt_best       : 29.542107379154057
 not_improved_count: 7
Train Epoch: 35 [1/1000 32/32000 (0%)] Loss: 1.96118 (semantic_loss: 0.01372, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=23.93156 
Train Epoch: 35 [6/1000 192/32000 (1%)] Loss: 1.96369 (semantic_loss: 0.01525, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18560 
Train Epoch: 35 [11/1000 352/32000 (1%)] Loss: 1.96030 (semantic_loss: 0.01186, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18568 
Train Epoch: 35 [16/1000 512/32000 (2%)] Loss: 1.96165 (semantic_loss: 0.01418, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.51537 
Train Epoch: 35 [21/1000 672/32000 (2%)] Loss: 1.96711 (semantic_loss: 0.01964, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18799 
Train Epoch: 35 [26/1000 832/32000 (3%)] Loss: 1.96364 (semantic_loss: 0.01618, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18632 
Train Epoch: 35 [31/1000 992/32000 (3%)] Loss: 1.95933 (semantic_loss: 0.01284, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20380 
Train Epoch: 35 [36/1000 1152/32000 (4%)] Loss: 1.96170 (semantic_loss: 0.01521, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19010 
Train Epoch: 35 [41/1000 1312/32000 (4%)] Loss: 1.96023 (semantic_loss: 0.01374, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.29852 
Train Epoch: 35 [46/1000 1472/32000 (5%)] Loss: 1.95837 (semantic_loss: 0.01188, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19153 
Train Epoch: 35 [51/1000 1632/32000 (5%)] Loss: 1.96541 (semantic_loss: 0.01794, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18906 
Train Epoch: 35 [56/1000 1792/32000 (6%)] Loss: 1.95939 (semantic_loss: 0.01290, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18962 
Train Epoch: 35 [61/1000 1952/32000 (6%)] Loss: 1.95917 (semantic_loss: 0.01268, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18862 
Train Epoch: 35 [66/1000 2112/32000 (7%)] Loss: 1.95967 (semantic_loss: 0.01220, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.39846 
Train Epoch: 35 [71/1000 2272/32000 (7%)] Loss: 1.96582 (semantic_loss: 0.01737, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21578 
Train Epoch: 35 [76/1000 2432/32000 (8%)] Loss: 1.96114 (semantic_loss: 0.01464, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24134 
Train Epoch: 35 [81/1000 2592/32000 (8%)] Loss: 1.96118 (semantic_loss: 0.01371, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20307 
Train Epoch: 35 [86/1000 2752/32000 (9%)] Loss: 1.96148 (semantic_loss: 0.01401, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19344 
Train Epoch: 35 [91/1000 2912/32000 (9%)] Loss: 1.96419 (semantic_loss: 0.01672, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19392 
Train Epoch: 35 [96/1000 3072/32000 (10%)] Loss: 1.95983 (semantic_loss: 0.01236, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19166 
Train Epoch: 35 [101/1000 3232/32000 (10%)] Loss: 1.96076 (semantic_loss: 0.01330, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19289 
Train Epoch: 35 [106/1000 3392/32000 (11%)] Loss: 1.96425 (semantic_loss: 0.01678, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19177 
Train Epoch: 35 [111/1000 3552/32000 (11%)] Loss: 1.96117 (semantic_loss: 0.01370, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18760 
Train Epoch: 35 [116/1000 3712/32000 (12%)] Loss: 1.95994 (semantic_loss: 0.01247, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19120 
Train Epoch: 35 [121/1000 3872/32000 (12%)] Loss: 1.96056 (semantic_loss: 0.01308, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22835 
Train Epoch: 35 [126/1000 4032/32000 (13%)] Loss: 1.96611 (semantic_loss: 0.01961, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18778 
Train Epoch: 35 [131/1000 4192/32000 (13%)] Loss: 1.96259 (semantic_loss: 0.01512, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18648 
Train Epoch: 35 [136/1000 4352/32000 (14%)] Loss: 1.96229 (semantic_loss: 0.01482, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19440 
Train Epoch: 35 [141/1000 4512/32000 (14%)] Loss: 1.96301 (semantic_loss: 0.01456, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19099 
Train Epoch: 35 [146/1000 4672/32000 (15%)] Loss: 1.96253 (semantic_loss: 0.01506, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19945 
Train Epoch: 35 [151/1000 4832/32000 (15%)] Loss: 1.96478 (semantic_loss: 0.01731, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18584 
Train Epoch: 35 [156/1000 4992/32000 (16%)] Loss: 1.96124 (semantic_loss: 0.01476, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18758 
Train Epoch: 35 [161/1000 5152/32000 (16%)] Loss: 1.96144 (semantic_loss: 0.01495, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18597 
Train Epoch: 35 [166/1000 5312/32000 (17%)] Loss: 1.96004 (semantic_loss: 0.01354, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18593 
Train Epoch: 35 [171/1000 5472/32000 (17%)] Loss: 1.96576 (semantic_loss: 0.01829, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.41047 
Train Epoch: 35 [176/1000 5632/32000 (18%)] Loss: 1.96032 (semantic_loss: 0.01383, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19006 
Train Epoch: 35 [181/1000 5792/32000 (18%)] Loss: 1.96115 (semantic_loss: 0.01466, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18911 
Train Epoch: 35 [186/1000 5952/32000 (19%)] Loss: 1.96007 (semantic_loss: 0.01163, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18512 
Train Epoch: 35 [191/1000 6112/32000 (19%)] Loss: 1.96042 (semantic_loss: 0.01491, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18878 
Train Epoch: 35 [196/1000 6272/32000 (20%)] Loss: 1.95995 (semantic_loss: 0.01247, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18581 
Train Epoch: 35 [201/1000 6432/32000 (20%)] Loss: 1.96271 (semantic_loss: 0.01525, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22299 
Train Epoch: 35 [206/1000 6592/32000 (21%)] Loss: 1.96313 (semantic_loss: 0.01663, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18647 
Train Epoch: 35 [211/1000 6752/32000 (21%)] Loss: 1.95956 (semantic_loss: 0.01307, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18845 
Train Epoch: 35 [216/1000 6912/32000 (22%)] Loss: 1.96370 (semantic_loss: 0.01721, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21742 
Train Epoch: 35 [221/1000 7072/32000 (22%)] Loss: 1.96438 (semantic_loss: 0.01789, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19360 
Train Epoch: 35 [226/1000 7232/32000 (23%)] Loss: 1.96405 (semantic_loss: 0.01755, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20223 
Train Epoch: 35 [231/1000 7392/32000 (23%)] Loss: 1.96004 (semantic_loss: 0.01355, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23338 
Train Epoch: 35 [236/1000 7552/32000 (24%)] Loss: 1.96304 (semantic_loss: 0.01654, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22109 
Train Epoch: 35 [241/1000 7712/32000 (24%)] Loss: 1.96424 (semantic_loss: 0.01579, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21678 
Train Epoch: 35 [246/1000 7872/32000 (25%)] Loss: 1.96399 (semantic_loss: 0.01750, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23459 
Train Epoch: 35 [251/1000 8032/32000 (25%)] Loss: 1.96804 (semantic_loss: 0.02055, quant_loss: 1.94727, bit_balance_loss: 0.00022) batch_time=0.19590 
Train Epoch: 35 [256/1000 8192/32000 (26%)] Loss: 1.96376 (semantic_loss: 0.01530, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21365 
Train Epoch: 35 [261/1000 8352/32000 (26%)] Loss: 1.96540 (semantic_loss: 0.01891, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22509 
Train Epoch: 35 [266/1000 8512/32000 (27%)] Loss: 1.96719 (semantic_loss: 0.01972, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22032 
Train Epoch: 35 [271/1000 8672/32000 (27%)] Loss: 1.96408 (semantic_loss: 0.01661, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18569 
Train Epoch: 35 [276/1000 8832/32000 (28%)] Loss: 1.96454 (semantic_loss: 0.01708, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18697 
Train Epoch: 35 [281/1000 8992/32000 (28%)] Loss: 1.96030 (semantic_loss: 0.01381, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.25249 
Train Epoch: 35 [286/1000 9152/32000 (29%)] Loss: 1.95941 (semantic_loss: 0.01194, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18815 
Train Epoch: 35 [291/1000 9312/32000 (29%)] Loss: 1.96549 (semantic_loss: 0.01607, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18974 
Train Epoch: 35 [296/1000 9472/32000 (30%)] Loss: 1.96219 (semantic_loss: 0.01571, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24347 
Train Epoch: 35 [301/1000 9632/32000 (30%)] Loss: 1.96095 (semantic_loss: 0.01446, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19269 
Train Epoch: 35 [306/1000 9792/32000 (31%)] Loss: 1.95904 (semantic_loss: 0.01255, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18992 
Train Epoch: 35 [311/1000 9952/32000 (31%)] Loss: 1.96676 (semantic_loss: 0.01831, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18446 
Train Epoch: 35 [316/1000 10112/32000 (32%)] Loss: 1.96068 (semantic_loss: 0.01419, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18544 
Train Epoch: 35 [321/1000 10272/32000 (32%)] Loss: 1.96205 (semantic_loss: 0.01458, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18776 
Train Epoch: 35 [326/1000 10432/32000 (33%)] Loss: 1.96515 (semantic_loss: 0.01768, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18623 
Train Epoch: 35 [331/1000 10592/32000 (33%)] Loss: 1.96230 (semantic_loss: 0.01581, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18755 
Train Epoch: 35 [336/1000 10752/32000 (34%)] Loss: 1.96362 (semantic_loss: 0.01615, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.47855 
Train Epoch: 35 [341/1000 10912/32000 (34%)] Loss: 1.96204 (semantic_loss: 0.01456, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18527 
Train Epoch: 35 [346/1000 11072/32000 (35%)] Loss: 1.96144 (semantic_loss: 0.01495, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18487 
Train Epoch: 35 [351/1000 11232/32000 (35%)] Loss: 1.96618 (semantic_loss: 0.01774, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20625 
Train Epoch: 35 [356/1000 11392/32000 (36%)] Loss: 1.96066 (semantic_loss: 0.01320, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18690 
Train Epoch: 35 [361/1000 11552/32000 (36%)] Loss: 1.96590 (semantic_loss: 0.01844, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.26604 
Train Epoch: 35 [366/1000 11712/32000 (37%)] Loss: 1.96227 (semantic_loss: 0.01382, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18524 
Train Epoch: 35 [371/1000 11872/32000 (37%)] Loss: 1.96247 (semantic_loss: 0.01695, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18613 
Train Epoch: 35 [376/1000 12032/32000 (38%)] Loss: 1.96600 (semantic_loss: 0.01853, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19912 
Train Epoch: 35 [381/1000 12192/32000 (38%)] Loss: 1.95904 (semantic_loss: 0.01254, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20173 
Train Epoch: 35 [386/1000 12352/32000 (39%)] Loss: 1.95912 (semantic_loss: 0.01166, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.48249 
Train Epoch: 35 [391/1000 12512/32000 (39%)] Loss: 1.96307 (semantic_loss: 0.01559, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21426 
Train Epoch: 35 [396/1000 12672/32000 (40%)] Loss: 1.96471 (semantic_loss: 0.01723, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19204 
Train Epoch: 35 [401/1000 12832/32000 (40%)] Loss: 1.96093 (semantic_loss: 0.01347, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21086 
Train Epoch: 35 [406/1000 12992/32000 (41%)] Loss: 1.96396 (semantic_loss: 0.01649, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19353 
Train Epoch: 35 [411/1000 13152/32000 (41%)] Loss: 1.96042 (semantic_loss: 0.01295, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19909 
Train Epoch: 35 [416/1000 13312/32000 (42%)] Loss: 1.96073 (semantic_loss: 0.01229, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19108 
Train Epoch: 35 [421/1000 13472/32000 (42%)] Loss: 1.96096 (semantic_loss: 0.01545, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18663 
Train Epoch: 35 [426/1000 13632/32000 (43%)] Loss: 1.96039 (semantic_loss: 0.01292, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18549 
Train Epoch: 35 [431/1000 13792/32000 (43%)] Loss: 1.95996 (semantic_loss: 0.01248, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18781 
Train Epoch: 35 [436/1000 13952/32000 (44%)] Loss: 1.96234 (semantic_loss: 0.01487, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18843 
Train Epoch: 35 [441/1000 14112/32000 (44%)] Loss: 1.96110 (semantic_loss: 0.01363, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21849 
Train Epoch: 35 [446/1000 14272/32000 (45%)] Loss: 1.96197 (semantic_loss: 0.01548, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18974 
Train Epoch: 35 [451/1000 14432/32000 (45%)] Loss: 1.96778 (semantic_loss: 0.01836, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.18968 
Train Epoch: 35 [456/1000 14592/32000 (46%)] Loss: 1.96274 (semantic_loss: 0.01430, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19426 
Train Epoch: 35 [461/1000 14752/32000 (46%)] Loss: 1.96058 (semantic_loss: 0.01311, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18763 
Train Epoch: 35 [466/1000 14912/32000 (47%)] Loss: 1.96001 (semantic_loss: 0.01254, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20914 
Train Epoch: 35 [471/1000 15072/32000 (47%)] Loss: 1.96219 (semantic_loss: 0.01374, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18689 
Train Epoch: 35 [476/1000 15232/32000 (48%)] Loss: 1.96120 (semantic_loss: 0.01373, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18516 
Train Epoch: 35 [481/1000 15392/32000 (48%)] Loss: 1.96504 (semantic_loss: 0.01758, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18722 
Train Epoch: 35 [486/1000 15552/32000 (49%)] Loss: 1.96323 (semantic_loss: 0.01478, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18803 
Train Epoch: 35 [491/1000 15712/32000 (49%)] Loss: 1.96146 (semantic_loss: 0.01204, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.40626 
Train Epoch: 35 [496/1000 15872/32000 (50%)] Loss: 1.96802 (semantic_loss: 0.02055, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21003 
Train Epoch: 35 [501/1000 16032/32000 (50%)] Loss: 1.96117 (semantic_loss: 0.01468, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19147 
Train Epoch: 35 [506/1000 16192/32000 (51%)] Loss: 1.96296 (semantic_loss: 0.01550, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18847 
Train Epoch: 35 [511/1000 16352/32000 (51%)] Loss: 1.95979 (semantic_loss: 0.01330, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18776 
Train Epoch: 35 [516/1000 16512/32000 (52%)] Loss: 1.95961 (semantic_loss: 0.01214, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18944 
Train Epoch: 35 [521/1000 16672/32000 (52%)] Loss: 1.95977 (semantic_loss: 0.01231, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18668 
Train Epoch: 35 [526/1000 16832/32000 (53%)] Loss: 1.96111 (semantic_loss: 0.01364, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18606 
Train Epoch: 35 [531/1000 16992/32000 (53%)] Loss: 1.96438 (semantic_loss: 0.01691, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18796 
Train Epoch: 35 [536/1000 17152/32000 (54%)] Loss: 1.96143 (semantic_loss: 0.01397, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24318 
Train Epoch: 35 [541/1000 17312/32000 (54%)] Loss: 1.96334 (semantic_loss: 0.01587, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19635 
Train Epoch: 35 [546/1000 17472/32000 (55%)] Loss: 1.96128 (semantic_loss: 0.01284, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22329 
Train Epoch: 35 [551/1000 17632/32000 (55%)] Loss: 1.95800 (semantic_loss: 0.01152, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20554 
Train Epoch: 35 [556/1000 17792/32000 (56%)] Loss: 1.95807 (semantic_loss: 0.01256, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.22013 
Train Epoch: 35 [561/1000 17952/32000 (56%)] Loss: 1.96181 (semantic_loss: 0.01337, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19578 
Train Epoch: 35 [566/1000 18112/32000 (57%)] Loss: 1.96066 (semantic_loss: 0.01320, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23856 
Train Epoch: 35 [571/1000 18272/32000 (57%)] Loss: 1.96121 (semantic_loss: 0.01472, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19482 
Train Epoch: 35 [576/1000 18432/32000 (58%)] Loss: 1.96771 (semantic_loss: 0.02023, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18757 
Train Epoch: 35 [581/1000 18592/32000 (58%)] Loss: 1.96261 (semantic_loss: 0.01416, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22346 
Train Epoch: 35 [586/1000 18752/32000 (59%)] Loss: 1.96041 (semantic_loss: 0.01294, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22227 
Train Epoch: 35 [591/1000 18912/32000 (59%)] Loss: 1.96321 (semantic_loss: 0.01575, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18614 
Train Epoch: 35 [596/1000 19072/32000 (60%)] Loss: 1.96293 (semantic_loss: 0.01546, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19412 
Train Epoch: 35 [601/1000 19232/32000 (60%)] Loss: 1.96162 (semantic_loss: 0.01415, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.25082 
Train Epoch: 35 [606/1000 19392/32000 (61%)] Loss: 1.96098 (semantic_loss: 0.01351, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20914 
Train Epoch: 35 [611/1000 19552/32000 (61%)] Loss: 1.96576 (semantic_loss: 0.01828, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20562 
Train Epoch: 35 [616/1000 19712/32000 (62%)] Loss: 1.96126 (semantic_loss: 0.01379, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24989 
Train Epoch: 35 [621/1000 19872/32000 (62%)] Loss: 1.96393 (semantic_loss: 0.01548, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19737 
Train Epoch: 35 [626/1000 20032/32000 (63%)] Loss: 1.96088 (semantic_loss: 0.01439, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18855 
Train Epoch: 35 [631/1000 20192/32000 (63%)] Loss: 1.96394 (semantic_loss: 0.01648, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18657 
Train Epoch: 35 [636/1000 20352/32000 (64%)] Loss: 1.95873 (semantic_loss: 0.01125, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18401 
Train Epoch: 35 [641/1000 20512/32000 (64%)] Loss: 1.96298 (semantic_loss: 0.01552, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18843 
Train Epoch: 35 [646/1000 20672/32000 (65%)] Loss: 1.96349 (semantic_loss: 0.01602, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18693 
Train Epoch: 35 [651/1000 20832/32000 (65%)] Loss: 1.96152 (semantic_loss: 0.01502, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19086 
Train Epoch: 35 [656/1000 20992/32000 (66%)] Loss: 1.95891 (semantic_loss: 0.01144, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.45667 
Train Epoch: 35 [661/1000 21152/32000 (66%)] Loss: 1.96041 (semantic_loss: 0.01393, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18662 
Train Epoch: 35 [666/1000 21312/32000 (67%)] Loss: 1.96385 (semantic_loss: 0.01638, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18473 
Train Epoch: 35 [671/1000 21472/32000 (67%)] Loss: 1.96081 (semantic_loss: 0.01334, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20423 
Train Epoch: 35 [676/1000 21632/32000 (68%)] Loss: 1.96236 (semantic_loss: 0.01490, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18512 
Train Epoch: 35 [681/1000 21792/32000 (68%)] Loss: 1.96203 (semantic_loss: 0.01358, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.27468 
Train Epoch: 35 [686/1000 21952/32000 (69%)] Loss: 1.96093 (semantic_loss: 0.01346, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18953 
Train Epoch: 35 [691/1000 22112/32000 (69%)] Loss: 1.95989 (semantic_loss: 0.01340, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21598 
Train Epoch: 35 [696/1000 22272/32000 (70%)] Loss: 1.96010 (semantic_loss: 0.01263, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23214 
Train Epoch: 35 [701/1000 22432/32000 (70%)] Loss: 1.96270 (semantic_loss: 0.01524, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23348 
Train Epoch: 35 [706/1000 22592/32000 (71%)] Loss: 1.96084 (semantic_loss: 0.01240, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.42444 
Train Epoch: 35 [711/1000 22752/32000 (71%)] Loss: 1.95945 (semantic_loss: 0.01198, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20218 
Train Epoch: 35 [716/1000 22912/32000 (72%)] Loss: 1.96094 (semantic_loss: 0.01445, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20486 
Train Epoch: 35 [721/1000 23072/32000 (72%)] Loss: 1.96246 (semantic_loss: 0.01402, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20482 
Train Epoch: 35 [726/1000 23232/32000 (73%)] Loss: 1.96232 (semantic_loss: 0.01485, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19086 
Train Epoch: 35 [731/1000 23392/32000 (73%)] Loss: 1.96019 (semantic_loss: 0.01174, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18934 
Train Epoch: 35 [736/1000 23552/32000 (74%)] Loss: 1.95927 (semantic_loss: 0.01278, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18802 
Train Epoch: 35 [741/1000 23712/32000 (74%)] Loss: 1.96078 (semantic_loss: 0.01429, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20942 
Train Epoch: 35 [746/1000 23872/32000 (75%)] Loss: 1.95983 (semantic_loss: 0.01236, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18854 
Train Epoch: 35 [751/1000 24032/32000 (75%)] Loss: 1.96056 (semantic_loss: 0.01407, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18547 
Train Epoch: 35 [756/1000 24192/32000 (76%)] Loss: 1.95967 (semantic_loss: 0.01318, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19966 
Train Epoch: 35 [761/1000 24352/32000 (76%)] Loss: 1.96220 (semantic_loss: 0.01474, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23042 
Train Epoch: 35 [766/1000 24512/32000 (77%)] Loss: 1.96081 (semantic_loss: 0.01334, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19872 
Train Epoch: 35 [771/1000 24672/32000 (77%)] Loss: 1.96267 (semantic_loss: 0.01618, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19041 
Train Epoch: 35 [776/1000 24832/32000 (78%)] Loss: 1.96026 (semantic_loss: 0.01279, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18556 
Train Epoch: 35 [781/1000 24992/32000 (78%)] Loss: 1.96301 (semantic_loss: 0.01652, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18795 
Train Epoch: 35 [786/1000 25152/32000 (79%)] Loss: 1.96119 (semantic_loss: 0.01373, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19217 
Train Epoch: 35 [791/1000 25312/32000 (79%)] Loss: 1.96228 (semantic_loss: 0.01481, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18723 
Train Epoch: 35 [796/1000 25472/32000 (80%)] Loss: 1.96340 (semantic_loss: 0.01593, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19209 
Train Epoch: 35 [801/1000 25632/32000 (80%)] Loss: 1.95888 (semantic_loss: 0.01141, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20098 
Train Epoch: 35 [806/1000 25792/32000 (81%)] Loss: 1.96067 (semantic_loss: 0.01320, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18575 
Train Epoch: 35 [811/1000 25952/32000 (81%)] Loss: 1.96175 (semantic_loss: 0.01429, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.40514 
Train Epoch: 35 [816/1000 26112/32000 (82%)] Loss: 1.96309 (semantic_loss: 0.01464, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18673 
Train Epoch: 35 [821/1000 26272/32000 (82%)] Loss: 1.96315 (semantic_loss: 0.01568, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18570 
Train Epoch: 35 [826/1000 26432/32000 (83%)] Loss: 1.96006 (semantic_loss: 0.01357, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18896 
Train Epoch: 35 [831/1000 26592/32000 (83%)] Loss: 1.96303 (semantic_loss: 0.01459, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18837 
Train Epoch: 35 [836/1000 26752/32000 (84%)] Loss: 1.95900 (semantic_loss: 0.01153, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18888 
Train Epoch: 35 [841/1000 26912/32000 (84%)] Loss: 1.96064 (semantic_loss: 0.01317, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18781 
Train Epoch: 35 [846/1000 27072/32000 (85%)] Loss: 1.95948 (semantic_loss: 0.01202, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21596 
Train Epoch: 35 [851/1000 27232/32000 (85%)] Loss: 1.95889 (semantic_loss: 0.01240, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21744 
Train Epoch: 35 [856/1000 27392/32000 (86%)] Loss: 1.96095 (semantic_loss: 0.01349, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27051 
Train Epoch: 35 [861/1000 27552/32000 (86%)] Loss: 1.96167 (semantic_loss: 0.01420, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21231 
Train Epoch: 35 [866/1000 27712/32000 (87%)] Loss: 1.96104 (semantic_loss: 0.01455, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20353 
Train Epoch: 35 [871/1000 27872/32000 (87%)] Loss: 1.96225 (semantic_loss: 0.01381, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19867 
Train Epoch: 35 [876/1000 28032/32000 (88%)] Loss: 1.96176 (semantic_loss: 0.01429, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21700 
Train Epoch: 35 [881/1000 28192/32000 (88%)] Loss: 1.96684 (semantic_loss: 0.01937, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19765 
Train Epoch: 35 [886/1000 28352/32000 (89%)] Loss: 1.96211 (semantic_loss: 0.01366, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22424 
Train Epoch: 35 [891/1000 28512/32000 (89%)] Loss: 1.96263 (semantic_loss: 0.01614, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19960 
Train Epoch: 35 [896/1000 28672/32000 (90%)] Loss: 1.96109 (semantic_loss: 0.01362, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18727 
Train Epoch: 35 [901/1000 28832/32000 (90%)] Loss: 1.96146 (semantic_loss: 0.01302, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22311 
Train Epoch: 35 [906/1000 28992/32000 (91%)] Loss: 1.96296 (semantic_loss: 0.01549, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21984 
Train Epoch: 35 [911/1000 29152/32000 (91%)] Loss: 1.96422 (semantic_loss: 0.01676, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19609 
Train Epoch: 35 [916/1000 29312/32000 (92%)] Loss: 1.96147 (semantic_loss: 0.01400, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21049 
Train Epoch: 35 [921/1000 29472/32000 (92%)] Loss: 1.96063 (semantic_loss: 0.01219, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.25939 
Train Epoch: 35 [926/1000 29632/32000 (93%)] Loss: 1.96441 (semantic_loss: 0.01597, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18935 
Train Epoch: 35 [931/1000 29792/32000 (93%)] Loss: 1.95895 (semantic_loss: 0.01148, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18642 
Train Epoch: 35 [936/1000 29952/32000 (94%)] Loss: 1.96241 (semantic_loss: 0.01494, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23567 
Train Epoch: 35 [941/1000 30112/32000 (94%)] Loss: 1.96323 (semantic_loss: 0.01479, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18594 
Train Epoch: 35 [946/1000 30272/32000 (95%)] Loss: 1.96059 (semantic_loss: 0.01313, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19191 
Train Epoch: 35 [951/1000 30432/32000 (95%)] Loss: 1.96302 (semantic_loss: 0.01653, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18455 
Train Epoch: 35 [956/1000 30592/32000 (96%)] Loss: 1.96261 (semantic_loss: 0.01612, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18759 
Train Epoch: 35 [961/1000 30752/32000 (96%)] Loss: 1.96215 (semantic_loss: 0.01469, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18614 
Train Epoch: 35 [966/1000 30912/32000 (97%)] Loss: 1.96019 (semantic_loss: 0.01175, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20524 
Train Epoch: 35 [971/1000 31072/32000 (97%)] Loss: 1.96339 (semantic_loss: 0.01689, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19787 
Train Epoch: 35 [976/1000 31232/32000 (98%)] Loss: 1.96115 (semantic_loss: 0.01367, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.45092 
Train Epoch: 35 [981/1000 31392/32000 (98%)] Loss: 1.95938 (semantic_loss: 0.01289, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19602 
Train Epoch: 35 [986/1000 31552/32000 (99%)] Loss: 1.96410 (semantic_loss: 0.01663, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19718 
Train Epoch: 35 [991/1000 31712/32000 (99%)] Loss: 1.96219 (semantic_loss: 0.01472, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20257 
Train Epoch: 35 [996/1000 31872/32000 (100%)] Loss: 1.96174 (semantic_loss: 0.01329, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18624 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch35.pth ...
Done in 4.795s
removing stale ckpt [epoch 34] [took 0.00s]
 epoch          : 35
 loss           : 1.9618601080179214
 learning_rate  : 1.3906419472184687e-06
 n_samples      : 1120000
 n_steps        : 35000
 ActivityNet_val1_test/t2v_metrics/R1: 11.958511287370348
 ActivityNet_val1_test/t2v_metrics/R5: 38.66178564165141
 ActivityNet_val1_test/t2v_metrics/R10: 55.37929631889364
 ActivityNet_val1_test/t2v_metrics/R50: 84.74679682733374
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 61.8302826926988
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.47375617651883
 ActivityNet_val1_test/v2t_metrics/R1: 12.8533658735001
 ActivityNet_val1_test/v2t_metrics/R5: 39.08887533048607
 ActivityNet_val1_test/v2t_metrics/R10: 55.37929631889364
 ActivityNet_val1_test/v2t_metrics/R50: 84.88916005694529
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 63.63595688427903
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 30.302083646426734
 mnt_best       : 29.542107379154057
 not_improved_count: 8
Train Epoch: 36 [1/1000 32/32000 (0%)] Loss: 1.96248 (semantic_loss: 0.01501, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=21.63821 
Train Epoch: 36 [6/1000 192/32000 (1%)] Loss: 1.96218 (semantic_loss: 0.01374, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.28389 
Train Epoch: 36 [11/1000 352/32000 (1%)] Loss: 1.95859 (semantic_loss: 0.01113, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19110 
Train Epoch: 36 [16/1000 512/32000 (2%)] Loss: 1.96650 (semantic_loss: 0.01902, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23654 
Train Epoch: 36 [21/1000 672/32000 (2%)] Loss: 1.96273 (semantic_loss: 0.01624, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19854 
Train Epoch: 36 [26/1000 832/32000 (3%)] Loss: 1.96228 (semantic_loss: 0.01481, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19184 
Train Epoch: 36 [31/1000 992/32000 (3%)] Loss: 1.96113 (semantic_loss: 0.01367, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20371 
Train Epoch: 36 [36/1000 1152/32000 (4%)] Loss: 1.96208 (semantic_loss: 0.01364, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18843 
Train Epoch: 36 [41/1000 1312/32000 (4%)] Loss: 1.95976 (semantic_loss: 0.01229, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18774 
Train Epoch: 36 [46/1000 1472/32000 (5%)] Loss: 1.96983 (semantic_loss: 0.02138, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20464 
Train Epoch: 36 [51/1000 1632/32000 (5%)] Loss: 1.96210 (semantic_loss: 0.01365, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21846 
Train Epoch: 36 [56/1000 1792/32000 (6%)] Loss: 1.96013 (semantic_loss: 0.01364, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19717 
Train Epoch: 36 [61/1000 1952/32000 (6%)] Loss: 1.95925 (semantic_loss: 0.01275, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19125 
Train Epoch: 36 [66/1000 2112/32000 (7%)] Loss: 1.96285 (semantic_loss: 0.01538, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.32514 
Train Epoch: 36 [71/1000 2272/32000 (7%)] Loss: 1.96477 (semantic_loss: 0.01536, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18624 
Train Epoch: 36 [76/1000 2432/32000 (8%)] Loss: 1.96802 (semantic_loss: 0.01957, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21942 
Train Epoch: 36 [81/1000 2592/32000 (8%)] Loss: 1.96116 (semantic_loss: 0.01467, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.75040 
Train Epoch: 36 [86/1000 2752/32000 (9%)] Loss: 1.96249 (semantic_loss: 0.01405, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19805 
Train Epoch: 36 [91/1000 2912/32000 (9%)] Loss: 1.95999 (semantic_loss: 0.01350, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20903 
Train Epoch: 36 [96/1000 3072/32000 (10%)] Loss: 1.96330 (semantic_loss: 0.01485, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20898 
Train Epoch: 36 [101/1000 3232/32000 (10%)] Loss: 1.96115 (semantic_loss: 0.01367, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18866 
Train Epoch: 36 [106/1000 3392/32000 (11%)] Loss: 1.95989 (semantic_loss: 0.01339, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18840 
Train Epoch: 36 [111/1000 3552/32000 (11%)] Loss: 1.96334 (semantic_loss: 0.01489, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18975 
Train Epoch: 36 [116/1000 3712/32000 (12%)] Loss: 1.96468 (semantic_loss: 0.01721, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18874 
Train Epoch: 36 [121/1000 3872/32000 (12%)] Loss: 1.96054 (semantic_loss: 0.01307, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20060 
Train Epoch: 36 [126/1000 4032/32000 (13%)] Loss: 1.96158 (semantic_loss: 0.01412, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20141 
Train Epoch: 36 [131/1000 4192/32000 (13%)] Loss: 1.96336 (semantic_loss: 0.01590, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18996 
Train Epoch: 36 [136/1000 4352/32000 (14%)] Loss: 1.95817 (semantic_loss: 0.01168, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24155 
Train Epoch: 36 [141/1000 4512/32000 (14%)] Loss: 1.96175 (semantic_loss: 0.01428, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19213 
Train Epoch: 36 [146/1000 4672/32000 (15%)] Loss: 1.96596 (semantic_loss: 0.01751, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20769 
Train Epoch: 36 [151/1000 4832/32000 (15%)] Loss: 1.96325 (semantic_loss: 0.01578, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22751 
Train Epoch: 36 [156/1000 4992/32000 (16%)] Loss: 1.96255 (semantic_loss: 0.01508, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21489 
Train Epoch: 36 [161/1000 5152/32000 (16%)] Loss: 1.96440 (semantic_loss: 0.01790, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19201 
Train Epoch: 36 [166/1000 5312/32000 (17%)] Loss: 1.96142 (semantic_loss: 0.01395, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20769 
Train Epoch: 36 [171/1000 5472/32000 (17%)] Loss: 1.96224 (semantic_loss: 0.01477, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19177 
Train Epoch: 36 [176/1000 5632/32000 (18%)] Loss: 1.96707 (semantic_loss: 0.01960, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20048 
Train Epoch: 36 [181/1000 5792/32000 (18%)] Loss: 1.95965 (semantic_loss: 0.01122, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19610 
Train Epoch: 36 [186/1000 5952/32000 (19%)] Loss: 1.96251 (semantic_loss: 0.01406, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18919 
Train Epoch: 36 [191/1000 6112/32000 (19%)] Loss: 1.96420 (semantic_loss: 0.01770, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18778 
Train Epoch: 36 [196/1000 6272/32000 (20%)] Loss: 1.96084 (semantic_loss: 0.01240, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19610 
Train Epoch: 36 [201/1000 6432/32000 (20%)] Loss: 1.96214 (semantic_loss: 0.01565, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18631 
Train Epoch: 36 [206/1000 6592/32000 (21%)] Loss: 1.96097 (semantic_loss: 0.01350, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22184 
Train Epoch: 36 [211/1000 6752/32000 (21%)] Loss: 1.96587 (semantic_loss: 0.01743, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19879 
Train Epoch: 36 [216/1000 6912/32000 (22%)] Loss: 1.96278 (semantic_loss: 0.01531, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20635 
Train Epoch: 36 [221/1000 7072/32000 (22%)] Loss: 1.96746 (semantic_loss: 0.01998, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18735 
Train Epoch: 36 [226/1000 7232/32000 (23%)] Loss: 1.96168 (semantic_loss: 0.01421, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.56033 
Train Epoch: 36 [231/1000 7392/32000 (23%)] Loss: 1.95997 (semantic_loss: 0.01348, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18841 
Train Epoch: 36 [236/1000 7552/32000 (24%)] Loss: 1.96009 (semantic_loss: 0.01263, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19211 
Train Epoch: 36 [241/1000 7712/32000 (24%)] Loss: 1.96004 (semantic_loss: 0.01257, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20416 
Train Epoch: 36 [246/1000 7872/32000 (25%)] Loss: 1.96170 (semantic_loss: 0.01423, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18717 
Train Epoch: 36 [251/1000 8032/32000 (25%)] Loss: 1.95892 (semantic_loss: 0.01243, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18664 
Train Epoch: 36 [256/1000 8192/32000 (26%)] Loss: 1.95863 (semantic_loss: 0.01214, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20770 
Train Epoch: 36 [261/1000 8352/32000 (26%)] Loss: 1.96139 (semantic_loss: 0.01490, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18974 
Train Epoch: 36 [266/1000 8512/32000 (27%)] Loss: 1.95960 (semantic_loss: 0.01213, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18548 
Train Epoch: 36 [271/1000 8672/32000 (27%)] Loss: 1.95768 (semantic_loss: 0.01119, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18414 
Train Epoch: 36 [276/1000 8832/32000 (28%)] Loss: 1.95776 (semantic_loss: 0.01127, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19210 
Train Epoch: 36 [281/1000 8992/32000 (28%)] Loss: 1.95924 (semantic_loss: 0.01177, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18805 
Train Epoch: 36 [286/1000 9152/32000 (29%)] Loss: 1.96360 (semantic_loss: 0.01515, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18984 
Train Epoch: 36 [291/1000 9312/32000 (29%)] Loss: 1.96250 (semantic_loss: 0.01504, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18568 
Train Epoch: 36 [296/1000 9472/32000 (30%)] Loss: 1.95945 (semantic_loss: 0.01199, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18651 
Train Epoch: 36 [301/1000 9632/32000 (30%)] Loss: 1.95936 (semantic_loss: 0.01190, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20748 
Train Epoch: 36 [306/1000 9792/32000 (31%)] Loss: 1.95930 (semantic_loss: 0.01281, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22017 
Train Epoch: 36 [311/1000 9952/32000 (31%)] Loss: 1.95948 (semantic_loss: 0.01299, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24569 
Train Epoch: 36 [316/1000 10112/32000 (32%)] Loss: 1.96299 (semantic_loss: 0.01553, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21076 
Train Epoch: 36 [321/1000 10272/32000 (32%)] Loss: 1.96346 (semantic_loss: 0.01599, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22953 
Train Epoch: 36 [326/1000 10432/32000 (33%)] Loss: 1.96071 (semantic_loss: 0.01423, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20404 
Train Epoch: 36 [331/1000 10592/32000 (33%)] Loss: 1.96154 (semantic_loss: 0.01505, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19892 
Train Epoch: 36 [336/1000 10752/32000 (34%)] Loss: 1.95939 (semantic_loss: 0.01290, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22199 
Train Epoch: 36 [341/1000 10912/32000 (34%)] Loss: 1.96324 (semantic_loss: 0.01479, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18927 
Train Epoch: 36 [346/1000 11072/32000 (35%)] Loss: 1.96014 (semantic_loss: 0.01365, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18581 
Train Epoch: 36 [351/1000 11232/32000 (35%)] Loss: 1.95835 (semantic_loss: 0.01186, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18695 
Train Epoch: 36 [356/1000 11392/32000 (36%)] Loss: 1.96170 (semantic_loss: 0.01423, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18603 
Train Epoch: 36 [361/1000 11552/32000 (36%)] Loss: 1.96288 (semantic_loss: 0.01444, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19373 
Train Epoch: 36 [366/1000 11712/32000 (37%)] Loss: 1.96202 (semantic_loss: 0.01455, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19096 
Train Epoch: 36 [371/1000 11872/32000 (37%)] Loss: 1.96187 (semantic_loss: 0.01440, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23211 
Train Epoch: 36 [376/1000 12032/32000 (38%)] Loss: 1.96225 (semantic_loss: 0.01479, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20822 
Train Epoch: 36 [381/1000 12192/32000 (38%)] Loss: 1.96220 (semantic_loss: 0.01375, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21075 
Train Epoch: 36 [386/1000 12352/32000 (39%)] Loss: 1.96086 (semantic_loss: 0.01340, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.30255 
Train Epoch: 36 [391/1000 12512/32000 (39%)] Loss: 1.96781 (semantic_loss: 0.01937, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20872 
Train Epoch: 36 [396/1000 12672/32000 (40%)] Loss: 1.96380 (semantic_loss: 0.01633, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23154 
Train Epoch: 36 [401/1000 12832/32000 (40%)] Loss: 1.96004 (semantic_loss: 0.01354, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.73201 
Train Epoch: 36 [406/1000 12992/32000 (41%)] Loss: 1.95971 (semantic_loss: 0.01224, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18825 
Train Epoch: 36 [411/1000 13152/32000 (41%)] Loss: 1.96165 (semantic_loss: 0.01419, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18604 
Train Epoch: 36 [416/1000 13312/32000 (42%)] Loss: 1.96218 (semantic_loss: 0.01374, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21495 
Train Epoch: 36 [421/1000 13472/32000 (42%)] Loss: 1.96359 (semantic_loss: 0.01612, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18576 
Train Epoch: 36 [426/1000 13632/32000 (43%)] Loss: 1.96120 (semantic_loss: 0.01373, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18749 
Train Epoch: 36 [431/1000 13792/32000 (43%)] Loss: 1.96123 (semantic_loss: 0.01377, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18143 
Train Epoch: 36 [436/1000 13952/32000 (44%)] Loss: 1.96174 (semantic_loss: 0.01427, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18921 
Train Epoch: 36 [441/1000 14112/32000 (44%)] Loss: 1.96268 (semantic_loss: 0.01619, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20673 
Train Epoch: 36 [446/1000 14272/32000 (45%)] Loss: 1.95889 (semantic_loss: 0.01240, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21562 
Train Epoch: 36 [451/1000 14432/32000 (45%)] Loss: 1.96392 (semantic_loss: 0.01645, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18796 
Train Epoch: 36 [456/1000 14592/32000 (46%)] Loss: 1.96477 (semantic_loss: 0.01632, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.27433 
Train Epoch: 36 [461/1000 14752/32000 (46%)] Loss: 1.96281 (semantic_loss: 0.01632, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23163 
Train Epoch: 36 [466/1000 14912/32000 (47%)] Loss: 1.96048 (semantic_loss: 0.01204, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21445 
Train Epoch: 36 [471/1000 15072/32000 (47%)] Loss: 1.96461 (semantic_loss: 0.01714, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20297 
Train Epoch: 36 [476/1000 15232/32000 (48%)] Loss: 1.95843 (semantic_loss: 0.01292, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.21621 
Train Epoch: 36 [481/1000 15392/32000 (48%)] Loss: 1.96336 (semantic_loss: 0.01492, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21147 
Train Epoch: 36 [486/1000 15552/32000 (49%)] Loss: 1.96655 (semantic_loss: 0.01810, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19760 
Train Epoch: 36 [491/1000 15712/32000 (49%)] Loss: 1.96067 (semantic_loss: 0.01320, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18789 
Train Epoch: 36 [496/1000 15872/32000 (50%)] Loss: 1.96161 (semantic_loss: 0.01413, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18629 
Train Epoch: 36 [501/1000 16032/32000 (50%)] Loss: 1.96013 (semantic_loss: 0.01267, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19332 
Train Epoch: 36 [506/1000 16192/32000 (51%)] Loss: 1.95789 (semantic_loss: 0.01140, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18982 
Train Epoch: 36 [511/1000 16352/32000 (51%)] Loss: 1.96212 (semantic_loss: 0.01465, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19169 
Train Epoch: 36 [516/1000 16512/32000 (52%)] Loss: 1.96403 (semantic_loss: 0.01656, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18839 
Train Epoch: 36 [521/1000 16672/32000 (52%)] Loss: 1.96266 (semantic_loss: 0.01617, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19737 
Train Epoch: 36 [526/1000 16832/32000 (53%)] Loss: 1.96163 (semantic_loss: 0.01417, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23262 
Train Epoch: 36 [531/1000 16992/32000 (53%)] Loss: 1.96156 (semantic_loss: 0.01409, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19614 
Train Epoch: 36 [536/1000 17152/32000 (54%)] Loss: 1.96062 (semantic_loss: 0.01413, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18390 
Train Epoch: 36 [541/1000 17312/32000 (54%)] Loss: 1.95909 (semantic_loss: 0.01163, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18541 
Train Epoch: 36 [546/1000 17472/32000 (55%)] Loss: 1.95807 (semantic_loss: 0.01158, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.56486 
Train Epoch: 36 [551/1000 17632/32000 (55%)] Loss: 1.96142 (semantic_loss: 0.01298, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18671 
Train Epoch: 36 [556/1000 17792/32000 (56%)] Loss: 1.96071 (semantic_loss: 0.01324, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18552 
Train Epoch: 36 [561/1000 17952/32000 (56%)] Loss: 1.96095 (semantic_loss: 0.01348, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20488 
Train Epoch: 36 [566/1000 18112/32000 (57%)] Loss: 1.96370 (semantic_loss: 0.01623, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19438 
Train Epoch: 36 [571/1000 18272/32000 (57%)] Loss: 1.96048 (semantic_loss: 0.01300, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18786 
Train Epoch: 36 [576/1000 18432/32000 (58%)] Loss: 1.96731 (semantic_loss: 0.01985, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19961 
Train Epoch: 36 [581/1000 18592/32000 (58%)] Loss: 1.96161 (semantic_loss: 0.01415, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18733 
Train Epoch: 36 [586/1000 18752/32000 (59%)] Loss: 1.96490 (semantic_loss: 0.01645, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18535 
Train Epoch: 36 [591/1000 18912/32000 (59%)] Loss: 1.96138 (semantic_loss: 0.01489, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18789 
Train Epoch: 36 [596/1000 19072/32000 (60%)] Loss: 1.96096 (semantic_loss: 0.01349, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18560 
Train Epoch: 36 [601/1000 19232/32000 (60%)] Loss: 1.96191 (semantic_loss: 0.01444, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18637 
Train Epoch: 36 [606/1000 19392/32000 (61%)] Loss: 1.96201 (semantic_loss: 0.01454, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18911 
Train Epoch: 36 [611/1000 19552/32000 (61%)] Loss: 1.96188 (semantic_loss: 0.01539, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19590 
Train Epoch: 36 [616/1000 19712/32000 (62%)] Loss: 1.95980 (semantic_loss: 0.01331, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21564 
Train Epoch: 36 [621/1000 19872/32000 (62%)] Loss: 1.96136 (semantic_loss: 0.01487, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23432 
Train Epoch: 36 [626/1000 20032/32000 (63%)] Loss: 1.96382 (semantic_loss: 0.01733, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21085 
Train Epoch: 36 [631/1000 20192/32000 (63%)] Loss: 1.96208 (semantic_loss: 0.01462, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19860 
Train Epoch: 36 [636/1000 20352/32000 (64%)] Loss: 1.96254 (semantic_loss: 0.01410, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19998 
Train Epoch: 36 [641/1000 20512/32000 (64%)] Loss: 1.96151 (semantic_loss: 0.01306, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22154 
Train Epoch: 36 [646/1000 20672/32000 (65%)] Loss: 1.96660 (semantic_loss: 0.01912, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19188 
Train Epoch: 36 [651/1000 20832/32000 (65%)] Loss: 1.96162 (semantic_loss: 0.01317, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19233 
Train Epoch: 36 [656/1000 20992/32000 (66%)] Loss: 1.95998 (semantic_loss: 0.01251, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20259 
Train Epoch: 36 [661/1000 21152/32000 (66%)] Loss: 1.96706 (semantic_loss: 0.01862, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18860 
Train Epoch: 36 [666/1000 21312/32000 (67%)] Loss: 1.95967 (semantic_loss: 0.01318, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18924 
Train Epoch: 36 [671/1000 21472/32000 (67%)] Loss: 1.95803 (semantic_loss: 0.01154, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18828 
Train Epoch: 36 [676/1000 21632/32000 (68%)] Loss: 1.96246 (semantic_loss: 0.01597, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20287 
Train Epoch: 36 [681/1000 21792/32000 (68%)] Loss: 1.96025 (semantic_loss: 0.01278, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18942 
Train Epoch: 36 [686/1000 21952/32000 (69%)] Loss: 1.96437 (semantic_loss: 0.01690, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18667 
Train Epoch: 36 [691/1000 22112/32000 (69%)] Loss: 1.96146 (semantic_loss: 0.01496, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20088 
Train Epoch: 36 [696/1000 22272/32000 (70%)] Loss: 1.95961 (semantic_loss: 0.01312, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18512 
Train Epoch: 36 [701/1000 22432/32000 (70%)] Loss: 1.95916 (semantic_loss: 0.01364, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18459 
Train Epoch: 36 [706/1000 22592/32000 (71%)] Loss: 1.96174 (semantic_loss: 0.01329, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.29890 
Train Epoch: 36 [711/1000 22752/32000 (71%)] Loss: 1.96023 (semantic_loss: 0.01374, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18379 
Train Epoch: 36 [716/1000 22912/32000 (72%)] Loss: 1.96064 (semantic_loss: 0.01317, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22086 
Train Epoch: 36 [721/1000 23072/32000 (72%)] Loss: 1.96456 (semantic_loss: 0.01709, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.69337 
Train Epoch: 36 [726/1000 23232/32000 (73%)] Loss: 1.96508 (semantic_loss: 0.01663, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18617 
Train Epoch: 36 [731/1000 23392/32000 (73%)] Loss: 1.96227 (semantic_loss: 0.01480, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18426 
Train Epoch: 36 [736/1000 23552/32000 (74%)] Loss: 1.96866 (semantic_loss: 0.02022, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20944 
Train Epoch: 36 [741/1000 23712/32000 (74%)] Loss: 1.96032 (semantic_loss: 0.01285, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22212 
Train Epoch: 36 [746/1000 23872/32000 (75%)] Loss: 1.96431 (semantic_loss: 0.01683, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18709 
Train Epoch: 36 [751/1000 24032/32000 (75%)] Loss: 1.96481 (semantic_loss: 0.01832, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18442 
Train Epoch: 36 [756/1000 24192/32000 (76%)] Loss: 1.96522 (semantic_loss: 0.01678, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18764 
Train Epoch: 36 [761/1000 24352/32000 (76%)] Loss: 1.96105 (semantic_loss: 0.01358, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18988 
Train Epoch: 36 [766/1000 24512/32000 (77%)] Loss: 1.96186 (semantic_loss: 0.01439, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19783 
Train Epoch: 36 [771/1000 24672/32000 (77%)] Loss: 1.96207 (semantic_loss: 0.01460, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18556 
Train Epoch: 36 [776/1000 24832/32000 (78%)] Loss: 1.96355 (semantic_loss: 0.01608, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23190 
Train Epoch: 36 [781/1000 24992/32000 (78%)] Loss: 1.96413 (semantic_loss: 0.01666, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20515 
Train Epoch: 36 [786/1000 25152/32000 (79%)] Loss: 1.96423 (semantic_loss: 0.01579, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21947 
Train Epoch: 36 [791/1000 25312/32000 (79%)] Loss: 1.96429 (semantic_loss: 0.01682, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21654 
Train Epoch: 36 [796/1000 25472/32000 (80%)] Loss: 1.96084 (semantic_loss: 0.01434, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20212 
Train Epoch: 36 [801/1000 25632/32000 (80%)] Loss: 1.95936 (semantic_loss: 0.01190, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20133 
Train Epoch: 36 [806/1000 25792/32000 (81%)] Loss: 1.96133 (semantic_loss: 0.01386, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20210 
Train Epoch: 36 [811/1000 25952/32000 (81%)] Loss: 1.96149 (semantic_loss: 0.01402, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20220 
Train Epoch: 36 [816/1000 26112/32000 (82%)] Loss: 1.96014 (semantic_loss: 0.01365, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20104 
Train Epoch: 36 [821/1000 26272/32000 (82%)] Loss: 1.96638 (semantic_loss: 0.01988, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18993 
Train Epoch: 36 [826/1000 26432/32000 (83%)] Loss: 1.96194 (semantic_loss: 0.01544, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18637 
Train Epoch: 36 [831/1000 26592/32000 (83%)] Loss: 1.96103 (semantic_loss: 0.01259, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18809 
Train Epoch: 36 [836/1000 26752/32000 (84%)] Loss: 1.95952 (semantic_loss: 0.01303, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18524 
Train Epoch: 36 [841/1000 26912/32000 (84%)] Loss: 1.96143 (semantic_loss: 0.01396, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18392 
Train Epoch: 36 [846/1000 27072/32000 (85%)] Loss: 1.96147 (semantic_loss: 0.01498, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23194 
Train Epoch: 36 [851/1000 27232/32000 (85%)] Loss: 1.96144 (semantic_loss: 0.01494, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18904 
Train Epoch: 36 [856/1000 27392/32000 (86%)] Loss: 1.95892 (semantic_loss: 0.01242, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18992 
Train Epoch: 36 [861/1000 27552/32000 (86%)] Loss: 1.96634 (semantic_loss: 0.01887, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18637 
Train Epoch: 36 [866/1000 27712/32000 (87%)] Loss: 1.95859 (semantic_loss: 0.01112, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.55328 
Train Epoch: 36 [871/1000 27872/32000 (87%)] Loss: 1.96246 (semantic_loss: 0.01401, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19421 
Train Epoch: 36 [876/1000 28032/32000 (88%)] Loss: 1.96329 (semantic_loss: 0.01680, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20707 
Train Epoch: 36 [881/1000 28192/32000 (88%)] Loss: 1.96272 (semantic_loss: 0.01526, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20398 
Train Epoch: 36 [886/1000 28352/32000 (89%)] Loss: 1.96301 (semantic_loss: 0.01456, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18624 
Train Epoch: 36 [891/1000 28512/32000 (89%)] Loss: 1.96077 (semantic_loss: 0.01428, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18586 
Train Epoch: 36 [896/1000 28672/32000 (90%)] Loss: 1.96420 (semantic_loss: 0.01575, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20090 
Train Epoch: 36 [901/1000 28832/32000 (90%)] Loss: 1.96619 (semantic_loss: 0.01774, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19140 
Train Epoch: 36 [906/1000 28992/32000 (91%)] Loss: 1.95971 (semantic_loss: 0.01322, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18609 
Train Epoch: 36 [911/1000 29152/32000 (91%)] Loss: 1.96314 (semantic_loss: 0.01567, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18753 
Train Epoch: 36 [916/1000 29312/32000 (92%)] Loss: 1.95985 (semantic_loss: 0.01336, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18857 
Train Epoch: 36 [921/1000 29472/32000 (92%)] Loss: 1.96016 (semantic_loss: 0.01465, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.21290 
Train Epoch: 36 [926/1000 29632/32000 (93%)] Loss: 1.96313 (semantic_loss: 0.01664, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19976 
Train Epoch: 36 [931/1000 29792/32000 (93%)] Loss: 1.96237 (semantic_loss: 0.01588, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18626 
Train Epoch: 36 [936/1000 29952/32000 (94%)] Loss: 1.96110 (semantic_loss: 0.01362, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18691 
Train Epoch: 36 [941/1000 30112/32000 (94%)] Loss: 1.96310 (semantic_loss: 0.01563, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18753 
Train Epoch: 36 [946/1000 30272/32000 (95%)] Loss: 1.96164 (semantic_loss: 0.01417, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20775 
Train Epoch: 36 [951/1000 30432/32000 (95%)] Loss: 1.96070 (semantic_loss: 0.01324, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21787 
Train Epoch: 36 [956/1000 30592/32000 (96%)] Loss: 1.96517 (semantic_loss: 0.01868, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23213 
Train Epoch: 36 [961/1000 30752/32000 (96%)] Loss: 1.96321 (semantic_loss: 0.01477, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22384 
Train Epoch: 36 [966/1000 30912/32000 (97%)] Loss: 1.96471 (semantic_loss: 0.01724, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19732 
Train Epoch: 36 [971/1000 31072/32000 (97%)] Loss: 1.96194 (semantic_loss: 0.01447, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19168 
Train Epoch: 36 [976/1000 31232/32000 (98%)] Loss: 1.96239 (semantic_loss: 0.01492, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22105 
Train Epoch: 36 [981/1000 31392/32000 (98%)] Loss: 1.96346 (semantic_loss: 0.01794, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19205 
Train Epoch: 36 [986/1000 31552/32000 (99%)] Loss: 1.95828 (semantic_loss: 0.01179, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18994 
Train Epoch: 36 [991/1000 31712/32000 (99%)] Loss: 1.96281 (semantic_loss: 0.01534, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19612 
Train Epoch: 36 [996/1000 31872/32000 (100%)] Loss: 1.96037 (semantic_loss: 0.01387, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18483 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch36.pth ...
Done in 4.085s
removing stale ckpt [epoch 35] [took 0.00s]
 epoch          : 36
 loss           : 1.9619778261184693
 learning_rate  : 1.251577752496622e-06
 n_samples      : 1152000
 n_steps        : 36000
 ActivityNet_val1_test/t2v_metrics/R1: 11.572096806996136
 ActivityNet_val1_test/t2v_metrics/R5: 38.173683140126094
 ActivityNet_val1_test/t2v_metrics/R10: 55.358958714663416
 ActivityNet_val1_test/t2v_metrics/R50: 85.03152328655685
 ActivityNet_val1_test/t2v_metrics/MedR: 8.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 61.59945088468579
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.02605485443345
 ActivityNet_val1_test/v2t_metrics/R1: 12.34492576774456
 ActivityNet_val1_test/v2t_metrics/R5: 38.90583689241407
 ActivityNet_val1_test/v2t_metrics/R10: 55.96908684157006
 ActivityNet_val1_test/v2t_metrics/R50: 85.11287370347773
 ActivityNet_val1_test/v2t_metrics/MedR: 8.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 63.67276794793573
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.956000001199026
 mnt_best       : 29.542107379154057
 not_improved_count: 9
Train Epoch: 37 [1/1000 32/32000 (0%)] Loss: 1.96269 (semantic_loss: 0.01425, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=22.76891 
Train Epoch: 37 [6/1000 192/32000 (1%)] Loss: 1.96078 (semantic_loss: 0.01526, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19145 
Train Epoch: 37 [11/1000 352/32000 (1%)] Loss: 1.96668 (semantic_loss: 0.01824, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19020 
Train Epoch: 37 [16/1000 512/32000 (2%)] Loss: 1.96329 (semantic_loss: 0.01485, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18660 
Train Epoch: 37 [21/1000 672/32000 (2%)] Loss: 1.96118 (semantic_loss: 0.01371, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19461 
Train Epoch: 37 [26/1000 832/32000 (3%)] Loss: 1.96330 (semantic_loss: 0.01486, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20689 
Train Epoch: 37 [31/1000 992/32000 (3%)] Loss: 1.96074 (semantic_loss: 0.01326, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18645 
Train Epoch: 37 [36/1000 1152/32000 (4%)] Loss: 1.96393 (semantic_loss: 0.01451, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.19804 
Train Epoch: 37 [41/1000 1312/32000 (4%)] Loss: 1.96147 (semantic_loss: 0.01401, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20168 
Train Epoch: 37 [46/1000 1472/32000 (5%)] Loss: 1.96085 (semantic_loss: 0.01337, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.32509 
Train Epoch: 37 [51/1000 1632/32000 (5%)] Loss: 1.96301 (semantic_loss: 0.01555, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19164 
Train Epoch: 37 [56/1000 1792/32000 (6%)] Loss: 1.96344 (semantic_loss: 0.01597, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18849 
Train Epoch: 37 [61/1000 1952/32000 (6%)] Loss: 1.96133 (semantic_loss: 0.01387, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18525 
Train Epoch: 37 [66/1000 2112/32000 (7%)] Loss: 1.96306 (semantic_loss: 0.01656, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18663 
Train Epoch: 37 [71/1000 2272/32000 (7%)] Loss: 1.96618 (semantic_loss: 0.01774, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18531 
Train Epoch: 37 [76/1000 2432/32000 (8%)] Loss: 1.96189 (semantic_loss: 0.01540, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18788 
Train Epoch: 37 [81/1000 2592/32000 (8%)] Loss: 1.96581 (semantic_loss: 0.01932, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18790 
Train Epoch: 37 [86/1000 2752/32000 (9%)] Loss: 1.95999 (semantic_loss: 0.01350, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20696 
Train Epoch: 37 [91/1000 2912/32000 (9%)] Loss: 1.96218 (semantic_loss: 0.01471, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20648 
Train Epoch: 37 [96/1000 3072/32000 (10%)] Loss: 1.95931 (semantic_loss: 0.01184, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20858 
Train Epoch: 37 [101/1000 3232/32000 (10%)] Loss: 1.96023 (semantic_loss: 0.01275, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20894 
Train Epoch: 37 [106/1000 3392/32000 (11%)] Loss: 1.96140 (semantic_loss: 0.01393, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19835 
Train Epoch: 37 [111/1000 3552/32000 (11%)] Loss: 1.96121 (semantic_loss: 0.01472, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19809 
Train Epoch: 37 [116/1000 3712/32000 (12%)] Loss: 1.96070 (semantic_loss: 0.01226, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19576 
Train Epoch: 37 [121/1000 3872/32000 (12%)] Loss: 1.95951 (semantic_loss: 0.01301, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20082 
Train Epoch: 37 [126/1000 4032/32000 (13%)] Loss: 1.95833 (semantic_loss: 0.01281, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19648 
Train Epoch: 37 [131/1000 4192/32000 (13%)] Loss: 1.96119 (semantic_loss: 0.01372, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18709 
Train Epoch: 37 [136/1000 4352/32000 (14%)] Loss: 1.96182 (semantic_loss: 0.01435, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18669 
Train Epoch: 37 [141/1000 4512/32000 (14%)] Loss: 1.96524 (semantic_loss: 0.01777, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18903 
Train Epoch: 37 [146/1000 4672/32000 (15%)] Loss: 1.96082 (semantic_loss: 0.01335, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19535 
Train Epoch: 37 [151/1000 4832/32000 (15%)] Loss: 1.95901 (semantic_loss: 0.01252, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19119 
Train Epoch: 37 [156/1000 4992/32000 (16%)] Loss: 1.96094 (semantic_loss: 0.01347, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19104 
Train Epoch: 37 [161/1000 5152/32000 (16%)] Loss: 1.96283 (semantic_loss: 0.01439, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18992 
Train Epoch: 37 [166/1000 5312/32000 (17%)] Loss: 1.96323 (semantic_loss: 0.01576, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19841 
Train Epoch: 37 [171/1000 5472/32000 (17%)] Loss: 1.96096 (semantic_loss: 0.01350, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19334 
Train Epoch: 37 [176/1000 5632/32000 (18%)] Loss: 1.96284 (semantic_loss: 0.01538, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18533 
Train Epoch: 37 [181/1000 5792/32000 (18%)] Loss: 1.96159 (semantic_loss: 0.01510, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19379 
Train Epoch: 37 [186/1000 5952/32000 (19%)] Loss: 1.96047 (semantic_loss: 0.01300, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24374 
Train Epoch: 37 [191/1000 6112/32000 (19%)] Loss: 1.95943 (semantic_loss: 0.01196, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21396 
Train Epoch: 37 [196/1000 6272/32000 (20%)] Loss: 1.96492 (semantic_loss: 0.01647, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18770 
Train Epoch: 37 [201/1000 6432/32000 (20%)] Loss: 1.95989 (semantic_loss: 0.01340, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18626 
Train Epoch: 37 [206/1000 6592/32000 (21%)] Loss: 1.96533 (semantic_loss: 0.01786, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18676 
Train Epoch: 37 [211/1000 6752/32000 (21%)] Loss: 1.95799 (semantic_loss: 0.01150, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19101 
Train Epoch: 37 [216/1000 6912/32000 (22%)] Loss: 1.96139 (semantic_loss: 0.01392, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18461 
Train Epoch: 37 [221/1000 7072/32000 (22%)] Loss: 1.96363 (semantic_loss: 0.01617, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18244 
Train Epoch: 37 [226/1000 7232/32000 (23%)] Loss: 1.96386 (semantic_loss: 0.01542, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19202 
Train Epoch: 37 [231/1000 7392/32000 (23%)] Loss: 1.96290 (semantic_loss: 0.01641, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18345 
Train Epoch: 37 [236/1000 7552/32000 (24%)] Loss: 1.96177 (semantic_loss: 0.01527, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18642 
Train Epoch: 37 [241/1000 7712/32000 (24%)] Loss: 1.96451 (semantic_loss: 0.01606, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19799 
Train Epoch: 37 [246/1000 7872/32000 (25%)] Loss: 1.96185 (semantic_loss: 0.01536, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18801 
Train Epoch: 37 [251/1000 8032/32000 (25%)] Loss: 1.96121 (semantic_loss: 0.01374, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21839 
Train Epoch: 37 [256/1000 8192/32000 (26%)] Loss: 1.96607 (semantic_loss: 0.01860, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23805 
Train Epoch: 37 [261/1000 8352/32000 (26%)] Loss: 1.96325 (semantic_loss: 0.01578, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21105 
Train Epoch: 37 [266/1000 8512/32000 (27%)] Loss: 1.95991 (semantic_loss: 0.01342, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19745 
Train Epoch: 37 [271/1000 8672/32000 (27%)] Loss: 1.96132 (semantic_loss: 0.01482, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22590 
Train Epoch: 37 [276/1000 8832/32000 (28%)] Loss: 1.96091 (semantic_loss: 0.01246, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20051 
Train Epoch: 37 [281/1000 8992/32000 (28%)] Loss: 1.95866 (semantic_loss: 0.01217, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19083 
Train Epoch: 37 [286/1000 9152/32000 (29%)] Loss: 1.96042 (semantic_loss: 0.01198, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18907 
Train Epoch: 37 [291/1000 9312/32000 (29%)] Loss: 1.96262 (semantic_loss: 0.01417, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18685 
Train Epoch: 37 [296/1000 9472/32000 (30%)] Loss: 1.96264 (semantic_loss: 0.01615, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18631 
Train Epoch: 37 [301/1000 9632/32000 (30%)] Loss: 1.96363 (semantic_loss: 0.01518, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.24270 
Train Epoch: 37 [306/1000 9792/32000 (31%)] Loss: 1.96149 (semantic_loss: 0.01402, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18674 
Train Epoch: 37 [311/1000 9952/32000 (31%)] Loss: 1.96455 (semantic_loss: 0.01708, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18521 
Train Epoch: 37 [316/1000 10112/32000 (32%)] Loss: 1.96158 (semantic_loss: 0.01509, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18852 
Train Epoch: 37 [321/1000 10272/32000 (32%)] Loss: 1.95902 (semantic_loss: 0.01253, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.96076 
Train Epoch: 37 [326/1000 10432/32000 (33%)] Loss: 1.96371 (semantic_loss: 0.01429, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.19004 
Train Epoch: 37 [331/1000 10592/32000 (33%)] Loss: 1.96092 (semantic_loss: 0.01345, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18525 
Train Epoch: 37 [336/1000 10752/32000 (34%)] Loss: 1.96234 (semantic_loss: 0.01390, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19233 
Train Epoch: 37 [341/1000 10912/32000 (34%)] Loss: 1.96085 (semantic_loss: 0.01436, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18633 
Train Epoch: 37 [346/1000 11072/32000 (35%)] Loss: 1.95946 (semantic_loss: 0.01297, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18703 
Train Epoch: 37 [351/1000 11232/32000 (35%)] Loss: 1.96143 (semantic_loss: 0.01397, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18727 
Train Epoch: 37 [356/1000 11392/32000 (36%)] Loss: 1.96025 (semantic_loss: 0.01278, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18501 
Train Epoch: 37 [361/1000 11552/32000 (36%)] Loss: 1.96132 (semantic_loss: 0.01288, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.28653 
Train Epoch: 37 [366/1000 11712/32000 (37%)] Loss: 1.96095 (semantic_loss: 0.01446, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.31669 
Train Epoch: 37 [371/1000 11872/32000 (37%)] Loss: 1.96262 (semantic_loss: 0.01516, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18655 
Train Epoch: 37 [376/1000 12032/32000 (38%)] Loss: 1.96164 (semantic_loss: 0.01417, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18613 
Train Epoch: 37 [381/1000 12192/32000 (38%)] Loss: 1.96285 (semantic_loss: 0.01636, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18572 
Train Epoch: 37 [386/1000 12352/32000 (39%)] Loss: 1.96195 (semantic_loss: 0.01449, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18585 
Train Epoch: 37 [391/1000 12512/32000 (39%)] Loss: 1.96184 (semantic_loss: 0.01437, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.42421 
Train Epoch: 37 [396/1000 12672/32000 (40%)] Loss: 1.96526 (semantic_loss: 0.01682, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18603 
Train Epoch: 37 [401/1000 12832/32000 (40%)] Loss: 1.96131 (semantic_loss: 0.01482, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18735 
Train Epoch: 37 [406/1000 12992/32000 (41%)] Loss: 1.96330 (semantic_loss: 0.01682, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19141 
Train Epoch: 37 [411/1000 13152/32000 (41%)] Loss: 1.96196 (semantic_loss: 0.01449, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20876 
Train Epoch: 37 [416/1000 13312/32000 (42%)] Loss: 1.96279 (semantic_loss: 0.01532, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23118 
Train Epoch: 37 [421/1000 13472/32000 (42%)] Loss: 1.96320 (semantic_loss: 0.01574, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21244 
Train Epoch: 37 [426/1000 13632/32000 (43%)] Loss: 1.96348 (semantic_loss: 0.01503, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19910 
Train Epoch: 37 [431/1000 13792/32000 (43%)] Loss: 1.96309 (semantic_loss: 0.01660, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20109 
Train Epoch: 37 [436/1000 13952/32000 (44%)] Loss: 1.96038 (semantic_loss: 0.01389, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21040 
Train Epoch: 37 [441/1000 14112/32000 (44%)] Loss: 1.96168 (semantic_loss: 0.01421, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19413 
Train Epoch: 37 [446/1000 14272/32000 (45%)] Loss: 1.96972 (semantic_loss: 0.02225, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19617 
Train Epoch: 37 [451/1000 14432/32000 (45%)] Loss: 1.96162 (semantic_loss: 0.01415, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18797 
Train Epoch: 37 [456/1000 14592/32000 (46%)] Loss: 1.95803 (semantic_loss: 0.01154, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18861 
Train Epoch: 37 [461/1000 14752/32000 (46%)] Loss: 1.96733 (semantic_loss: 0.01888, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18600 
Train Epoch: 37 [466/1000 14912/32000 (47%)] Loss: 1.96252 (semantic_loss: 0.01408, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18745 
Train Epoch: 37 [471/1000 15072/32000 (47%)] Loss: 1.96332 (semantic_loss: 0.01488, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19606 
Train Epoch: 37 [476/1000 15232/32000 (48%)] Loss: 1.96205 (semantic_loss: 0.01556, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20330 
Train Epoch: 37 [481/1000 15392/32000 (48%)] Loss: 1.95878 (semantic_loss: 0.01229, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19491 
Train Epoch: 37 [486/1000 15552/32000 (49%)] Loss: 1.96237 (semantic_loss: 0.01392, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19930 
Train Epoch: 37 [491/1000 15712/32000 (49%)] Loss: 1.95956 (semantic_loss: 0.01210, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18891 
Train Epoch: 37 [496/1000 15872/32000 (50%)] Loss: 1.96351 (semantic_loss: 0.01507, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18368 
Train Epoch: 37 [501/1000 16032/32000 (50%)] Loss: 1.96330 (semantic_loss: 0.01583, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21857 
Train Epoch: 37 [506/1000 16192/32000 (51%)] Loss: 1.96118 (semantic_loss: 0.01371, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23455 
Train Epoch: 37 [511/1000 16352/32000 (51%)] Loss: 1.95843 (semantic_loss: 0.01291, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.20173 
Train Epoch: 37 [516/1000 16512/32000 (52%)] Loss: 1.96144 (semantic_loss: 0.01397, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18726 
Train Epoch: 37 [521/1000 16672/32000 (52%)] Loss: 1.96089 (semantic_loss: 0.01342, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18633 
Train Epoch: 37 [526/1000 16832/32000 (53%)] Loss: 1.96445 (semantic_loss: 0.01699, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18395 
Train Epoch: 37 [531/1000 16992/32000 (53%)] Loss: 1.96175 (semantic_loss: 0.01428, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18366 
Train Epoch: 37 [536/1000 17152/32000 (54%)] Loss: 1.96347 (semantic_loss: 0.01502, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20172 
Train Epoch: 37 [541/1000 17312/32000 (54%)] Loss: 1.96355 (semantic_loss: 0.01608, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18801 
Train Epoch: 37 [546/1000 17472/32000 (55%)] Loss: 1.96237 (semantic_loss: 0.01490, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18659 
Train Epoch: 37 [551/1000 17632/32000 (55%)] Loss: 1.96266 (semantic_loss: 0.01617, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20040 
Train Epoch: 37 [556/1000 17792/32000 (56%)] Loss: 1.95779 (semantic_loss: 0.01130, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18968 
Train Epoch: 37 [561/1000 17952/32000 (56%)] Loss: 1.96230 (semantic_loss: 0.01483, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18615 
Train Epoch: 37 [566/1000 18112/32000 (57%)] Loss: 1.96259 (semantic_loss: 0.01610, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18767 
Train Epoch: 37 [571/1000 18272/32000 (57%)] Loss: 1.95944 (semantic_loss: 0.01197, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20682 
Train Epoch: 37 [576/1000 18432/32000 (58%)] Loss: 1.96228 (semantic_loss: 0.01383, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19741 
Train Epoch: 37 [581/1000 18592/32000 (58%)] Loss: 1.96526 (semantic_loss: 0.01877, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21085 
Train Epoch: 37 [586/1000 18752/32000 (59%)] Loss: 1.96527 (semantic_loss: 0.01878, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20520 
Train Epoch: 37 [591/1000 18912/32000 (59%)] Loss: 1.96805 (semantic_loss: 0.02058, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21260 
Train Epoch: 37 [596/1000 19072/32000 (60%)] Loss: 1.96520 (semantic_loss: 0.01578, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.20701 
Train Epoch: 37 [601/1000 19232/32000 (60%)] Loss: 1.96197 (semantic_loss: 0.01353, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20522 
Train Epoch: 37 [606/1000 19392/32000 (61%)] Loss: 1.96411 (semantic_loss: 0.01761, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20704 
Train Epoch: 37 [611/1000 19552/32000 (61%)] Loss: 1.96245 (semantic_loss: 0.01401, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19339 
Train Epoch: 37 [616/1000 19712/32000 (62%)] Loss: 1.95809 (semantic_loss: 0.01258, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18560 
Train Epoch: 37 [621/1000 19872/32000 (62%)] Loss: 1.96864 (semantic_loss: 0.02214, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23477 
Train Epoch: 37 [626/1000 20032/32000 (63%)] Loss: 1.96092 (semantic_loss: 0.01443, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18806 
Train Epoch: 37 [631/1000 20192/32000 (63%)] Loss: 1.96375 (semantic_loss: 0.01628, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19542 
Train Epoch: 37 [636/1000 20352/32000 (64%)] Loss: 1.96310 (semantic_loss: 0.01661, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20153 
Train Epoch: 37 [641/1000 20512/32000 (64%)] Loss: 1.96150 (semantic_loss: 0.01403, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=1.01119 
Train Epoch: 37 [646/1000 20672/32000 (65%)] Loss: 1.96142 (semantic_loss: 0.01395, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19337 
Train Epoch: 37 [651/1000 20832/32000 (65%)] Loss: 1.96275 (semantic_loss: 0.01528, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18431 
Train Epoch: 37 [656/1000 20992/32000 (66%)] Loss: 1.96172 (semantic_loss: 0.01426, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18809 
Train Epoch: 37 [661/1000 21152/32000 (66%)] Loss: 1.96240 (semantic_loss: 0.01493, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18482 
Train Epoch: 37 [666/1000 21312/32000 (67%)] Loss: 1.96228 (semantic_loss: 0.01482, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18630 
Train Epoch: 37 [671/1000 21472/32000 (67%)] Loss: 1.95998 (semantic_loss: 0.01252, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18568 
Train Epoch: 37 [676/1000 21632/32000 (68%)] Loss: 1.96151 (semantic_loss: 0.01307, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18547 
Train Epoch: 37 [681/1000 21792/32000 (68%)] Loss: 1.96171 (semantic_loss: 0.01424, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.28500 
Train Epoch: 37 [686/1000 21952/32000 (69%)] Loss: 1.96124 (semantic_loss: 0.01280, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.31593 
Train Epoch: 37 [691/1000 22112/32000 (69%)] Loss: 1.95983 (semantic_loss: 0.01431, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19891 
Train Epoch: 37 [696/1000 22272/32000 (70%)] Loss: 1.96617 (semantic_loss: 0.01870, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19379 
Train Epoch: 37 [701/1000 22432/32000 (70%)] Loss: 1.96287 (semantic_loss: 0.01541, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18638 
Train Epoch: 37 [706/1000 22592/32000 (71%)] Loss: 1.96211 (semantic_loss: 0.01660, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18639 
Train Epoch: 37 [711/1000 22752/32000 (71%)] Loss: 1.96362 (semantic_loss: 0.01615, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18563 
Train Epoch: 37 [716/1000 22912/32000 (72%)] Loss: 1.96483 (semantic_loss: 0.01736, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19693 
Train Epoch: 37 [721/1000 23072/32000 (72%)] Loss: 1.96500 (semantic_loss: 0.01851, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19094 
Train Epoch: 37 [726/1000 23232/32000 (73%)] Loss: 1.96452 (semantic_loss: 0.01705, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18808 
Train Epoch: 37 [731/1000 23392/32000 (73%)] Loss: 1.96199 (semantic_loss: 0.01452, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20536 
Train Epoch: 37 [736/1000 23552/32000 (74%)] Loss: 1.95874 (semantic_loss: 0.01127, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21121 
Train Epoch: 37 [741/1000 23712/32000 (74%)] Loss: 1.96130 (semantic_loss: 0.01286, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21670 
Train Epoch: 37 [746/1000 23872/32000 (75%)] Loss: 1.96183 (semantic_loss: 0.01436, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21147 
Train Epoch: 37 [751/1000 24032/32000 (75%)] Loss: 1.96256 (semantic_loss: 0.01509, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19991 
Train Epoch: 37 [756/1000 24192/32000 (76%)] Loss: 1.95888 (semantic_loss: 0.01239, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19338 
Train Epoch: 37 [761/1000 24352/32000 (76%)] Loss: 1.96122 (semantic_loss: 0.01473, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21351 
Train Epoch: 37 [766/1000 24512/32000 (77%)] Loss: 1.96349 (semantic_loss: 0.01700, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19531 
Train Epoch: 37 [771/1000 24672/32000 (77%)] Loss: 1.96117 (semantic_loss: 0.01468, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18797 
Train Epoch: 37 [776/1000 24832/32000 (78%)] Loss: 1.96272 (semantic_loss: 0.01427, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18916 
Train Epoch: 37 [781/1000 24992/32000 (78%)] Loss: 1.96315 (semantic_loss: 0.01470, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18980 
Train Epoch: 37 [786/1000 25152/32000 (79%)] Loss: 1.96080 (semantic_loss: 0.01236, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18638 
Train Epoch: 37 [791/1000 25312/32000 (79%)] Loss: 1.96137 (semantic_loss: 0.01488, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18679 
Train Epoch: 37 [796/1000 25472/32000 (80%)] Loss: 1.96492 (semantic_loss: 0.01745, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20295 
Train Epoch: 37 [801/1000 25632/32000 (80%)] Loss: 1.95933 (semantic_loss: 0.01187, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20326 
Train Epoch: 37 [806/1000 25792/32000 (81%)] Loss: 1.96121 (semantic_loss: 0.01375, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20338 
Train Epoch: 37 [811/1000 25952/32000 (81%)] Loss: 1.96601 (semantic_loss: 0.01854, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18761 
Train Epoch: 37 [816/1000 26112/32000 (82%)] Loss: 1.96437 (semantic_loss: 0.01788, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18565 
Train Epoch: 37 [821/1000 26272/32000 (82%)] Loss: 1.96010 (semantic_loss: 0.01263, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19370 
Train Epoch: 37 [826/1000 26432/32000 (83%)] Loss: 1.96132 (semantic_loss: 0.01483, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23284 
Train Epoch: 37 [831/1000 26592/32000 (83%)] Loss: 1.96186 (semantic_loss: 0.01439, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20298 
Train Epoch: 37 [836/1000 26752/32000 (84%)] Loss: 1.96062 (semantic_loss: 0.01414, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18672 
Train Epoch: 37 [841/1000 26912/32000 (84%)] Loss: 1.96001 (semantic_loss: 0.01255, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18408 
Train Epoch: 37 [846/1000 27072/32000 (85%)] Loss: 1.96356 (semantic_loss: 0.01511, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18588 
Train Epoch: 37 [851/1000 27232/32000 (85%)] Loss: 1.96401 (semantic_loss: 0.01654, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18765 
Train Epoch: 37 [856/1000 27392/32000 (86%)] Loss: 1.96077 (semantic_loss: 0.01428, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18250 
Train Epoch: 37 [861/1000 27552/32000 (86%)] Loss: 1.95976 (semantic_loss: 0.01229, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18718 
Train Epoch: 37 [866/1000 27712/32000 (87%)] Loss: 1.96471 (semantic_loss: 0.01725, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19016 
Train Epoch: 37 [871/1000 27872/32000 (87%)] Loss: 1.96316 (semantic_loss: 0.01472, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18851 
Train Epoch: 37 [876/1000 28032/32000 (88%)] Loss: 1.95844 (semantic_loss: 0.01195, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18517 
Train Epoch: 37 [881/1000 28192/32000 (88%)] Loss: 1.95973 (semantic_loss: 0.01324, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18781 
Train Epoch: 37 [886/1000 28352/32000 (89%)] Loss: 1.96071 (semantic_loss: 0.01325, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18667 
Train Epoch: 37 [891/1000 28512/32000 (89%)] Loss: 1.96374 (semantic_loss: 0.01530, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18540 
Train Epoch: 37 [896/1000 28672/32000 (90%)] Loss: 1.96068 (semantic_loss: 0.01516, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.21560 
Train Epoch: 37 [901/1000 28832/32000 (90%)] Loss: 1.96055 (semantic_loss: 0.01309, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20825 
Train Epoch: 37 [906/1000 28992/32000 (91%)] Loss: 1.96114 (semantic_loss: 0.01466, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20925 
Train Epoch: 37 [911/1000 29152/32000 (91%)] Loss: 1.96105 (semantic_loss: 0.01260, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20008 
Train Epoch: 37 [916/1000 29312/32000 (92%)] Loss: 1.95937 (semantic_loss: 0.01190, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22266 
Train Epoch: 37 [921/1000 29472/32000 (92%)] Loss: 1.96333 (semantic_loss: 0.01586, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23024 
Train Epoch: 37 [926/1000 29632/32000 (93%)] Loss: 1.96078 (semantic_loss: 0.01331, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19984 
Train Epoch: 37 [931/1000 29792/32000 (93%)] Loss: 1.96167 (semantic_loss: 0.01517, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20684 
Train Epoch: 37 [936/1000 29952/32000 (94%)] Loss: 1.96123 (semantic_loss: 0.01376, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18649 
Train Epoch: 37 [941/1000 30112/32000 (94%)] Loss: 1.96280 (semantic_loss: 0.01533, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23044 
Train Epoch: 37 [946/1000 30272/32000 (95%)] Loss: 1.95875 (semantic_loss: 0.01226, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18526 
Train Epoch: 37 [951/1000 30432/32000 (95%)] Loss: 1.96135 (semantic_loss: 0.01485, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18405 
Train Epoch: 37 [956/1000 30592/32000 (96%)] Loss: 1.96116 (semantic_loss: 0.01369, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19943 
Train Epoch: 37 [961/1000 30752/32000 (96%)] Loss: 1.95966 (semantic_loss: 0.01317, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=1.20386 
Train Epoch: 37 [966/1000 30912/32000 (97%)] Loss: 1.95949 (semantic_loss: 0.01300, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18931 
Train Epoch: 37 [971/1000 31072/32000 (97%)] Loss: 1.95940 (semantic_loss: 0.01291, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19368 
Train Epoch: 37 [976/1000 31232/32000 (98%)] Loss: 1.96255 (semantic_loss: 0.01704, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18837 
Train Epoch: 37 [981/1000 31392/32000 (98%)] Loss: 1.95956 (semantic_loss: 0.01210, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19163 
Train Epoch: 37 [986/1000 31552/32000 (99%)] Loss: 1.96680 (semantic_loss: 0.01836, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18772 
Train Epoch: 37 [991/1000 31712/32000 (99%)] Loss: 1.96331 (semantic_loss: 0.01584, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18728 
Train Epoch: 37 [996/1000 31872/32000 (100%)] Loss: 1.96244 (semantic_loss: 0.01497, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19348 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch37.pth ...
Done in 4.682s
removing stale ckpt [epoch 36] [took 0.00s]
 epoch          : 37
 loss           : 1.9619026166200637
 learning_rate  : 1.1264199772469597e-06
 n_samples      : 1184000
 n_steps        : 37000
 ActivityNet_val1_test/t2v_metrics/R1: 11.938173683140127
 ActivityNet_val1_test/t2v_metrics/R5: 37.949969493593656
 ActivityNet_val1_test/t2v_metrics/R10: 55.562334756965626
 ActivityNet_val1_test/t2v_metrics/R50: 84.8078096400244
 ActivityNet_val1_test/t2v_metrics/MedR: 8.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 63.437055114907466
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.30735355824207
 ActivityNet_val1_test/v2t_metrics/R1: 12.263575350823674
 ActivityNet_val1_test/v2t_metrics/R5: 38.66178564165141
 ActivityNet_val1_test/v2t_metrics/R10: 55.9080740288794
 ActivityNet_val1_test/v2t_metrics/R50: 84.90949766117551
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 64.67286963595689
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.81658138366242
 mnt_best       : 29.542107379154057
 not_improved_count: 10
Train Epoch: 38 [1/1000 32/32000 (0%)] Loss: 1.96270 (semantic_loss: 0.01425, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=21.54556 
Train Epoch: 38 [6/1000 192/32000 (1%)] Loss: 1.96014 (semantic_loss: 0.01267, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18927 
Train Epoch: 38 [11/1000 352/32000 (1%)] Loss: 1.96237 (semantic_loss: 0.01490, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21464 
Train Epoch: 38 [16/1000 512/32000 (2%)] Loss: 1.96090 (semantic_loss: 0.01343, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.48914 
Train Epoch: 38 [21/1000 672/32000 (2%)] Loss: 1.96095 (semantic_loss: 0.01349, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22574 
Train Epoch: 38 [26/1000 832/32000 (3%)] Loss: 1.96093 (semantic_loss: 0.01346, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21262 
Train Epoch: 38 [31/1000 992/32000 (3%)] Loss: 1.96327 (semantic_loss: 0.01580, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19939 
Train Epoch: 38 [36/1000 1152/32000 (4%)] Loss: 1.96378 (semantic_loss: 0.01632, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19576 
Train Epoch: 38 [41/1000 1312/32000 (4%)] Loss: 1.96322 (semantic_loss: 0.01478, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20065 
Train Epoch: 38 [46/1000 1472/32000 (5%)] Loss: 1.96124 (semantic_loss: 0.01377, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18897 
Train Epoch: 38 [51/1000 1632/32000 (5%)] Loss: 1.96326 (semantic_loss: 0.01482, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21064 
Train Epoch: 38 [56/1000 1792/32000 (6%)] Loss: 1.96199 (semantic_loss: 0.01354, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21014 
Train Epoch: 38 [61/1000 1952/32000 (6%)] Loss: 1.96248 (semantic_loss: 0.01404, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19510 
Train Epoch: 38 [66/1000 2112/32000 (7%)] Loss: 1.96356 (semantic_loss: 0.01511, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18804 
Train Epoch: 38 [71/1000 2272/32000 (7%)] Loss: 1.96303 (semantic_loss: 0.01555, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18830 
Train Epoch: 38 [76/1000 2432/32000 (8%)] Loss: 1.95950 (semantic_loss: 0.01106, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21625 
Train Epoch: 38 [81/1000 2592/32000 (8%)] Loss: 1.96230 (semantic_loss: 0.01483, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19489 
Train Epoch: 38 [86/1000 2752/32000 (9%)] Loss: 1.96155 (semantic_loss: 0.01407, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.33630 
Train Epoch: 38 [91/1000 2912/32000 (9%)] Loss: 1.96331 (semantic_loss: 0.01584, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18540 
Train Epoch: 38 [96/1000 3072/32000 (10%)] Loss: 1.96043 (semantic_loss: 0.01297, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18531 
Train Epoch: 38 [101/1000 3232/32000 (10%)] Loss: 1.96235 (semantic_loss: 0.01488, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18558 
Train Epoch: 38 [106/1000 3392/32000 (11%)] Loss: 1.95957 (semantic_loss: 0.01405, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18686 
Train Epoch: 38 [111/1000 3552/32000 (11%)] Loss: 1.96135 (semantic_loss: 0.01486, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18738 
Train Epoch: 38 [116/1000 3712/32000 (12%)] Loss: 1.96037 (semantic_loss: 0.01290, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18482 
Train Epoch: 38 [121/1000 3872/32000 (12%)] Loss: 1.96427 (semantic_loss: 0.01583, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18853 
Train Epoch: 38 [126/1000 4032/32000 (13%)] Loss: 1.96066 (semantic_loss: 0.01319, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18416 
Train Epoch: 38 [131/1000 4192/32000 (13%)] Loss: 1.95943 (semantic_loss: 0.01294, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.37051 
Train Epoch: 38 [136/1000 4352/32000 (14%)] Loss: 1.96292 (semantic_loss: 0.01544, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18937 
Train Epoch: 38 [141/1000 4512/32000 (14%)] Loss: 1.96413 (semantic_loss: 0.01569, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18855 
Train Epoch: 38 [146/1000 4672/32000 (15%)] Loss: 1.96087 (semantic_loss: 0.01536, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18805 
Train Epoch: 38 [151/1000 4832/32000 (15%)] Loss: 1.96224 (semantic_loss: 0.01379, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18452 
Train Epoch: 38 [156/1000 4992/32000 (16%)] Loss: 1.96416 (semantic_loss: 0.01669, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18422 
Train Epoch: 38 [161/1000 5152/32000 (16%)] Loss: 1.96046 (semantic_loss: 0.01300, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.33313 
Train Epoch: 38 [166/1000 5312/32000 (17%)] Loss: 1.96027 (semantic_loss: 0.01281, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19258 
Train Epoch: 38 [171/1000 5472/32000 (17%)] Loss: 1.96194 (semantic_loss: 0.01448, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20983 
Train Epoch: 38 [176/1000 5632/32000 (18%)] Loss: 1.96409 (semantic_loss: 0.01759, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21239 
Train Epoch: 38 [181/1000 5792/32000 (18%)] Loss: 1.95749 (semantic_loss: 0.01100, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21803 
Train Epoch: 38 [186/1000 5952/32000 (19%)] Loss: 1.96213 (semantic_loss: 0.01467, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21968 
Train Epoch: 38 [191/1000 6112/32000 (19%)] Loss: 1.96047 (semantic_loss: 0.01398, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21358 
Train Epoch: 38 [196/1000 6272/32000 (20%)] Loss: 1.95945 (semantic_loss: 0.01198, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18948 
Train Epoch: 38 [201/1000 6432/32000 (20%)] Loss: 1.96418 (semantic_loss: 0.01768, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20522 
Train Epoch: 38 [206/1000 6592/32000 (21%)] Loss: 1.96090 (semantic_loss: 0.01343, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19910 
Train Epoch: 38 [211/1000 6752/32000 (21%)] Loss: 1.95902 (semantic_loss: 0.01156, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18746 
Train Epoch: 38 [216/1000 6912/32000 (22%)] Loss: 1.96115 (semantic_loss: 0.01270, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18895 
Train Epoch: 38 [221/1000 7072/32000 (22%)] Loss: 1.95947 (semantic_loss: 0.01200, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18792 
Train Epoch: 38 [226/1000 7232/32000 (23%)] Loss: 1.96311 (semantic_loss: 0.01564, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18539 
Train Epoch: 38 [231/1000 7392/32000 (23%)] Loss: 1.96412 (semantic_loss: 0.01763, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.70609 
Train Epoch: 38 [236/1000 7552/32000 (24%)] Loss: 1.95976 (semantic_loss: 0.01132, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18847 
Train Epoch: 38 [241/1000 7712/32000 (24%)] Loss: 1.96343 (semantic_loss: 0.01596, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19442 
Train Epoch: 38 [246/1000 7872/32000 (25%)] Loss: 1.96120 (semantic_loss: 0.01472, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19330 
Train Epoch: 38 [251/1000 8032/32000 (25%)] Loss: 1.96007 (semantic_loss: 0.01261, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19664 
Train Epoch: 38 [256/1000 8192/32000 (26%)] Loss: 1.96746 (semantic_loss: 0.02096, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20988 
Train Epoch: 38 [261/1000 8352/32000 (26%)] Loss: 1.96462 (semantic_loss: 0.01618, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21463 
Train Epoch: 38 [266/1000 8512/32000 (27%)] Loss: 1.95886 (semantic_loss: 0.01140, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22002 
Train Epoch: 38 [271/1000 8672/32000 (27%)] Loss: 1.95938 (semantic_loss: 0.01289, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18559 
Train Epoch: 38 [276/1000 8832/32000 (28%)] Loss: 1.96210 (semantic_loss: 0.01366, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18547 
Train Epoch: 38 [281/1000 8992/32000 (28%)] Loss: 1.96641 (semantic_loss: 0.01895, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18755 
Train Epoch: 38 [286/1000 9152/32000 (29%)] Loss: 1.96192 (semantic_loss: 0.01349, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18298 
Train Epoch: 38 [291/1000 9312/32000 (29%)] Loss: 1.96143 (semantic_loss: 0.01298, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18472 
Train Epoch: 38 [296/1000 9472/32000 (30%)] Loss: 1.95956 (semantic_loss: 0.01307, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18731 
Train Epoch: 38 [301/1000 9632/32000 (30%)] Loss: 1.95904 (semantic_loss: 0.01158, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19126 
Train Epoch: 38 [306/1000 9792/32000 (31%)] Loss: 1.96169 (semantic_loss: 0.01421, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18923 
Train Epoch: 38 [311/1000 9952/32000 (31%)] Loss: 1.96419 (semantic_loss: 0.01769, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18651 
Train Epoch: 38 [316/1000 10112/32000 (32%)] Loss: 1.96282 (semantic_loss: 0.01535, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18734 
Train Epoch: 38 [321/1000 10272/32000 (32%)] Loss: 1.96354 (semantic_loss: 0.01705, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18953 
Train Epoch: 38 [326/1000 10432/32000 (33%)] Loss: 1.96180 (semantic_loss: 0.01433, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18591 
Train Epoch: 38 [331/1000 10592/32000 (33%)] Loss: 1.96052 (semantic_loss: 0.01402, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20669 
Train Epoch: 38 [336/1000 10752/32000 (34%)] Loss: 1.95881 (semantic_loss: 0.01232, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.51878 
Train Epoch: 38 [341/1000 10912/32000 (34%)] Loss: 1.95900 (semantic_loss: 0.01154, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23010 
Train Epoch: 38 [346/1000 11072/32000 (35%)] Loss: 1.95999 (semantic_loss: 0.01350, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19041 
Train Epoch: 38 [351/1000 11232/32000 (35%)] Loss: 1.95859 (semantic_loss: 0.01113, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19334 
Train Epoch: 38 [356/1000 11392/32000 (36%)] Loss: 1.95942 (semantic_loss: 0.01195, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20199 
Train Epoch: 38 [361/1000 11552/32000 (36%)] Loss: 1.96164 (semantic_loss: 0.01418, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18889 
Train Epoch: 38 [366/1000 11712/32000 (37%)] Loss: 1.96117 (semantic_loss: 0.01468, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20222 
Train Epoch: 38 [371/1000 11872/32000 (37%)] Loss: 1.96421 (semantic_loss: 0.01772, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18849 
Train Epoch: 38 [376/1000 12032/32000 (38%)] Loss: 1.96004 (semantic_loss: 0.01257, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20949 
Train Epoch: 38 [381/1000 12192/32000 (38%)] Loss: 1.96251 (semantic_loss: 0.01406, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19574 
Train Epoch: 38 [386/1000 12352/32000 (39%)] Loss: 1.96353 (semantic_loss: 0.01509, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18985 
Train Epoch: 38 [391/1000 12512/32000 (39%)] Loss: 1.96049 (semantic_loss: 0.01498, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18650 
Train Epoch: 38 [396/1000 12672/32000 (40%)] Loss: 1.96531 (semantic_loss: 0.01687, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18951 
Train Epoch: 38 [401/1000 12832/32000 (40%)] Loss: 1.96156 (semantic_loss: 0.01410, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18990 
Train Epoch: 38 [406/1000 12992/32000 (41%)] Loss: 1.95892 (semantic_loss: 0.01145, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.32484 
Train Epoch: 38 [411/1000 13152/32000 (41%)] Loss: 1.96152 (semantic_loss: 0.01405, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18924 
Train Epoch: 38 [416/1000 13312/32000 (42%)] Loss: 1.96015 (semantic_loss: 0.01269, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18580 
Train Epoch: 38 [421/1000 13472/32000 (42%)] Loss: 1.96007 (semantic_loss: 0.01456, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18675 
Train Epoch: 38 [426/1000 13632/32000 (43%)] Loss: 1.96103 (semantic_loss: 0.01455, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18613 
Train Epoch: 38 [431/1000 13792/32000 (43%)] Loss: 1.95854 (semantic_loss: 0.01205, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18685 
Train Epoch: 38 [436/1000 13952/32000 (44%)] Loss: 1.96635 (semantic_loss: 0.01887, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18874 
Train Epoch: 38 [441/1000 14112/32000 (44%)] Loss: 1.95940 (semantic_loss: 0.01193, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18864 
Train Epoch: 38 [446/1000 14272/32000 (45%)] Loss: 1.96490 (semantic_loss: 0.01743, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18673 
Train Epoch: 38 [451/1000 14432/32000 (45%)] Loss: 1.96002 (semantic_loss: 0.01255, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.35283 
Train Epoch: 38 [456/1000 14592/32000 (46%)] Loss: 1.96288 (semantic_loss: 0.01542, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18620 
Train Epoch: 38 [461/1000 14752/32000 (46%)] Loss: 1.96187 (semantic_loss: 0.01538, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18597 
Train Epoch: 38 [466/1000 14912/32000 (47%)] Loss: 1.96070 (semantic_loss: 0.01324, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18902 
Train Epoch: 38 [471/1000 15072/32000 (47%)] Loss: 1.95977 (semantic_loss: 0.01328, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18628 
Train Epoch: 38 [476/1000 15232/32000 (48%)] Loss: 1.96284 (semantic_loss: 0.01538, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18582 
Train Epoch: 38 [481/1000 15392/32000 (48%)] Loss: 1.95863 (semantic_loss: 0.01116, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.33410 
Train Epoch: 38 [486/1000 15552/32000 (49%)] Loss: 1.96015 (semantic_loss: 0.01269, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18446 
Train Epoch: 38 [491/1000 15712/32000 (49%)] Loss: 1.96203 (semantic_loss: 0.01554, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21002 
Train Epoch: 38 [496/1000 15872/32000 (50%)] Loss: 1.95973 (semantic_loss: 0.01226, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21275 
Train Epoch: 38 [501/1000 16032/32000 (50%)] Loss: 1.96021 (semantic_loss: 0.01275, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23046 
Train Epoch: 38 [506/1000 16192/32000 (51%)] Loss: 1.96280 (semantic_loss: 0.01534, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20093 
Train Epoch: 38 [511/1000 16352/32000 (51%)] Loss: 1.95936 (semantic_loss: 0.01189, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21301 
Train Epoch: 38 [516/1000 16512/32000 (52%)] Loss: 1.96213 (semantic_loss: 0.01368, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20742 
Train Epoch: 38 [521/1000 16672/32000 (52%)] Loss: 1.96183 (semantic_loss: 0.01436, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19583 
Train Epoch: 38 [526/1000 16832/32000 (53%)] Loss: 1.96385 (semantic_loss: 0.01638, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20819 
Train Epoch: 38 [531/1000 16992/32000 (53%)] Loss: 1.96205 (semantic_loss: 0.01361, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19355 
Train Epoch: 38 [536/1000 17152/32000 (54%)] Loss: 1.96176 (semantic_loss: 0.01429, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18581 
Train Epoch: 38 [541/1000 17312/32000 (54%)] Loss: 1.96193 (semantic_loss: 0.01349, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18668 
Train Epoch: 38 [546/1000 17472/32000 (55%)] Loss: 1.96061 (semantic_loss: 0.01412, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18236 
Train Epoch: 38 [551/1000 17632/32000 (55%)] Loss: 1.95912 (semantic_loss: 0.01263, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.64991 
Train Epoch: 38 [556/1000 17792/32000 (56%)] Loss: 1.96614 (semantic_loss: 0.01769, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19285 
Train Epoch: 38 [561/1000 17952/32000 (56%)] Loss: 1.96149 (semantic_loss: 0.01304, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21498 
Train Epoch: 38 [566/1000 18112/32000 (57%)] Loss: 1.96148 (semantic_loss: 0.01400, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21532 
Train Epoch: 38 [571/1000 18272/32000 (57%)] Loss: 1.96394 (semantic_loss: 0.01647, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19206 
Train Epoch: 38 [576/1000 18432/32000 (58%)] Loss: 1.96212 (semantic_loss: 0.01465, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20672 
Train Epoch: 38 [581/1000 18592/32000 (58%)] Loss: 1.96081 (semantic_loss: 0.01431, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23343 
Train Epoch: 38 [586/1000 18752/32000 (59%)] Loss: 1.95825 (semantic_loss: 0.01176, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22155 
Train Epoch: 38 [591/1000 18912/32000 (59%)] Loss: 1.96154 (semantic_loss: 0.01408, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18821 
Train Epoch: 38 [596/1000 19072/32000 (60%)] Loss: 1.96333 (semantic_loss: 0.01586, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18723 
Train Epoch: 38 [601/1000 19232/32000 (60%)] Loss: 1.96395 (semantic_loss: 0.01453, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18828 
Train Epoch: 38 [606/1000 19392/32000 (61%)] Loss: 1.96055 (semantic_loss: 0.01210, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18710 
Train Epoch: 38 [611/1000 19552/32000 (61%)] Loss: 1.96295 (semantic_loss: 0.01645, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18948 
Train Epoch: 38 [616/1000 19712/32000 (62%)] Loss: 1.96167 (semantic_loss: 0.01518, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18572 
Train Epoch: 38 [621/1000 19872/32000 (62%)] Loss: 1.96264 (semantic_loss: 0.01322, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18743 
Train Epoch: 38 [626/1000 20032/32000 (63%)] Loss: 1.96159 (semantic_loss: 0.01412, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18642 
Train Epoch: 38 [631/1000 20192/32000 (63%)] Loss: 1.95908 (semantic_loss: 0.01260, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18975 
Train Epoch: 38 [636/1000 20352/32000 (64%)] Loss: 1.96029 (semantic_loss: 0.01282, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18727 
Train Epoch: 38 [641/1000 20512/32000 (64%)] Loss: 1.96160 (semantic_loss: 0.01412, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18854 
Train Epoch: 38 [646/1000 20672/32000 (65%)] Loss: 1.95962 (semantic_loss: 0.01313, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18767 
Train Epoch: 38 [651/1000 20832/32000 (65%)] Loss: 1.96110 (semantic_loss: 0.01364, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19800 
Train Epoch: 38 [656/1000 20992/32000 (66%)] Loss: 1.96088 (semantic_loss: 0.01438, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.47893 
Train Epoch: 38 [661/1000 21152/32000 (66%)] Loss: 1.95900 (semantic_loss: 0.01349, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.22766 
Train Epoch: 38 [666/1000 21312/32000 (67%)] Loss: 1.95980 (semantic_loss: 0.01331, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20676 
Train Epoch: 38 [671/1000 21472/32000 (67%)] Loss: 1.96057 (semantic_loss: 0.01310, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21872 
Train Epoch: 38 [676/1000 21632/32000 (68%)] Loss: 1.95935 (semantic_loss: 0.01287, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20124 
Train Epoch: 38 [681/1000 21792/32000 (68%)] Loss: 1.95981 (semantic_loss: 0.01137, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19407 
Train Epoch: 38 [686/1000 21952/32000 (69%)] Loss: 1.96325 (semantic_loss: 0.01577, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20064 
Train Epoch: 38 [691/1000 22112/32000 (69%)] Loss: 1.96106 (semantic_loss: 0.01360, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19146 
Train Epoch: 38 [696/1000 22272/32000 (70%)] Loss: 1.96387 (semantic_loss: 0.01640, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18916 
Train Epoch: 38 [701/1000 22432/32000 (70%)] Loss: 1.96500 (semantic_loss: 0.01754, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19488 
Train Epoch: 38 [706/1000 22592/32000 (71%)] Loss: 1.96078 (semantic_loss: 0.01233, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19885 
Train Epoch: 38 [711/1000 22752/32000 (71%)] Loss: 1.95877 (semantic_loss: 0.01130, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18917 
Train Epoch: 38 [716/1000 22912/32000 (72%)] Loss: 1.96372 (semantic_loss: 0.01625, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19945 
Train Epoch: 38 [721/1000 23072/32000 (72%)] Loss: 1.95977 (semantic_loss: 0.01425, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.20235 
Train Epoch: 38 [726/1000 23232/32000 (73%)] Loss: 1.96437 (semantic_loss: 0.01691, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.32310 
Train Epoch: 38 [731/1000 23392/32000 (73%)] Loss: 1.95969 (semantic_loss: 0.01223, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18688 
Train Epoch: 38 [736/1000 23552/32000 (74%)] Loss: 1.95950 (semantic_loss: 0.01204, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18527 
Train Epoch: 38 [741/1000 23712/32000 (74%)] Loss: 1.96232 (semantic_loss: 0.01485, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19616 
Train Epoch: 38 [746/1000 23872/32000 (75%)] Loss: 1.95809 (semantic_loss: 0.01160, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18601 
Train Epoch: 38 [751/1000 24032/32000 (75%)] Loss: 1.96127 (semantic_loss: 0.01478, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20838 
Train Epoch: 38 [756/1000 24192/32000 (76%)] Loss: 1.96375 (semantic_loss: 0.01725, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19075 
Train Epoch: 38 [761/1000 24352/32000 (76%)] Loss: 1.96024 (semantic_loss: 0.01277, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18838 
Train Epoch: 38 [766/1000 24512/32000 (77%)] Loss: 1.96056 (semantic_loss: 0.01406, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18437 
Train Epoch: 38 [771/1000 24672/32000 (77%)] Loss: 1.96073 (semantic_loss: 0.01327, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.37295 
Train Epoch: 38 [776/1000 24832/32000 (78%)] Loss: 1.96027 (semantic_loss: 0.01378, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18900 
Train Epoch: 38 [781/1000 24992/32000 (78%)] Loss: 1.96025 (semantic_loss: 0.01376, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18600 
Train Epoch: 38 [786/1000 25152/32000 (79%)] Loss: 1.96361 (semantic_loss: 0.01615, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18499 
Train Epoch: 38 [791/1000 25312/32000 (79%)] Loss: 1.96154 (semantic_loss: 0.01310, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19161 
Train Epoch: 38 [796/1000 25472/32000 (80%)] Loss: 1.96199 (semantic_loss: 0.01354, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18658 
Train Epoch: 38 [801/1000 25632/32000 (80%)] Loss: 1.95961 (semantic_loss: 0.01312, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.33813 
Train Epoch: 38 [806/1000 25792/32000 (81%)] Loss: 1.95995 (semantic_loss: 0.01346, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18607 
Train Epoch: 38 [811/1000 25952/32000 (81%)] Loss: 1.96078 (semantic_loss: 0.01332, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20635 
Train Epoch: 38 [816/1000 26112/32000 (82%)] Loss: 1.96144 (semantic_loss: 0.01397, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21135 
Train Epoch: 38 [821/1000 26272/32000 (82%)] Loss: 1.96536 (semantic_loss: 0.01887, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23090 
Train Epoch: 38 [826/1000 26432/32000 (83%)] Loss: 1.95966 (semantic_loss: 0.01219, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22629 
Train Epoch: 38 [831/1000 26592/32000 (83%)] Loss: 1.96259 (semantic_loss: 0.01512, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21700 
Train Epoch: 38 [836/1000 26752/32000 (84%)] Loss: 1.96106 (semantic_loss: 0.01359, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22314 
Train Epoch: 38 [841/1000 26912/32000 (84%)] Loss: 1.96047 (semantic_loss: 0.01300, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20511 
Train Epoch: 38 [846/1000 27072/32000 (85%)] Loss: 1.96404 (semantic_loss: 0.01560, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19382 
Train Epoch: 38 [851/1000 27232/32000 (85%)] Loss: 1.96346 (semantic_loss: 0.01598, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18548 
Train Epoch: 38 [856/1000 27392/32000 (86%)] Loss: 1.96329 (semantic_loss: 0.01485, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19408 
Train Epoch: 38 [861/1000 27552/32000 (86%)] Loss: 1.96193 (semantic_loss: 0.01349, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18905 
Train Epoch: 38 [866/1000 27712/32000 (87%)] Loss: 1.96179 (semantic_loss: 0.01530, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18787 
Train Epoch: 38 [871/1000 27872/32000 (87%)] Loss: 1.95996 (semantic_loss: 0.01249, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.65898 
Train Epoch: 38 [876/1000 28032/32000 (88%)] Loss: 1.96420 (semantic_loss: 0.01576, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20868 
Train Epoch: 38 [881/1000 28192/32000 (88%)] Loss: 1.96157 (semantic_loss: 0.01410, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19666 
Train Epoch: 38 [886/1000 28352/32000 (89%)] Loss: 1.96969 (semantic_loss: 0.02222, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19963 
Train Epoch: 38 [891/1000 28512/32000 (89%)] Loss: 1.95697 (semantic_loss: 0.01048, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18957 
Train Epoch: 38 [896/1000 28672/32000 (90%)] Loss: 1.96027 (semantic_loss: 0.01280, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21358 
Train Epoch: 38 [901/1000 28832/32000 (90%)] Loss: 1.96091 (semantic_loss: 0.01344, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23202 
Train Epoch: 38 [906/1000 28992/32000 (91%)] Loss: 1.96295 (semantic_loss: 0.01646, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21935 
Train Epoch: 38 [911/1000 29152/32000 (91%)] Loss: 1.96048 (semantic_loss: 0.01302, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19002 
Train Epoch: 38 [916/1000 29312/32000 (92%)] Loss: 1.96188 (semantic_loss: 0.01538, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18944 
Train Epoch: 38 [921/1000 29472/32000 (92%)] Loss: 1.96398 (semantic_loss: 0.01651, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18754 
Train Epoch: 38 [926/1000 29632/32000 (93%)] Loss: 1.96083 (semantic_loss: 0.01336, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18412 
Train Epoch: 38 [931/1000 29792/32000 (93%)] Loss: 1.96469 (semantic_loss: 0.01820, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18444 
Train Epoch: 38 [936/1000 29952/32000 (94%)] Loss: 1.95776 (semantic_loss: 0.01127, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18350 
Train Epoch: 38 [941/1000 30112/32000 (94%)] Loss: 1.96319 (semantic_loss: 0.01572, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20440 
Train Epoch: 38 [946/1000 30272/32000 (95%)] Loss: 1.96139 (semantic_loss: 0.01295, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18876 
Train Epoch: 38 [951/1000 30432/32000 (95%)] Loss: 1.95838 (semantic_loss: 0.01189, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18854 
Train Epoch: 38 [956/1000 30592/32000 (96%)] Loss: 1.96115 (semantic_loss: 0.01369, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18887 
Train Epoch: 38 [961/1000 30752/32000 (96%)] Loss: 1.95902 (semantic_loss: 0.01350, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18947 
Train Epoch: 38 [966/1000 30912/32000 (97%)] Loss: 1.95912 (semantic_loss: 0.01166, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21972 
Train Epoch: 38 [971/1000 31072/32000 (97%)] Loss: 1.96406 (semantic_loss: 0.01562, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22047 
Train Epoch: 38 [976/1000 31232/32000 (98%)] Loss: 1.96187 (semantic_loss: 0.01440, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.53024 
Train Epoch: 38 [981/1000 31392/32000 (98%)] Loss: 1.96099 (semantic_loss: 0.01450, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19088 
Train Epoch: 38 [986/1000 31552/32000 (99%)] Loss: 1.95786 (semantic_loss: 0.01138, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20830 
Train Epoch: 38 [991/1000 31712/32000 (99%)] Loss: 1.95887 (semantic_loss: 0.01238, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19260 
Train Epoch: 38 [996/1000 31872/32000 (100%)] Loss: 1.96455 (semantic_loss: 0.01611, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19632 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch38.pth ...
Done in 11.908s
removing stale ckpt [epoch 37] [took 0.00s]
 epoch          : 38
 loss           : 1.9617827310562135
 learning_rate  : 1.0137779795222638e-06
 n_samples      : 1216000
 n_steps        : 38000
 ActivityNet_val1_test/t2v_metrics/R1: 12.039861704291235
 ActivityNet_val1_test/t2v_metrics/R5: 38.03131991051454
 ActivityNet_val1_test/t2v_metrics/R10: 55.257270693512304
 ActivityNet_val1_test/t2v_metrics/R50: 84.74679682733374
 ActivityNet_val1_test/t2v_metrics/MedR: 8.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 62.67805572503559
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.357389998790154
 ActivityNet_val1_test/v2t_metrics/R1: 12.548301810046777
 ActivityNet_val1_test/v2t_metrics/R5: 39.31258897701851
 ActivityNet_val1_test/v2t_metrics/R10: 55.013219442749644
 ActivityNet_val1_test/v2t_metrics/R50: 84.66544641041286
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 64.37380516575148
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 30.051158621913
 mnt_best       : 29.542107379154057
 not_improved_count: 11
Train Epoch: 39 [1/1000 32/32000 (0%)] Loss: 1.96313 (semantic_loss: 0.01566, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=22.54058 
Train Epoch: 39 [6/1000 192/32000 (1%)] Loss: 1.96349 (semantic_loss: 0.01505, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18368 
Train Epoch: 39 [11/1000 352/32000 (1%)] Loss: 1.96177 (semantic_loss: 0.01431, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18311 
Train Epoch: 39 [16/1000 512/32000 (2%)] Loss: 1.96510 (semantic_loss: 0.01763, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.51395 
Train Epoch: 39 [21/1000 672/32000 (2%)] Loss: 1.95968 (semantic_loss: 0.01221, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18380 
Train Epoch: 39 [26/1000 832/32000 (3%)] Loss: 1.96198 (semantic_loss: 0.01451, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20624 
Train Epoch: 39 [31/1000 992/32000 (3%)] Loss: 1.96147 (semantic_loss: 0.01401, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18318 
Train Epoch: 39 [36/1000 1152/32000 (4%)] Loss: 1.96369 (semantic_loss: 0.01622, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18318 
Train Epoch: 39 [41/1000 1312/32000 (4%)] Loss: 1.96549 (semantic_loss: 0.01705, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18422 
Train Epoch: 39 [46/1000 1472/32000 (5%)] Loss: 1.96130 (semantic_loss: 0.01384, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20943 
Train Epoch: 39 [51/1000 1632/32000 (5%)] Loss: 1.96225 (semantic_loss: 0.01478, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18853 
Train Epoch: 39 [56/1000 1792/32000 (6%)] Loss: 1.95955 (semantic_loss: 0.01208, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18780 
Train Epoch: 39 [61/1000 1952/32000 (6%)] Loss: 1.96406 (semantic_loss: 0.01658, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18639 
Train Epoch: 39 [66/1000 2112/32000 (7%)] Loss: 1.96391 (semantic_loss: 0.01742, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.34316 
Train Epoch: 39 [71/1000 2272/32000 (7%)] Loss: 1.95860 (semantic_loss: 0.01114, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21917 
Train Epoch: 39 [76/1000 2432/32000 (8%)] Loss: 1.96251 (semantic_loss: 0.01504, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20292 
Train Epoch: 39 [81/1000 2592/32000 (8%)] Loss: 1.96045 (semantic_loss: 0.01299, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.65711 
Train Epoch: 39 [86/1000 2752/32000 (9%)] Loss: 1.96204 (semantic_loss: 0.01360, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21589 
Train Epoch: 39 [91/1000 2912/32000 (9%)] Loss: 1.96317 (semantic_loss: 0.01668, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19412 
Train Epoch: 39 [96/1000 3072/32000 (10%)] Loss: 1.96594 (semantic_loss: 0.01847, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20717 
Train Epoch: 39 [101/1000 3232/32000 (10%)] Loss: 1.96263 (semantic_loss: 0.01614, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20191 
Train Epoch: 39 [106/1000 3392/32000 (11%)] Loss: 1.96224 (semantic_loss: 0.01477, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20647 
Train Epoch: 39 [111/1000 3552/32000 (11%)] Loss: 1.95798 (semantic_loss: 0.01149, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19939 
Train Epoch: 39 [116/1000 3712/32000 (12%)] Loss: 1.95992 (semantic_loss: 0.01245, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19156 
Train Epoch: 39 [121/1000 3872/32000 (12%)] Loss: 1.96176 (semantic_loss: 0.01429, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18660 
Train Epoch: 39 [126/1000 4032/32000 (13%)] Loss: 1.96142 (semantic_loss: 0.01493, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19271 
Train Epoch: 39 [131/1000 4192/32000 (13%)] Loss: 1.96038 (semantic_loss: 0.01292, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20197 
Train Epoch: 39 [136/1000 4352/32000 (14%)] Loss: 1.96387 (semantic_loss: 0.01543, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18752 
Train Epoch: 39 [141/1000 4512/32000 (14%)] Loss: 1.96274 (semantic_loss: 0.01430, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18717 
Train Epoch: 39 [146/1000 4672/32000 (15%)] Loss: 1.96416 (semantic_loss: 0.01668, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19355 
Train Epoch: 39 [151/1000 4832/32000 (15%)] Loss: 1.96246 (semantic_loss: 0.01597, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19301 
Train Epoch: 39 [156/1000 4992/32000 (16%)] Loss: 1.96514 (semantic_loss: 0.01865, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19121 
Train Epoch: 39 [161/1000 5152/32000 (16%)] Loss: 1.96002 (semantic_loss: 0.01256, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18571 
Train Epoch: 39 [166/1000 5312/32000 (17%)] Loss: 1.96060 (semantic_loss: 0.01411, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.25078 
Train Epoch: 39 [171/1000 5472/32000 (17%)] Loss: 1.95831 (semantic_loss: 0.01085, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18756 
Train Epoch: 39 [176/1000 5632/32000 (18%)] Loss: 1.95861 (semantic_loss: 0.01211, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18605 
Train Epoch: 39 [181/1000 5792/32000 (18%)] Loss: 1.96087 (semantic_loss: 0.01437, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18328 
Train Epoch: 39 [186/1000 5952/32000 (19%)] Loss: 1.96267 (semantic_loss: 0.01617, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18487 
Train Epoch: 39 [191/1000 6112/32000 (19%)] Loss: 1.96504 (semantic_loss: 0.01854, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18692 
Train Epoch: 39 [196/1000 6272/32000 (20%)] Loss: 1.95837 (semantic_loss: 0.01188, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18901 
Train Epoch: 39 [201/1000 6432/32000 (20%)] Loss: 1.96473 (semantic_loss: 0.01628, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18450 
Train Epoch: 39 [206/1000 6592/32000 (21%)] Loss: 1.96089 (semantic_loss: 0.01245, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18651 
Train Epoch: 39 [211/1000 6752/32000 (21%)] Loss: 1.95933 (semantic_loss: 0.01187, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19081 
Train Epoch: 39 [216/1000 6912/32000 (22%)] Loss: 1.95996 (semantic_loss: 0.01152, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19109 
Train Epoch: 39 [221/1000 7072/32000 (22%)] Loss: 1.95907 (semantic_loss: 0.01160, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.26586 
Train Epoch: 39 [226/1000 7232/32000 (23%)] Loss: 1.96054 (semantic_loss: 0.01405, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19493 
Train Epoch: 39 [231/1000 7392/32000 (23%)] Loss: 1.96104 (semantic_loss: 0.01357, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20352 
Train Epoch: 39 [236/1000 7552/32000 (24%)] Loss: 1.96119 (semantic_loss: 0.01470, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21633 
Train Epoch: 39 [241/1000 7712/32000 (24%)] Loss: 1.96330 (semantic_loss: 0.01583, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22148 
Train Epoch: 39 [246/1000 7872/32000 (25%)] Loss: 1.96012 (semantic_loss: 0.01265, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22447 
Train Epoch: 39 [251/1000 8032/32000 (25%)] Loss: 1.96259 (semantic_loss: 0.01414, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19177 
Train Epoch: 39 [256/1000 8192/32000 (26%)] Loss: 1.95880 (semantic_loss: 0.01133, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20324 
Train Epoch: 39 [261/1000 8352/32000 (26%)] Loss: 1.96074 (semantic_loss: 0.01327, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21546 
Train Epoch: 39 [266/1000 8512/32000 (27%)] Loss: 1.96725 (semantic_loss: 0.01881, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21500 
Train Epoch: 39 [271/1000 8672/32000 (27%)] Loss: 1.96797 (semantic_loss: 0.01953, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21702 
Train Epoch: 39 [276/1000 8832/32000 (28%)] Loss: 1.96076 (semantic_loss: 0.01330, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18789 
Train Epoch: 39 [281/1000 8992/32000 (28%)] Loss: 1.96246 (semantic_loss: 0.01402, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18889 
Train Epoch: 39 [286/1000 9152/32000 (29%)] Loss: 1.95916 (semantic_loss: 0.01365, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.37957 
Train Epoch: 39 [291/1000 9312/32000 (29%)] Loss: 1.96033 (semantic_loss: 0.01287, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18660 
Train Epoch: 39 [296/1000 9472/32000 (30%)] Loss: 1.96070 (semantic_loss: 0.01226, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18477 
Train Epoch: 39 [301/1000 9632/32000 (30%)] Loss: 1.95917 (semantic_loss: 0.01170, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19603 
Train Epoch: 39 [306/1000 9792/32000 (31%)] Loss: 1.96232 (semantic_loss: 0.01485, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21122 
Train Epoch: 39 [311/1000 9952/32000 (31%)] Loss: 1.95893 (semantic_loss: 0.01341, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18848 
Train Epoch: 39 [316/1000 10112/32000 (32%)] Loss: 1.96086 (semantic_loss: 0.01437, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18483 
Train Epoch: 39 [321/1000 10272/32000 (32%)] Loss: 1.96756 (semantic_loss: 0.02009, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19275 
Train Epoch: 39 [326/1000 10432/32000 (33%)] Loss: 1.96136 (semantic_loss: 0.01291, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18915 
Train Epoch: 39 [331/1000 10592/32000 (33%)] Loss: 1.96099 (semantic_loss: 0.01352, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20394 
Train Epoch: 39 [336/1000 10752/32000 (34%)] Loss: 1.96094 (semantic_loss: 0.01347, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.54334 
Train Epoch: 39 [341/1000 10912/32000 (34%)] Loss: 1.96152 (semantic_loss: 0.01405, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19041 
Train Epoch: 39 [346/1000 11072/32000 (35%)] Loss: 1.96332 (semantic_loss: 0.01487, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18675 
Train Epoch: 39 [351/1000 11232/32000 (35%)] Loss: 1.96033 (semantic_loss: 0.01287, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18524 
Train Epoch: 39 [356/1000 11392/32000 (36%)] Loss: 1.96403 (semantic_loss: 0.01656, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18464 
Train Epoch: 39 [361/1000 11552/32000 (36%)] Loss: 1.96089 (semantic_loss: 0.01343, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18602 
Train Epoch: 39 [366/1000 11712/32000 (37%)] Loss: 1.96210 (semantic_loss: 0.01463, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18471 
Train Epoch: 39 [371/1000 11872/32000 (37%)] Loss: 1.95902 (semantic_loss: 0.01155, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18894 
Train Epoch: 39 [376/1000 12032/32000 (38%)] Loss: 1.96282 (semantic_loss: 0.01633, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18684 
Train Epoch: 39 [381/1000 12192/32000 (38%)] Loss: 1.96276 (semantic_loss: 0.01529, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18992 
Train Epoch: 39 [386/1000 12352/32000 (39%)] Loss: 1.96599 (semantic_loss: 0.01853, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.33169 
Train Epoch: 39 [391/1000 12512/32000 (39%)] Loss: 1.96027 (semantic_loss: 0.01378, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23889 
Train Epoch: 39 [396/1000 12672/32000 (40%)] Loss: 1.96043 (semantic_loss: 0.01296, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21792 
Train Epoch: 39 [401/1000 12832/32000 (40%)] Loss: 1.96537 (semantic_loss: 0.01692, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.66496 
Train Epoch: 39 [406/1000 12992/32000 (41%)] Loss: 1.96155 (semantic_loss: 0.01409, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20595 
Train Epoch: 39 [411/1000 13152/32000 (41%)] Loss: 1.96500 (semantic_loss: 0.01656, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20662 
Train Epoch: 39 [416/1000 13312/32000 (42%)] Loss: 1.96241 (semantic_loss: 0.01591, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19451 
Train Epoch: 39 [421/1000 13472/32000 (42%)] Loss: 1.96293 (semantic_loss: 0.01547, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19793 
Train Epoch: 39 [426/1000 13632/32000 (43%)] Loss: 1.96628 (semantic_loss: 0.01881, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18826 
Train Epoch: 39 [431/1000 13792/32000 (43%)] Loss: 1.95917 (semantic_loss: 0.01171, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21029 
Train Epoch: 39 [436/1000 13952/32000 (44%)] Loss: 1.96513 (semantic_loss: 0.01767, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18869 
Train Epoch: 39 [441/1000 14112/32000 (44%)] Loss: 1.96137 (semantic_loss: 0.01391, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19038 
Train Epoch: 39 [446/1000 14272/32000 (45%)] Loss: 1.96080 (semantic_loss: 0.01431, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19173 
Train Epoch: 39 [451/1000 14432/32000 (45%)] Loss: 1.96074 (semantic_loss: 0.01230, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18871 
Train Epoch: 39 [456/1000 14592/32000 (46%)] Loss: 1.95914 (semantic_loss: 0.01168, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19583 
Train Epoch: 39 [461/1000 14752/32000 (46%)] Loss: 1.96855 (semantic_loss: 0.02011, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19190 
Train Epoch: 39 [466/1000 14912/32000 (47%)] Loss: 1.96195 (semantic_loss: 0.01448, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19920 
Train Epoch: 39 [471/1000 15072/32000 (47%)] Loss: 1.96212 (semantic_loss: 0.01465, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18735 
Train Epoch: 39 [476/1000 15232/32000 (48%)] Loss: 1.96005 (semantic_loss: 0.01258, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18730 
Train Epoch: 39 [481/1000 15392/32000 (48%)] Loss: 1.96189 (semantic_loss: 0.01344, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18305 
Train Epoch: 39 [486/1000 15552/32000 (49%)] Loss: 1.96378 (semantic_loss: 0.01631, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.25262 
Train Epoch: 39 [491/1000 15712/32000 (49%)] Loss: 1.95918 (semantic_loss: 0.01172, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19114 
Train Epoch: 39 [496/1000 15872/32000 (50%)] Loss: 1.96582 (semantic_loss: 0.01835, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18485 
Train Epoch: 39 [501/1000 16032/32000 (50%)] Loss: 1.96285 (semantic_loss: 0.01538, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18820 
Train Epoch: 39 [506/1000 16192/32000 (51%)] Loss: 1.96709 (semantic_loss: 0.01961, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18782 
Train Epoch: 39 [511/1000 16352/32000 (51%)] Loss: 1.96535 (semantic_loss: 0.01691, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18657 
Train Epoch: 39 [516/1000 16512/32000 (52%)] Loss: 1.95815 (semantic_loss: 0.01263, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18941 
Train Epoch: 39 [521/1000 16672/32000 (52%)] Loss: 1.96128 (semantic_loss: 0.01382, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18896 
Train Epoch: 39 [526/1000 16832/32000 (53%)] Loss: 1.96070 (semantic_loss: 0.01420, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18621 
Train Epoch: 39 [531/1000 16992/32000 (53%)] Loss: 1.96557 (semantic_loss: 0.01907, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18525 
Train Epoch: 39 [536/1000 17152/32000 (54%)] Loss: 1.96024 (semantic_loss: 0.01278, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20638 
Train Epoch: 39 [541/1000 17312/32000 (54%)] Loss: 1.96589 (semantic_loss: 0.01843, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.32350 
Train Epoch: 39 [546/1000 17472/32000 (55%)] Loss: 1.95864 (semantic_loss: 0.01313, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.21097 
Train Epoch: 39 [551/1000 17632/32000 (55%)] Loss: 1.96165 (semantic_loss: 0.01418, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18946 
Train Epoch: 39 [556/1000 17792/32000 (56%)] Loss: 1.96499 (semantic_loss: 0.01654, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19169 
Train Epoch: 39 [561/1000 17952/32000 (56%)] Loss: 1.96252 (semantic_loss: 0.01408, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18986 
Train Epoch: 39 [566/1000 18112/32000 (57%)] Loss: 1.96300 (semantic_loss: 0.01651, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19603 
Train Epoch: 39 [571/1000 18272/32000 (57%)] Loss: 1.96066 (semantic_loss: 0.01319, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19271 
Train Epoch: 39 [576/1000 18432/32000 (58%)] Loss: 1.96337 (semantic_loss: 0.01688, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21677 
Train Epoch: 39 [581/1000 18592/32000 (58%)] Loss: 1.95920 (semantic_loss: 0.01173, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20426 
Train Epoch: 39 [586/1000 18752/32000 (59%)] Loss: 1.95881 (semantic_loss: 0.01232, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19261 
Train Epoch: 39 [591/1000 18912/32000 (59%)] Loss: 1.95929 (semantic_loss: 0.01280, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18439 
Train Epoch: 39 [596/1000 19072/32000 (60%)] Loss: 1.95830 (semantic_loss: 0.01181, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18707 
Train Epoch: 39 [601/1000 19232/32000 (60%)] Loss: 1.96071 (semantic_loss: 0.01325, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19696 
Train Epoch: 39 [606/1000 19392/32000 (61%)] Loss: 1.96528 (semantic_loss: 0.01781, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.41656 
Train Epoch: 39 [611/1000 19552/32000 (61%)] Loss: 1.95944 (semantic_loss: 0.01295, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20609 
Train Epoch: 39 [616/1000 19712/32000 (62%)] Loss: 1.96317 (semantic_loss: 0.01668, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19194 
Train Epoch: 39 [621/1000 19872/32000 (62%)] Loss: 1.95992 (semantic_loss: 0.01148, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20344 
Train Epoch: 39 [626/1000 20032/32000 (63%)] Loss: 1.96198 (semantic_loss: 0.01451, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18799 
Train Epoch: 39 [631/1000 20192/32000 (63%)] Loss: 1.96268 (semantic_loss: 0.01521, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18675 
Train Epoch: 39 [636/1000 20352/32000 (64%)] Loss: 1.96308 (semantic_loss: 0.01561, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18988 
Train Epoch: 39 [641/1000 20512/32000 (64%)] Loss: 1.95986 (semantic_loss: 0.01239, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18215 
Train Epoch: 39 [646/1000 20672/32000 (65%)] Loss: 1.95986 (semantic_loss: 0.01141, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18702 
Train Epoch: 39 [651/1000 20832/32000 (65%)] Loss: 1.96205 (semantic_loss: 0.01458, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18405 
Train Epoch: 39 [656/1000 20992/32000 (66%)] Loss: 1.96253 (semantic_loss: 0.01506, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.53678 
Train Epoch: 39 [661/1000 21152/32000 (66%)] Loss: 1.96040 (semantic_loss: 0.01294, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18877 
Train Epoch: 39 [666/1000 21312/32000 (67%)] Loss: 1.96094 (semantic_loss: 0.01445, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18577 
Train Epoch: 39 [671/1000 21472/32000 (67%)] Loss: 1.96336 (semantic_loss: 0.01590, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18686 
Train Epoch: 39 [676/1000 21632/32000 (68%)] Loss: 1.96162 (semantic_loss: 0.01513, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19011 
Train Epoch: 39 [681/1000 21792/32000 (68%)] Loss: 1.96035 (semantic_loss: 0.01385, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19178 
Train Epoch: 39 [686/1000 21952/32000 (69%)] Loss: 1.96048 (semantic_loss: 0.01301, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19732 
Train Epoch: 39 [691/1000 22112/32000 (69%)] Loss: 1.95887 (semantic_loss: 0.01238, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18850 
Train Epoch: 39 [696/1000 22272/32000 (70%)] Loss: 1.95971 (semantic_loss: 0.01224, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18593 
Train Epoch: 39 [701/1000 22432/32000 (70%)] Loss: 1.96581 (semantic_loss: 0.01736, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20544 
Train Epoch: 39 [706/1000 22592/32000 (71%)] Loss: 1.95896 (semantic_loss: 0.01247, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.40616 
Train Epoch: 39 [711/1000 22752/32000 (71%)] Loss: 1.96075 (semantic_loss: 0.01329, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24760 
Train Epoch: 39 [716/1000 22912/32000 (72%)] Loss: 1.96331 (semantic_loss: 0.01584, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19697 
Train Epoch: 39 [721/1000 23072/32000 (72%)] Loss: 1.96198 (semantic_loss: 0.01548, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.63109 
Train Epoch: 39 [726/1000 23232/32000 (73%)] Loss: 1.96023 (semantic_loss: 0.01179, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21251 
Train Epoch: 39 [731/1000 23392/32000 (73%)] Loss: 1.96472 (semantic_loss: 0.01822, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21844 
Train Epoch: 39 [736/1000 23552/32000 (74%)] Loss: 1.95928 (semantic_loss: 0.01279, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18982 
Train Epoch: 39 [741/1000 23712/32000 (74%)] Loss: 1.95913 (semantic_loss: 0.01166, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18365 
Train Epoch: 39 [746/1000 23872/32000 (75%)] Loss: 1.96164 (semantic_loss: 0.01418, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18504 
Train Epoch: 39 [751/1000 24032/32000 (75%)] Loss: 1.96074 (semantic_loss: 0.01328, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20830 
Train Epoch: 39 [756/1000 24192/32000 (76%)] Loss: 1.96123 (semantic_loss: 0.01474, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18863 
Train Epoch: 39 [761/1000 24352/32000 (76%)] Loss: 1.95934 (semantic_loss: 0.01187, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18858 
Train Epoch: 39 [766/1000 24512/32000 (77%)] Loss: 1.95915 (semantic_loss: 0.01168, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19957 
Train Epoch: 39 [771/1000 24672/32000 (77%)] Loss: 1.95945 (semantic_loss: 0.01199, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18905 
Train Epoch: 39 [776/1000 24832/32000 (78%)] Loss: 1.96014 (semantic_loss: 0.01267, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20527 
Train Epoch: 39 [781/1000 24992/32000 (78%)] Loss: 1.96520 (semantic_loss: 0.01773, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18939 
Train Epoch: 39 [786/1000 25152/32000 (79%)] Loss: 1.96014 (semantic_loss: 0.01267, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21960 
Train Epoch: 39 [791/1000 25312/32000 (79%)] Loss: 1.96009 (semantic_loss: 0.01262, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18604 
Train Epoch: 39 [796/1000 25472/32000 (80%)] Loss: 1.96421 (semantic_loss: 0.01577, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18572 
Train Epoch: 39 [801/1000 25632/32000 (80%)] Loss: 1.95934 (semantic_loss: 0.01285, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19704 
Train Epoch: 39 [806/1000 25792/32000 (81%)] Loss: 1.95941 (semantic_loss: 0.01292, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.25236 
Train Epoch: 39 [811/1000 25952/32000 (81%)] Loss: 1.96337 (semantic_loss: 0.01590, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18322 
Train Epoch: 39 [816/1000 26112/32000 (82%)] Loss: 1.96473 (semantic_loss: 0.01726, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18522 
Train Epoch: 39 [821/1000 26272/32000 (82%)] Loss: 1.95902 (semantic_loss: 0.01155, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18882 
Train Epoch: 39 [826/1000 26432/32000 (83%)] Loss: 1.96055 (semantic_loss: 0.01309, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18483 
Train Epoch: 39 [831/1000 26592/32000 (83%)] Loss: 1.96134 (semantic_loss: 0.01387, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18898 
Train Epoch: 39 [836/1000 26752/32000 (84%)] Loss: 1.96208 (semantic_loss: 0.01461, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18677 
Train Epoch: 39 [841/1000 26912/32000 (84%)] Loss: 1.96714 (semantic_loss: 0.01967, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19285 
Train Epoch: 39 [846/1000 27072/32000 (85%)] Loss: 1.96190 (semantic_loss: 0.01443, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18686 
Train Epoch: 39 [851/1000 27232/32000 (85%)] Loss: 1.96137 (semantic_loss: 0.01390, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20195 
Train Epoch: 39 [856/1000 27392/32000 (86%)] Loss: 1.96032 (semantic_loss: 0.01285, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22229 
Train Epoch: 39 [861/1000 27552/32000 (86%)] Loss: 1.96182 (semantic_loss: 0.01435, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.33153 
Train Epoch: 39 [866/1000 27712/32000 (87%)] Loss: 1.96444 (semantic_loss: 0.01697, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19798 
Train Epoch: 39 [871/1000 27872/32000 (87%)] Loss: 1.96208 (semantic_loss: 0.01462, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20954 
Train Epoch: 39 [876/1000 28032/32000 (88%)] Loss: 1.96208 (semantic_loss: 0.01462, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19397 
Train Epoch: 39 [881/1000 28192/32000 (88%)] Loss: 1.96215 (semantic_loss: 0.01370, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22326 
Train Epoch: 39 [886/1000 28352/32000 (89%)] Loss: 1.96684 (semantic_loss: 0.01937, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21105 
Train Epoch: 39 [891/1000 28512/32000 (89%)] Loss: 1.96509 (semantic_loss: 0.01762, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18992 
Train Epoch: 39 [896/1000 28672/32000 (90%)] Loss: 1.96337 (semantic_loss: 0.01591, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19454 
Train Epoch: 39 [901/1000 28832/32000 (90%)] Loss: 1.96406 (semantic_loss: 0.01659, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20824 
Train Epoch: 39 [906/1000 28992/32000 (91%)] Loss: 1.96497 (semantic_loss: 0.01750, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19564 
Train Epoch: 39 [911/1000 29152/32000 (91%)] Loss: 1.96192 (semantic_loss: 0.01446, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18591 
Train Epoch: 39 [916/1000 29312/32000 (92%)] Loss: 1.96301 (semantic_loss: 0.01457, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19338 
Train Epoch: 39 [921/1000 29472/32000 (92%)] Loss: 1.96411 (semantic_loss: 0.01664, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20260 
Train Epoch: 39 [926/1000 29632/32000 (93%)] Loss: 1.96297 (semantic_loss: 0.01551, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.40249 
Train Epoch: 39 [931/1000 29792/32000 (93%)] Loss: 1.96321 (semantic_loss: 0.01574, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18813 
Train Epoch: 39 [936/1000 29952/32000 (94%)] Loss: 1.96208 (semantic_loss: 0.01363, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18733 
Train Epoch: 39 [941/1000 30112/32000 (94%)] Loss: 1.96134 (semantic_loss: 0.01387, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18488 
Train Epoch: 39 [946/1000 30272/32000 (95%)] Loss: 1.96435 (semantic_loss: 0.01688, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18580 
Train Epoch: 39 [951/1000 30432/32000 (95%)] Loss: 1.96036 (semantic_loss: 0.01289, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18516 
Train Epoch: 39 [956/1000 30592/32000 (96%)] Loss: 1.96282 (semantic_loss: 0.01438, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18533 
Train Epoch: 39 [961/1000 30752/32000 (96%)] Loss: 1.96374 (semantic_loss: 0.01432, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18760 
Train Epoch: 39 [966/1000 30912/32000 (97%)] Loss: 1.95691 (semantic_loss: 0.01140, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18818 
Train Epoch: 39 [971/1000 31072/32000 (97%)] Loss: 1.95993 (semantic_loss: 0.01246, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18799 
Train Epoch: 39 [976/1000 31232/32000 (98%)] Loss: 1.96110 (semantic_loss: 0.01461, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.53260 
Train Epoch: 39 [981/1000 31392/32000 (98%)] Loss: 1.96262 (semantic_loss: 0.01516, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20265 
Train Epoch: 39 [986/1000 31552/32000 (99%)] Loss: 1.95954 (semantic_loss: 0.01305, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18448 
Train Epoch: 39 [991/1000 31712/32000 (99%)] Loss: 1.96338 (semantic_loss: 0.01494, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18566 
Train Epoch: 39 [996/1000 31872/32000 (100%)] Loss: 1.96319 (semantic_loss: 0.01474, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18973 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch39.pth ...
Done in 4.363s
removing stale ckpt [epoch 38] [took 0.00s]
 epoch          : 39
 loss           : 1.961737043261528
 learning_rate  : 9.124001815700375e-07
 n_samples      : 1248000
 n_steps        : 39000
 ActivityNet_val1_test/t2v_metrics/R1: 11.551759202765915
 ActivityNet_val1_test/t2v_metrics/R5: 38.417734390888754
 ActivityNet_val1_test/t2v_metrics/R10: 55.257270693512304
 ActivityNet_val1_test/t2v_metrics/R50: 84.72645922310352
 ActivityNet_val1_test/t2v_metrics/MedR: 8.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 62.820622330689446
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.052918717649046
 ActivityNet_val1_test/v2t_metrics/R1: 12.121212121212121
 ActivityNet_val1_test/v2t_metrics/R5: 39.39393939393939
 ActivityNet_val1_test/v2t_metrics/R10: 55.582672361195854
 ActivityNet_val1_test/v2t_metrics/R50: 84.74679682733374
 ActivityNet_val1_test/v2t_metrics/MedR: 8.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 64.66981899532234
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.828972638176392
 mnt_best       : 29.542107379154057
 not_improved_count: 12
Train Epoch: 40 [1/1000 32/32000 (0%)] Loss: 1.95946 (semantic_loss: 0.01200, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=22.92582 
Train Epoch: 40 [6/1000 192/32000 (1%)] Loss: 1.96089 (semantic_loss: 0.01342, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21730 
Train Epoch: 40 [11/1000 352/32000 (1%)] Loss: 1.95949 (semantic_loss: 0.01203, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19818 
Train Epoch: 40 [16/1000 512/32000 (2%)] Loss: 1.96030 (semantic_loss: 0.01381, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23905 
Train Epoch: 40 [21/1000 672/32000 (2%)] Loss: 1.96055 (semantic_loss: 0.01308, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.31657 
Train Epoch: 40 [26/1000 832/32000 (3%)] Loss: 1.95922 (semantic_loss: 0.01273, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18629 
Train Epoch: 40 [31/1000 992/32000 (3%)] Loss: 1.96331 (semantic_loss: 0.01584, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18720 
Train Epoch: 40 [36/1000 1152/32000 (4%)] Loss: 1.96089 (semantic_loss: 0.01343, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.41364 
Train Epoch: 40 [41/1000 1312/32000 (4%)] Loss: 1.96002 (semantic_loss: 0.01255, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20285 
Train Epoch: 40 [46/1000 1472/32000 (5%)] Loss: 1.96260 (semantic_loss: 0.01513, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20286 
Train Epoch: 40 [51/1000 1632/32000 (5%)] Loss: 1.96422 (semantic_loss: 0.01675, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27350 
Train Epoch: 40 [56/1000 1792/32000 (6%)] Loss: 1.96135 (semantic_loss: 0.01485, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18174 
Train Epoch: 40 [61/1000 1952/32000 (6%)] Loss: 1.96402 (semantic_loss: 0.01655, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18618 
Train Epoch: 40 [66/1000 2112/32000 (7%)] Loss: 1.96521 (semantic_loss: 0.01676, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20236 
Train Epoch: 40 [71/1000 2272/32000 (7%)] Loss: 1.96123 (semantic_loss: 0.01376, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22058 
Train Epoch: 40 [76/1000 2432/32000 (8%)] Loss: 1.96305 (semantic_loss: 0.01460, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18750 
Train Epoch: 40 [81/1000 2592/32000 (8%)] Loss: 1.95954 (semantic_loss: 0.01304, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23661 
Train Epoch: 40 [86/1000 2752/32000 (9%)] Loss: 1.96571 (semantic_loss: 0.01922, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18591 
Train Epoch: 40 [91/1000 2912/32000 (9%)] Loss: 1.96138 (semantic_loss: 0.01489, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.40562 
Train Epoch: 40 [96/1000 3072/32000 (10%)] Loss: 1.95968 (semantic_loss: 0.01319, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19377 
Train Epoch: 40 [101/1000 3232/32000 (10%)] Loss: 1.96211 (semantic_loss: 0.01562, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18740 
Train Epoch: 40 [106/1000 3392/32000 (11%)] Loss: 1.96080 (semantic_loss: 0.01333, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19057 
Train Epoch: 40 [111/1000 3552/32000 (11%)] Loss: 1.96136 (semantic_loss: 0.01292, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18592 
Train Epoch: 40 [116/1000 3712/32000 (12%)] Loss: 1.96224 (semantic_loss: 0.01576, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18513 
Train Epoch: 40 [121/1000 3872/32000 (12%)] Loss: 1.96309 (semantic_loss: 0.01660, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18692 
Train Epoch: 40 [126/1000 4032/32000 (13%)] Loss: 1.96371 (semantic_loss: 0.01625, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18468 
Train Epoch: 40 [131/1000 4192/32000 (13%)] Loss: 1.96393 (semantic_loss: 0.01647, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23670 
Train Epoch: 40 [136/1000 4352/32000 (14%)] Loss: 1.96550 (semantic_loss: 0.01608, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18911 
Train Epoch: 40 [141/1000 4512/32000 (14%)] Loss: 1.96055 (semantic_loss: 0.01308, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20391 
Train Epoch: 40 [146/1000 4672/32000 (15%)] Loss: 1.96234 (semantic_loss: 0.01390, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20495 
Train Epoch: 40 [151/1000 4832/32000 (15%)] Loss: 1.95963 (semantic_loss: 0.01314, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20928 
Train Epoch: 40 [156/1000 4992/32000 (16%)] Loss: 1.96226 (semantic_loss: 0.01381, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22286 
Train Epoch: 40 [161/1000 5152/32000 (16%)] Loss: 1.95904 (semantic_loss: 0.01255, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21548 
Train Epoch: 40 [166/1000 5312/32000 (17%)] Loss: 1.96157 (semantic_loss: 0.01410, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19733 
Train Epoch: 40 [171/1000 5472/32000 (17%)] Loss: 1.96070 (semantic_loss: 0.01420, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19971 
Train Epoch: 40 [176/1000 5632/32000 (18%)] Loss: 1.96320 (semantic_loss: 0.01671, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21524 
Train Epoch: 40 [181/1000 5792/32000 (18%)] Loss: 1.96066 (semantic_loss: 0.01417, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20900 
Train Epoch: 40 [186/1000 5952/32000 (19%)] Loss: 1.96155 (semantic_loss: 0.01409, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18907 
Train Epoch: 40 [191/1000 6112/32000 (19%)] Loss: 1.95890 (semantic_loss: 0.01241, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20220 
Train Epoch: 40 [196/1000 6272/32000 (20%)] Loss: 1.96247 (semantic_loss: 0.01500, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18644 
Train Epoch: 40 [201/1000 6432/32000 (20%)] Loss: 1.95982 (semantic_loss: 0.01235, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18575 
Train Epoch: 40 [206/1000 6592/32000 (21%)] Loss: 1.96208 (semantic_loss: 0.01461, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18824 
Train Epoch: 40 [211/1000 6752/32000 (21%)] Loss: 1.96232 (semantic_loss: 0.01486, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18867 
Train Epoch: 40 [216/1000 6912/32000 (22%)] Loss: 1.95912 (semantic_loss: 0.01263, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20297 
Train Epoch: 40 [221/1000 7072/32000 (22%)] Loss: 1.95980 (semantic_loss: 0.01233, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19127 
Train Epoch: 40 [226/1000 7232/32000 (23%)] Loss: 1.95954 (semantic_loss: 0.01305, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19125 
Train Epoch: 40 [231/1000 7392/32000 (23%)] Loss: 1.96109 (semantic_loss: 0.01265, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.33935 
Train Epoch: 40 [236/1000 7552/32000 (24%)] Loss: 1.96096 (semantic_loss: 0.01349, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18823 
Train Epoch: 40 [241/1000 7712/32000 (24%)] Loss: 1.96269 (semantic_loss: 0.01522, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18603 
Train Epoch: 40 [246/1000 7872/32000 (25%)] Loss: 1.96483 (semantic_loss: 0.01639, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18621 
Train Epoch: 40 [251/1000 8032/32000 (25%)] Loss: 1.95995 (semantic_loss: 0.01248, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18465 
Train Epoch: 40 [256/1000 8192/32000 (26%)] Loss: 1.96207 (semantic_loss: 0.01461, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24333 
Train Epoch: 40 [261/1000 8352/32000 (26%)] Loss: 1.95964 (semantic_loss: 0.01217, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.28920 
Train Epoch: 40 [266/1000 8512/32000 (27%)] Loss: 1.95809 (semantic_loss: 0.01160, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18631 
Train Epoch: 40 [271/1000 8672/32000 (27%)] Loss: 1.96005 (semantic_loss: 0.01258, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18626 
Train Epoch: 40 [276/1000 8832/32000 (28%)] Loss: 1.96189 (semantic_loss: 0.01442, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18787 
Train Epoch: 40 [281/1000 8992/32000 (28%)] Loss: 1.96119 (semantic_loss: 0.01372, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19211 
Train Epoch: 40 [286/1000 9152/32000 (29%)] Loss: 1.96512 (semantic_loss: 0.01765, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18940 
Train Epoch: 40 [291/1000 9312/32000 (29%)] Loss: 1.96363 (semantic_loss: 0.01616, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18782 
Train Epoch: 40 [296/1000 9472/32000 (30%)] Loss: 1.95994 (semantic_loss: 0.01248, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21628 
Train Epoch: 40 [301/1000 9632/32000 (30%)] Loss: 1.96425 (semantic_loss: 0.01776, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21810 
Train Epoch: 40 [306/1000 9792/32000 (31%)] Loss: 1.96120 (semantic_loss: 0.01471, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21275 
Train Epoch: 40 [311/1000 9952/32000 (31%)] Loss: 1.96408 (semantic_loss: 0.01661, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23241 
Train Epoch: 40 [316/1000 10112/32000 (32%)] Loss: 1.96216 (semantic_loss: 0.01470, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19928 
Train Epoch: 40 [321/1000 10272/32000 (32%)] Loss: 1.96277 (semantic_loss: 0.01335, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.19946 
Train Epoch: 40 [326/1000 10432/32000 (33%)] Loss: 1.95964 (semantic_loss: 0.01217, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21821 
Train Epoch: 40 [331/1000 10592/32000 (33%)] Loss: 1.96229 (semantic_loss: 0.01579, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19503 
Train Epoch: 40 [336/1000 10752/32000 (34%)] Loss: 1.96020 (semantic_loss: 0.01371, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24690 
Train Epoch: 40 [341/1000 10912/32000 (34%)] Loss: 1.95963 (semantic_loss: 0.01315, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.30624 
Train Epoch: 40 [346/1000 11072/32000 (35%)] Loss: 1.96226 (semantic_loss: 0.01674, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18813 
Train Epoch: 40 [351/1000 11232/32000 (35%)] Loss: 1.96240 (semantic_loss: 0.01493, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18911 
Train Epoch: 40 [356/1000 11392/32000 (36%)] Loss: 1.96043 (semantic_loss: 0.01394, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.39136 
Train Epoch: 40 [361/1000 11552/32000 (36%)] Loss: 1.96228 (semantic_loss: 0.01385, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18447 
Train Epoch: 40 [366/1000 11712/32000 (37%)] Loss: 1.96445 (semantic_loss: 0.01698, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19586 
Train Epoch: 40 [371/1000 11872/32000 (37%)] Loss: 1.96211 (semantic_loss: 0.01464, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.26791 
Train Epoch: 40 [376/1000 12032/32000 (38%)] Loss: 1.95974 (semantic_loss: 0.01325, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19143 
Train Epoch: 40 [381/1000 12192/32000 (38%)] Loss: 1.95904 (semantic_loss: 0.01256, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19093 
Train Epoch: 40 [386/1000 12352/32000 (39%)] Loss: 1.95999 (semantic_loss: 0.01350, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18429 
Train Epoch: 40 [391/1000 12512/32000 (39%)] Loss: 1.96168 (semantic_loss: 0.01324, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22308 
Train Epoch: 40 [396/1000 12672/32000 (40%)] Loss: 1.96372 (semantic_loss: 0.01722, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18778 
Train Epoch: 40 [401/1000 12832/32000 (40%)] Loss: 1.96313 (semantic_loss: 0.01566, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23187 
Train Epoch: 40 [406/1000 12992/32000 (41%)] Loss: 1.96044 (semantic_loss: 0.01395, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18719 
Train Epoch: 40 [411/1000 13152/32000 (41%)] Loss: 1.95950 (semantic_loss: 0.01203, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.41203 
Train Epoch: 40 [416/1000 13312/32000 (42%)] Loss: 1.96100 (semantic_loss: 0.01255, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18457 
Train Epoch: 40 [421/1000 13472/32000 (42%)] Loss: 1.96171 (semantic_loss: 0.01425, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18749 
Train Epoch: 40 [426/1000 13632/32000 (43%)] Loss: 1.96551 (semantic_loss: 0.01609, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.20567 
Train Epoch: 40 [431/1000 13792/32000 (43%)] Loss: 1.96055 (semantic_loss: 0.01406, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20984 
Train Epoch: 40 [436/1000 13952/32000 (44%)] Loss: 1.96367 (semantic_loss: 0.01620, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18826 
Train Epoch: 40 [441/1000 14112/32000 (44%)] Loss: 1.96379 (semantic_loss: 0.01632, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18915 
Train Epoch: 40 [446/1000 14272/32000 (45%)] Loss: 1.95999 (semantic_loss: 0.01350, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18621 
Train Epoch: 40 [451/1000 14432/32000 (45%)] Loss: 1.95946 (semantic_loss: 0.01297, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24826 
Train Epoch: 40 [456/1000 14592/32000 (46%)] Loss: 1.96261 (semantic_loss: 0.01514, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20693 
Train Epoch: 40 [461/1000 14752/32000 (46%)] Loss: 1.96574 (semantic_loss: 0.01730, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.24678 
Train Epoch: 40 [466/1000 14912/32000 (47%)] Loss: 1.96237 (semantic_loss: 0.01393, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21784 
Train Epoch: 40 [471/1000 15072/32000 (47%)] Loss: 1.95967 (semantic_loss: 0.01318, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19879 
Train Epoch: 40 [476/1000 15232/32000 (48%)] Loss: 1.95964 (semantic_loss: 0.01217, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20530 
Train Epoch: 40 [481/1000 15392/32000 (48%)] Loss: 1.96417 (semantic_loss: 0.01573, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20392 
Train Epoch: 40 [486/1000 15552/32000 (49%)] Loss: 1.96121 (semantic_loss: 0.01375, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21075 
Train Epoch: 40 [491/1000 15712/32000 (49%)] Loss: 1.96160 (semantic_loss: 0.01413, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20395 
Train Epoch: 40 [496/1000 15872/32000 (50%)] Loss: 1.96053 (semantic_loss: 0.01209, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18719 
Train Epoch: 40 [501/1000 16032/32000 (50%)] Loss: 1.95885 (semantic_loss: 0.01138, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18965 
Train Epoch: 40 [506/1000 16192/32000 (51%)] Loss: 1.96308 (semantic_loss: 0.01561, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18751 
Train Epoch: 40 [511/1000 16352/32000 (51%)] Loss: 1.96392 (semantic_loss: 0.01744, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20352 
Train Epoch: 40 [516/1000 16512/32000 (52%)] Loss: 1.95944 (semantic_loss: 0.01197, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18682 
Train Epoch: 40 [521/1000 16672/32000 (52%)] Loss: 1.96194 (semantic_loss: 0.01545, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19733 
Train Epoch: 40 [526/1000 16832/32000 (53%)] Loss: 1.96582 (semantic_loss: 0.01835, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20582 
Train Epoch: 40 [531/1000 16992/32000 (53%)] Loss: 1.96413 (semantic_loss: 0.01569, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19713 
Train Epoch: 40 [536/1000 17152/32000 (54%)] Loss: 1.96529 (semantic_loss: 0.01684, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18837 
Train Epoch: 40 [541/1000 17312/32000 (54%)] Loss: 1.96200 (semantic_loss: 0.01454, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18915 
Train Epoch: 40 [546/1000 17472/32000 (55%)] Loss: 1.96251 (semantic_loss: 0.01504, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18224 
Train Epoch: 40 [551/1000 17632/32000 (55%)] Loss: 1.95952 (semantic_loss: 0.01303, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.34796 
Train Epoch: 40 [556/1000 17792/32000 (56%)] Loss: 1.96094 (semantic_loss: 0.01250, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18772 
Train Epoch: 40 [561/1000 17952/32000 (56%)] Loss: 1.96303 (semantic_loss: 0.01556, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18459 
Train Epoch: 40 [566/1000 18112/32000 (57%)] Loss: 1.96010 (semantic_loss: 0.01263, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18743 
Train Epoch: 40 [571/1000 18272/32000 (57%)] Loss: 1.96171 (semantic_loss: 0.01424, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18604 
Train Epoch: 40 [576/1000 18432/32000 (58%)] Loss: 1.96114 (semantic_loss: 0.01368, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23625 
Train Epoch: 40 [581/1000 18592/32000 (58%)] Loss: 1.96275 (semantic_loss: 0.01722, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.28246 
Train Epoch: 40 [586/1000 18752/32000 (59%)] Loss: 1.96160 (semantic_loss: 0.01316, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18765 
Train Epoch: 40 [591/1000 18912/32000 (59%)] Loss: 1.96255 (semantic_loss: 0.01509, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19018 
Train Epoch: 40 [596/1000 19072/32000 (60%)] Loss: 1.96080 (semantic_loss: 0.01333, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18740 
Train Epoch: 40 [601/1000 19232/32000 (60%)] Loss: 1.96082 (semantic_loss: 0.01335, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18843 
Train Epoch: 40 [606/1000 19392/32000 (61%)] Loss: 1.95907 (semantic_loss: 0.01160, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18723 
Train Epoch: 40 [611/1000 19552/32000 (61%)] Loss: 1.95973 (semantic_loss: 0.01324, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18579 
Train Epoch: 40 [616/1000 19712/32000 (62%)] Loss: 1.96442 (semantic_loss: 0.01695, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20654 
Train Epoch: 40 [621/1000 19872/32000 (62%)] Loss: 1.96132 (semantic_loss: 0.01385, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20736 
Train Epoch: 40 [626/1000 20032/32000 (63%)] Loss: 1.96045 (semantic_loss: 0.01397, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22905 
Train Epoch: 40 [631/1000 20192/32000 (63%)] Loss: 1.96272 (semantic_loss: 0.01526, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20276 
Train Epoch: 40 [636/1000 20352/32000 (64%)] Loss: 1.96374 (semantic_loss: 0.01530, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20494 
Train Epoch: 40 [641/1000 20512/32000 (64%)] Loss: 1.96157 (semantic_loss: 0.01507, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19178 
Train Epoch: 40 [646/1000 20672/32000 (65%)] Loss: 1.95823 (semantic_loss: 0.01076, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19250 
Train Epoch: 40 [651/1000 20832/32000 (65%)] Loss: 1.95931 (semantic_loss: 0.01184, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18888 
Train Epoch: 40 [656/1000 20992/32000 (66%)] Loss: 1.96206 (semantic_loss: 0.01557, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21764 
Train Epoch: 40 [661/1000 21152/32000 (66%)] Loss: 1.95979 (semantic_loss: 0.01428, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.33196 
Train Epoch: 40 [666/1000 21312/32000 (67%)] Loss: 1.96118 (semantic_loss: 0.01372, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19001 
Train Epoch: 40 [671/1000 21472/32000 (67%)] Loss: 1.96517 (semantic_loss: 0.01770, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18646 
Train Epoch: 40 [676/1000 21632/32000 (68%)] Loss: 1.96116 (semantic_loss: 0.01467, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.40387 
Train Epoch: 40 [681/1000 21792/32000 (68%)] Loss: 1.95798 (semantic_loss: 0.01149, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19192 
Train Epoch: 40 [686/1000 21952/32000 (69%)] Loss: 1.96293 (semantic_loss: 0.01546, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20595 
Train Epoch: 40 [691/1000 22112/32000 (69%)] Loss: 1.95988 (semantic_loss: 0.01242, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27195 
Train Epoch: 40 [696/1000 22272/32000 (70%)] Loss: 1.96366 (semantic_loss: 0.01619, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18428 
Train Epoch: 40 [701/1000 22432/32000 (70%)] Loss: 1.96352 (semantic_loss: 0.01605, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18539 
Train Epoch: 40 [706/1000 22592/32000 (71%)] Loss: 1.97166 (semantic_loss: 0.02419, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18720 
Train Epoch: 40 [711/1000 22752/32000 (71%)] Loss: 1.96204 (semantic_loss: 0.01457, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24366 
Train Epoch: 40 [716/1000 22912/32000 (72%)] Loss: 1.96642 (semantic_loss: 0.01896, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21484 
Train Epoch: 40 [721/1000 23072/32000 (72%)] Loss: 1.96085 (semantic_loss: 0.01241, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23796 
Train Epoch: 40 [726/1000 23232/32000 (73%)] Loss: 1.96157 (semantic_loss: 0.01410, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18644 
Train Epoch: 40 [731/1000 23392/32000 (73%)] Loss: 1.96271 (semantic_loss: 0.01524, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.42201 
Train Epoch: 40 [736/1000 23552/32000 (74%)] Loss: 1.95910 (semantic_loss: 0.01261, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18718 
Train Epoch: 40 [741/1000 23712/32000 (74%)] Loss: 1.96264 (semantic_loss: 0.01615, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18739 
Train Epoch: 40 [746/1000 23872/32000 (75%)] Loss: 1.96092 (semantic_loss: 0.01443, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19076 
Train Epoch: 40 [751/1000 24032/32000 (75%)] Loss: 1.96155 (semantic_loss: 0.01408, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18879 
Train Epoch: 40 [756/1000 24192/32000 (76%)] Loss: 1.96027 (semantic_loss: 0.01378, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18894 
Train Epoch: 40 [761/1000 24352/32000 (76%)] Loss: 1.96311 (semantic_loss: 0.01466, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18542 
Train Epoch: 40 [766/1000 24512/32000 (77%)] Loss: 1.96217 (semantic_loss: 0.01568, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20199 
Train Epoch: 40 [771/1000 24672/32000 (77%)] Loss: 1.95843 (semantic_loss: 0.01194, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.28618 
Train Epoch: 40 [776/1000 24832/32000 (78%)] Loss: 1.96236 (semantic_loss: 0.01489, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23660 
Train Epoch: 40 [781/1000 24992/32000 (78%)] Loss: 1.96136 (semantic_loss: 0.01486, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21087 
Train Epoch: 40 [786/1000 25152/32000 (79%)] Loss: 1.96267 (semantic_loss: 0.01520, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20880 
Train Epoch: 40 [791/1000 25312/32000 (79%)] Loss: 1.96146 (semantic_loss: 0.01302, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18925 
Train Epoch: 40 [796/1000 25472/32000 (80%)] Loss: 1.96223 (semantic_loss: 0.01377, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19418 
Train Epoch: 40 [801/1000 25632/32000 (80%)] Loss: 1.95684 (semantic_loss: 0.01133, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.20899 
Train Epoch: 40 [806/1000 25792/32000 (81%)] Loss: 1.96050 (semantic_loss: 0.01303, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18777 
Train Epoch: 40 [811/1000 25952/32000 (81%)] Loss: 1.95843 (semantic_loss: 0.01097, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18937 
Train Epoch: 40 [816/1000 26112/32000 (82%)] Loss: 1.96092 (semantic_loss: 0.01248, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19540 
Train Epoch: 40 [821/1000 26272/32000 (82%)] Loss: 1.96364 (semantic_loss: 0.01716, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19919 
Train Epoch: 40 [826/1000 26432/32000 (83%)] Loss: 1.96091 (semantic_loss: 0.01344, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18688 
Train Epoch: 40 [831/1000 26592/32000 (83%)] Loss: 1.96118 (semantic_loss: 0.01370, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21963 
Train Epoch: 40 [836/1000 26752/32000 (84%)] Loss: 1.96327 (semantic_loss: 0.01579, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20860 
Train Epoch: 40 [841/1000 26912/32000 (84%)] Loss: 1.96106 (semantic_loss: 0.01359, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19815 
Train Epoch: 40 [846/1000 27072/32000 (85%)] Loss: 1.96353 (semantic_loss: 0.01509, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18923 
Train Epoch: 40 [851/1000 27232/32000 (85%)] Loss: 1.96019 (semantic_loss: 0.01272, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18620 
Train Epoch: 40 [856/1000 27392/32000 (86%)] Loss: 1.96086 (semantic_loss: 0.01437, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18606 
Train Epoch: 40 [861/1000 27552/32000 (86%)] Loss: 1.96439 (semantic_loss: 0.01693, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21026 
Train Epoch: 40 [866/1000 27712/32000 (87%)] Loss: 1.96094 (semantic_loss: 0.01445, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20713 
Train Epoch: 40 [871/1000 27872/32000 (87%)] Loss: 1.95962 (semantic_loss: 0.01411, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.34774 
Train Epoch: 40 [876/1000 28032/32000 (88%)] Loss: 1.96117 (semantic_loss: 0.01468, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19221 
Train Epoch: 40 [881/1000 28192/32000 (88%)] Loss: 1.95989 (semantic_loss: 0.01243, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19031 
Train Epoch: 40 [886/1000 28352/32000 (89%)] Loss: 1.95908 (semantic_loss: 0.01161, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18696 
Train Epoch: 40 [891/1000 28512/32000 (89%)] Loss: 1.96352 (semantic_loss: 0.01606, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18310 
Train Epoch: 40 [896/1000 28672/32000 (90%)] Loss: 1.95807 (semantic_loss: 0.01158, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23549 
Train Epoch: 40 [901/1000 28832/32000 (90%)] Loss: 1.96029 (semantic_loss: 0.01380, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.28241 
Train Epoch: 40 [906/1000 28992/32000 (91%)] Loss: 1.96018 (semantic_loss: 0.01272, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19375 
Train Epoch: 40 [911/1000 29152/32000 (91%)] Loss: 1.95943 (semantic_loss: 0.01197, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18846 
Train Epoch: 40 [916/1000 29312/32000 (92%)] Loss: 1.96382 (semantic_loss: 0.01538, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19226 
Train Epoch: 40 [921/1000 29472/32000 (92%)] Loss: 1.96206 (semantic_loss: 0.01362, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20399 
Train Epoch: 40 [926/1000 29632/32000 (93%)] Loss: 1.95974 (semantic_loss: 0.01227, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21814 
Train Epoch: 40 [931/1000 29792/32000 (93%)] Loss: 1.96071 (semantic_loss: 0.01422, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24157 
Train Epoch: 40 [936/1000 29952/32000 (94%)] Loss: 1.96485 (semantic_loss: 0.01738, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20817 
Train Epoch: 40 [941/1000 30112/32000 (94%)] Loss: 1.95993 (semantic_loss: 0.01246, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20180 
Train Epoch: 40 [946/1000 30272/32000 (95%)] Loss: 1.96262 (semantic_loss: 0.01516, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20840 
Train Epoch: 40 [951/1000 30432/32000 (95%)] Loss: 1.96533 (semantic_loss: 0.01688, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20861 
Train Epoch: 40 [956/1000 30592/32000 (96%)] Loss: 1.95972 (semantic_loss: 0.01226, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20136 
Train Epoch: 40 [961/1000 30752/32000 (96%)] Loss: 1.95845 (semantic_loss: 0.01196, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20792 
Train Epoch: 40 [966/1000 30912/32000 (97%)] Loss: 1.96194 (semantic_loss: 0.01447, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19814 
Train Epoch: 40 [971/1000 31072/32000 (97%)] Loss: 1.96403 (semantic_loss: 0.01656, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18927 
Train Epoch: 40 [976/1000 31232/32000 (98%)] Loss: 1.96393 (semantic_loss: 0.01549, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22412 
Train Epoch: 40 [981/1000 31392/32000 (98%)] Loss: 1.96183 (semantic_loss: 0.01435, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.31254 
Train Epoch: 40 [986/1000 31552/32000 (99%)] Loss: 1.95856 (semantic_loss: 0.01109, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18603 
Train Epoch: 40 [991/1000 31712/32000 (99%)] Loss: 1.96113 (semantic_loss: 0.01464, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19204 
Train Epoch: 40 [996/1000 31872/32000 (100%)] Loss: 1.96119 (semantic_loss: 0.01373, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.44016 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch40.pth ...
Done in 4.270s
removing stale ckpt [epoch 39] [took 0.00s]
 epoch          : 40
 loss           : 1.9618284034729003
 learning_rate  : 8.211601634130337e-07
 n_samples      : 1280000
 n_steps        : 40000
 ActivityNet_val1_test/t2v_metrics/R1: 11.958511287370348
 ActivityNet_val1_test/t2v_metrics/R5: 38.29570876550742
 ActivityNet_val1_test/t2v_metrics/R10: 55.46064673581452
 ActivityNet_val1_test/t2v_metrics/R50: 84.52308318080131
 ActivityNet_val1_test/t2v_metrics/MedR: 8.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 63.02003254016677
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.39481421994606
 ActivityNet_val1_test/v2t_metrics/R1: 12.71100264388855
 ActivityNet_val1_test/v2t_metrics/R5: 39.2109009558674
 ActivityNet_val1_test/v2t_metrics/R10: 55.50132194427496
 ActivityNet_val1_test/v2t_metrics/R50: 84.7061216188733
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 65.30618263168599
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 30.243356296131257
 mnt_best       : 29.542107379154057
 not_improved_count: 13
Train Epoch: 41 [1/1000 32/32000 (0%)] Loss: 1.96147 (semantic_loss: 0.01302, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=22.38656 
Train Epoch: 41 [6/1000 192/32000 (1%)] Loss: 1.96334 (semantic_loss: 0.01587, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18564 
Train Epoch: 41 [11/1000 352/32000 (1%)] Loss: 1.95830 (semantic_loss: 0.01083, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19689 
Train Epoch: 41 [16/1000 512/32000 (2%)] Loss: 1.96378 (semantic_loss: 0.01729, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18377 
Train Epoch: 41 [21/1000 672/32000 (2%)] Loss: 1.95864 (semantic_loss: 0.01118, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.15988 
Train Epoch: 41 [26/1000 832/32000 (3%)] Loss: 1.96401 (semantic_loss: 0.01557, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18294 
Train Epoch: 41 [31/1000 992/32000 (3%)] Loss: 1.96013 (semantic_loss: 0.01364, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18856 
Train Epoch: 41 [36/1000 1152/32000 (4%)] Loss: 1.96248 (semantic_loss: 0.01404, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18761 
Train Epoch: 41 [41/1000 1312/32000 (4%)] Loss: 1.96225 (semantic_loss: 0.01576, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18350 
Train Epoch: 41 [46/1000 1472/32000 (5%)] Loss: 1.96105 (semantic_loss: 0.01260, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18569 
Train Epoch: 41 [51/1000 1632/32000 (5%)] Loss: 1.96622 (semantic_loss: 0.01778, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20649 
Train Epoch: 41 [56/1000 1792/32000 (6%)] Loss: 1.96125 (semantic_loss: 0.01378, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20608 
Train Epoch: 41 [61/1000 1952/32000 (6%)] Loss: 1.96079 (semantic_loss: 0.01234, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.25053 
Train Epoch: 41 [66/1000 2112/32000 (7%)] Loss: 1.96105 (semantic_loss: 0.01457, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19958 
Train Epoch: 41 [71/1000 2272/32000 (7%)] Loss: 1.96142 (semantic_loss: 0.01298, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19987 
Train Epoch: 41 [76/1000 2432/32000 (8%)] Loss: 1.96440 (semantic_loss: 0.01596, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=1.35730 
Train Epoch: 41 [81/1000 2592/32000 (8%)] Loss: 1.96483 (semantic_loss: 0.01736, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19000 
Train Epoch: 41 [86/1000 2752/32000 (9%)] Loss: 1.95994 (semantic_loss: 0.01345, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18959 
Train Epoch: 41 [91/1000 2912/32000 (9%)] Loss: 1.96322 (semantic_loss: 0.01575, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20578 
Train Epoch: 41 [96/1000 3072/32000 (10%)] Loss: 1.96032 (semantic_loss: 0.01285, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19455 
Train Epoch: 41 [101/1000 3232/32000 (10%)] Loss: 1.96605 (semantic_loss: 0.01663, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.19617 
Train Epoch: 41 [106/1000 3392/32000 (11%)] Loss: 1.95842 (semantic_loss: 0.01193, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18343 
Train Epoch: 41 [111/1000 3552/32000 (11%)] Loss: 1.96430 (semantic_loss: 0.01488, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18368 
Train Epoch: 41 [116/1000 3712/32000 (12%)] Loss: 1.96165 (semantic_loss: 0.01515, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18592 
Train Epoch: 41 [121/1000 3872/32000 (12%)] Loss: 1.96164 (semantic_loss: 0.01417, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18350 
Train Epoch: 41 [126/1000 4032/32000 (13%)] Loss: 1.96270 (semantic_loss: 0.01621, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19462 
Train Epoch: 41 [131/1000 4192/32000 (13%)] Loss: 1.95951 (semantic_loss: 0.01302, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19732 
Train Epoch: 41 [136/1000 4352/32000 (14%)] Loss: 1.96194 (semantic_loss: 0.01350, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20668 
Train Epoch: 41 [141/1000 4512/32000 (14%)] Loss: 1.96180 (semantic_loss: 0.01335, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18198 
Train Epoch: 41 [146/1000 4672/32000 (15%)] Loss: 1.95812 (semantic_loss: 0.01162, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18303 
Train Epoch: 41 [151/1000 4832/32000 (15%)] Loss: 1.96020 (semantic_loss: 0.01273, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18364 
Train Epoch: 41 [156/1000 4992/32000 (16%)] Loss: 1.96091 (semantic_loss: 0.01344, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18808 
Train Epoch: 41 [161/1000 5152/32000 (16%)] Loss: 1.96076 (semantic_loss: 0.01329, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18983 
Train Epoch: 41 [166/1000 5312/32000 (17%)] Loss: 1.96184 (semantic_loss: 0.01437, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18689 
Train Epoch: 41 [171/1000 5472/32000 (17%)] Loss: 1.95987 (semantic_loss: 0.01240, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18601 
Train Epoch: 41 [176/1000 5632/32000 (18%)] Loss: 1.96177 (semantic_loss: 0.01236, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18493 
Train Epoch: 41 [181/1000 5792/32000 (18%)] Loss: 1.96165 (semantic_loss: 0.01515, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18520 
Train Epoch: 41 [186/1000 5952/32000 (19%)] Loss: 1.95966 (semantic_loss: 0.01219, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18549 
Train Epoch: 41 [191/1000 6112/32000 (19%)] Loss: 1.95966 (semantic_loss: 0.01317, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18361 
Train Epoch: 41 [196/1000 6272/32000 (20%)] Loss: 1.96463 (semantic_loss: 0.01716, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18884 
Train Epoch: 41 [201/1000 6432/32000 (20%)] Loss: 1.96025 (semantic_loss: 0.01279, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18307 
Train Epoch: 41 [206/1000 6592/32000 (21%)] Loss: 1.96084 (semantic_loss: 0.01337, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18214 
Train Epoch: 41 [211/1000 6752/32000 (21%)] Loss: 1.96306 (semantic_loss: 0.01462, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19500 
Train Epoch: 41 [216/1000 6912/32000 (22%)] Loss: 1.96209 (semantic_loss: 0.01462, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21326 
Train Epoch: 41 [221/1000 7072/32000 (22%)] Loss: 1.96588 (semantic_loss: 0.01842, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20799 
Train Epoch: 41 [226/1000 7232/32000 (23%)] Loss: 1.95819 (semantic_loss: 0.01170, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22261 
Train Epoch: 41 [231/1000 7392/32000 (23%)] Loss: 1.95751 (semantic_loss: 0.01102, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20163 
Train Epoch: 41 [236/1000 7552/32000 (24%)] Loss: 1.95866 (semantic_loss: 0.01216, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20539 
Train Epoch: 41 [241/1000 7712/32000 (24%)] Loss: 1.96195 (semantic_loss: 0.01351, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19691 
Train Epoch: 41 [246/1000 7872/32000 (25%)] Loss: 1.96027 (semantic_loss: 0.01379, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19591 
Train Epoch: 41 [251/1000 8032/32000 (25%)] Loss: 1.96209 (semantic_loss: 0.01365, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20628 
Train Epoch: 41 [256/1000 8192/32000 (26%)] Loss: 1.96425 (semantic_loss: 0.01679, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18644 
Train Epoch: 41 [261/1000 8352/32000 (26%)] Loss: 1.96418 (semantic_loss: 0.01671, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18418 
Train Epoch: 41 [266/1000 8512/32000 (27%)] Loss: 1.95924 (semantic_loss: 0.01373, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18419 
Train Epoch: 41 [271/1000 8672/32000 (27%)] Loss: 1.95991 (semantic_loss: 0.01342, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19382 
Train Epoch: 41 [276/1000 8832/32000 (28%)] Loss: 1.96432 (semantic_loss: 0.01684, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19971 
Train Epoch: 41 [281/1000 8992/32000 (28%)] Loss: 1.96004 (semantic_loss: 0.01356, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18832 
Train Epoch: 41 [286/1000 9152/32000 (29%)] Loss: 1.96164 (semantic_loss: 0.01418, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19057 
Train Epoch: 41 [291/1000 9312/32000 (29%)] Loss: 1.96431 (semantic_loss: 0.01685, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.34080 
Train Epoch: 41 [296/1000 9472/32000 (30%)] Loss: 1.96366 (semantic_loss: 0.01716, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20218 
Train Epoch: 41 [301/1000 9632/32000 (30%)] Loss: 1.95998 (semantic_loss: 0.01154, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19971 
Train Epoch: 41 [306/1000 9792/32000 (31%)] Loss: 1.95967 (semantic_loss: 0.01318, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18782 
Train Epoch: 41 [311/1000 9952/32000 (31%)] Loss: 1.96481 (semantic_loss: 0.01734, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19148 
Train Epoch: 41 [316/1000 10112/32000 (32%)] Loss: 1.96244 (semantic_loss: 0.01498, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19368 
Train Epoch: 41 [321/1000 10272/32000 (32%)] Loss: 1.95929 (semantic_loss: 0.01183, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27330 
Train Epoch: 41 [326/1000 10432/32000 (33%)] Loss: 1.96629 (semantic_loss: 0.01980, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18691 
Train Epoch: 41 [331/1000 10592/32000 (33%)] Loss: 1.96098 (semantic_loss: 0.01449, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18901 
Train Epoch: 41 [336/1000 10752/32000 (34%)] Loss: 1.96589 (semantic_loss: 0.01647, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.18986 
Train Epoch: 41 [341/1000 10912/32000 (34%)] Loss: 1.96066 (semantic_loss: 0.01319, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19340 
Train Epoch: 41 [346/1000 11072/32000 (35%)] Loss: 1.97080 (semantic_loss: 0.02333, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18678 
Train Epoch: 41 [351/1000 11232/32000 (35%)] Loss: 1.96252 (semantic_loss: 0.01505, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18740 
Train Epoch: 41 [356/1000 11392/32000 (36%)] Loss: 1.96131 (semantic_loss: 0.01384, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18663 
Train Epoch: 41 [361/1000 11552/32000 (36%)] Loss: 1.95761 (semantic_loss: 0.01209, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18463 
Train Epoch: 41 [366/1000 11712/32000 (37%)] Loss: 1.96108 (semantic_loss: 0.01361, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19027 
Train Epoch: 41 [371/1000 11872/32000 (37%)] Loss: 1.96175 (semantic_loss: 0.01429, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18608 
Train Epoch: 41 [376/1000 12032/32000 (38%)] Loss: 1.96631 (semantic_loss: 0.01884, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21108 
Train Epoch: 41 [381/1000 12192/32000 (38%)] Loss: 1.96275 (semantic_loss: 0.01528, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21368 
Train Epoch: 41 [386/1000 12352/32000 (39%)] Loss: 1.96041 (semantic_loss: 0.01294, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21318 
Train Epoch: 41 [391/1000 12512/32000 (39%)] Loss: 1.96472 (semantic_loss: 0.01726, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21299 
Train Epoch: 41 [396/1000 12672/32000 (40%)] Loss: 1.96466 (semantic_loss: 0.01719, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=1.29682 
Train Epoch: 41 [401/1000 12832/32000 (40%)] Loss: 1.96133 (semantic_loss: 0.01484, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19796 
Train Epoch: 41 [406/1000 12992/32000 (41%)] Loss: 1.95906 (semantic_loss: 0.01257, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20131 
Train Epoch: 41 [411/1000 13152/32000 (41%)] Loss: 1.96320 (semantic_loss: 0.01573, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19408 
Train Epoch: 41 [416/1000 13312/32000 (42%)] Loss: 1.95983 (semantic_loss: 0.01236, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18904 
Train Epoch: 41 [421/1000 13472/32000 (42%)] Loss: 1.96231 (semantic_loss: 0.01484, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18789 
Train Epoch: 41 [426/1000 13632/32000 (43%)] Loss: 1.95993 (semantic_loss: 0.01245, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18825 
Train Epoch: 41 [431/1000 13792/32000 (43%)] Loss: 1.95894 (semantic_loss: 0.01245, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18494 
Train Epoch: 41 [436/1000 13952/32000 (44%)] Loss: 1.95907 (semantic_loss: 0.01258, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18960 
Train Epoch: 41 [441/1000 14112/32000 (44%)] Loss: 1.96451 (semantic_loss: 0.01704, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19513 
Train Epoch: 41 [446/1000 14272/32000 (45%)] Loss: 1.96608 (semantic_loss: 0.01861, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19411 
Train Epoch: 41 [451/1000 14432/32000 (45%)] Loss: 1.96295 (semantic_loss: 0.01548, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19460 
Train Epoch: 41 [456/1000 14592/32000 (46%)] Loss: 1.96427 (semantic_loss: 0.01582, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20753 
Train Epoch: 41 [461/1000 14752/32000 (46%)] Loss: 1.96563 (semantic_loss: 0.01718, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18666 
Train Epoch: 41 [466/1000 14912/32000 (47%)] Loss: 1.95881 (semantic_loss: 0.01330, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19298 
Train Epoch: 41 [471/1000 15072/32000 (47%)] Loss: 1.96294 (semantic_loss: 0.01548, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18924 
Train Epoch: 41 [476/1000 15232/32000 (48%)] Loss: 1.96057 (semantic_loss: 0.01213, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18796 
Train Epoch: 41 [481/1000 15392/32000 (48%)] Loss: 1.96283 (semantic_loss: 0.01536, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19079 
Train Epoch: 41 [486/1000 15552/32000 (49%)] Loss: 1.96434 (semantic_loss: 0.01493, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18530 
Train Epoch: 41 [491/1000 15712/32000 (49%)] Loss: 1.96361 (semantic_loss: 0.01615, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18591 
Train Epoch: 41 [496/1000 15872/32000 (50%)] Loss: 1.96647 (semantic_loss: 0.01998, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18754 
Train Epoch: 41 [501/1000 16032/32000 (50%)] Loss: 1.96043 (semantic_loss: 0.01296, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18568 
Train Epoch: 41 [506/1000 16192/32000 (51%)] Loss: 1.96298 (semantic_loss: 0.01649, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18793 
Train Epoch: 41 [511/1000 16352/32000 (51%)] Loss: 1.96250 (semantic_loss: 0.01405, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18535 
Train Epoch: 41 [516/1000 16512/32000 (52%)] Loss: 1.96110 (semantic_loss: 0.01462, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19427 
Train Epoch: 41 [521/1000 16672/32000 (52%)] Loss: 1.96690 (semantic_loss: 0.01845, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18694 
Train Epoch: 41 [526/1000 16832/32000 (53%)] Loss: 1.96284 (semantic_loss: 0.01635, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18708 
Train Epoch: 41 [531/1000 16992/32000 (53%)] Loss: 1.96002 (semantic_loss: 0.01353, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21872 
Train Epoch: 41 [536/1000 17152/32000 (54%)] Loss: 1.96741 (semantic_loss: 0.01897, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22678 
Train Epoch: 41 [541/1000 17312/32000 (54%)] Loss: 1.95886 (semantic_loss: 0.01238, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20318 
Train Epoch: 41 [546/1000 17472/32000 (55%)] Loss: 1.96202 (semantic_loss: 0.01455, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20147 
Train Epoch: 41 [551/1000 17632/32000 (55%)] Loss: 1.96247 (semantic_loss: 0.01597, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.22021 
Train Epoch: 41 [556/1000 17792/32000 (56%)] Loss: 1.96259 (semantic_loss: 0.01513, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20425 
Train Epoch: 41 [561/1000 17952/32000 (56%)] Loss: 1.96641 (semantic_loss: 0.01797, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20241 
Train Epoch: 41 [566/1000 18112/32000 (57%)] Loss: 1.96024 (semantic_loss: 0.01375, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20399 
Train Epoch: 41 [571/1000 18272/32000 (57%)] Loss: 1.96149 (semantic_loss: 0.01402, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19404 
Train Epoch: 41 [576/1000 18432/32000 (58%)] Loss: 1.96242 (semantic_loss: 0.01496, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18696 
Train Epoch: 41 [581/1000 18592/32000 (58%)] Loss: 1.96364 (semantic_loss: 0.01618, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18513 
Train Epoch: 41 [586/1000 18752/32000 (59%)] Loss: 1.96548 (semantic_loss: 0.01801, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18592 
Train Epoch: 41 [591/1000 18912/32000 (59%)] Loss: 1.96074 (semantic_loss: 0.01328, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18560 
Train Epoch: 41 [596/1000 19072/32000 (60%)] Loss: 1.96128 (semantic_loss: 0.01381, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19489 
Train Epoch: 41 [601/1000 19232/32000 (60%)] Loss: 1.96150 (semantic_loss: 0.01403, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19565 
Train Epoch: 41 [606/1000 19392/32000 (61%)] Loss: 1.96226 (semantic_loss: 0.01480, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20884 
Train Epoch: 41 [611/1000 19552/32000 (61%)] Loss: 1.96186 (semantic_loss: 0.01342, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.32204 
Train Epoch: 41 [616/1000 19712/32000 (62%)] Loss: 1.96412 (semantic_loss: 0.01665, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18696 
Train Epoch: 41 [621/1000 19872/32000 (62%)] Loss: 1.95830 (semantic_loss: 0.01181, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18822 
Train Epoch: 41 [626/1000 20032/32000 (63%)] Loss: 1.95953 (semantic_loss: 0.01206, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18353 
Train Epoch: 41 [631/1000 20192/32000 (63%)] Loss: 1.95970 (semantic_loss: 0.01223, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18955 
Train Epoch: 41 [636/1000 20352/32000 (64%)] Loss: 1.96141 (semantic_loss: 0.01394, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19481 
Train Epoch: 41 [641/1000 20512/32000 (64%)] Loss: 1.96012 (semantic_loss: 0.01363, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24853 
Train Epoch: 41 [646/1000 20672/32000 (65%)] Loss: 1.96378 (semantic_loss: 0.01730, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18680 
Train Epoch: 41 [651/1000 20832/32000 (65%)] Loss: 1.96009 (semantic_loss: 0.01262, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18384 
Train Epoch: 41 [656/1000 20992/32000 (66%)] Loss: 1.96083 (semantic_loss: 0.01336, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18554 
Train Epoch: 41 [661/1000 21152/32000 (66%)] Loss: 1.96038 (semantic_loss: 0.01292, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19057 
Train Epoch: 41 [666/1000 21312/32000 (67%)] Loss: 1.96155 (semantic_loss: 0.01506, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18853 
Train Epoch: 41 [671/1000 21472/32000 (67%)] Loss: 1.95775 (semantic_loss: 0.01127, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18722 
Train Epoch: 41 [676/1000 21632/32000 (68%)] Loss: 1.96333 (semantic_loss: 0.01489, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18345 
Train Epoch: 41 [681/1000 21792/32000 (68%)] Loss: 1.96458 (semantic_loss: 0.01614, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19310 
Train Epoch: 41 [686/1000 21952/32000 (69%)] Loss: 1.96025 (semantic_loss: 0.01180, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20530 
Train Epoch: 41 [691/1000 22112/32000 (69%)] Loss: 1.96422 (semantic_loss: 0.01578, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21019 
Train Epoch: 41 [696/1000 22272/32000 (70%)] Loss: 1.95832 (semantic_loss: 0.01183, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21797 
Train Epoch: 41 [701/1000 22432/32000 (70%)] Loss: 1.96691 (semantic_loss: 0.01944, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20546 
Train Epoch: 41 [706/1000 22592/32000 (71%)] Loss: 1.95922 (semantic_loss: 0.01274, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19958 
Train Epoch: 41 [711/1000 22752/32000 (71%)] Loss: 1.95963 (semantic_loss: 0.01314, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18966 
Train Epoch: 41 [716/1000 22912/32000 (72%)] Loss: 1.96462 (semantic_loss: 0.01617, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=1.39119 
Train Epoch: 41 [721/1000 23072/32000 (72%)] Loss: 1.96220 (semantic_loss: 0.01571, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21010 
Train Epoch: 41 [726/1000 23232/32000 (73%)] Loss: 1.96436 (semantic_loss: 0.01689, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19029 
Train Epoch: 41 [731/1000 23392/32000 (73%)] Loss: 1.96177 (semantic_loss: 0.01430, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20358 
Train Epoch: 41 [736/1000 23552/32000 (74%)] Loss: 1.96066 (semantic_loss: 0.01417, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18646 
Train Epoch: 41 [741/1000 23712/32000 (74%)] Loss: 1.96138 (semantic_loss: 0.01391, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18899 
Train Epoch: 41 [746/1000 23872/32000 (75%)] Loss: 1.96048 (semantic_loss: 0.01302, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18876 
Train Epoch: 41 [751/1000 24032/32000 (75%)] Loss: 1.96359 (semantic_loss: 0.01515, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18937 
Train Epoch: 41 [756/1000 24192/32000 (76%)] Loss: 1.96185 (semantic_loss: 0.01633, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19225 
Train Epoch: 41 [761/1000 24352/32000 (76%)] Loss: 1.96053 (semantic_loss: 0.01208, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19896 
Train Epoch: 41 [766/1000 24512/32000 (77%)] Loss: 1.96401 (semantic_loss: 0.01654, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18948 
Train Epoch: 41 [771/1000 24672/32000 (77%)] Loss: 1.96123 (semantic_loss: 0.01375, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18618 
Train Epoch: 41 [776/1000 24832/32000 (78%)] Loss: 1.96374 (semantic_loss: 0.01627, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20170 
Train Epoch: 41 [781/1000 24992/32000 (78%)] Loss: 1.96110 (semantic_loss: 0.01266, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18481 
Train Epoch: 41 [786/1000 25152/32000 (79%)] Loss: 1.96016 (semantic_loss: 0.01269, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18874 
Train Epoch: 41 [791/1000 25312/32000 (79%)] Loss: 1.96336 (semantic_loss: 0.01589, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18336 
Train Epoch: 41 [796/1000 25472/32000 (80%)] Loss: 1.95856 (semantic_loss: 0.01110, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18553 
Train Epoch: 41 [801/1000 25632/32000 (80%)] Loss: 1.95910 (semantic_loss: 0.01261, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18966 
Train Epoch: 41 [806/1000 25792/32000 (81%)] Loss: 1.96102 (semantic_loss: 0.01356, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18867 
Train Epoch: 41 [811/1000 25952/32000 (81%)] Loss: 1.96399 (semantic_loss: 0.01554, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18582 
Train Epoch: 41 [816/1000 26112/32000 (82%)] Loss: 1.96476 (semantic_loss: 0.01729, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18534 
Train Epoch: 41 [821/1000 26272/32000 (82%)] Loss: 1.96175 (semantic_loss: 0.01429, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18901 
Train Epoch: 41 [826/1000 26432/32000 (83%)] Loss: 1.96165 (semantic_loss: 0.01321, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18776 
Train Epoch: 41 [831/1000 26592/32000 (83%)] Loss: 1.96428 (semantic_loss: 0.01779, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18887 
Train Epoch: 41 [836/1000 26752/32000 (84%)] Loss: 1.95876 (semantic_loss: 0.01227, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19358 
Train Epoch: 41 [841/1000 26912/32000 (84%)] Loss: 1.96181 (semantic_loss: 0.01435, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18547 
Train Epoch: 41 [846/1000 27072/32000 (85%)] Loss: 1.96208 (semantic_loss: 0.01462, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20462 
Train Epoch: 41 [851/1000 27232/32000 (85%)] Loss: 1.96093 (semantic_loss: 0.01444, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20928 
Train Epoch: 41 [856/1000 27392/32000 (86%)] Loss: 1.96427 (semantic_loss: 0.01680, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22395 
Train Epoch: 41 [861/1000 27552/32000 (86%)] Loss: 1.96084 (semantic_loss: 0.01435, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19633 
Train Epoch: 41 [866/1000 27712/32000 (87%)] Loss: 1.96286 (semantic_loss: 0.01539, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20381 
Train Epoch: 41 [871/1000 27872/32000 (87%)] Loss: 1.95909 (semantic_loss: 0.01260, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19643 
Train Epoch: 41 [876/1000 28032/32000 (88%)] Loss: 1.96158 (semantic_loss: 0.01411, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19956 
Train Epoch: 41 [881/1000 28192/32000 (88%)] Loss: 1.95948 (semantic_loss: 0.01299, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19014 
Train Epoch: 41 [886/1000 28352/32000 (89%)] Loss: 1.96522 (semantic_loss: 0.01775, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19105 
Train Epoch: 41 [891/1000 28512/32000 (89%)] Loss: 1.95873 (semantic_loss: 0.01224, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18724 
Train Epoch: 41 [896/1000 28672/32000 (90%)] Loss: 1.96049 (semantic_loss: 0.01205, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18826 
Train Epoch: 41 [901/1000 28832/32000 (90%)] Loss: 1.96765 (semantic_loss: 0.02018, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18949 
Train Epoch: 41 [906/1000 28992/32000 (91%)] Loss: 1.96767 (semantic_loss: 0.02020, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18580 
Train Epoch: 41 [911/1000 29152/32000 (91%)] Loss: 1.95917 (semantic_loss: 0.01268, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20163 
Train Epoch: 41 [916/1000 29312/32000 (92%)] Loss: 1.96308 (semantic_loss: 0.01659, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19982 
Train Epoch: 41 [921/1000 29472/32000 (92%)] Loss: 1.95908 (semantic_loss: 0.01258, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20400 
Train Epoch: 41 [926/1000 29632/32000 (93%)] Loss: 1.96494 (semantic_loss: 0.01845, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19802 
Train Epoch: 41 [931/1000 29792/32000 (93%)] Loss: 1.95829 (semantic_loss: 0.01181, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19607 
Train Epoch: 41 [936/1000 29952/32000 (94%)] Loss: 1.96628 (semantic_loss: 0.01881, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19005 
Train Epoch: 41 [941/1000 30112/32000 (94%)] Loss: 1.96084 (semantic_loss: 0.01337, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18540 
Train Epoch: 41 [946/1000 30272/32000 (95%)] Loss: 1.96175 (semantic_loss: 0.01429, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18573 
Train Epoch: 41 [951/1000 30432/32000 (95%)] Loss: 1.96163 (semantic_loss: 0.01416, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18492 
Train Epoch: 41 [956/1000 30592/32000 (96%)] Loss: 1.96074 (semantic_loss: 0.01327, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.65052 
Train Epoch: 41 [961/1000 30752/32000 (96%)] Loss: 1.96223 (semantic_loss: 0.01476, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18390 
Train Epoch: 41 [966/1000 30912/32000 (97%)] Loss: 1.96565 (semantic_loss: 0.01916, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18307 
Train Epoch: 41 [971/1000 31072/32000 (97%)] Loss: 1.96387 (semantic_loss: 0.01640, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18511 
Train Epoch: 41 [976/1000 31232/32000 (98%)] Loss: 1.96134 (semantic_loss: 0.01388, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18628 
Train Epoch: 41 [981/1000 31392/32000 (98%)] Loss: 1.96062 (semantic_loss: 0.01315, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18341 
Train Epoch: 41 [986/1000 31552/32000 (99%)] Loss: 1.95903 (semantic_loss: 0.01254, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18589 
Train Epoch: 41 [991/1000 31712/32000 (99%)] Loss: 1.96329 (semantic_loss: 0.01681, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18878 
Train Epoch: 41 [996/1000 31872/32000 (100%)] Loss: 1.95997 (semantic_loss: 0.01251, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20038 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch41.pth ...
Done in 16.894s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch41.pth ...
Done in 20.780s
removing stale ckpt [epoch 40] [took 0.01s]
 epoch          : 41
 loss           : 1.9618619058132172
 learning_rate  : 7.390441470717303e-07
 n_samples      : 1312000
 n_steps        : 41000
 ActivityNet_val1_test/t2v_metrics/R1: 12.141549725442342
 ActivityNet_val1_test/t2v_metrics/R5: 38.33638397396787
 ActivityNet_val1_test/t2v_metrics/R10: 55.64368517388652
 ActivityNet_val1_test/t2v_metrics/R50: 84.9908480780964
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 62.644295302013425
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.58696272768771
 ActivityNet_val1_test/v2t_metrics/R1: 12.182224933902786
 ActivityNet_val1_test/v2t_metrics/R5: 39.06853772625585
 ActivityNet_val1_test/v2t_metrics/R10: 56.05043725849095
 ActivityNet_val1_test/v2t_metrics/R50: 85.01118568232663
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 64.60423022167988
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.879793509517032
 mnt_best       : 29.58696272768771
 not_improved_count: 0
Train Epoch: 42 [1/1000 32/32000 (0%)] Loss: 1.96208 (semantic_loss: 0.01461, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=23.29839 
Train Epoch: 42 [6/1000 192/32000 (1%)] Loss: 1.96141 (semantic_loss: 0.01395, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18474 
Train Epoch: 42 [11/1000 352/32000 (1%)] Loss: 1.96318 (semantic_loss: 0.01473, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18520 
Train Epoch: 42 [16/1000 512/32000 (2%)] Loss: 1.96446 (semantic_loss: 0.01797, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.65390 
Train Epoch: 42 [21/1000 672/32000 (2%)] Loss: 1.96144 (semantic_loss: 0.01495, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18447 
Train Epoch: 42 [26/1000 832/32000 (3%)] Loss: 1.96135 (semantic_loss: 0.01486, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18945 
Train Epoch: 42 [31/1000 992/32000 (3%)] Loss: 1.95851 (semantic_loss: 0.01201, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18314 
Train Epoch: 42 [36/1000 1152/32000 (4%)] Loss: 1.95960 (semantic_loss: 0.01213, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18443 
Train Epoch: 42 [41/1000 1312/32000 (4%)] Loss: 1.96242 (semantic_loss: 0.01496, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18563 
Train Epoch: 42 [46/1000 1472/32000 (5%)] Loss: 1.96093 (semantic_loss: 0.01346, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24790 
Train Epoch: 42 [51/1000 1632/32000 (5%)] Loss: 1.96770 (semantic_loss: 0.02122, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24490 
Train Epoch: 42 [56/1000 1792/32000 (6%)] Loss: 1.95942 (semantic_loss: 0.01294, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18504 
Train Epoch: 42 [61/1000 1952/32000 (6%)] Loss: 1.95816 (semantic_loss: 0.01265, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.21471 
Train Epoch: 42 [66/1000 2112/32000 (7%)] Loss: 1.96484 (semantic_loss: 0.01737, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21581 
Train Epoch: 42 [71/1000 2272/32000 (7%)] Loss: 1.96084 (semantic_loss: 0.01337, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21832 
Train Epoch: 42 [76/1000 2432/32000 (8%)] Loss: 1.95961 (semantic_loss: 0.01311, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20197 
Train Epoch: 42 [81/1000 2592/32000 (8%)] Loss: 1.96289 (semantic_loss: 0.01445, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20114 
Train Epoch: 42 [86/1000 2752/32000 (9%)] Loss: 1.96254 (semantic_loss: 0.01507, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19392 
Train Epoch: 42 [91/1000 2912/32000 (9%)] Loss: 1.96032 (semantic_loss: 0.01383, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19801 
Train Epoch: 42 [96/1000 3072/32000 (10%)] Loss: 1.96397 (semantic_loss: 0.01552, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19805 
Train Epoch: 42 [101/1000 3232/32000 (10%)] Loss: 1.96446 (semantic_loss: 0.01600, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18935 
Train Epoch: 42 [106/1000 3392/32000 (11%)] Loss: 1.96328 (semantic_loss: 0.01582, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18443 
Train Epoch: 42 [111/1000 3552/32000 (11%)] Loss: 1.96015 (semantic_loss: 0.01268, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18619 
Train Epoch: 42 [116/1000 3712/32000 (12%)] Loss: 1.96493 (semantic_loss: 0.01746, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18559 
Train Epoch: 42 [121/1000 3872/32000 (12%)] Loss: 1.96311 (semantic_loss: 0.01565, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18465 
Train Epoch: 42 [126/1000 4032/32000 (13%)] Loss: 1.96033 (semantic_loss: 0.01482, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19799 
Train Epoch: 42 [131/1000 4192/32000 (13%)] Loss: 1.96098 (semantic_loss: 0.01449, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19905 
Train Epoch: 42 [136/1000 4352/32000 (14%)] Loss: 1.96044 (semantic_loss: 0.01298, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20181 
Train Epoch: 42 [141/1000 4512/32000 (14%)] Loss: 1.96037 (semantic_loss: 0.01387, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18830 
Train Epoch: 42 [146/1000 4672/32000 (15%)] Loss: 1.96289 (semantic_loss: 0.01542, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18402 
Train Epoch: 42 [151/1000 4832/32000 (15%)] Loss: 1.96196 (semantic_loss: 0.01450, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18376 
Train Epoch: 42 [156/1000 4992/32000 (16%)] Loss: 1.96382 (semantic_loss: 0.01537, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20041 
Train Epoch: 42 [161/1000 5152/32000 (16%)] Loss: 1.96374 (semantic_loss: 0.01529, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18371 
Train Epoch: 42 [166/1000 5312/32000 (17%)] Loss: 1.96135 (semantic_loss: 0.01290, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18749 
Train Epoch: 42 [171/1000 5472/32000 (17%)] Loss: 1.96219 (semantic_loss: 0.01472, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24620 
Train Epoch: 42 [176/1000 5632/32000 (18%)] Loss: 1.96279 (semantic_loss: 0.01630, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.25520 
Train Epoch: 42 [181/1000 5792/32000 (18%)] Loss: 1.96205 (semantic_loss: 0.01555, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18523 
Train Epoch: 42 [186/1000 5952/32000 (19%)] Loss: 1.96263 (semantic_loss: 0.01614, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18656 
Train Epoch: 42 [191/1000 6112/32000 (19%)] Loss: 1.96153 (semantic_loss: 0.01309, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20369 
Train Epoch: 42 [196/1000 6272/32000 (20%)] Loss: 1.96303 (semantic_loss: 0.01557, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19293 
Train Epoch: 42 [201/1000 6432/32000 (20%)] Loss: 1.96232 (semantic_loss: 0.01583, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19590 
Train Epoch: 42 [206/1000 6592/32000 (21%)] Loss: 1.95966 (semantic_loss: 0.01219, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18714 
Train Epoch: 42 [211/1000 6752/32000 (21%)] Loss: 1.96190 (semantic_loss: 0.01443, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18901 
Train Epoch: 42 [216/1000 6912/32000 (22%)] Loss: 1.96034 (semantic_loss: 0.01287, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18654 
Train Epoch: 42 [221/1000 7072/32000 (22%)] Loss: 1.96393 (semantic_loss: 0.01548, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20720 
Train Epoch: 42 [226/1000 7232/32000 (23%)] Loss: 1.96079 (semantic_loss: 0.01234, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20209 
Train Epoch: 42 [231/1000 7392/32000 (23%)] Loss: 1.96310 (semantic_loss: 0.01660, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21112 
Train Epoch: 42 [236/1000 7552/32000 (24%)] Loss: 1.96136 (semantic_loss: 0.01488, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.39361 
Train Epoch: 42 [241/1000 7712/32000 (24%)] Loss: 1.96153 (semantic_loss: 0.01308, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20077 
Train Epoch: 42 [246/1000 7872/32000 (25%)] Loss: 1.96066 (semantic_loss: 0.01222, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20688 
Train Epoch: 42 [251/1000 8032/32000 (25%)] Loss: 1.96100 (semantic_loss: 0.01451, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19373 
Train Epoch: 42 [256/1000 8192/32000 (26%)] Loss: 1.95912 (semantic_loss: 0.01361, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19475 
Train Epoch: 42 [261/1000 8352/32000 (26%)] Loss: 1.96274 (semantic_loss: 0.01430, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18940 
Train Epoch: 42 [266/1000 8512/32000 (27%)] Loss: 1.96024 (semantic_loss: 0.01277, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20422 
Train Epoch: 42 [271/1000 8672/32000 (27%)] Loss: 1.95913 (semantic_loss: 0.01264, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19543 
Train Epoch: 42 [276/1000 8832/32000 (28%)] Loss: 1.95999 (semantic_loss: 0.01349, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24968 
Train Epoch: 42 [281/1000 8992/32000 (28%)] Loss: 1.96247 (semantic_loss: 0.01500, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18815 
Train Epoch: 42 [286/1000 9152/32000 (29%)] Loss: 1.95681 (semantic_loss: 0.01032, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18639 
Train Epoch: 42 [291/1000 9312/32000 (29%)] Loss: 1.96117 (semantic_loss: 0.01468, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22647 
Train Epoch: 42 [296/1000 9472/32000 (30%)] Loss: 1.95830 (semantic_loss: 0.01278, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19863 
Train Epoch: 42 [301/1000 9632/32000 (30%)] Loss: 1.96022 (semantic_loss: 0.01276, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19110 
Train Epoch: 42 [306/1000 9792/32000 (31%)] Loss: 1.96142 (semantic_loss: 0.01396, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.26177 
Train Epoch: 42 [311/1000 9952/32000 (31%)] Loss: 1.96079 (semantic_loss: 0.01527, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18658 
Train Epoch: 42 [316/1000 10112/32000 (32%)] Loss: 1.96056 (semantic_loss: 0.01407, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19293 
Train Epoch: 42 [321/1000 10272/32000 (32%)] Loss: 1.95945 (semantic_loss: 0.01198, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.30727 
Train Epoch: 42 [326/1000 10432/32000 (33%)] Loss: 1.96056 (semantic_loss: 0.01309, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18701 
Train Epoch: 42 [331/1000 10592/32000 (33%)] Loss: 1.96205 (semantic_loss: 0.01361, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18789 
Train Epoch: 42 [336/1000 10752/32000 (34%)] Loss: 1.95847 (semantic_loss: 0.01296, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.65353 
Train Epoch: 42 [341/1000 10912/32000 (34%)] Loss: 1.96667 (semantic_loss: 0.02017, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18665 
Train Epoch: 42 [346/1000 11072/32000 (35%)] Loss: 1.96031 (semantic_loss: 0.01285, quant_loss: 1.94727, bit_balance_loss: 0.00019) batch_time=0.18544 
Train Epoch: 42 [351/1000 11232/32000 (35%)] Loss: 1.96149 (semantic_loss: 0.01304, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18691 
Train Epoch: 42 [356/1000 11392/32000 (36%)] Loss: 1.95985 (semantic_loss: 0.01337, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18795 
Train Epoch: 42 [361/1000 11552/32000 (36%)] Loss: 1.96049 (semantic_loss: 0.01302, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18741 
Train Epoch: 42 [366/1000 11712/32000 (37%)] Loss: 1.96121 (semantic_loss: 0.01277, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.25287 
Train Epoch: 42 [371/1000 11872/32000 (37%)] Loss: 1.96396 (semantic_loss: 0.01649, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24330 
Train Epoch: 42 [376/1000 12032/32000 (38%)] Loss: 1.96453 (semantic_loss: 0.01705, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20556 
Train Epoch: 42 [381/1000 12192/32000 (38%)] Loss: 1.96350 (semantic_loss: 0.01505, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22682 
Train Epoch: 42 [386/1000 12352/32000 (39%)] Loss: 1.96284 (semantic_loss: 0.01635, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21125 
Train Epoch: 42 [391/1000 12512/32000 (39%)] Loss: 1.96387 (semantic_loss: 0.01542, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19158 
Train Epoch: 42 [396/1000 12672/32000 (40%)] Loss: 1.96115 (semantic_loss: 0.01272, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20164 
Train Epoch: 42 [401/1000 12832/32000 (40%)] Loss: 1.96312 (semantic_loss: 0.01565, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19853 
Train Epoch: 42 [406/1000 12992/32000 (41%)] Loss: 1.96155 (semantic_loss: 0.01409, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19195 
Train Epoch: 42 [411/1000 13152/32000 (41%)] Loss: 1.95974 (semantic_loss: 0.01325, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19127 
Train Epoch: 42 [416/1000 13312/32000 (42%)] Loss: 1.96093 (semantic_loss: 0.01248, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21745 
Train Epoch: 42 [421/1000 13472/32000 (42%)] Loss: 1.96183 (semantic_loss: 0.01339, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21336 
Train Epoch: 42 [426/1000 13632/32000 (43%)] Loss: 1.96384 (semantic_loss: 0.01735, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.44606 
Train Epoch: 42 [431/1000 13792/32000 (43%)] Loss: 1.96327 (semantic_loss: 0.01580, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18546 
Train Epoch: 42 [436/1000 13952/32000 (44%)] Loss: 1.96138 (semantic_loss: 0.01392, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18934 
Train Epoch: 42 [441/1000 14112/32000 (44%)] Loss: 1.96025 (semantic_loss: 0.01375, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19322 
Train Epoch: 42 [446/1000 14272/32000 (45%)] Loss: 1.96274 (semantic_loss: 0.01624, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20281 
Train Epoch: 42 [451/1000 14432/32000 (45%)] Loss: 1.96036 (semantic_loss: 0.01290, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19384 
Train Epoch: 42 [456/1000 14592/32000 (46%)] Loss: 1.96334 (semantic_loss: 0.01587, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18582 
Train Epoch: 42 [461/1000 14752/32000 (46%)] Loss: 1.96070 (semantic_loss: 0.01226, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18789 
Train Epoch: 42 [466/1000 14912/32000 (47%)] Loss: 1.95974 (semantic_loss: 0.01325, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18586 
Train Epoch: 42 [471/1000 15072/32000 (47%)] Loss: 1.96570 (semantic_loss: 0.01726, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18746 
Train Epoch: 42 [476/1000 15232/32000 (48%)] Loss: 1.95961 (semantic_loss: 0.01214, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20497 
Train Epoch: 42 [481/1000 15392/32000 (48%)] Loss: 1.96317 (semantic_loss: 0.01570, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18775 
Train Epoch: 42 [486/1000 15552/32000 (49%)] Loss: 1.96382 (semantic_loss: 0.01733, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18877 
Train Epoch: 42 [491/1000 15712/32000 (49%)] Loss: 1.96122 (semantic_loss: 0.01375, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.24838 
Train Epoch: 42 [496/1000 15872/32000 (50%)] Loss: 1.96108 (semantic_loss: 0.01263, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.26215 
Train Epoch: 42 [501/1000 16032/32000 (50%)] Loss: 1.96128 (semantic_loss: 0.01283, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18204 
Train Epoch: 42 [506/1000 16192/32000 (51%)] Loss: 1.96081 (semantic_loss: 0.01334, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20894 
Train Epoch: 42 [511/1000 16352/32000 (51%)] Loss: 1.96192 (semantic_loss: 0.01445, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23060 
Train Epoch: 42 [516/1000 16512/32000 (52%)] Loss: 1.96092 (semantic_loss: 0.01345, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18700 
Train Epoch: 42 [521/1000 16672/32000 (52%)] Loss: 1.96051 (semantic_loss: 0.01304, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18477 
Train Epoch: 42 [526/1000 16832/32000 (53%)] Loss: 1.95983 (semantic_loss: 0.01236, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18549 
Train Epoch: 42 [531/1000 16992/32000 (53%)] Loss: 1.96225 (semantic_loss: 0.01478, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18607 
Train Epoch: 42 [536/1000 17152/32000 (54%)] Loss: 1.96125 (semantic_loss: 0.01378, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20885 
Train Epoch: 42 [541/1000 17312/32000 (54%)] Loss: 1.95792 (semantic_loss: 0.01143, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23596 
Train Epoch: 42 [546/1000 17472/32000 (55%)] Loss: 1.95874 (semantic_loss: 0.01225, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22196 
Train Epoch: 42 [551/1000 17632/32000 (55%)] Loss: 1.96006 (semantic_loss: 0.01357, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19932 
Train Epoch: 42 [556/1000 17792/32000 (56%)] Loss: 1.96145 (semantic_loss: 0.01496, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.34869 
Train Epoch: 42 [561/1000 17952/32000 (56%)] Loss: 1.95988 (semantic_loss: 0.01339, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20284 
Train Epoch: 42 [566/1000 18112/32000 (57%)] Loss: 1.96487 (semantic_loss: 0.01643, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19103 
Train Epoch: 42 [571/1000 18272/32000 (57%)] Loss: 1.95835 (semantic_loss: 0.01187, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19608 
Train Epoch: 42 [576/1000 18432/32000 (58%)] Loss: 1.96119 (semantic_loss: 0.01470, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19249 
Train Epoch: 42 [581/1000 18592/32000 (58%)] Loss: 1.96400 (semantic_loss: 0.01653, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18437 
Train Epoch: 42 [586/1000 18752/32000 (59%)] Loss: 1.95774 (semantic_loss: 0.01125, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20074 
Train Epoch: 42 [591/1000 18912/32000 (59%)] Loss: 1.95981 (semantic_loss: 0.01235, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19278 
Train Epoch: 42 [596/1000 19072/32000 (60%)] Loss: 1.96239 (semantic_loss: 0.01589, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.25461 
Train Epoch: 42 [601/1000 19232/32000 (60%)] Loss: 1.96249 (semantic_loss: 0.01698, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19076 
Train Epoch: 42 [606/1000 19392/32000 (61%)] Loss: 1.95826 (semantic_loss: 0.01177, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19667 
Train Epoch: 42 [611/1000 19552/32000 (61%)] Loss: 1.96055 (semantic_loss: 0.01406, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22555 
Train Epoch: 42 [616/1000 19712/32000 (62%)] Loss: 1.96119 (semantic_loss: 0.01274, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18817 
Train Epoch: 42 [621/1000 19872/32000 (62%)] Loss: 1.96243 (semantic_loss: 0.01495, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18818 
Train Epoch: 42 [626/1000 20032/32000 (63%)] Loss: 1.96219 (semantic_loss: 0.01373, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.26761 
Train Epoch: 42 [631/1000 20192/32000 (63%)] Loss: 1.96025 (semantic_loss: 0.01181, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18733 
Train Epoch: 42 [636/1000 20352/32000 (64%)] Loss: 1.96069 (semantic_loss: 0.01518, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18732 
Train Epoch: 42 [641/1000 20512/32000 (64%)] Loss: 1.96308 (semantic_loss: 0.01658, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.28966 
Train Epoch: 42 [646/1000 20672/32000 (65%)] Loss: 1.95837 (semantic_loss: 0.01188, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18580 
Train Epoch: 42 [651/1000 20832/32000 (65%)] Loss: 1.96082 (semantic_loss: 0.01336, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18628 
Train Epoch: 42 [656/1000 20992/32000 (66%)] Loss: 1.96074 (semantic_loss: 0.01425, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.63281 
Train Epoch: 42 [661/1000 21152/32000 (66%)] Loss: 1.96142 (semantic_loss: 0.01298, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18674 
Train Epoch: 42 [666/1000 21312/32000 (67%)] Loss: 1.96101 (semantic_loss: 0.01451, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18821 
Train Epoch: 42 [671/1000 21472/32000 (67%)] Loss: 1.96340 (semantic_loss: 0.01594, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20191 
Train Epoch: 42 [676/1000 21632/32000 (68%)] Loss: 1.95854 (semantic_loss: 0.01107, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18654 
Train Epoch: 42 [681/1000 21792/32000 (68%)] Loss: 1.96463 (semantic_loss: 0.01814, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18388 
Train Epoch: 42 [686/1000 21952/32000 (69%)] Loss: 1.96265 (semantic_loss: 0.01519, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27217 
Train Epoch: 42 [691/1000 22112/32000 (69%)] Loss: 1.95975 (semantic_loss: 0.01228, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.29337 
Train Epoch: 42 [696/1000 22272/32000 (70%)] Loss: 1.96035 (semantic_loss: 0.01191, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.24747 
Train Epoch: 42 [701/1000 22432/32000 (70%)] Loss: 1.96031 (semantic_loss: 0.01284, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21240 
Train Epoch: 42 [706/1000 22592/32000 (71%)] Loss: 1.96301 (semantic_loss: 0.01554, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20874 
Train Epoch: 42 [711/1000 22752/32000 (71%)] Loss: 1.96143 (semantic_loss: 0.01397, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20989 
Train Epoch: 42 [716/1000 22912/32000 (72%)] Loss: 1.96482 (semantic_loss: 0.01735, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19287 
Train Epoch: 42 [721/1000 23072/32000 (72%)] Loss: 1.96488 (semantic_loss: 0.01741, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18945 
Train Epoch: 42 [726/1000 23232/32000 (73%)] Loss: 1.96208 (semantic_loss: 0.01461, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18574 
Train Epoch: 42 [731/1000 23392/32000 (73%)] Loss: 1.96070 (semantic_loss: 0.01323, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18387 
Train Epoch: 42 [736/1000 23552/32000 (74%)] Loss: 1.96587 (semantic_loss: 0.01743, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18576 
Train Epoch: 42 [741/1000 23712/32000 (74%)] Loss: 1.96301 (semantic_loss: 0.01456, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19008 
Train Epoch: 42 [746/1000 23872/32000 (75%)] Loss: 1.96568 (semantic_loss: 0.01723, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18708 
Train Epoch: 42 [751/1000 24032/32000 (75%)] Loss: 1.96090 (semantic_loss: 0.01440, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19837 
Train Epoch: 42 [756/1000 24192/32000 (76%)] Loss: 1.96371 (semantic_loss: 0.01527, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19868 
Train Epoch: 42 [761/1000 24352/32000 (76%)] Loss: 1.96001 (semantic_loss: 0.01253, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18961 
Train Epoch: 42 [766/1000 24512/32000 (77%)] Loss: 1.96443 (semantic_loss: 0.01598, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19594 
Train Epoch: 42 [771/1000 24672/32000 (77%)] Loss: 1.96067 (semantic_loss: 0.01320, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18742 
Train Epoch: 42 [776/1000 24832/32000 (78%)] Loss: 1.96271 (semantic_loss: 0.01524, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18565 
Train Epoch: 42 [781/1000 24992/32000 (78%)] Loss: 1.96353 (semantic_loss: 0.01607, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18558 
Train Epoch: 42 [786/1000 25152/32000 (79%)] Loss: 1.96112 (semantic_loss: 0.01365, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18507 
Train Epoch: 42 [791/1000 25312/32000 (79%)] Loss: 1.95997 (semantic_loss: 0.01348, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18460 
Train Epoch: 42 [796/1000 25472/32000 (80%)] Loss: 1.95982 (semantic_loss: 0.01234, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20230 
Train Epoch: 42 [801/1000 25632/32000 (80%)] Loss: 1.96138 (semantic_loss: 0.01392, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18619 
Train Epoch: 42 [806/1000 25792/32000 (81%)] Loss: 1.96378 (semantic_loss: 0.01631, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18345 
Train Epoch: 42 [811/1000 25952/32000 (81%)] Loss: 1.96090 (semantic_loss: 0.01344, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.26015 
Train Epoch: 42 [816/1000 26112/32000 (82%)] Loss: 1.96351 (semantic_loss: 0.01507, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.25775 
Train Epoch: 42 [821/1000 26272/32000 (82%)] Loss: 1.95971 (semantic_loss: 0.01225, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18543 
Train Epoch: 42 [826/1000 26432/32000 (83%)] Loss: 1.96394 (semantic_loss: 0.01745, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18895 
Train Epoch: 42 [831/1000 26592/32000 (83%)] Loss: 1.96133 (semantic_loss: 0.01484, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20325 
Train Epoch: 42 [836/1000 26752/32000 (84%)] Loss: 1.96354 (semantic_loss: 0.01607, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18608 
Train Epoch: 42 [841/1000 26912/32000 (84%)] Loss: 1.96001 (semantic_loss: 0.01352, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18571 
Train Epoch: 42 [846/1000 27072/32000 (85%)] Loss: 1.96168 (semantic_loss: 0.01519, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20813 
Train Epoch: 42 [851/1000 27232/32000 (85%)] Loss: 1.96512 (semantic_loss: 0.01765, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22533 
Train Epoch: 42 [856/1000 27392/32000 (86%)] Loss: 1.96143 (semantic_loss: 0.01494, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21831 
Train Epoch: 42 [861/1000 27552/32000 (86%)] Loss: 1.96110 (semantic_loss: 0.01266, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21782 
Train Epoch: 42 [866/1000 27712/32000 (87%)] Loss: 1.96120 (semantic_loss: 0.01472, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20939 
Train Epoch: 42 [871/1000 27872/32000 (87%)] Loss: 1.96437 (semantic_loss: 0.01690, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19413 
Train Epoch: 42 [876/1000 28032/32000 (88%)] Loss: 1.96213 (semantic_loss: 0.01368, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.34708 
Train Epoch: 42 [881/1000 28192/32000 (88%)] Loss: 1.96184 (semantic_loss: 0.01340, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18814 
Train Epoch: 42 [886/1000 28352/32000 (89%)] Loss: 1.95838 (semantic_loss: 0.01287, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18848 
Train Epoch: 42 [891/1000 28512/32000 (89%)] Loss: 1.96632 (semantic_loss: 0.01787, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18747 
Train Epoch: 42 [896/1000 28672/32000 (90%)] Loss: 1.96049 (semantic_loss: 0.01302, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18575 
Train Epoch: 42 [901/1000 28832/32000 (90%)] Loss: 1.96129 (semantic_loss: 0.01383, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18616 
Train Epoch: 42 [906/1000 28992/32000 (91%)] Loss: 1.96329 (semantic_loss: 0.01485, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20399 
Train Epoch: 42 [911/1000 29152/32000 (91%)] Loss: 1.96270 (semantic_loss: 0.01523, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21435 
Train Epoch: 42 [916/1000 29312/32000 (92%)] Loss: 1.96184 (semantic_loss: 0.01438, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.25841 
Train Epoch: 42 [921/1000 29472/32000 (92%)] Loss: 1.96525 (semantic_loss: 0.01778, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19313 
Train Epoch: 42 [926/1000 29632/32000 (93%)] Loss: 1.96224 (semantic_loss: 0.01478, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18382 
Train Epoch: 42 [931/1000 29792/32000 (93%)] Loss: 1.96270 (semantic_loss: 0.01620, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21645 
Train Epoch: 42 [936/1000 29952/32000 (94%)] Loss: 1.96110 (semantic_loss: 0.01461, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18474 
Train Epoch: 42 [941/1000 30112/32000 (94%)] Loss: 1.96175 (semantic_loss: 0.01331, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18446 
Train Epoch: 42 [946/1000 30272/32000 (95%)] Loss: 1.96457 (semantic_loss: 0.01613, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.26277 
Train Epoch: 42 [951/1000 30432/32000 (95%)] Loss: 1.96200 (semantic_loss: 0.01454, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18427 
Train Epoch: 42 [956/1000 30592/32000 (96%)] Loss: 1.96494 (semantic_loss: 0.01649, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18384 
Train Epoch: 42 [961/1000 30752/32000 (96%)] Loss: 1.95860 (semantic_loss: 0.01309, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.30434 
Train Epoch: 42 [966/1000 30912/32000 (97%)] Loss: 1.96019 (semantic_loss: 0.01272, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20370 
Train Epoch: 42 [971/1000 31072/32000 (97%)] Loss: 1.95995 (semantic_loss: 0.01249, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18724 
Train Epoch: 42 [976/1000 31232/32000 (98%)] Loss: 1.96509 (semantic_loss: 0.01860, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.66447 
Train Epoch: 42 [981/1000 31392/32000 (98%)] Loss: 1.95811 (semantic_loss: 0.01162, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18887 
Train Epoch: 42 [986/1000 31552/32000 (99%)] Loss: 1.96216 (semantic_loss: 0.01371, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19453 
Train Epoch: 42 [991/1000 31712/32000 (99%)] Loss: 1.96299 (semantic_loss: 0.01454, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20278 
Train Epoch: 42 [996/1000 31872/32000 (100%)] Loss: 1.96072 (semantic_loss: 0.01228, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20775 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch42.pth ...
Done in 4.432s
Updating 'best' checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch42.pth ...
Done in 8.546s
removing stale ckpt [epoch 41] [took 0.01s]
 epoch          : 42
 loss           : 1.9618283565044403
 learning_rate  : 6.651397323645573e-07
 n_samples      : 1344000
 n_steps        : 42000
 ActivityNet_val1_test/t2v_metrics/R1: 12.34492576774456
 ActivityNet_val1_test/t2v_metrics/R5: 38.39739678665853
 ActivityNet_val1_test/t2v_metrics/R10: 55.623347569656296
 ActivityNet_val1_test/t2v_metrics/R50: 84.62477120195241
 ActivityNet_val1_test/t2v_metrics/MedR: 8.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 63.4082774049217
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.76339460386141
 ActivityNet_val1_test/v2t_metrics/R1: 12.446613788895668
 ActivityNet_val1_test/v2t_metrics/R5: 39.190563351637174
 ActivityNet_val1_test/v2t_metrics/R10: 55.521659548505184
 ActivityNet_val1_test/v2t_metrics/R50: 84.58409599349197
 ActivityNet_val1_test/v2t_metrics/MedR: 8.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 65.42942851332113
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 30.030672167495368
 mnt_best       : 29.76339460386141
 not_improved_count: 0
Train Epoch: 43 [1/1000 32/32000 (0%)] Loss: 1.96105 (semantic_loss: 0.01456, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=23.02622 
Train Epoch: 43 [6/1000 192/32000 (1%)] Loss: 1.96288 (semantic_loss: 0.01444, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18709 
Train Epoch: 43 [11/1000 352/32000 (1%)] Loss: 1.96321 (semantic_loss: 0.01477, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18784 
Train Epoch: 43 [16/1000 512/32000 (2%)] Loss: 1.96063 (semantic_loss: 0.01317, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19566 
Train Epoch: 43 [21/1000 672/32000 (2%)] Loss: 1.96054 (semantic_loss: 0.01405, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19522 
Train Epoch: 43 [26/1000 832/32000 (3%)] Loss: 1.96245 (semantic_loss: 0.01498, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20811 
Train Epoch: 43 [31/1000 992/32000 (3%)] Loss: 1.96124 (semantic_loss: 0.01475, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20034 
Train Epoch: 43 [36/1000 1152/32000 (4%)] Loss: 1.96359 (semantic_loss: 0.01515, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18746 
Train Epoch: 43 [41/1000 1312/32000 (4%)] Loss: 1.96262 (semantic_loss: 0.01516, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18684 
Train Epoch: 43 [46/1000 1472/32000 (5%)] Loss: 1.96410 (semantic_loss: 0.01566, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18662 
Train Epoch: 43 [51/1000 1632/32000 (5%)] Loss: 1.96649 (semantic_loss: 0.01707, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18867 
Train Epoch: 43 [56/1000 1792/32000 (6%)] Loss: 1.96230 (semantic_loss: 0.01385, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19025 
Train Epoch: 43 [61/1000 1952/32000 (6%)] Loss: 1.96045 (semantic_loss: 0.01299, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18537 
Train Epoch: 43 [66/1000 2112/32000 (7%)] Loss: 1.96628 (semantic_loss: 0.01882, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18728 
Train Epoch: 43 [71/1000 2272/32000 (7%)] Loss: 1.95933 (semantic_loss: 0.01284, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19782 
Train Epoch: 43 [76/1000 2432/32000 (8%)] Loss: 1.96190 (semantic_loss: 0.01345, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18636 
Train Epoch: 43 [81/1000 2592/32000 (8%)] Loss: 1.96477 (semantic_loss: 0.01731, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18571 
Train Epoch: 43 [86/1000 2752/32000 (9%)] Loss: 1.95746 (semantic_loss: 0.01195, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18575 
Train Epoch: 43 [91/1000 2912/32000 (9%)] Loss: 1.96111 (semantic_loss: 0.01364, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.43745 
Train Epoch: 43 [96/1000 3072/32000 (10%)] Loss: 1.96312 (semantic_loss: 0.01663, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19773 
Train Epoch: 43 [101/1000 3232/32000 (10%)] Loss: 1.96130 (semantic_loss: 0.01383, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18918 
Train Epoch: 43 [106/1000 3392/32000 (11%)] Loss: 1.96069 (semantic_loss: 0.01225, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18715 
Train Epoch: 43 [111/1000 3552/32000 (11%)] Loss: 1.96177 (semantic_loss: 0.01429, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18757 
Train Epoch: 43 [116/1000 3712/32000 (12%)] Loss: 1.96163 (semantic_loss: 0.01416, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.33314 
Train Epoch: 43 [121/1000 3872/32000 (12%)] Loss: 1.95903 (semantic_loss: 0.01253, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23232 
Train Epoch: 43 [126/1000 4032/32000 (13%)] Loss: 1.96187 (semantic_loss: 0.01538, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22213 
Train Epoch: 43 [131/1000 4192/32000 (13%)] Loss: 1.96037 (semantic_loss: 0.01192, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22182 
Train Epoch: 43 [136/1000 4352/32000 (14%)] Loss: 1.96115 (semantic_loss: 0.01368, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22833 
Train Epoch: 43 [141/1000 4512/32000 (14%)] Loss: 1.96399 (semantic_loss: 0.01652, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20629 
Train Epoch: 43 [146/1000 4672/32000 (15%)] Loss: 1.96092 (semantic_loss: 0.01247, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21128 
Train Epoch: 43 [151/1000 4832/32000 (15%)] Loss: 1.95868 (semantic_loss: 0.01317, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18903 
Train Epoch: 43 [156/1000 4992/32000 (16%)] Loss: 1.95881 (semantic_loss: 0.01232, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18529 
Train Epoch: 43 [161/1000 5152/32000 (16%)] Loss: 1.96248 (semantic_loss: 0.01501, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19066 
Train Epoch: 43 [166/1000 5312/32000 (17%)] Loss: 1.96597 (semantic_loss: 0.01753, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18959 
Train Epoch: 43 [171/1000 5472/32000 (17%)] Loss: 1.96306 (semantic_loss: 0.01559, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20922 
Train Epoch: 43 [176/1000 5632/32000 (18%)] Loss: 1.96277 (semantic_loss: 0.01530, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18497 
Train Epoch: 43 [181/1000 5792/32000 (18%)] Loss: 1.96237 (semantic_loss: 0.01588, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19042 
Train Epoch: 43 [186/1000 5952/32000 (19%)] Loss: 1.96080 (semantic_loss: 0.01333, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19123 
Train Epoch: 43 [191/1000 6112/32000 (19%)] Loss: 1.96116 (semantic_loss: 0.01467, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20074 
Train Epoch: 43 [196/1000 6272/32000 (20%)] Loss: 1.96093 (semantic_loss: 0.01444, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23422 
Train Epoch: 43 [201/1000 6432/32000 (20%)] Loss: 1.96169 (semantic_loss: 0.01423, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23837 
Train Epoch: 43 [206/1000 6592/32000 (21%)] Loss: 1.95899 (semantic_loss: 0.01152, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19587 
Train Epoch: 43 [211/1000 6752/32000 (21%)] Loss: 1.96027 (semantic_loss: 0.01378, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18817 
Train Epoch: 43 [216/1000 6912/32000 (22%)] Loss: 1.95950 (semantic_loss: 0.01203, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18873 
Train Epoch: 43 [221/1000 7072/32000 (22%)] Loss: 1.96240 (semantic_loss: 0.01297, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.18810 
Train Epoch: 43 [226/1000 7232/32000 (23%)] Loss: 1.95940 (semantic_loss: 0.01291, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18737 
Train Epoch: 43 [231/1000 7392/32000 (23%)] Loss: 1.96113 (semantic_loss: 0.01464, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18861 
Train Epoch: 43 [236/1000 7552/32000 (24%)] Loss: 1.96286 (semantic_loss: 0.01539, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19424 
Train Epoch: 43 [241/1000 7712/32000 (24%)] Loss: 1.96822 (semantic_loss: 0.02074, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18839 
Train Epoch: 43 [246/1000 7872/32000 (25%)] Loss: 1.96287 (semantic_loss: 0.01540, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18482 
Train Epoch: 43 [251/1000 8032/32000 (25%)] Loss: 1.96249 (semantic_loss: 0.01600, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19821 
Train Epoch: 43 [256/1000 8192/32000 (26%)] Loss: 1.96371 (semantic_loss: 0.01527, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18681 
Train Epoch: 43 [261/1000 8352/32000 (26%)] Loss: 1.96182 (semantic_loss: 0.01435, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18807 
Train Epoch: 43 [266/1000 8512/32000 (27%)] Loss: 1.96446 (semantic_loss: 0.01602, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.26903 
Train Epoch: 43 [271/1000 8672/32000 (27%)] Loss: 1.96021 (semantic_loss: 0.01469, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.25044 
Train Epoch: 43 [276/1000 8832/32000 (28%)] Loss: 1.96272 (semantic_loss: 0.01428, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=1.11733 
Train Epoch: 43 [281/1000 8992/32000 (28%)] Loss: 1.96177 (semantic_loss: 0.01332, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21329 
Train Epoch: 43 [286/1000 9152/32000 (29%)] Loss: 1.95951 (semantic_loss: 0.01302, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20718 
Train Epoch: 43 [291/1000 9312/32000 (29%)] Loss: 1.96227 (semantic_loss: 0.01577, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20541 
Train Epoch: 43 [296/1000 9472/32000 (30%)] Loss: 1.96089 (semantic_loss: 0.01342, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20205 
Train Epoch: 43 [301/1000 9632/32000 (30%)] Loss: 1.96038 (semantic_loss: 0.01389, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21545 
Train Epoch: 43 [306/1000 9792/32000 (31%)] Loss: 1.96117 (semantic_loss: 0.01370, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19278 
Train Epoch: 43 [311/1000 9952/32000 (31%)] Loss: 1.96142 (semantic_loss: 0.01396, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19946 
Train Epoch: 43 [316/1000 10112/32000 (32%)] Loss: 1.96145 (semantic_loss: 0.01397, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18962 
Train Epoch: 43 [321/1000 10272/32000 (32%)] Loss: 1.96058 (semantic_loss: 0.01312, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22209 
Train Epoch: 43 [326/1000 10432/32000 (33%)] Loss: 1.95819 (semantic_loss: 0.01170, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18798 
Train Epoch: 43 [331/1000 10592/32000 (33%)] Loss: 1.96468 (semantic_loss: 0.01623, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18757 
Train Epoch: 43 [336/1000 10752/32000 (34%)] Loss: 1.96357 (semantic_loss: 0.01610, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18854 
Train Epoch: 43 [341/1000 10912/32000 (34%)] Loss: 1.96308 (semantic_loss: 0.01464, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19040 
Train Epoch: 43 [346/1000 11072/32000 (35%)] Loss: 1.96186 (semantic_loss: 0.01440, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.48181 
Train Epoch: 43 [351/1000 11232/32000 (35%)] Loss: 1.96094 (semantic_loss: 0.01348, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18889 
Train Epoch: 43 [356/1000 11392/32000 (36%)] Loss: 1.96244 (semantic_loss: 0.01400, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19112 
Train Epoch: 43 [361/1000 11552/32000 (36%)] Loss: 1.96484 (semantic_loss: 0.01737, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19003 
Train Epoch: 43 [366/1000 11712/32000 (37%)] Loss: 1.96310 (semantic_loss: 0.01563, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19274 
Train Epoch: 43 [371/1000 11872/32000 (37%)] Loss: 1.96128 (semantic_loss: 0.01284, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19003 
Train Epoch: 43 [376/1000 12032/32000 (38%)] Loss: 1.95940 (semantic_loss: 0.01389, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18835 
Train Epoch: 43 [381/1000 12192/32000 (38%)] Loss: 1.96374 (semantic_loss: 0.01628, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18827 
Train Epoch: 43 [386/1000 12352/32000 (39%)] Loss: 1.95880 (semantic_loss: 0.01231, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18813 
Train Epoch: 43 [391/1000 12512/32000 (39%)] Loss: 1.96098 (semantic_loss: 0.01351, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19480 
Train Epoch: 43 [396/1000 12672/32000 (40%)] Loss: 1.95942 (semantic_loss: 0.01292, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18694 
Train Epoch: 43 [401/1000 12832/32000 (40%)] Loss: 1.96254 (semantic_loss: 0.01507, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18522 
Train Epoch: 43 [406/1000 12992/32000 (41%)] Loss: 1.96055 (semantic_loss: 0.01406, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18687 
Train Epoch: 43 [411/1000 13152/32000 (41%)] Loss: 1.96737 (semantic_loss: 0.01893, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.41990 
Train Epoch: 43 [416/1000 13312/32000 (42%)] Loss: 1.95908 (semantic_loss: 0.01162, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18824 
Train Epoch: 43 [421/1000 13472/32000 (42%)] Loss: 1.95988 (semantic_loss: 0.01339, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18538 
Train Epoch: 43 [426/1000 13632/32000 (43%)] Loss: 1.96282 (semantic_loss: 0.01633, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18919 
Train Epoch: 43 [431/1000 13792/32000 (43%)] Loss: 1.96362 (semantic_loss: 0.01615, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18847 
Train Epoch: 43 [436/1000 13952/32000 (44%)] Loss: 1.96523 (semantic_loss: 0.01679, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.31872 
Train Epoch: 43 [441/1000 14112/32000 (44%)] Loss: 1.96167 (semantic_loss: 0.01323, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23019 
Train Epoch: 43 [446/1000 14272/32000 (45%)] Loss: 1.96265 (semantic_loss: 0.01421, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22345 
Train Epoch: 43 [451/1000 14432/32000 (45%)] Loss: 1.96021 (semantic_loss: 0.01274, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21322 
Train Epoch: 43 [456/1000 14592/32000 (46%)] Loss: 1.96042 (semantic_loss: 0.01295, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20896 
Train Epoch: 43 [461/1000 14752/32000 (46%)] Loss: 1.96115 (semantic_loss: 0.01466, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19222 
Train Epoch: 43 [466/1000 14912/32000 (47%)] Loss: 1.96530 (semantic_loss: 0.01685, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21540 
Train Epoch: 43 [471/1000 15072/32000 (47%)] Loss: 1.96456 (semantic_loss: 0.01710, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20305 
Train Epoch: 43 [476/1000 15232/32000 (48%)] Loss: 1.96210 (semantic_loss: 0.01463, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18302 
Train Epoch: 43 [481/1000 15392/32000 (48%)] Loss: 1.96344 (semantic_loss: 0.01597, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18121 
Train Epoch: 43 [486/1000 15552/32000 (49%)] Loss: 1.95949 (semantic_loss: 0.01202, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18790 
Train Epoch: 43 [491/1000 15712/32000 (49%)] Loss: 1.95997 (semantic_loss: 0.01250, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18984 
Train Epoch: 43 [496/1000 15872/32000 (50%)] Loss: 1.95996 (semantic_loss: 0.01346, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18965 
Train Epoch: 43 [501/1000 16032/32000 (50%)] Loss: 1.96663 (semantic_loss: 0.01818, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19827 
Train Epoch: 43 [506/1000 16192/32000 (51%)] Loss: 1.96224 (semantic_loss: 0.01477, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19076 
Train Epoch: 43 [511/1000 16352/32000 (51%)] Loss: 1.96218 (semantic_loss: 0.01472, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19388 
Train Epoch: 43 [516/1000 16512/32000 (52%)] Loss: 1.96222 (semantic_loss: 0.01572, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.23614 
Train Epoch: 43 [521/1000 16672/32000 (52%)] Loss: 1.95933 (semantic_loss: 0.01186, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18840 
Train Epoch: 43 [526/1000 16832/32000 (53%)] Loss: 1.96170 (semantic_loss: 0.01423, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19336 
Train Epoch: 43 [531/1000 16992/32000 (53%)] Loss: 1.95941 (semantic_loss: 0.01389, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.22564 
Train Epoch: 43 [536/1000 17152/32000 (54%)] Loss: 1.96149 (semantic_loss: 0.01305, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18588 
Train Epoch: 43 [541/1000 17312/32000 (54%)] Loss: 1.96247 (semantic_loss: 0.01598, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19539 
Train Epoch: 43 [546/1000 17472/32000 (55%)] Loss: 1.95941 (semantic_loss: 0.01291, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18857 
Train Epoch: 43 [551/1000 17632/32000 (55%)] Loss: 1.96513 (semantic_loss: 0.01668, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18850 
Train Epoch: 43 [556/1000 17792/32000 (56%)] Loss: 1.95929 (semantic_loss: 0.01182, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18823 
Train Epoch: 43 [561/1000 17952/32000 (56%)] Loss: 1.96068 (semantic_loss: 0.01126, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18799 
Train Epoch: 43 [566/1000 18112/32000 (57%)] Loss: 1.96153 (semantic_loss: 0.01212, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18773 
Train Epoch: 43 [571/1000 18272/32000 (57%)] Loss: 1.96445 (semantic_loss: 0.01698, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18617 
Train Epoch: 43 [576/1000 18432/32000 (58%)] Loss: 1.96099 (semantic_loss: 0.01255, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18720 
Train Epoch: 43 [581/1000 18592/32000 (58%)] Loss: 1.96236 (semantic_loss: 0.01586, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18794 
Train Epoch: 43 [586/1000 18752/32000 (59%)] Loss: 1.96082 (semantic_loss: 0.01433, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.28766 
Train Epoch: 43 [591/1000 18912/32000 (59%)] Loss: 1.96184 (semantic_loss: 0.01437, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.30306 
Train Epoch: 43 [596/1000 19072/32000 (60%)] Loss: 1.96532 (semantic_loss: 0.01688, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=1.20086 
Train Epoch: 43 [601/1000 19232/32000 (60%)] Loss: 1.95984 (semantic_loss: 0.01237, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19976 
Train Epoch: 43 [606/1000 19392/32000 (61%)] Loss: 1.96249 (semantic_loss: 0.01502, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21954 
Train Epoch: 43 [611/1000 19552/32000 (61%)] Loss: 1.96395 (semantic_loss: 0.01551, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20138 
Train Epoch: 43 [616/1000 19712/32000 (62%)] Loss: 1.96132 (semantic_loss: 0.01482, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19906 
Train Epoch: 43 [621/1000 19872/32000 (62%)] Loss: 1.96134 (semantic_loss: 0.01387, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20433 
Train Epoch: 43 [626/1000 20032/32000 (63%)] Loss: 1.96555 (semantic_loss: 0.01809, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19565 
Train Epoch: 43 [631/1000 20192/32000 (63%)] Loss: 1.95994 (semantic_loss: 0.01247, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18716 
Train Epoch: 43 [636/1000 20352/32000 (64%)] Loss: 1.96130 (semantic_loss: 0.01480, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19782 
Train Epoch: 43 [641/1000 20512/32000 (64%)] Loss: 1.96494 (semantic_loss: 0.01746, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21969 
Train Epoch: 43 [646/1000 20672/32000 (65%)] Loss: 1.96498 (semantic_loss: 0.01752, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19159 
Train Epoch: 43 [651/1000 20832/32000 (65%)] Loss: 1.95989 (semantic_loss: 0.01242, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20381 
Train Epoch: 43 [656/1000 20992/32000 (66%)] Loss: 1.96065 (semantic_loss: 0.01318, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19106 
Train Epoch: 43 [661/1000 21152/32000 (66%)] Loss: 1.96398 (semantic_loss: 0.01554, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23096 
Train Epoch: 43 [666/1000 21312/32000 (67%)] Loss: 1.96097 (semantic_loss: 0.01350, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18742 
Train Epoch: 43 [671/1000 21472/32000 (67%)] Loss: 1.96188 (semantic_loss: 0.01442, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18613 
Train Epoch: 43 [676/1000 21632/32000 (68%)] Loss: 1.96067 (semantic_loss: 0.01320, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18719 
Train Epoch: 43 [681/1000 21792/32000 (68%)] Loss: 1.96152 (semantic_loss: 0.01405, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18560 
Train Epoch: 43 [686/1000 21952/32000 (69%)] Loss: 1.95808 (semantic_loss: 0.01158, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18507 
Train Epoch: 43 [691/1000 22112/32000 (69%)] Loss: 1.96147 (semantic_loss: 0.01401, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18475 
Train Epoch: 43 [696/1000 22272/32000 (70%)] Loss: 1.96427 (semantic_loss: 0.01485, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18654 
Train Epoch: 43 [701/1000 22432/32000 (70%)] Loss: 1.95992 (semantic_loss: 0.01246, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18459 
Train Epoch: 43 [706/1000 22592/32000 (71%)] Loss: 1.96055 (semantic_loss: 0.01309, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18525 
Train Epoch: 43 [711/1000 22752/32000 (71%)] Loss: 1.95995 (semantic_loss: 0.01151, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19312 
Train Epoch: 43 [716/1000 22912/32000 (72%)] Loss: 1.95725 (semantic_loss: 0.01076, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18903 
Train Epoch: 43 [721/1000 23072/32000 (72%)] Loss: 1.96102 (semantic_loss: 0.01355, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19139 
Train Epoch: 43 [726/1000 23232/32000 (73%)] Loss: 1.96411 (semantic_loss: 0.01664, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20084 
Train Epoch: 43 [731/1000 23392/32000 (73%)] Loss: 1.96415 (semantic_loss: 0.01571, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.46032 
Train Epoch: 43 [736/1000 23552/32000 (74%)] Loss: 1.96356 (semantic_loss: 0.01512, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21044 
Train Epoch: 43 [741/1000 23712/32000 (74%)] Loss: 1.96162 (semantic_loss: 0.01415, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18761 
Train Epoch: 43 [746/1000 23872/32000 (75%)] Loss: 1.96084 (semantic_loss: 0.01435, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18797 
Train Epoch: 43 [751/1000 24032/32000 (75%)] Loss: 1.96120 (semantic_loss: 0.01373, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23582 
Train Epoch: 43 [756/1000 24192/32000 (76%)] Loss: 1.96198 (semantic_loss: 0.01354, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.35660 
Train Epoch: 43 [761/1000 24352/32000 (76%)] Loss: 1.96146 (semantic_loss: 0.01399, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21488 
Train Epoch: 43 [766/1000 24512/32000 (77%)] Loss: 1.95917 (semantic_loss: 0.01268, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21957 
Train Epoch: 43 [771/1000 24672/32000 (77%)] Loss: 1.95966 (semantic_loss: 0.01415, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.20617 
Train Epoch: 43 [776/1000 24832/32000 (78%)] Loss: 1.96152 (semantic_loss: 0.01406, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20569 
Train Epoch: 43 [781/1000 24992/32000 (78%)] Loss: 1.96633 (semantic_loss: 0.01886, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20356 
Train Epoch: 43 [786/1000 25152/32000 (79%)] Loss: 1.96014 (semantic_loss: 0.01267, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22200 
Train Epoch: 43 [791/1000 25312/32000 (79%)] Loss: 1.96249 (semantic_loss: 0.01405, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18886 
Train Epoch: 43 [796/1000 25472/32000 (80%)] Loss: 1.95997 (semantic_loss: 0.01250, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18728 
Train Epoch: 43 [801/1000 25632/32000 (80%)] Loss: 1.96050 (semantic_loss: 0.01401, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19137 
Train Epoch: 43 [806/1000 25792/32000 (81%)] Loss: 1.95814 (semantic_loss: 0.01165, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19058 
Train Epoch: 43 [811/1000 25952/32000 (81%)] Loss: 1.96284 (semantic_loss: 0.01537, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19944 
Train Epoch: 43 [816/1000 26112/32000 (82%)] Loss: 1.96081 (semantic_loss: 0.01334, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20157 
Train Epoch: 43 [821/1000 26272/32000 (82%)] Loss: 1.95952 (semantic_loss: 0.01303, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19217 
Train Epoch: 43 [826/1000 26432/32000 (83%)] Loss: 1.96003 (semantic_loss: 0.01354, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19841 
Train Epoch: 43 [831/1000 26592/32000 (83%)] Loss: 1.96189 (semantic_loss: 0.01443, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18808 
Train Epoch: 43 [836/1000 26752/32000 (84%)] Loss: 1.96083 (semantic_loss: 0.01336, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23734 
Train Epoch: 43 [841/1000 26912/32000 (84%)] Loss: 1.96097 (semantic_loss: 0.01350, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22111 
Train Epoch: 43 [846/1000 27072/32000 (85%)] Loss: 1.96141 (semantic_loss: 0.01394, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18532 
Train Epoch: 43 [851/1000 27232/32000 (85%)] Loss: 1.96360 (semantic_loss: 0.01711, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18717 
Train Epoch: 43 [856/1000 27392/32000 (86%)] Loss: 1.96171 (semantic_loss: 0.01424, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18799 
Train Epoch: 43 [861/1000 27552/32000 (86%)] Loss: 1.96277 (semantic_loss: 0.01530, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18512 
Train Epoch: 43 [866/1000 27712/32000 (87%)] Loss: 1.96114 (semantic_loss: 0.01368, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18451 
Train Epoch: 43 [871/1000 27872/32000 (87%)] Loss: 1.96107 (semantic_loss: 0.01263, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18775 
Train Epoch: 43 [876/1000 28032/32000 (88%)] Loss: 1.95804 (semantic_loss: 0.01155, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18881 
Train Epoch: 43 [881/1000 28192/32000 (88%)] Loss: 1.96504 (semantic_loss: 0.01854, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18729 
Train Epoch: 43 [886/1000 28352/32000 (89%)] Loss: 1.96026 (semantic_loss: 0.01377, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18805 
Train Epoch: 43 [891/1000 28512/32000 (89%)] Loss: 1.96085 (semantic_loss: 0.01339, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18831 
Train Epoch: 43 [896/1000 28672/32000 (90%)] Loss: 1.96505 (semantic_loss: 0.01759, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18997 
Train Epoch: 43 [901/1000 28832/32000 (90%)] Loss: 1.96579 (semantic_loss: 0.01832, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19274 
Train Epoch: 43 [906/1000 28992/32000 (91%)] Loss: 1.95994 (semantic_loss: 0.01246, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26350 
Train Epoch: 43 [911/1000 29152/32000 (91%)] Loss: 1.96219 (semantic_loss: 0.01374, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.29101 
Train Epoch: 43 [916/1000 29312/32000 (92%)] Loss: 1.96163 (semantic_loss: 0.01514, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=1.20122 
Train Epoch: 43 [921/1000 29472/32000 (92%)] Loss: 1.95867 (semantic_loss: 0.01218, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20850 
Train Epoch: 43 [926/1000 29632/32000 (93%)] Loss: 1.96311 (semantic_loss: 0.01662, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20247 
Train Epoch: 43 [931/1000 29792/32000 (93%)] Loss: 1.96199 (semantic_loss: 0.01453, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19611 
Train Epoch: 43 [936/1000 29952/32000 (94%)] Loss: 1.96016 (semantic_loss: 0.01368, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20720 
Train Epoch: 43 [941/1000 30112/32000 (94%)] Loss: 1.96647 (semantic_loss: 0.01802, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21476 
Train Epoch: 43 [946/1000 30272/32000 (95%)] Loss: 1.95954 (semantic_loss: 0.01207, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18954 
Train Epoch: 43 [951/1000 30432/32000 (95%)] Loss: 1.95923 (semantic_loss: 0.01275, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18723 
Train Epoch: 43 [956/1000 30592/32000 (96%)] Loss: 1.95979 (semantic_loss: 0.01330, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18875 
Train Epoch: 43 [961/1000 30752/32000 (96%)] Loss: 1.96648 (semantic_loss: 0.01901, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22553 
Train Epoch: 43 [966/1000 30912/32000 (97%)] Loss: 1.96001 (semantic_loss: 0.01254, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18860 
Train Epoch: 43 [971/1000 31072/32000 (97%)] Loss: 1.96061 (semantic_loss: 0.01510, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.20971 
Train Epoch: 43 [976/1000 31232/32000 (98%)] Loss: 1.95971 (semantic_loss: 0.01322, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19235 
Train Epoch: 43 [981/1000 31392/32000 (98%)] Loss: 1.96337 (semantic_loss: 0.01590, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19005 
Train Epoch: 43 [986/1000 31552/32000 (99%)] Loss: 1.96093 (semantic_loss: 0.01346, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19144 
Train Epoch: 43 [991/1000 31712/32000 (99%)] Loss: 1.96087 (semantic_loss: 0.01341, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18901 
Train Epoch: 43 [996/1000 31872/32000 (100%)] Loss: 1.95831 (semantic_loss: 0.01182, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18860 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch43.pth ...
Done in 5.707s
removing stale ckpt [epoch 42] [took 0.00s]
 epoch          : 43
 loss           : 1.9616126945018768
 learning_rate  : 5.986257591281016e-07
 n_samples      : 1376000
 n_steps        : 43000
 ActivityNet_val1_test/t2v_metrics/R1: 11.470408785845027
 ActivityNet_val1_test/t2v_metrics/R5: 38.33638397396787
 ActivityNet_val1_test/t2v_metrics/R10: 55.521659548505184
 ActivityNet_val1_test/t2v_metrics/R50: 84.84848484848484
 ActivityNet_val1_test/t2v_metrics/MedR: 8.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 62.711307707952
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.010206947322136
 ActivityNet_val1_test/v2t_metrics/R1: 12.487288997356112
 ActivityNet_val1_test/v2t_metrics/R5: 38.62111043319097
 ActivityNet_val1_test/v2t_metrics/R10: 55.277608297742525
 ActivityNet_val1_test/v2t_metrics/R50: 84.86882245271507
 ActivityNet_val1_test/v2t_metrics/MedR: 9.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 65.06202969290217
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.873129471723686
 mnt_best       : 29.76339460386141
 not_improved_count: 1
Train Epoch: 44 [1/1000 32/32000 (0%)] Loss: 1.95800 (semantic_loss: 0.01151, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=22.31973 
Train Epoch: 44 [6/1000 192/32000 (1%)] Loss: 1.95953 (semantic_loss: 0.01207, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18722 
Train Epoch: 44 [11/1000 352/32000 (1%)] Loss: 1.96046 (semantic_loss: 0.01398, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18762 
Train Epoch: 44 [16/1000 512/32000 (2%)] Loss: 1.96278 (semantic_loss: 0.01434, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.53441 
Train Epoch: 44 [21/1000 672/32000 (2%)] Loss: 1.96072 (semantic_loss: 0.01423, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.30467 
Train Epoch: 44 [26/1000 832/32000 (3%)] Loss: 1.96497 (semantic_loss: 0.01847, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19542 
Train Epoch: 44 [31/1000 992/32000 (3%)] Loss: 1.96140 (semantic_loss: 0.01296, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19544 
Train Epoch: 44 [36/1000 1152/32000 (4%)] Loss: 1.96255 (semantic_loss: 0.01508, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18735 
Train Epoch: 44 [41/1000 1312/32000 (4%)] Loss: 1.96107 (semantic_loss: 0.01360, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.28814 
Train Epoch: 44 [46/1000 1472/32000 (5%)] Loss: 1.96060 (semantic_loss: 0.01314, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24966 
Train Epoch: 44 [51/1000 1632/32000 (5%)] Loss: 1.95959 (semantic_loss: 0.01212, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22109 
Train Epoch: 44 [56/1000 1792/32000 (6%)] Loss: 1.96021 (semantic_loss: 0.01274, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23316 
Train Epoch: 44 [61/1000 1952/32000 (6%)] Loss: 1.96116 (semantic_loss: 0.01369, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.25390 
Train Epoch: 44 [66/1000 2112/32000 (7%)] Loss: 1.96468 (semantic_loss: 0.01721, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22367 
Train Epoch: 44 [71/1000 2272/32000 (7%)] Loss: 1.95923 (semantic_loss: 0.01176, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20921 
Train Epoch: 44 [76/1000 2432/32000 (8%)] Loss: 1.95859 (semantic_loss: 0.01112, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20864 
Train Epoch: 44 [81/1000 2592/32000 (8%)] Loss: 1.95926 (semantic_loss: 0.01179, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18600 
Train Epoch: 44 [86/1000 2752/32000 (9%)] Loss: 1.96173 (semantic_loss: 0.01621, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19249 
Train Epoch: 44 [91/1000 2912/32000 (9%)] Loss: 1.96000 (semantic_loss: 0.01350, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18780 
Train Epoch: 44 [96/1000 3072/32000 (10%)] Loss: 1.96097 (semantic_loss: 0.01350, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18647 
Train Epoch: 44 [101/1000 3232/32000 (10%)] Loss: 1.96136 (semantic_loss: 0.01291, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18859 
Train Epoch: 44 [106/1000 3392/32000 (11%)] Loss: 1.96145 (semantic_loss: 0.01398, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.33604 
Train Epoch: 44 [111/1000 3552/32000 (11%)] Loss: 1.95901 (semantic_loss: 0.01154, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20083 
Train Epoch: 44 [116/1000 3712/32000 (12%)] Loss: 1.96070 (semantic_loss: 0.01324, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.43190 
Train Epoch: 44 [121/1000 3872/32000 (12%)] Loss: 1.96287 (semantic_loss: 0.01540, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18489 
Train Epoch: 44 [126/1000 4032/32000 (13%)] Loss: 1.96147 (semantic_loss: 0.01499, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19473 
Train Epoch: 44 [131/1000 4192/32000 (13%)] Loss: 1.96043 (semantic_loss: 0.01199, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19032 
Train Epoch: 44 [136/1000 4352/32000 (14%)] Loss: 1.96257 (semantic_loss: 0.01511, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19102 
Train Epoch: 44 [141/1000 4512/32000 (14%)] Loss: 1.96341 (semantic_loss: 0.01496, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18817 
Train Epoch: 44 [146/1000 4672/32000 (15%)] Loss: 1.96071 (semantic_loss: 0.01323, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18540 
Train Epoch: 44 [151/1000 4832/32000 (15%)] Loss: 1.96091 (semantic_loss: 0.01345, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20282 
Train Epoch: 44 [156/1000 4992/32000 (16%)] Loss: 1.96411 (semantic_loss: 0.01567, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18819 
Train Epoch: 44 [161/1000 5152/32000 (16%)] Loss: 1.96317 (semantic_loss: 0.01473, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18577 
Train Epoch: 44 [166/1000 5312/32000 (17%)] Loss: 1.96186 (semantic_loss: 0.01439, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18609 
Train Epoch: 44 [171/1000 5472/32000 (17%)] Loss: 1.95955 (semantic_loss: 0.01404, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18516 
Train Epoch: 44 [176/1000 5632/32000 (18%)] Loss: 1.96185 (semantic_loss: 0.01438, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25318 
Train Epoch: 44 [181/1000 5792/32000 (18%)] Loss: 1.96232 (semantic_loss: 0.01388, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19058 
Train Epoch: 44 [186/1000 5952/32000 (19%)] Loss: 1.96090 (semantic_loss: 0.01344, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18855 
Train Epoch: 44 [191/1000 6112/32000 (19%)] Loss: 1.95857 (semantic_loss: 0.01209, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18618 
Train Epoch: 44 [196/1000 6272/32000 (20%)] Loss: 1.96034 (semantic_loss: 0.01287, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18561 
Train Epoch: 44 [201/1000 6432/32000 (20%)] Loss: 1.96004 (semantic_loss: 0.01355, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20872 
Train Epoch: 44 [206/1000 6592/32000 (21%)] Loss: 1.96200 (semantic_loss: 0.01454, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22171 
Train Epoch: 44 [211/1000 6752/32000 (21%)] Loss: 1.96225 (semantic_loss: 0.01479, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21949 
Train Epoch: 44 [216/1000 6912/32000 (22%)] Loss: 1.96171 (semantic_loss: 0.01424, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20387 
Train Epoch: 44 [221/1000 7072/32000 (22%)] Loss: 1.96148 (semantic_loss: 0.01402, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20724 
Train Epoch: 44 [226/1000 7232/32000 (23%)] Loss: 1.95975 (semantic_loss: 0.01326, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20695 
Train Epoch: 44 [231/1000 7392/32000 (23%)] Loss: 1.95684 (semantic_loss: 0.01132, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.20680 
Train Epoch: 44 [236/1000 7552/32000 (24%)] Loss: 1.95943 (semantic_loss: 0.01294, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19898 
Train Epoch: 44 [241/1000 7712/32000 (24%)] Loss: 1.96099 (semantic_loss: 0.01352, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.26669 
Train Epoch: 44 [246/1000 7872/32000 (25%)] Loss: 1.96063 (semantic_loss: 0.01315, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18568 
Train Epoch: 44 [251/1000 8032/32000 (25%)] Loss: 1.95914 (semantic_loss: 0.01265, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18955 
Train Epoch: 44 [256/1000 8192/32000 (26%)] Loss: 1.96208 (semantic_loss: 0.01461, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19247 
Train Epoch: 44 [261/1000 8352/32000 (26%)] Loss: 1.96280 (semantic_loss: 0.01435, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20611 
Train Epoch: 44 [266/1000 8512/32000 (27%)] Loss: 1.96262 (semantic_loss: 0.01613, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20492 
Train Epoch: 44 [271/1000 8672/32000 (27%)] Loss: 1.96291 (semantic_loss: 0.01545, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21093 
Train Epoch: 44 [276/1000 8832/32000 (28%)] Loss: 1.96199 (semantic_loss: 0.01452, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20421 
Train Epoch: 44 [281/1000 8992/32000 (28%)] Loss: 1.96263 (semantic_loss: 0.01516, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18801 
Train Epoch: 44 [286/1000 9152/32000 (29%)] Loss: 1.96248 (semantic_loss: 0.01501, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21092 
Train Epoch: 44 [291/1000 9312/32000 (29%)] Loss: 1.96210 (semantic_loss: 0.01464, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19812 
Train Epoch: 44 [296/1000 9472/32000 (30%)] Loss: 1.96432 (semantic_loss: 0.01588, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21416 
Train Epoch: 44 [301/1000 9632/32000 (30%)] Loss: 1.96062 (semantic_loss: 0.01511, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18608 
Train Epoch: 44 [306/1000 9792/32000 (31%)] Loss: 1.96172 (semantic_loss: 0.01426, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22981 
Train Epoch: 44 [311/1000 9952/32000 (31%)] Loss: 1.95979 (semantic_loss: 0.01232, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18841 
Train Epoch: 44 [316/1000 10112/32000 (32%)] Loss: 1.95958 (semantic_loss: 0.01308, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19415 
Train Epoch: 44 [321/1000 10272/32000 (32%)] Loss: 1.96424 (semantic_loss: 0.01678, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27167 
Train Epoch: 44 [326/1000 10432/32000 (33%)] Loss: 1.96153 (semantic_loss: 0.01504, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19174 
Train Epoch: 44 [331/1000 10592/32000 (33%)] Loss: 1.95901 (semantic_loss: 0.01252, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18991 
Train Epoch: 44 [336/1000 10752/32000 (34%)] Loss: 1.96151 (semantic_loss: 0.01405, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.52550 
Train Epoch: 44 [341/1000 10912/32000 (34%)] Loss: 1.96245 (semantic_loss: 0.01303, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.29970 
Train Epoch: 44 [346/1000 11072/32000 (35%)] Loss: 1.96435 (semantic_loss: 0.01688, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18755 
Train Epoch: 44 [351/1000 11232/32000 (35%)] Loss: 1.96268 (semantic_loss: 0.01424, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18805 
Train Epoch: 44 [356/1000 11392/32000 (36%)] Loss: 1.96189 (semantic_loss: 0.01443, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18806 
Train Epoch: 44 [361/1000 11552/32000 (36%)] Loss: 1.96457 (semantic_loss: 0.01710, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27650 
Train Epoch: 44 [366/1000 11712/32000 (37%)] Loss: 1.95789 (semantic_loss: 0.01238, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.22375 
Train Epoch: 44 [371/1000 11872/32000 (37%)] Loss: 1.96041 (semantic_loss: 0.01197, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.24339 
Train Epoch: 44 [376/1000 12032/32000 (38%)] Loss: 1.96442 (semantic_loss: 0.01890, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.27683 
Train Epoch: 44 [381/1000 12192/32000 (38%)] Loss: 1.95847 (semantic_loss: 0.01199, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20386 
Train Epoch: 44 [386/1000 12352/32000 (39%)] Loss: 1.95984 (semantic_loss: 0.01237, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20888 
Train Epoch: 44 [391/1000 12512/32000 (39%)] Loss: 1.96037 (semantic_loss: 0.01290, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19898 
Train Epoch: 44 [396/1000 12672/32000 (40%)] Loss: 1.96632 (semantic_loss: 0.01983, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19276 
Train Epoch: 44 [401/1000 12832/32000 (40%)] Loss: 1.96272 (semantic_loss: 0.01623, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18807 
Train Epoch: 44 [406/1000 12992/32000 (41%)] Loss: 1.96363 (semantic_loss: 0.01616, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19039 
Train Epoch: 44 [411/1000 13152/32000 (41%)] Loss: 1.96108 (semantic_loss: 0.01264, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21137 
Train Epoch: 44 [416/1000 13312/32000 (42%)] Loss: 1.95818 (semantic_loss: 0.01266, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19160 
Train Epoch: 44 [421/1000 13472/32000 (42%)] Loss: 1.96357 (semantic_loss: 0.01707, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19385 
Train Epoch: 44 [426/1000 13632/32000 (43%)] Loss: 1.96378 (semantic_loss: 0.01631, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20366 
Train Epoch: 44 [431/1000 13792/32000 (43%)] Loss: 1.96205 (semantic_loss: 0.01459, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19268 
Train Epoch: 44 [436/1000 13952/32000 (44%)] Loss: 1.96136 (semantic_loss: 0.01389, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22837 
Train Epoch: 44 [441/1000 14112/32000 (44%)] Loss: 1.96169 (semantic_loss: 0.01422, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18336 
Train Epoch: 44 [446/1000 14272/32000 (45%)] Loss: 1.96004 (semantic_loss: 0.01257, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18526 
Train Epoch: 44 [451/1000 14432/32000 (45%)] Loss: 1.96429 (semantic_loss: 0.01780, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18855 
Train Epoch: 44 [456/1000 14592/32000 (46%)] Loss: 1.96223 (semantic_loss: 0.01476, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19065 
Train Epoch: 44 [461/1000 14752/32000 (46%)] Loss: 1.96150 (semantic_loss: 0.01501, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19070 
Train Epoch: 44 [466/1000 14912/32000 (47%)] Loss: 1.95904 (semantic_loss: 0.01157, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18472 
Train Epoch: 44 [471/1000 15072/32000 (47%)] Loss: 1.96250 (semantic_loss: 0.01502, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18728 
Train Epoch: 44 [476/1000 15232/32000 (48%)] Loss: 1.95941 (semantic_loss: 0.01195, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21183 
Train Epoch: 44 [481/1000 15392/32000 (48%)] Loss: 1.96157 (semantic_loss: 0.01411, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18901 
Train Epoch: 44 [486/1000 15552/32000 (49%)] Loss: 1.96088 (semantic_loss: 0.01439, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18768 
Train Epoch: 44 [491/1000 15712/32000 (49%)] Loss: 1.96582 (semantic_loss: 0.01933, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18889 
Train Epoch: 44 [496/1000 15872/32000 (50%)] Loss: 1.96185 (semantic_loss: 0.01439, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18527 
Train Epoch: 44 [501/1000 16032/32000 (50%)] Loss: 1.96444 (semantic_loss: 0.01698, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21855 
Train Epoch: 44 [506/1000 16192/32000 (51%)] Loss: 1.96091 (semantic_loss: 0.01247, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.95088 
Train Epoch: 44 [511/1000 16352/32000 (51%)] Loss: 1.96420 (semantic_loss: 0.01771, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20758 
Train Epoch: 44 [516/1000 16512/32000 (52%)] Loss: 1.96105 (semantic_loss: 0.01359, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21519 
Train Epoch: 44 [521/1000 16672/32000 (52%)] Loss: 1.95891 (semantic_loss: 0.01144, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23010 
Train Epoch: 44 [526/1000 16832/32000 (53%)] Loss: 1.96132 (semantic_loss: 0.01385, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22074 
Train Epoch: 44 [531/1000 16992/32000 (53%)] Loss: 1.96070 (semantic_loss: 0.01226, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20687 
Train Epoch: 44 [536/1000 17152/32000 (54%)] Loss: 1.96310 (semantic_loss: 0.01563, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21288 
Train Epoch: 44 [541/1000 17312/32000 (54%)] Loss: 1.95950 (semantic_loss: 0.01399, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19462 
Train Epoch: 44 [546/1000 17472/32000 (55%)] Loss: 1.96094 (semantic_loss: 0.01445, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19754 
Train Epoch: 44 [551/1000 17632/32000 (55%)] Loss: 1.96386 (semantic_loss: 0.01639, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19234 
Train Epoch: 44 [556/1000 17792/32000 (56%)] Loss: 1.96263 (semantic_loss: 0.01418, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18807 
Train Epoch: 44 [561/1000 17952/32000 (56%)] Loss: 1.96118 (semantic_loss: 0.01468, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18599 
Train Epoch: 44 [566/1000 18112/32000 (57%)] Loss: 1.96417 (semantic_loss: 0.01670, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18945 
Train Epoch: 44 [571/1000 18272/32000 (57%)] Loss: 1.95974 (semantic_loss: 0.01325, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19406 
Train Epoch: 44 [576/1000 18432/32000 (58%)] Loss: 1.95915 (semantic_loss: 0.01169, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20013 
Train Epoch: 44 [581/1000 18592/32000 (58%)] Loss: 1.96541 (semantic_loss: 0.01892, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19855 
Train Epoch: 44 [586/1000 18752/32000 (59%)] Loss: 1.95994 (semantic_loss: 0.01345, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19341 
Train Epoch: 44 [591/1000 18912/32000 (59%)] Loss: 1.96081 (semantic_loss: 0.01432, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18872 
Train Epoch: 44 [596/1000 19072/32000 (60%)] Loss: 1.96086 (semantic_loss: 0.01340, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19232 
Train Epoch: 44 [601/1000 19232/32000 (60%)] Loss: 1.96051 (semantic_loss: 0.01401, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19027 
Train Epoch: 44 [606/1000 19392/32000 (61%)] Loss: 1.95998 (semantic_loss: 0.01349, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19252 
Train Epoch: 44 [611/1000 19552/32000 (61%)] Loss: 1.96158 (semantic_loss: 0.01412, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19380 
Train Epoch: 44 [616/1000 19712/32000 (62%)] Loss: 1.96398 (semantic_loss: 0.01651, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19919 
Train Epoch: 44 [621/1000 19872/32000 (62%)] Loss: 1.95973 (semantic_loss: 0.01129, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19573 
Train Epoch: 44 [626/1000 20032/32000 (63%)] Loss: 1.96311 (semantic_loss: 0.01466, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21388 
Train Epoch: 44 [631/1000 20192/32000 (63%)] Loss: 1.95835 (semantic_loss: 0.01186, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19323 
Train Epoch: 44 [636/1000 20352/32000 (64%)] Loss: 1.96245 (semantic_loss: 0.01498, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18871 
Train Epoch: 44 [641/1000 20512/32000 (64%)] Loss: 1.96364 (semantic_loss: 0.01618, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20677 
Train Epoch: 44 [646/1000 20672/32000 (65%)] Loss: 1.96553 (semantic_loss: 0.01708, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19025 
Train Epoch: 44 [651/1000 20832/32000 (65%)] Loss: 1.96950 (semantic_loss: 0.02203, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18963 
Train Epoch: 44 [656/1000 20992/32000 (66%)] Loss: 1.96121 (semantic_loss: 0.01374, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19627 
Train Epoch: 44 [661/1000 21152/32000 (66%)] Loss: 1.96381 (semantic_loss: 0.01635, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18770 
Train Epoch: 44 [666/1000 21312/32000 (67%)] Loss: 1.96252 (semantic_loss: 0.01603, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19350 
Train Epoch: 44 [671/1000 21472/32000 (67%)] Loss: 1.96157 (semantic_loss: 0.01411, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19098 
Train Epoch: 44 [676/1000 21632/32000 (68%)] Loss: 1.96086 (semantic_loss: 0.01438, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21523 
Train Epoch: 44 [681/1000 21792/32000 (68%)] Loss: 1.96133 (semantic_loss: 0.01485, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20610 
Train Epoch: 44 [686/1000 21952/32000 (69%)] Loss: 1.96343 (semantic_loss: 0.01694, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20931 
Train Epoch: 44 [691/1000 22112/32000 (69%)] Loss: 1.96334 (semantic_loss: 0.01587, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22452 
Train Epoch: 44 [696/1000 22272/32000 (70%)] Loss: 1.96051 (semantic_loss: 0.01402, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.25936 
Train Epoch: 44 [701/1000 22432/32000 (70%)] Loss: 1.96496 (semantic_loss: 0.01749, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19301 
Train Epoch: 44 [706/1000 22592/32000 (71%)] Loss: 1.96012 (semantic_loss: 0.01364, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20905 
Train Epoch: 44 [711/1000 22752/32000 (71%)] Loss: 1.95951 (semantic_loss: 0.01301, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19078 
Train Epoch: 44 [716/1000 22912/32000 (72%)] Loss: 1.95855 (semantic_loss: 0.01206, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18694 
Train Epoch: 44 [721/1000 23072/32000 (72%)] Loss: 1.96337 (semantic_loss: 0.01687, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18708 
Train Epoch: 44 [726/1000 23232/32000 (73%)] Loss: 1.96259 (semantic_loss: 0.01414, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19212 
Train Epoch: 44 [731/1000 23392/32000 (73%)] Loss: 1.96157 (semantic_loss: 0.01410, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18878 
Train Epoch: 44 [736/1000 23552/32000 (74%)] Loss: 1.96113 (semantic_loss: 0.01269, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18824 
Train Epoch: 44 [741/1000 23712/32000 (74%)] Loss: 1.96220 (semantic_loss: 0.01473, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19091 
Train Epoch: 44 [746/1000 23872/32000 (75%)] Loss: 1.95794 (semantic_loss: 0.01144, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19118 
Train Epoch: 44 [751/1000 24032/32000 (75%)] Loss: 1.96056 (semantic_loss: 0.01309, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19193 
Train Epoch: 44 [756/1000 24192/32000 (76%)] Loss: 1.96300 (semantic_loss: 0.01553, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.43851 
Train Epoch: 44 [761/1000 24352/32000 (76%)] Loss: 1.96218 (semantic_loss: 0.01471, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19810 
Train Epoch: 44 [766/1000 24512/32000 (77%)] Loss: 1.95830 (semantic_loss: 0.01182, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18739 
Train Epoch: 44 [771/1000 24672/32000 (77%)] Loss: 1.95936 (semantic_loss: 0.01189, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18790 
Train Epoch: 44 [776/1000 24832/32000 (78%)] Loss: 1.95950 (semantic_loss: 0.01203, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18790 
Train Epoch: 44 [781/1000 24992/32000 (78%)] Loss: 1.96109 (semantic_loss: 0.01362, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18925 
Train Epoch: 44 [786/1000 25152/32000 (79%)] Loss: 1.96494 (semantic_loss: 0.01651, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19337 
Train Epoch: 44 [791/1000 25312/32000 (79%)] Loss: 1.96011 (semantic_loss: 0.01265, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19945 
Train Epoch: 44 [796/1000 25472/32000 (80%)] Loss: 1.96282 (semantic_loss: 0.01536, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19228 
Train Epoch: 44 [801/1000 25632/32000 (80%)] Loss: 1.96452 (semantic_loss: 0.01705, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19048 
Train Epoch: 44 [806/1000 25792/32000 (81%)] Loss: 1.96202 (semantic_loss: 0.01554, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18767 
Train Epoch: 44 [811/1000 25952/32000 (81%)] Loss: 1.96071 (semantic_loss: 0.01324, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18649 
Train Epoch: 44 [816/1000 26112/32000 (82%)] Loss: 1.95811 (semantic_loss: 0.01065, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18820 
Train Epoch: 44 [821/1000 26272/32000 (82%)] Loss: 1.95979 (semantic_loss: 0.01330, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18910 
Train Epoch: 44 [826/1000 26432/32000 (83%)] Loss: 1.95954 (semantic_loss: 0.01110, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.97859 
Train Epoch: 44 [831/1000 26592/32000 (83%)] Loss: 1.96292 (semantic_loss: 0.01545, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20050 
Train Epoch: 44 [836/1000 26752/32000 (84%)] Loss: 1.96309 (semantic_loss: 0.01561, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22023 
Train Epoch: 44 [841/1000 26912/32000 (84%)] Loss: 1.96025 (semantic_loss: 0.01181, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22435 
Train Epoch: 44 [846/1000 27072/32000 (85%)] Loss: 1.96066 (semantic_loss: 0.01417, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21716 
Train Epoch: 44 [851/1000 27232/32000 (85%)] Loss: 1.96233 (semantic_loss: 0.01389, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20622 
Train Epoch: 44 [856/1000 27392/32000 (86%)] Loss: 1.95975 (semantic_loss: 0.01229, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22672 
Train Epoch: 44 [861/1000 27552/32000 (86%)] Loss: 1.96310 (semantic_loss: 0.01661, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19408 
Train Epoch: 44 [866/1000 27712/32000 (87%)] Loss: 1.95921 (semantic_loss: 0.01174, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21246 
Train Epoch: 44 [871/1000 27872/32000 (87%)] Loss: 1.95980 (semantic_loss: 0.01136, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20668 
Train Epoch: 44 [876/1000 28032/32000 (88%)] Loss: 1.96289 (semantic_loss: 0.01543, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18504 
Train Epoch: 44 [881/1000 28192/32000 (88%)] Loss: 1.96341 (semantic_loss: 0.01595, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18824 
Train Epoch: 44 [886/1000 28352/32000 (89%)] Loss: 1.96178 (semantic_loss: 0.01431, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18794 
Train Epoch: 44 [891/1000 28512/32000 (89%)] Loss: 1.95985 (semantic_loss: 0.01238, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18968 
Train Epoch: 44 [896/1000 28672/32000 (90%)] Loss: 1.96323 (semantic_loss: 0.01576, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20140 
Train Epoch: 44 [901/1000 28832/32000 (90%)] Loss: 1.95981 (semantic_loss: 0.01234, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19450 
Train Epoch: 44 [906/1000 28992/32000 (91%)] Loss: 1.96298 (semantic_loss: 0.01649, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20616 
Train Epoch: 44 [911/1000 29152/32000 (91%)] Loss: 1.95947 (semantic_loss: 0.01298, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18775 
Train Epoch: 44 [916/1000 29312/32000 (92%)] Loss: 1.96422 (semantic_loss: 0.01675, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18801 
Train Epoch: 44 [921/1000 29472/32000 (92%)] Loss: 1.96330 (semantic_loss: 0.01583, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20970 
Train Epoch: 44 [926/1000 29632/32000 (93%)] Loss: 1.96321 (semantic_loss: 0.01574, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18659 
Train Epoch: 44 [931/1000 29792/32000 (93%)] Loss: 1.95784 (semantic_loss: 0.01233, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.20313 
Train Epoch: 44 [936/1000 29952/32000 (94%)] Loss: 1.96618 (semantic_loss: 0.01774, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18755 
Train Epoch: 44 [941/1000 30112/32000 (94%)] Loss: 1.96027 (semantic_loss: 0.01280, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18656 
Train Epoch: 44 [946/1000 30272/32000 (95%)] Loss: 1.96330 (semantic_loss: 0.01681, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18768 
Train Epoch: 44 [951/1000 30432/32000 (95%)] Loss: 1.96192 (semantic_loss: 0.01348, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19123 
Train Epoch: 44 [956/1000 30592/32000 (96%)] Loss: 1.96327 (semantic_loss: 0.01482, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18679 
Train Epoch: 44 [961/1000 30752/32000 (96%)] Loss: 1.96221 (semantic_loss: 0.01475, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18581 
Train Epoch: 44 [966/1000 30912/32000 (97%)] Loss: 1.96543 (semantic_loss: 0.01894, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18759 
Train Epoch: 44 [971/1000 31072/32000 (97%)] Loss: 1.96224 (semantic_loss: 0.01477, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19018 
Train Epoch: 44 [976/1000 31232/32000 (98%)] Loss: 1.95953 (semantic_loss: 0.01206, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19835 
Train Epoch: 44 [981/1000 31392/32000 (98%)] Loss: 1.96159 (semantic_loss: 0.01314, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18568 
Train Epoch: 44 [986/1000 31552/32000 (99%)] Loss: 1.96031 (semantic_loss: 0.01186, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18939 
Train Epoch: 44 [991/1000 31712/32000 (99%)] Loss: 1.96279 (semantic_loss: 0.01532, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18740 
Train Epoch: 44 [996/1000 31872/32000 (100%)] Loss: 1.95872 (semantic_loss: 0.01223, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19132 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch44.pth ...
Done in 4.557s
removing stale ckpt [epoch 43] [took 0.01s]
 epoch          : 44
 loss           : 1.961636638879776
 learning_rate  : 5.387631832152914e-07
 n_samples      : 1408000
 n_steps        : 44000
 ActivityNet_val1_test/t2v_metrics/R1: 11.73479764083791
 ActivityNet_val1_test/t2v_metrics/R5: 38.417734390888754
 ActivityNet_val1_test/t2v_metrics/R10: 55.37929631889364
 ActivityNet_val1_test/t2v_metrics/R50: 84.90949766117551
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 63.00183038438072
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.227046409431193
 ActivityNet_val1_test/v2t_metrics/R1: 12.8533658735001
 ActivityNet_val1_test/v2t_metrics/R5: 38.98718730933496
 ActivityNet_val1_test/v2t_metrics/R10: 55.603009965426075
 ActivityNet_val1_test/v2t_metrics/R50: 84.76713443156396
 ActivityNet_val1_test/v2t_metrics/MedR: 8.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 64.68619076672768
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 30.316497513445444
 mnt_best       : 29.76339460386141
 not_improved_count: 2
Train Epoch: 45 [1/1000 32/32000 (0%)] Loss: 1.96207 (semantic_loss: 0.01558, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=23.05582 
Train Epoch: 45 [6/1000 192/32000 (1%)] Loss: 1.96579 (semantic_loss: 0.01833, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20674 
Train Epoch: 45 [11/1000 352/32000 (1%)] Loss: 1.95869 (semantic_loss: 0.01221, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18719 
Train Epoch: 45 [16/1000 512/32000 (2%)] Loss: 1.95904 (semantic_loss: 0.01158, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.53689 
Train Epoch: 45 [21/1000 672/32000 (2%)] Loss: 1.96345 (semantic_loss: 0.01598, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18789 
Train Epoch: 45 [26/1000 832/32000 (3%)] Loss: 1.96067 (semantic_loss: 0.01223, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19239 
Train Epoch: 45 [31/1000 992/32000 (3%)] Loss: 1.95887 (semantic_loss: 0.01237, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.27635 
Train Epoch: 45 [36/1000 1152/32000 (4%)] Loss: 1.96381 (semantic_loss: 0.01635, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21446 
Train Epoch: 45 [41/1000 1312/32000 (4%)] Loss: 1.95967 (semantic_loss: 0.01220, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20111 
Train Epoch: 45 [46/1000 1472/32000 (5%)] Loss: 1.96105 (semantic_loss: 0.01358, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18727 
Train Epoch: 45 [51/1000 1632/32000 (5%)] Loss: 1.95971 (semantic_loss: 0.01224, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19000 
Train Epoch: 45 [56/1000 1792/32000 (6%)] Loss: 1.95930 (semantic_loss: 0.01184, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18510 
Train Epoch: 45 [61/1000 1952/32000 (6%)] Loss: 1.96782 (semantic_loss: 0.01937, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19736 
Train Epoch: 45 [66/1000 2112/32000 (7%)] Loss: 1.96161 (semantic_loss: 0.01511, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18657 
Train Epoch: 45 [71/1000 2272/32000 (7%)] Loss: 1.95881 (semantic_loss: 0.01232, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24052 
Train Epoch: 45 [76/1000 2432/32000 (8%)] Loss: 1.95956 (semantic_loss: 0.01209, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18627 
Train Epoch: 45 [81/1000 2592/32000 (8%)] Loss: 1.96181 (semantic_loss: 0.01336, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18680 
Train Epoch: 45 [86/1000 2752/32000 (9%)] Loss: 1.96131 (semantic_loss: 0.01482, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.39759 
Train Epoch: 45 [91/1000 2912/32000 (9%)] Loss: 1.96076 (semantic_loss: 0.01428, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18769 
Train Epoch: 45 [96/1000 3072/32000 (10%)] Loss: 1.96116 (semantic_loss: 0.01564, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19220 
Train Epoch: 45 [101/1000 3232/32000 (10%)] Loss: 1.96881 (semantic_loss: 0.02232, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19405 
Train Epoch: 45 [106/1000 3392/32000 (11%)] Loss: 1.96298 (semantic_loss: 0.01551, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18815 
Train Epoch: 45 [111/1000 3552/32000 (11%)] Loss: 1.96232 (semantic_loss: 0.01583, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19099 
Train Epoch: 45 [116/1000 3712/32000 (12%)] Loss: 1.96082 (semantic_loss: 0.01238, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18854 
Train Epoch: 45 [121/1000 3872/32000 (12%)] Loss: 1.95981 (semantic_loss: 0.01234, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.22027 
Train Epoch: 45 [126/1000 4032/32000 (13%)] Loss: 1.96181 (semantic_loss: 0.01434, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21721 
Train Epoch: 45 [131/1000 4192/32000 (13%)] Loss: 1.96038 (semantic_loss: 0.01291, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.39758 
Train Epoch: 45 [136/1000 4352/32000 (14%)] Loss: 1.95838 (semantic_loss: 0.01189, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24159 
Train Epoch: 45 [141/1000 4512/32000 (14%)] Loss: 1.96420 (semantic_loss: 0.01673, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20311 
Train Epoch: 45 [146/1000 4672/32000 (15%)] Loss: 1.96143 (semantic_loss: 0.01397, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20233 
Train Epoch: 45 [151/1000 4832/32000 (15%)] Loss: 1.96150 (semantic_loss: 0.01403, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19541 
Train Epoch: 45 [156/1000 4992/32000 (16%)] Loss: 1.96003 (semantic_loss: 0.01255, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21084 
Train Epoch: 45 [161/1000 5152/32000 (16%)] Loss: 1.96426 (semantic_loss: 0.01679, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18794 
Train Epoch: 45 [166/1000 5312/32000 (17%)] Loss: 1.96315 (semantic_loss: 0.01666, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18673 
Train Epoch: 45 [171/1000 5472/32000 (17%)] Loss: 1.96126 (semantic_loss: 0.01379, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19168 
Train Epoch: 45 [176/1000 5632/32000 (18%)] Loss: 1.96117 (semantic_loss: 0.01272, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18906 
Train Epoch: 45 [181/1000 5792/32000 (18%)] Loss: 1.96028 (semantic_loss: 0.01282, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18733 
Train Epoch: 45 [186/1000 5952/32000 (19%)] Loss: 1.96278 (semantic_loss: 0.01532, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20133 
Train Epoch: 45 [191/1000 6112/32000 (19%)] Loss: 1.96748 (semantic_loss: 0.01903, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20163 
Train Epoch: 45 [196/1000 6272/32000 (20%)] Loss: 1.96064 (semantic_loss: 0.01318, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20094 
Train Epoch: 45 [201/1000 6432/32000 (20%)] Loss: 1.96906 (semantic_loss: 0.02159, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18718 
Train Epoch: 45 [206/1000 6592/32000 (21%)] Loss: 1.96050 (semantic_loss: 0.01304, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18814 
Train Epoch: 45 [211/1000 6752/32000 (21%)] Loss: 1.96098 (semantic_loss: 0.01254, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18433 
Train Epoch: 45 [216/1000 6912/32000 (22%)] Loss: 1.96444 (semantic_loss: 0.01600, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.24609 
Train Epoch: 45 [221/1000 7072/32000 (22%)] Loss: 1.96524 (semantic_loss: 0.01875, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18931 
Train Epoch: 45 [226/1000 7232/32000 (23%)] Loss: 1.95960 (semantic_loss: 0.01311, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18655 
Train Epoch: 45 [231/1000 7392/32000 (23%)] Loss: 1.96475 (semantic_loss: 0.01728, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18550 
Train Epoch: 45 [236/1000 7552/32000 (24%)] Loss: 1.96377 (semantic_loss: 0.01532, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18730 
Train Epoch: 45 [241/1000 7712/32000 (24%)] Loss: 1.96036 (semantic_loss: 0.01192, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19562 
Train Epoch: 45 [246/1000 7872/32000 (25%)] Loss: 1.96080 (semantic_loss: 0.01235, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19157 
Train Epoch: 45 [251/1000 8032/32000 (25%)] Loss: 1.96129 (semantic_loss: 0.01383, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20417 
Train Epoch: 45 [256/1000 8192/32000 (26%)] Loss: 1.95827 (semantic_loss: 0.01178, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20380 
Train Epoch: 45 [261/1000 8352/32000 (26%)] Loss: 1.96135 (semantic_loss: 0.01388, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18908 
Train Epoch: 45 [266/1000 8512/32000 (27%)] Loss: 1.96254 (semantic_loss: 0.01507, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20359 
Train Epoch: 45 [271/1000 8672/32000 (27%)] Loss: 1.96322 (semantic_loss: 0.01576, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18937 
Train Epoch: 45 [276/1000 8832/32000 (28%)] Loss: 1.96125 (semantic_loss: 0.01280, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.52413 
Train Epoch: 45 [281/1000 8992/32000 (28%)] Loss: 1.95832 (semantic_loss: 0.01085, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22286 
Train Epoch: 45 [286/1000 9152/32000 (29%)] Loss: 1.96166 (semantic_loss: 0.01322, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23389 
Train Epoch: 45 [291/1000 9312/32000 (29%)] Loss: 1.96005 (semantic_loss: 0.01258, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21535 
Train Epoch: 45 [296/1000 9472/32000 (30%)] Loss: 1.95961 (semantic_loss: 0.01214, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19596 
Train Epoch: 45 [301/1000 9632/32000 (30%)] Loss: 1.95954 (semantic_loss: 0.01207, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21067 
Train Epoch: 45 [306/1000 9792/32000 (31%)] Loss: 1.95962 (semantic_loss: 0.01411, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19429 
Train Epoch: 45 [311/1000 9952/32000 (31%)] Loss: 1.96038 (semantic_loss: 0.01291, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20878 
Train Epoch: 45 [316/1000 10112/32000 (32%)] Loss: 1.95945 (semantic_loss: 0.01394, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19524 
Train Epoch: 45 [321/1000 10272/32000 (32%)] Loss: 1.96433 (semantic_loss: 0.01686, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20665 
Train Epoch: 45 [326/1000 10432/32000 (33%)] Loss: 1.96321 (semantic_loss: 0.01575, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19898 
Train Epoch: 45 [331/1000 10592/32000 (33%)] Loss: 1.96088 (semantic_loss: 0.01342, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19260 
Train Epoch: 45 [336/1000 10752/32000 (34%)] Loss: 1.96327 (semantic_loss: 0.01678, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.50342 
Train Epoch: 45 [341/1000 10912/32000 (34%)] Loss: 1.95965 (semantic_loss: 0.01219, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18665 
Train Epoch: 45 [346/1000 11072/32000 (35%)] Loss: 1.96086 (semantic_loss: 0.01339, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19312 
Train Epoch: 45 [351/1000 11232/32000 (35%)] Loss: 1.96023 (semantic_loss: 0.01276, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27753 
Train Epoch: 45 [356/1000 11392/32000 (36%)] Loss: 1.96239 (semantic_loss: 0.01492, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21124 
Train Epoch: 45 [361/1000 11552/32000 (36%)] Loss: 1.96295 (semantic_loss: 0.01451, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18671 
Train Epoch: 45 [366/1000 11712/32000 (37%)] Loss: 1.95976 (semantic_loss: 0.01228, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18464 
Train Epoch: 45 [371/1000 11872/32000 (37%)] Loss: 1.96224 (semantic_loss: 0.01477, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18453 
Train Epoch: 45 [376/1000 12032/32000 (38%)] Loss: 1.96228 (semantic_loss: 0.01482, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18994 
Train Epoch: 45 [381/1000 12192/32000 (38%)] Loss: 1.96106 (semantic_loss: 0.01262, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18876 
Train Epoch: 45 [386/1000 12352/32000 (39%)] Loss: 1.95818 (semantic_loss: 0.01169, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19041 
Train Epoch: 45 [391/1000 12512/32000 (39%)] Loss: 1.96619 (semantic_loss: 0.01872, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23651 
Train Epoch: 45 [396/1000 12672/32000 (40%)] Loss: 1.96193 (semantic_loss: 0.01446, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18756 
Train Epoch: 45 [401/1000 12832/32000 (40%)] Loss: 1.96300 (semantic_loss: 0.01455, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.22248 
Train Epoch: 45 [406/1000 12992/32000 (41%)] Loss: 1.96212 (semantic_loss: 0.01465, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.40520 
Train Epoch: 45 [411/1000 13152/32000 (41%)] Loss: 1.96343 (semantic_loss: 0.01596, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21249 
Train Epoch: 45 [416/1000 13312/32000 (42%)] Loss: 1.96054 (semantic_loss: 0.01308, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18982 
Train Epoch: 45 [421/1000 13472/32000 (42%)] Loss: 1.96220 (semantic_loss: 0.01571, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19826 
Train Epoch: 45 [426/1000 13632/32000 (43%)] Loss: 1.96113 (semantic_loss: 0.01367, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19260 
Train Epoch: 45 [431/1000 13792/32000 (43%)] Loss: 1.96017 (semantic_loss: 0.01270, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19953 
Train Epoch: 45 [436/1000 13952/32000 (44%)] Loss: 1.96094 (semantic_loss: 0.01347, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21615 
Train Epoch: 45 [441/1000 14112/32000 (44%)] Loss: 1.96083 (semantic_loss: 0.01336, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23396 
Train Epoch: 45 [446/1000 14272/32000 (45%)] Loss: 1.96147 (semantic_loss: 0.01400, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22684 
Train Epoch: 45 [451/1000 14432/32000 (45%)] Loss: 1.96104 (semantic_loss: 0.01455, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.33981 
Train Epoch: 45 [456/1000 14592/32000 (46%)] Loss: 1.96075 (semantic_loss: 0.01329, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.25062 
Train Epoch: 45 [461/1000 14752/32000 (46%)] Loss: 1.96042 (semantic_loss: 0.01295, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19667 
Train Epoch: 45 [466/1000 14912/32000 (47%)] Loss: 1.96177 (semantic_loss: 0.01431, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20264 
Train Epoch: 45 [471/1000 15072/32000 (47%)] Loss: 1.95960 (semantic_loss: 0.01312, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19340 
Train Epoch: 45 [476/1000 15232/32000 (48%)] Loss: 1.96095 (semantic_loss: 0.01348, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20741 
Train Epoch: 45 [481/1000 15392/32000 (48%)] Loss: 1.96169 (semantic_loss: 0.01521, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18641 
Train Epoch: 45 [486/1000 15552/32000 (49%)] Loss: 1.95927 (semantic_loss: 0.01376, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18567 
Train Epoch: 45 [491/1000 15712/32000 (49%)] Loss: 1.95894 (semantic_loss: 0.01147, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18576 
Train Epoch: 45 [496/1000 15872/32000 (50%)] Loss: 1.96245 (semantic_loss: 0.01498, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19055 
Train Epoch: 45 [501/1000 16032/32000 (50%)] Loss: 1.96122 (semantic_loss: 0.01473, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19152 
Train Epoch: 45 [506/1000 16192/32000 (51%)] Loss: 1.95883 (semantic_loss: 0.01137, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18853 
Train Epoch: 45 [511/1000 16352/32000 (51%)] Loss: 1.96075 (semantic_loss: 0.01329, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20140 
Train Epoch: 45 [516/1000 16512/32000 (52%)] Loss: 1.96285 (semantic_loss: 0.01637, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18637 
Train Epoch: 45 [521/1000 16672/32000 (52%)] Loss: 1.96358 (semantic_loss: 0.01514, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19511 
Train Epoch: 45 [526/1000 16832/32000 (53%)] Loss: 1.96141 (semantic_loss: 0.01492, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18923 
Train Epoch: 45 [531/1000 16992/32000 (53%)] Loss: 1.96200 (semantic_loss: 0.01356, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18768 
Train Epoch: 45 [536/1000 17152/32000 (54%)] Loss: 1.95944 (semantic_loss: 0.01197, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23850 
Train Epoch: 45 [541/1000 17312/32000 (54%)] Loss: 1.96278 (semantic_loss: 0.01433, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18898 
Train Epoch: 45 [546/1000 17472/32000 (55%)] Loss: 1.96166 (semantic_loss: 0.01322, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18560 
Train Epoch: 45 [551/1000 17632/32000 (55%)] Loss: 1.96108 (semantic_loss: 0.01361, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18709 
Train Epoch: 45 [556/1000 17792/32000 (56%)] Loss: 1.96267 (semantic_loss: 0.01618, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18900 
Train Epoch: 45 [561/1000 17952/32000 (56%)] Loss: 1.96208 (semantic_loss: 0.01559, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19003 
Train Epoch: 45 [566/1000 18112/32000 (57%)] Loss: 1.96243 (semantic_loss: 0.01496, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18763 
Train Epoch: 45 [571/1000 18272/32000 (57%)] Loss: 1.95980 (semantic_loss: 0.01233, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20490 
Train Epoch: 45 [576/1000 18432/32000 (58%)] Loss: 1.96685 (semantic_loss: 0.02036, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19437 
Train Epoch: 45 [581/1000 18592/32000 (58%)] Loss: 1.96150 (semantic_loss: 0.01403, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18862 
Train Epoch: 45 [586/1000 18752/32000 (59%)] Loss: 1.96523 (semantic_loss: 0.01776, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20298 
Train Epoch: 45 [591/1000 18912/32000 (59%)] Loss: 1.96236 (semantic_loss: 0.01490, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19096 
Train Epoch: 45 [596/1000 19072/32000 (60%)] Loss: 1.96225 (semantic_loss: 0.01478, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.61272 
Train Epoch: 45 [601/1000 19232/32000 (60%)] Loss: 1.95773 (semantic_loss: 0.01124, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23444 
Train Epoch: 45 [606/1000 19392/32000 (61%)] Loss: 1.95985 (semantic_loss: 0.01337, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21992 
Train Epoch: 45 [611/1000 19552/32000 (61%)] Loss: 1.96543 (semantic_loss: 0.01600, quant_loss: 1.94922, bit_balance_loss: 0.00021) batch_time=0.20323 
Train Epoch: 45 [616/1000 19712/32000 (62%)] Loss: 1.96004 (semantic_loss: 0.01355, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20090 
Train Epoch: 45 [621/1000 19872/32000 (62%)] Loss: 1.96406 (semantic_loss: 0.01561, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19620 
Train Epoch: 45 [626/1000 20032/32000 (63%)] Loss: 1.95902 (semantic_loss: 0.01155, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19913 
Train Epoch: 45 [631/1000 20192/32000 (63%)] Loss: 1.96190 (semantic_loss: 0.01346, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20456 
Train Epoch: 45 [636/1000 20352/32000 (64%)] Loss: 1.95957 (semantic_loss: 0.01308, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19043 
Train Epoch: 45 [641/1000 20512/32000 (64%)] Loss: 1.96552 (semantic_loss: 0.01806, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20054 
Train Epoch: 45 [646/1000 20672/32000 (65%)] Loss: 1.96522 (semantic_loss: 0.01774, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18928 
Train Epoch: 45 [651/1000 20832/32000 (65%)] Loss: 1.96615 (semantic_loss: 0.01868, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18537 
Train Epoch: 45 [656/1000 20992/32000 (66%)] Loss: 1.96115 (semantic_loss: 0.01369, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.55287 
Train Epoch: 45 [661/1000 21152/32000 (66%)] Loss: 1.96371 (semantic_loss: 0.01624, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19093 
Train Epoch: 45 [666/1000 21312/32000 (67%)] Loss: 1.96174 (semantic_loss: 0.01427, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19198 
Train Epoch: 45 [671/1000 21472/32000 (67%)] Loss: 1.96270 (semantic_loss: 0.01523, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27292 
Train Epoch: 45 [676/1000 21632/32000 (68%)] Loss: 1.96040 (semantic_loss: 0.01294, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18805 
Train Epoch: 45 [681/1000 21792/32000 (68%)] Loss: 1.95982 (semantic_loss: 0.01333, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18504 
Train Epoch: 45 [686/1000 21952/32000 (69%)] Loss: 1.96099 (semantic_loss: 0.01351, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18508 
Train Epoch: 45 [691/1000 22112/32000 (69%)] Loss: 1.95969 (semantic_loss: 0.01417, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18675 
Train Epoch: 45 [696/1000 22272/32000 (70%)] Loss: 1.95855 (semantic_loss: 0.01304, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18815 
Train Epoch: 45 [701/1000 22432/32000 (70%)] Loss: 1.96596 (semantic_loss: 0.01849, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18818 
Train Epoch: 45 [706/1000 22592/32000 (71%)] Loss: 1.95816 (semantic_loss: 0.01265, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18287 
Train Epoch: 45 [711/1000 22752/32000 (71%)] Loss: 1.96420 (semantic_loss: 0.01576, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23426 
Train Epoch: 45 [716/1000 22912/32000 (72%)] Loss: 1.96063 (semantic_loss: 0.01413, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18584 
Train Epoch: 45 [721/1000 23072/32000 (72%)] Loss: 1.96181 (semantic_loss: 0.01337, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19111 
Train Epoch: 45 [726/1000 23232/32000 (73%)] Loss: 1.95937 (semantic_loss: 0.01190, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.60172 
Train Epoch: 45 [731/1000 23392/32000 (73%)] Loss: 1.95797 (semantic_loss: 0.01148, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18627 
Train Epoch: 45 [736/1000 23552/32000 (74%)] Loss: 1.96706 (semantic_loss: 0.01959, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18626 
Train Epoch: 45 [741/1000 23712/32000 (74%)] Loss: 1.96093 (semantic_loss: 0.01248, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18597 
Train Epoch: 45 [746/1000 23872/32000 (75%)] Loss: 1.95730 (semantic_loss: 0.01179, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18629 
Train Epoch: 45 [751/1000 24032/32000 (75%)] Loss: 1.96288 (semantic_loss: 0.01541, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21222 
Train Epoch: 45 [756/1000 24192/32000 (76%)] Loss: 1.95934 (semantic_loss: 0.01090, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21310 
Train Epoch: 45 [761/1000 24352/32000 (76%)] Loss: 1.96030 (semantic_loss: 0.01283, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20707 
Train Epoch: 45 [766/1000 24512/32000 (77%)] Loss: 1.96667 (semantic_loss: 0.01920, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20288 
Train Epoch: 45 [771/1000 24672/32000 (77%)] Loss: 1.95996 (semantic_loss: 0.01445, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.35377 
Train Epoch: 45 [776/1000 24832/32000 (78%)] Loss: 1.96010 (semantic_loss: 0.01263, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.25190 
Train Epoch: 45 [781/1000 24992/32000 (78%)] Loss: 1.96726 (semantic_loss: 0.01881, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20402 
Train Epoch: 45 [786/1000 25152/32000 (79%)] Loss: 1.96123 (semantic_loss: 0.01279, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19952 
Train Epoch: 45 [791/1000 25312/32000 (79%)] Loss: 1.95893 (semantic_loss: 0.01146, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18910 
Train Epoch: 45 [796/1000 25472/32000 (80%)] Loss: 1.96059 (semantic_loss: 0.01312, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20702 
Train Epoch: 45 [801/1000 25632/32000 (80%)] Loss: 1.96116 (semantic_loss: 0.01467, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18786 
Train Epoch: 45 [806/1000 25792/32000 (81%)] Loss: 1.95827 (semantic_loss: 0.01178, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18616 
Train Epoch: 45 [811/1000 25952/32000 (81%)] Loss: 1.96078 (semantic_loss: 0.01331, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18574 
Train Epoch: 45 [816/1000 26112/32000 (82%)] Loss: 1.95768 (semantic_loss: 0.01217, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.21137 
Train Epoch: 45 [821/1000 26272/32000 (82%)] Loss: 1.96443 (semantic_loss: 0.01599, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19220 
Train Epoch: 45 [826/1000 26432/32000 (83%)] Loss: 1.96032 (semantic_loss: 0.01286, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20557 
Train Epoch: 45 [831/1000 26592/32000 (83%)] Loss: 1.96229 (semantic_loss: 0.01482, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19889 
Train Epoch: 45 [836/1000 26752/32000 (84%)] Loss: 1.95911 (semantic_loss: 0.01165, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18725 
Train Epoch: 45 [841/1000 26912/32000 (84%)] Loss: 1.96060 (semantic_loss: 0.01216, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18626 
Train Epoch: 45 [846/1000 27072/32000 (85%)] Loss: 1.96094 (semantic_loss: 0.01445, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18593 
Train Epoch: 45 [851/1000 27232/32000 (85%)] Loss: 1.96011 (semantic_loss: 0.01166, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18559 
Train Epoch: 45 [856/1000 27392/32000 (86%)] Loss: 1.95952 (semantic_loss: 0.01206, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24139 
Train Epoch: 45 [861/1000 27552/32000 (86%)] Loss: 1.96318 (semantic_loss: 0.01571, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18592 
Train Epoch: 45 [866/1000 27712/32000 (87%)] Loss: 1.96181 (semantic_loss: 0.01434, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18544 
Train Epoch: 45 [871/1000 27872/32000 (87%)] Loss: 1.95949 (semantic_loss: 0.01202, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18736 
Train Epoch: 45 [876/1000 28032/32000 (88%)] Loss: 1.95927 (semantic_loss: 0.01278, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19142 
Train Epoch: 45 [881/1000 28192/32000 (88%)] Loss: 1.96578 (semantic_loss: 0.01831, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20000 
Train Epoch: 45 [886/1000 28352/32000 (89%)] Loss: 1.96282 (semantic_loss: 0.01536, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18969 
Train Epoch: 45 [891/1000 28512/32000 (89%)] Loss: 1.96353 (semantic_loss: 0.01606, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19692 
Train Epoch: 45 [896/1000 28672/32000 (90%)] Loss: 1.96332 (semantic_loss: 0.01683, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20414 
Train Epoch: 45 [901/1000 28832/32000 (90%)] Loss: 1.96047 (semantic_loss: 0.01203, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18890 
Train Epoch: 45 [906/1000 28992/32000 (91%)] Loss: 1.96033 (semantic_loss: 0.01286, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21578 
Train Epoch: 45 [911/1000 29152/32000 (91%)] Loss: 1.95869 (semantic_loss: 0.01220, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20521 
Train Epoch: 45 [916/1000 29312/32000 (92%)] Loss: 1.96566 (semantic_loss: 0.01819, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.68325 
Train Epoch: 45 [921/1000 29472/32000 (92%)] Loss: 1.95958 (semantic_loss: 0.01309, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19612 
Train Epoch: 45 [926/1000 29632/32000 (93%)] Loss: 1.96122 (semantic_loss: 0.01278, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20651 
Train Epoch: 45 [931/1000 29792/32000 (93%)] Loss: 1.96320 (semantic_loss: 0.01768, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19501 
Train Epoch: 45 [936/1000 29952/32000 (94%)] Loss: 1.96156 (semantic_loss: 0.01507, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19868 
Train Epoch: 45 [941/1000 30112/32000 (94%)] Loss: 1.95892 (semantic_loss: 0.01145, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19514 
Train Epoch: 45 [946/1000 30272/32000 (95%)] Loss: 1.96250 (semantic_loss: 0.01406, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18999 
Train Epoch: 45 [951/1000 30432/32000 (95%)] Loss: 1.96361 (semantic_loss: 0.01615, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18897 
Train Epoch: 45 [956/1000 30592/32000 (96%)] Loss: 1.96168 (semantic_loss: 0.01519, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18836 
Train Epoch: 45 [961/1000 30752/32000 (96%)] Loss: 1.96265 (semantic_loss: 0.01518, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19944 
Train Epoch: 45 [966/1000 30912/32000 (97%)] Loss: 1.96375 (semantic_loss: 0.01531, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18786 
Train Epoch: 45 [971/1000 31072/32000 (97%)] Loss: 1.95900 (semantic_loss: 0.01252, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19948 
Train Epoch: 45 [976/1000 31232/32000 (98%)] Loss: 1.96156 (semantic_loss: 0.01409, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.53701 
Train Epoch: 45 [981/1000 31392/32000 (98%)] Loss: 1.95845 (semantic_loss: 0.01099, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19802 
Train Epoch: 45 [986/1000 31552/32000 (99%)] Loss: 1.95856 (semantic_loss: 0.01305, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.21151 
Train Epoch: 45 [991/1000 31712/32000 (99%)] Loss: 1.96137 (semantic_loss: 0.01390, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26607 
Train Epoch: 45 [996/1000 31872/32000 (100%)] Loss: 1.95960 (semantic_loss: 0.01213, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18904 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch45.pth ...
Done in 4.760s
removing stale ckpt [epoch 44] [took 0.29s]
 epoch          : 45
 loss           : 1.961657550573349
 learning_rate  : 4.848868648937623e-07
 n_samples      : 1440000
 n_steps        : 45000
 ActivityNet_val1_test/t2v_metrics/R1: 12.019524100061012
 ActivityNet_val1_test/t2v_metrics/R5: 38.39739678665853
 ActivityNet_val1_test/t2v_metrics/R10: 55.33862111043319
 ActivityNet_val1_test/t2v_metrics/R50: 84.68578401464308
 ActivityNet_val1_test/t2v_metrics/MedR: 8.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 62.98454342078503
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.449129552641413
 ActivityNet_val1_test/v2t_metrics/R1: 12.34492576774456
 ActivityNet_val1_test/v2t_metrics/R5: 38.98718730933496
 ActivityNet_val1_test/v2t_metrics/R10: 55.78604840349807
 ActivityNet_val1_test/v2t_metrics/R50: 84.97051047386618
 ActivityNet_val1_test/v2t_metrics/MedR: 8.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 64.4640024405125
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.9441504087386
 mnt_best       : 29.76339460386141
 not_improved_count: 3
Train Epoch: 46 [1/1000 32/32000 (0%)] Loss: 1.96146 (semantic_loss: 0.01398, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=22.39810 
Train Epoch: 46 [6/1000 192/32000 (1%)] Loss: 1.96042 (semantic_loss: 0.01393, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19488 
Train Epoch: 46 [11/1000 352/32000 (1%)] Loss: 1.96337 (semantic_loss: 0.01591, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18630 
Train Epoch: 46 [16/1000 512/32000 (2%)] Loss: 1.96171 (semantic_loss: 0.01424, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19360 
Train Epoch: 46 [21/1000 672/32000 (2%)] Loss: 1.96274 (semantic_loss: 0.01527, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19118 
Train Epoch: 46 [26/1000 832/32000 (3%)] Loss: 1.96080 (semantic_loss: 0.01333, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18962 
Train Epoch: 46 [31/1000 992/32000 (3%)] Loss: 1.96725 (semantic_loss: 0.01978, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.78896 
Train Epoch: 46 [36/1000 1152/32000 (4%)] Loss: 1.95894 (semantic_loss: 0.01244, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.35012 
Train Epoch: 46 [41/1000 1312/32000 (4%)] Loss: 1.96269 (semantic_loss: 0.01620, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23120 
Train Epoch: 46 [46/1000 1472/32000 (5%)] Loss: 1.96597 (semantic_loss: 0.01851, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20230 
Train Epoch: 46 [51/1000 1632/32000 (5%)] Loss: 1.96003 (semantic_loss: 0.01257, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23098 
Train Epoch: 46 [56/1000 1792/32000 (6%)] Loss: 1.96385 (semantic_loss: 0.01736, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20684 
Train Epoch: 46 [61/1000 1952/32000 (6%)] Loss: 1.96149 (semantic_loss: 0.01304, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21084 
Train Epoch: 46 [66/1000 2112/32000 (7%)] Loss: 1.96380 (semantic_loss: 0.01633, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18982 
Train Epoch: 46 [71/1000 2272/32000 (7%)] Loss: 1.96172 (semantic_loss: 0.01425, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19066 
Train Epoch: 46 [76/1000 2432/32000 (8%)] Loss: 1.96000 (semantic_loss: 0.01254, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18565 
Train Epoch: 46 [81/1000 2592/32000 (8%)] Loss: 1.96273 (semantic_loss: 0.01429, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19051 
Train Epoch: 46 [86/1000 2752/32000 (9%)] Loss: 1.95742 (semantic_loss: 0.01093, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18892 
Train Epoch: 46 [91/1000 2912/32000 (9%)] Loss: 1.96350 (semantic_loss: 0.01505, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18881 
Train Epoch: 46 [96/1000 3072/32000 (10%)] Loss: 1.96368 (semantic_loss: 0.01524, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19360 
Train Epoch: 46 [101/1000 3232/32000 (10%)] Loss: 1.96367 (semantic_loss: 0.01621, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19328 
Train Epoch: 46 [106/1000 3392/32000 (11%)] Loss: 1.96585 (semantic_loss: 0.01935, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20352 
Train Epoch: 46 [111/1000 3552/32000 (11%)] Loss: 1.96094 (semantic_loss: 0.01250, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19016 
Train Epoch: 46 [116/1000 3712/32000 (12%)] Loss: 1.96228 (semantic_loss: 0.01481, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19497 
Train Epoch: 46 [121/1000 3872/32000 (12%)] Loss: 1.96325 (semantic_loss: 0.01578, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18774 
Train Epoch: 46 [126/1000 4032/32000 (13%)] Loss: 1.96274 (semantic_loss: 0.01527, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18854 
Train Epoch: 46 [131/1000 4192/32000 (13%)] Loss: 1.96396 (semantic_loss: 0.01747, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18577 
Train Epoch: 46 [136/1000 4352/32000 (14%)] Loss: 1.95995 (semantic_loss: 0.01248, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18790 
Train Epoch: 46 [141/1000 4512/32000 (14%)] Loss: 1.96042 (semantic_loss: 0.01295, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19395 
Train Epoch: 46 [146/1000 4672/32000 (15%)] Loss: 1.96232 (semantic_loss: 0.01388, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18668 
Train Epoch: 46 [151/1000 4832/32000 (15%)] Loss: 1.96047 (semantic_loss: 0.01398, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18685 
Train Epoch: 46 [156/1000 4992/32000 (16%)] Loss: 1.95888 (semantic_loss: 0.01141, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18750 
Train Epoch: 46 [161/1000 5152/32000 (16%)] Loss: 1.96193 (semantic_loss: 0.01446, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.25478 
Train Epoch: 46 [166/1000 5312/32000 (17%)] Loss: 1.96090 (semantic_loss: 0.01441, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18708 
Train Epoch: 46 [171/1000 5472/32000 (17%)] Loss: 1.96209 (semantic_loss: 0.01560, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18733 
Train Epoch: 46 [176/1000 5632/32000 (18%)] Loss: 1.96038 (semantic_loss: 0.01291, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.37646 
Train Epoch: 46 [181/1000 5792/32000 (18%)] Loss: 1.96283 (semantic_loss: 0.01438, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18881 
Train Epoch: 46 [186/1000 5952/32000 (19%)] Loss: 1.96186 (semantic_loss: 0.01439, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25996 
Train Epoch: 46 [191/1000 6112/32000 (19%)] Loss: 1.96127 (semantic_loss: 0.01478, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22989 
Train Epoch: 46 [196/1000 6272/32000 (20%)] Loss: 1.96143 (semantic_loss: 0.01494, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21261 
Train Epoch: 46 [201/1000 6432/32000 (20%)] Loss: 1.96116 (semantic_loss: 0.01272, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21605 
Train Epoch: 46 [206/1000 6592/32000 (21%)] Loss: 1.96235 (semantic_loss: 0.01488, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21173 
Train Epoch: 46 [211/1000 6752/32000 (21%)] Loss: 1.95999 (semantic_loss: 0.01252, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19234 
Train Epoch: 46 [216/1000 6912/32000 (22%)] Loss: 1.95972 (semantic_loss: 0.01226, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20576 
Train Epoch: 46 [221/1000 7072/32000 (22%)] Loss: 1.96317 (semantic_loss: 0.01473, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19325 
Train Epoch: 46 [226/1000 7232/32000 (23%)] Loss: 1.96204 (semantic_loss: 0.01360, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18753 
Train Epoch: 46 [231/1000 7392/32000 (23%)] Loss: 1.96452 (semantic_loss: 0.01705, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19772 
Train Epoch: 46 [236/1000 7552/32000 (24%)] Loss: 1.96058 (semantic_loss: 0.01312, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18650 
Train Epoch: 46 [241/1000 7712/32000 (24%)] Loss: 1.95888 (semantic_loss: 0.01141, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18713 
Train Epoch: 46 [246/1000 7872/32000 (25%)] Loss: 1.96198 (semantic_loss: 0.01354, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18757 
Train Epoch: 46 [251/1000 8032/32000 (25%)] Loss: 1.95967 (semantic_loss: 0.01317, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19281 
Train Epoch: 46 [256/1000 8192/32000 (26%)] Loss: 1.96065 (semantic_loss: 0.01318, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20044 
Train Epoch: 46 [261/1000 8352/32000 (26%)] Loss: 1.96172 (semantic_loss: 0.01425, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18894 
Train Epoch: 46 [266/1000 8512/32000 (27%)] Loss: 1.96098 (semantic_loss: 0.01254, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18918 
Train Epoch: 46 [271/1000 8672/32000 (27%)] Loss: 1.96053 (semantic_loss: 0.01307, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18600 
Train Epoch: 46 [276/1000 8832/32000 (28%)] Loss: 1.96340 (semantic_loss: 0.01690, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18492 
Train Epoch: 46 [281/1000 8992/32000 (28%)] Loss: 1.96018 (semantic_loss: 0.01369, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18653 
Train Epoch: 46 [286/1000 9152/32000 (29%)] Loss: 1.96192 (semantic_loss: 0.01543, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19087 
Train Epoch: 46 [291/1000 9312/32000 (29%)] Loss: 1.96670 (semantic_loss: 0.01923, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18804 
Train Epoch: 46 [296/1000 9472/32000 (30%)] Loss: 1.96342 (semantic_loss: 0.01693, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18892 
Train Epoch: 46 [301/1000 9632/32000 (30%)] Loss: 1.96584 (semantic_loss: 0.01837, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18818 
Train Epoch: 46 [306/1000 9792/32000 (31%)] Loss: 1.95995 (semantic_loss: 0.01248, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18845 
Train Epoch: 46 [311/1000 9952/32000 (31%)] Loss: 1.96274 (semantic_loss: 0.01527, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19797 
Train Epoch: 46 [316/1000 10112/32000 (32%)] Loss: 1.96878 (semantic_loss: 0.02034, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=1.05131 
Train Epoch: 46 [321/1000 10272/32000 (32%)] Loss: 1.95842 (semantic_loss: 0.01193, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20331 
Train Epoch: 46 [326/1000 10432/32000 (33%)] Loss: 1.96385 (semantic_loss: 0.01541, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18699 
Train Epoch: 46 [331/1000 10592/32000 (33%)] Loss: 1.95894 (semantic_loss: 0.01342, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18665 
Train Epoch: 46 [336/1000 10752/32000 (34%)] Loss: 1.96177 (semantic_loss: 0.01528, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19671 
Train Epoch: 46 [341/1000 10912/32000 (34%)] Loss: 1.96182 (semantic_loss: 0.01436, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21730 
Train Epoch: 46 [346/1000 11072/32000 (35%)] Loss: 1.96055 (semantic_loss: 0.01211, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22190 
Train Epoch: 46 [351/1000 11232/32000 (35%)] Loss: 1.96340 (semantic_loss: 0.01495, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22947 
Train Epoch: 46 [356/1000 11392/32000 (36%)] Loss: 1.96172 (semantic_loss: 0.01328, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21608 
Train Epoch: 46 [361/1000 11552/32000 (36%)] Loss: 1.96473 (semantic_loss: 0.01727, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20456 
Train Epoch: 46 [366/1000 11712/32000 (37%)] Loss: 1.96077 (semantic_loss: 0.01428, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19026 
Train Epoch: 46 [371/1000 11872/32000 (37%)] Loss: 1.95892 (semantic_loss: 0.01244, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18982 
Train Epoch: 46 [376/1000 12032/32000 (38%)] Loss: 1.96070 (semantic_loss: 0.01324, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23142 
Train Epoch: 46 [381/1000 12192/32000 (38%)] Loss: 1.96048 (semantic_loss: 0.01400, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21621 
Train Epoch: 46 [386/1000 12352/32000 (39%)] Loss: 1.95880 (semantic_loss: 0.01329, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19004 
Train Epoch: 46 [391/1000 12512/32000 (39%)] Loss: 1.95893 (semantic_loss: 0.01244, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21834 
Train Epoch: 46 [396/1000 12672/32000 (40%)] Loss: 1.96176 (semantic_loss: 0.01429, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19018 
Train Epoch: 46 [401/1000 12832/32000 (40%)] Loss: 1.96414 (semantic_loss: 0.01668, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22557 
Train Epoch: 46 [406/1000 12992/32000 (41%)] Loss: 1.96253 (semantic_loss: 0.01408, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19669 
Train Epoch: 46 [411/1000 13152/32000 (41%)] Loss: 1.96630 (semantic_loss: 0.01785, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20152 
Train Epoch: 46 [416/1000 13312/32000 (42%)] Loss: 1.95951 (semantic_loss: 0.01302, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19947 
Train Epoch: 46 [421/1000 13472/32000 (42%)] Loss: 1.96146 (semantic_loss: 0.01398, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18975 
Train Epoch: 46 [426/1000 13632/32000 (43%)] Loss: 1.96053 (semantic_loss: 0.01208, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19668 
Train Epoch: 46 [431/1000 13792/32000 (43%)] Loss: 1.95945 (semantic_loss: 0.01199, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19767 
Train Epoch: 46 [436/1000 13952/32000 (44%)] Loss: 1.95891 (semantic_loss: 0.01242, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19866 
Train Epoch: 46 [441/1000 14112/32000 (44%)] Loss: 1.95876 (semantic_loss: 0.01227, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19420 
Train Epoch: 46 [446/1000 14272/32000 (45%)] Loss: 1.96076 (semantic_loss: 0.01330, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19345 
Train Epoch: 46 [451/1000 14432/32000 (45%)] Loss: 1.96360 (semantic_loss: 0.01614, quant_loss: 1.94727, bit_balance_loss: 0.00019) batch_time=0.19168 
Train Epoch: 46 [456/1000 14592/32000 (46%)] Loss: 1.96020 (semantic_loss: 0.01273, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19189 
Train Epoch: 46 [461/1000 14752/32000 (46%)] Loss: 1.96106 (semantic_loss: 0.01262, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18934 
Train Epoch: 46 [466/1000 14912/32000 (47%)] Loss: 1.96114 (semantic_loss: 0.01466, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18786 
Train Epoch: 46 [471/1000 15072/32000 (47%)] Loss: 1.96152 (semantic_loss: 0.01502, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19076 
Train Epoch: 46 [476/1000 15232/32000 (48%)] Loss: 1.96388 (semantic_loss: 0.01641, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20461 
Train Epoch: 46 [481/1000 15392/32000 (48%)] Loss: 1.96187 (semantic_loss: 0.01441, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19114 
Train Epoch: 46 [486/1000 15552/32000 (49%)] Loss: 1.96462 (semantic_loss: 0.01520, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.19095 
Train Epoch: 46 [491/1000 15712/32000 (49%)] Loss: 1.95959 (semantic_loss: 0.01309, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18920 
Train Epoch: 46 [496/1000 15872/32000 (50%)] Loss: 1.95758 (semantic_loss: 0.01109, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.36541 
Train Epoch: 46 [501/1000 16032/32000 (50%)] Loss: 1.95748 (semantic_loss: 0.01099, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22464 
Train Epoch: 46 [506/1000 16192/32000 (51%)] Loss: 1.96372 (semantic_loss: 0.01528, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.27932 
Train Epoch: 46 [511/1000 16352/32000 (51%)] Loss: 1.96484 (semantic_loss: 0.01737, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21702 
Train Epoch: 46 [516/1000 16512/32000 (52%)] Loss: 1.96019 (semantic_loss: 0.01272, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22152 
Train Epoch: 46 [521/1000 16672/32000 (52%)] Loss: 1.96177 (semantic_loss: 0.01333, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20435 
Train Epoch: 46 [526/1000 16832/32000 (53%)] Loss: 1.95953 (semantic_loss: 0.01206, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19620 
Train Epoch: 46 [531/1000 16992/32000 (53%)] Loss: 1.96185 (semantic_loss: 0.01341, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20059 
Train Epoch: 46 [536/1000 17152/32000 (54%)] Loss: 1.96009 (semantic_loss: 0.01262, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21156 
Train Epoch: 46 [541/1000 17312/32000 (54%)] Loss: 1.96270 (semantic_loss: 0.01523, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18834 
Train Epoch: 46 [546/1000 17472/32000 (55%)] Loss: 1.96086 (semantic_loss: 0.01242, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18533 
Train Epoch: 46 [551/1000 17632/32000 (55%)] Loss: 1.95977 (semantic_loss: 0.01231, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18625 
Train Epoch: 46 [556/1000 17792/32000 (56%)] Loss: 1.95979 (semantic_loss: 0.01232, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18678 
Train Epoch: 46 [561/1000 17952/32000 (56%)] Loss: 1.95864 (semantic_loss: 0.01313, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19011 
Train Epoch: 46 [566/1000 18112/32000 (57%)] Loss: 1.96209 (semantic_loss: 0.01560, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21104 
Train Epoch: 46 [571/1000 18272/32000 (57%)] Loss: 1.96219 (semantic_loss: 0.01471, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20712 
Train Epoch: 46 [576/1000 18432/32000 (58%)] Loss: 1.96311 (semantic_loss: 0.01564, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18868 
Train Epoch: 46 [581/1000 18592/32000 (58%)] Loss: 1.96465 (semantic_loss: 0.01718, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18601 
Train Epoch: 46 [586/1000 18752/32000 (59%)] Loss: 1.96118 (semantic_loss: 0.01274, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18697 
Train Epoch: 46 [591/1000 18912/32000 (59%)] Loss: 1.96236 (semantic_loss: 0.01489, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18850 
Train Epoch: 46 [596/1000 19072/32000 (60%)] Loss: 1.95851 (semantic_loss: 0.01201, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18829 
Train Epoch: 46 [601/1000 19232/32000 (60%)] Loss: 1.96078 (semantic_loss: 0.01332, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18748 
Train Epoch: 46 [606/1000 19392/32000 (61%)] Loss: 1.96179 (semantic_loss: 0.01237, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18990 
Train Epoch: 46 [611/1000 19552/32000 (61%)] Loss: 1.96468 (semantic_loss: 0.01623, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18884 
Train Epoch: 46 [616/1000 19712/32000 (62%)] Loss: 1.96097 (semantic_loss: 0.01448, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19600 
Train Epoch: 46 [621/1000 19872/32000 (62%)] Loss: 1.96273 (semantic_loss: 0.01526, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18861 
Train Epoch: 46 [626/1000 20032/32000 (63%)] Loss: 1.95879 (semantic_loss: 0.01133, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18987 
Train Epoch: 46 [631/1000 20192/32000 (63%)] Loss: 1.96511 (semantic_loss: 0.01764, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18988 
Train Epoch: 46 [636/1000 20352/32000 (64%)] Loss: 1.96362 (semantic_loss: 0.01518, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=1.07727 
Train Epoch: 46 [641/1000 20512/32000 (64%)] Loss: 1.96000 (semantic_loss: 0.01350, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18903 
Train Epoch: 46 [646/1000 20672/32000 (65%)] Loss: 1.96377 (semantic_loss: 0.01630, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18860 
Train Epoch: 46 [651/1000 20832/32000 (65%)] Loss: 1.95895 (semantic_loss: 0.01246, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21741 
Train Epoch: 46 [656/1000 20992/32000 (66%)] Loss: 1.96124 (semantic_loss: 0.01378, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22550 
Train Epoch: 46 [661/1000 21152/32000 (66%)] Loss: 1.95862 (semantic_loss: 0.01213, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20318 
Train Epoch: 46 [666/1000 21312/32000 (67%)] Loss: 1.96094 (semantic_loss: 0.01347, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20065 
Train Epoch: 46 [671/1000 21472/32000 (67%)] Loss: 1.95965 (semantic_loss: 0.01218, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20172 
Train Epoch: 46 [676/1000 21632/32000 (68%)] Loss: 1.96191 (semantic_loss: 0.01346, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20775 
Train Epoch: 46 [681/1000 21792/32000 (68%)] Loss: 1.96756 (semantic_loss: 0.02010, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20785 
Train Epoch: 46 [686/1000 21952/32000 (69%)] Loss: 1.96087 (semantic_loss: 0.01341, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20625 
Train Epoch: 46 [691/1000 22112/32000 (69%)] Loss: 1.96582 (semantic_loss: 0.01835, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18809 
Train Epoch: 46 [696/1000 22272/32000 (70%)] Loss: 1.95928 (semantic_loss: 0.01278, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18609 
Train Epoch: 46 [701/1000 22432/32000 (70%)] Loss: 1.96019 (semantic_loss: 0.01273, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20060 
Train Epoch: 46 [706/1000 22592/32000 (71%)] Loss: 1.96283 (semantic_loss: 0.01438, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19086 
Train Epoch: 46 [711/1000 22752/32000 (71%)] Loss: 1.96266 (semantic_loss: 0.01617, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19947 
Train Epoch: 46 [716/1000 22912/32000 (72%)] Loss: 1.95830 (semantic_loss: 0.01181, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20435 
Train Epoch: 46 [721/1000 23072/32000 (72%)] Loss: 1.96807 (semantic_loss: 0.02061, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20038 
Train Epoch: 46 [726/1000 23232/32000 (73%)] Loss: 1.96146 (semantic_loss: 0.01399, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19024 
Train Epoch: 46 [731/1000 23392/32000 (73%)] Loss: 1.95924 (semantic_loss: 0.01177, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18846 
Train Epoch: 46 [736/1000 23552/32000 (74%)] Loss: 1.96179 (semantic_loss: 0.01433, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18873 
Train Epoch: 46 [741/1000 23712/32000 (74%)] Loss: 1.96146 (semantic_loss: 0.01399, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18865 
Train Epoch: 46 [746/1000 23872/32000 (75%)] Loss: 1.95976 (semantic_loss: 0.01229, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18721 
Train Epoch: 46 [751/1000 24032/32000 (75%)] Loss: 1.95968 (semantic_loss: 0.01221, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18927 
Train Epoch: 46 [756/1000 24192/32000 (76%)] Loss: 1.96010 (semantic_loss: 0.01263, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19683 
Train Epoch: 46 [761/1000 24352/32000 (76%)] Loss: 1.95920 (semantic_loss: 0.01271, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18826 
Train Epoch: 46 [766/1000 24512/32000 (77%)] Loss: 1.96334 (semantic_loss: 0.01393, quant_loss: 1.94922, bit_balance_loss: 0.00019) batch_time=0.18811 
Train Epoch: 46 [771/1000 24672/32000 (77%)] Loss: 1.95944 (semantic_loss: 0.01197, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19195 
Train Epoch: 46 [776/1000 24832/32000 (78%)] Loss: 1.96094 (semantic_loss: 0.01444, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19425 
Train Epoch: 46 [781/1000 24992/32000 (78%)] Loss: 1.96058 (semantic_loss: 0.01409, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18921 
Train Epoch: 46 [786/1000 25152/32000 (79%)] Loss: 1.96044 (semantic_loss: 0.01297, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18789 
Train Epoch: 46 [791/1000 25312/32000 (79%)] Loss: 1.96295 (semantic_loss: 0.01451, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19445 
Train Epoch: 46 [796/1000 25472/32000 (80%)] Loss: 1.95999 (semantic_loss: 0.01252, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19200 
Train Epoch: 46 [801/1000 25632/32000 (80%)] Loss: 1.96135 (semantic_loss: 0.01291, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18930 
Train Epoch: 46 [806/1000 25792/32000 (81%)] Loss: 1.96131 (semantic_loss: 0.01384, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18934 
Train Epoch: 46 [811/1000 25952/32000 (81%)] Loss: 1.96120 (semantic_loss: 0.01276, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18821 
Train Epoch: 46 [816/1000 26112/32000 (82%)] Loss: 1.96258 (semantic_loss: 0.01511, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.42086 
Train Epoch: 46 [821/1000 26272/32000 (82%)] Loss: 1.95949 (semantic_loss: 0.01202, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20258 
Train Epoch: 46 [826/1000 26432/32000 (83%)] Loss: 1.95999 (semantic_loss: 0.01253, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24784 
Train Epoch: 46 [831/1000 26592/32000 (83%)] Loss: 1.96012 (semantic_loss: 0.01364, quant_loss: 1.94629, bit_balance_loss: 0.00019) batch_time=0.26707 
Train Epoch: 46 [836/1000 26752/32000 (84%)] Loss: 1.96227 (semantic_loss: 0.01383, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20642 
Train Epoch: 46 [841/1000 26912/32000 (84%)] Loss: 1.96588 (semantic_loss: 0.01841, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21192 
Train Epoch: 46 [846/1000 27072/32000 (85%)] Loss: 1.96003 (semantic_loss: 0.01355, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21475 
Train Epoch: 46 [851/1000 27232/32000 (85%)] Loss: 1.96065 (semantic_loss: 0.01319, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19858 
Train Epoch: 46 [856/1000 27392/32000 (86%)] Loss: 1.96036 (semantic_loss: 0.01387, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19311 
Train Epoch: 46 [861/1000 27552/32000 (86%)] Loss: 1.96035 (semantic_loss: 0.01288, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18731 
Train Epoch: 46 [866/1000 27712/32000 (87%)] Loss: 1.96180 (semantic_loss: 0.01336, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18924 
Train Epoch: 46 [871/1000 27872/32000 (87%)] Loss: 1.96740 (semantic_loss: 0.01896, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19604 
Train Epoch: 46 [876/1000 28032/32000 (88%)] Loss: 1.96158 (semantic_loss: 0.01314, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20403 
Train Epoch: 46 [881/1000 28192/32000 (88%)] Loss: 1.96408 (semantic_loss: 0.01661, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19429 
Train Epoch: 46 [886/1000 28352/32000 (89%)] Loss: 1.96050 (semantic_loss: 0.01402, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19853 
Train Epoch: 46 [891/1000 28512/32000 (89%)] Loss: 1.95984 (semantic_loss: 0.01335, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19200 
Train Epoch: 46 [896/1000 28672/32000 (90%)] Loss: 1.96227 (semantic_loss: 0.01383, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18636 
Train Epoch: 46 [901/1000 28832/32000 (90%)] Loss: 1.96206 (semantic_loss: 0.01363, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18850 
Train Epoch: 46 [906/1000 28992/32000 (91%)] Loss: 1.96498 (semantic_loss: 0.01751, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18833 
Train Epoch: 46 [911/1000 29152/32000 (91%)] Loss: 1.96045 (semantic_loss: 0.01299, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19371 
Train Epoch: 46 [916/1000 29312/32000 (92%)] Loss: 1.95783 (semantic_loss: 0.01135, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20030 
Train Epoch: 46 [921/1000 29472/32000 (92%)] Loss: 1.96110 (semantic_loss: 0.01363, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18974 
Train Epoch: 46 [926/1000 29632/32000 (93%)] Loss: 1.95730 (semantic_loss: 0.01179, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19976 
Train Epoch: 46 [931/1000 29792/32000 (93%)] Loss: 1.96059 (semantic_loss: 0.01312, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18758 
Train Epoch: 46 [936/1000 29952/32000 (94%)] Loss: 1.96777 (semantic_loss: 0.01933, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18834 
Train Epoch: 46 [941/1000 30112/32000 (94%)] Loss: 1.96589 (semantic_loss: 0.01842, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18818 
Train Epoch: 46 [946/1000 30272/32000 (95%)] Loss: 1.96439 (semantic_loss: 0.01790, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22620 
Train Epoch: 46 [951/1000 30432/32000 (95%)] Loss: 1.96224 (semantic_loss: 0.01380, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18478 
Train Epoch: 46 [956/1000 30592/32000 (96%)] Loss: 1.96370 (semantic_loss: 0.01525, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=1.06201 
Train Epoch: 46 [961/1000 30752/32000 (96%)] Loss: 1.96037 (semantic_loss: 0.01290, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18490 
Train Epoch: 46 [966/1000 30912/32000 (97%)] Loss: 1.95917 (semantic_loss: 0.01171, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19963 
Train Epoch: 46 [971/1000 31072/32000 (97%)] Loss: 1.96122 (semantic_loss: 0.01278, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23277 
Train Epoch: 46 [976/1000 31232/32000 (98%)] Loss: 1.96081 (semantic_loss: 0.01334, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21500 
Train Epoch: 46 [981/1000 31392/32000 (98%)] Loss: 1.95934 (semantic_loss: 0.01285, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.26405 
Train Epoch: 46 [986/1000 31552/32000 (99%)] Loss: 1.96131 (semantic_loss: 0.01384, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19164 
Train Epoch: 46 [991/1000 31712/32000 (99%)] Loss: 1.96530 (semantic_loss: 0.01783, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20279 
Train Epoch: 46 [996/1000 31872/32000 (100%)] Loss: 1.95962 (semantic_loss: 0.01313, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18919 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch46.pth ...
Done in 4.214s
removing stale ckpt [epoch 45] [took 0.01s]
 epoch          : 46
 loss           : 1.961747077703476
 learning_rate  : 4.3639817840438605e-07
 n_samples      : 1472000
 n_steps        : 46000
 ActivityNet_val1_test/t2v_metrics/R1: 11.897498474679683
 ActivityNet_val1_test/t2v_metrics/R5: 37.848281472442544
 ActivityNet_val1_test/t2v_metrics/R10: 55.1555826723612
 ActivityNet_val1_test/t2v_metrics/R50: 84.58409599349197
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 62.93685173886516
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.17631459817641
 ActivityNet_val1_test/v2t_metrics/R1: 12.58897701850722
 ActivityNet_val1_test/v2t_metrics/R5: 39.35326418547895
 ActivityNet_val1_test/v2t_metrics/R10: 55.277608297742525
 ActivityNet_val1_test/v2t_metrics/R50: 84.90949766117551
 ActivityNet_val1_test/v2t_metrics/MedR: 8.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 64.37339841366688
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 30.14209830925354
 mnt_best       : 29.76339460386141
 not_improved_count: 4
Train Epoch: 47 [1/1000 32/32000 (0%)] Loss: 1.96012 (semantic_loss: 0.01265, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=24.39092 
Train Epoch: 47 [6/1000 192/32000 (1%)] Loss: 1.96012 (semantic_loss: 0.01363, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20436 
Train Epoch: 47 [11/1000 352/32000 (1%)] Loss: 1.96341 (semantic_loss: 0.01595, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18987 
Train Epoch: 47 [16/1000 512/32000 (2%)] Loss: 1.96014 (semantic_loss: 0.01365, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.48030 
Train Epoch: 47 [21/1000 672/32000 (2%)] Loss: 1.96342 (semantic_loss: 0.01595, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18756 
Train Epoch: 47 [26/1000 832/32000 (3%)] Loss: 1.96147 (semantic_loss: 0.01400, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18684 
Train Epoch: 47 [31/1000 992/32000 (3%)] Loss: 1.96318 (semantic_loss: 0.01571, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19287 
Train Epoch: 47 [36/1000 1152/32000 (4%)] Loss: 1.96024 (semantic_loss: 0.01277, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19153 
Train Epoch: 47 [41/1000 1312/32000 (4%)] Loss: 1.96019 (semantic_loss: 0.01273, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18674 
Train Epoch: 47 [46/1000 1472/32000 (5%)] Loss: 1.96383 (semantic_loss: 0.01734, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20336 
Train Epoch: 47 [51/1000 1632/32000 (5%)] Loss: 1.96178 (semantic_loss: 0.01431, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18968 
Train Epoch: 47 [56/1000 1792/32000 (6%)] Loss: 1.96236 (semantic_loss: 0.01490, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18898 
Train Epoch: 47 [61/1000 1952/32000 (6%)] Loss: 1.96140 (semantic_loss: 0.01393, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19213 
Train Epoch: 47 [66/1000 2112/32000 (7%)] Loss: 1.96080 (semantic_loss: 0.01431, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21901 
Train Epoch: 47 [71/1000 2272/32000 (7%)] Loss: 1.96204 (semantic_loss: 0.01457, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18768 
Train Epoch: 47 [76/1000 2432/32000 (8%)] Loss: 1.96195 (semantic_loss: 0.01350, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20349 
Train Epoch: 47 [81/1000 2592/32000 (8%)] Loss: 1.95964 (semantic_loss: 0.01315, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.30005 
Train Epoch: 47 [86/1000 2752/32000 (9%)] Loss: 1.96172 (semantic_loss: 0.01327, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21158 
Train Epoch: 47 [91/1000 2912/32000 (9%)] Loss: 1.96204 (semantic_loss: 0.01556, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24591 
Train Epoch: 47 [96/1000 3072/32000 (10%)] Loss: 1.96414 (semantic_loss: 0.01668, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20547 
Train Epoch: 47 [101/1000 3232/32000 (10%)] Loss: 1.95852 (semantic_loss: 0.01300, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.19200 
Train Epoch: 47 [106/1000 3392/32000 (11%)] Loss: 1.95852 (semantic_loss: 0.01203, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20036 
Train Epoch: 47 [111/1000 3552/32000 (11%)] Loss: 1.96553 (semantic_loss: 0.01710, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19250 
Train Epoch: 47 [116/1000 3712/32000 (12%)] Loss: 1.96117 (semantic_loss: 0.01370, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21017 
Train Epoch: 47 [121/1000 3872/32000 (12%)] Loss: 1.96058 (semantic_loss: 0.01311, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21553 
Train Epoch: 47 [126/1000 4032/32000 (13%)] Loss: 1.96211 (semantic_loss: 0.01464, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18764 
Train Epoch: 47 [131/1000 4192/32000 (13%)] Loss: 1.96061 (semantic_loss: 0.01315, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.32840 
Train Epoch: 47 [136/1000 4352/32000 (14%)] Loss: 1.96562 (semantic_loss: 0.01912, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18834 
Train Epoch: 47 [141/1000 4512/32000 (14%)] Loss: 1.96178 (semantic_loss: 0.01529, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22592 
Train Epoch: 47 [146/1000 4672/32000 (15%)] Loss: 1.96086 (semantic_loss: 0.01437, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19470 
Train Epoch: 47 [151/1000 4832/32000 (15%)] Loss: 1.96061 (semantic_loss: 0.01510, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19787 
Train Epoch: 47 [156/1000 4992/32000 (16%)] Loss: 1.96137 (semantic_loss: 0.01292, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19398 
Train Epoch: 47 [161/1000 5152/32000 (16%)] Loss: 1.96207 (semantic_loss: 0.01558, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18483 
Train Epoch: 47 [166/1000 5312/32000 (17%)] Loss: 1.96135 (semantic_loss: 0.01388, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18710 
Train Epoch: 47 [171/1000 5472/32000 (17%)] Loss: 1.96043 (semantic_loss: 0.01297, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18975 
Train Epoch: 47 [176/1000 5632/32000 (18%)] Loss: 1.96092 (semantic_loss: 0.01442, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18516 
Train Epoch: 47 [181/1000 5792/32000 (18%)] Loss: 1.96261 (semantic_loss: 0.01514, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18768 
Train Epoch: 47 [186/1000 5952/32000 (19%)] Loss: 1.96069 (semantic_loss: 0.01322, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18773 
Train Epoch: 47 [191/1000 6112/32000 (19%)] Loss: 1.96124 (semantic_loss: 0.01475, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18831 
Train Epoch: 47 [196/1000 6272/32000 (20%)] Loss: 1.96050 (semantic_loss: 0.01303, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19572 
Train Epoch: 47 [201/1000 6432/32000 (20%)] Loss: 1.96149 (semantic_loss: 0.01402, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18767 
Train Epoch: 47 [206/1000 6592/32000 (21%)] Loss: 1.96700 (semantic_loss: 0.02051, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20669 
Train Epoch: 47 [211/1000 6752/32000 (21%)] Loss: 1.96082 (semantic_loss: 0.01433, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18809 
Train Epoch: 47 [216/1000 6912/32000 (22%)] Loss: 1.95950 (semantic_loss: 0.01301, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19087 
Train Epoch: 47 [221/1000 7072/32000 (22%)] Loss: 1.95974 (semantic_loss: 0.01227, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.85705 
Train Epoch: 47 [226/1000 7232/32000 (23%)] Loss: 1.96345 (semantic_loss: 0.01501, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19293 
Train Epoch: 47 [231/1000 7392/32000 (23%)] Loss: 1.96294 (semantic_loss: 0.01644, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19759 
Train Epoch: 47 [236/1000 7552/32000 (24%)] Loss: 1.96228 (semantic_loss: 0.01383, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21453 
Train Epoch: 47 [241/1000 7712/32000 (24%)] Loss: 1.95914 (semantic_loss: 0.01363, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.22446 
Train Epoch: 47 [246/1000 7872/32000 (25%)] Loss: 1.96861 (semantic_loss: 0.02017, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22483 
Train Epoch: 47 [251/1000 8032/32000 (25%)] Loss: 1.96422 (semantic_loss: 0.01675, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21765 
Train Epoch: 47 [256/1000 8192/32000 (26%)] Loss: 1.96075 (semantic_loss: 0.01328, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20862 
Train Epoch: 47 [261/1000 8352/32000 (26%)] Loss: 1.96309 (semantic_loss: 0.01464, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19567 
Train Epoch: 47 [266/1000 8512/32000 (27%)] Loss: 1.96034 (semantic_loss: 0.01287, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19322 
Train Epoch: 47 [271/1000 8672/32000 (27%)] Loss: 1.96619 (semantic_loss: 0.01969, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19605 
Train Epoch: 47 [276/1000 8832/32000 (28%)] Loss: 1.96085 (semantic_loss: 0.01436, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19309 
Train Epoch: 47 [281/1000 8992/32000 (28%)] Loss: 1.95853 (semantic_loss: 0.01204, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18852 
Train Epoch: 47 [286/1000 9152/32000 (29%)] Loss: 1.96351 (semantic_loss: 0.01604, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19068 
Train Epoch: 47 [291/1000 9312/32000 (29%)] Loss: 1.95971 (semantic_loss: 0.01322, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19226 
Train Epoch: 47 [296/1000 9472/32000 (30%)] Loss: 1.96302 (semantic_loss: 0.01457, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18524 
Train Epoch: 47 [301/1000 9632/32000 (30%)] Loss: 1.96508 (semantic_loss: 0.01761, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19662 
Train Epoch: 47 [306/1000 9792/32000 (31%)] Loss: 1.96069 (semantic_loss: 0.01225, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23896 
Train Epoch: 47 [311/1000 9952/32000 (31%)] Loss: 1.96408 (semantic_loss: 0.01661, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19778 
Train Epoch: 47 [316/1000 10112/32000 (32%)] Loss: 1.95852 (semantic_loss: 0.01105, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19537 
Train Epoch: 47 [321/1000 10272/32000 (32%)] Loss: 1.96143 (semantic_loss: 0.01397, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18632 
Train Epoch: 47 [326/1000 10432/32000 (33%)] Loss: 1.96188 (semantic_loss: 0.01442, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18659 
Train Epoch: 47 [331/1000 10592/32000 (33%)] Loss: 1.96074 (semantic_loss: 0.01425, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19924 
Train Epoch: 47 [336/1000 10752/32000 (34%)] Loss: 1.96001 (semantic_loss: 0.01254, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.48360 
Train Epoch: 47 [341/1000 10912/32000 (34%)] Loss: 1.96308 (semantic_loss: 0.01659, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19403 
Train Epoch: 47 [346/1000 11072/32000 (35%)] Loss: 1.96214 (semantic_loss: 0.01369, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18814 
Train Epoch: 47 [351/1000 11232/32000 (35%)] Loss: 1.95988 (semantic_loss: 0.01339, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18575 
Train Epoch: 47 [356/1000 11392/32000 (36%)] Loss: 1.95889 (semantic_loss: 0.01241, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18897 
Train Epoch: 47 [361/1000 11552/32000 (36%)] Loss: 1.96191 (semantic_loss: 0.01445, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18635 
Train Epoch: 47 [366/1000 11712/32000 (37%)] Loss: 1.96267 (semantic_loss: 0.01520, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20374 
Train Epoch: 47 [371/1000 11872/32000 (37%)] Loss: 1.96118 (semantic_loss: 0.01273, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20887 
Train Epoch: 47 [376/1000 12032/32000 (38%)] Loss: 1.96328 (semantic_loss: 0.01581, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18598 
Train Epoch: 47 [381/1000 12192/32000 (38%)] Loss: 1.95873 (semantic_loss: 0.01224, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19774 
Train Epoch: 47 [386/1000 12352/32000 (39%)] Loss: 1.96056 (semantic_loss: 0.01212, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21716 
Train Epoch: 47 [391/1000 12512/32000 (39%)] Loss: 1.96086 (semantic_loss: 0.01241, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18833 
Train Epoch: 47 [396/1000 12672/32000 (40%)] Loss: 1.95856 (semantic_loss: 0.01207, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23127 
Train Epoch: 47 [401/1000 12832/32000 (40%)] Loss: 1.95913 (semantic_loss: 0.01166, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.35421 
Train Epoch: 47 [406/1000 12992/32000 (41%)] Loss: 1.96307 (semantic_loss: 0.01561, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21572 
Train Epoch: 47 [411/1000 13152/32000 (41%)] Loss: 1.96084 (semantic_loss: 0.01435, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22255 
Train Epoch: 47 [416/1000 13312/32000 (42%)] Loss: 1.96007 (semantic_loss: 0.01358, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20277 
Train Epoch: 47 [421/1000 13472/32000 (42%)] Loss: 1.95989 (semantic_loss: 0.01340, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20954 
Train Epoch: 47 [426/1000 13632/32000 (43%)] Loss: 1.96039 (semantic_loss: 0.01390, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19485 
Train Epoch: 47 [431/1000 13792/32000 (43%)] Loss: 1.96097 (semantic_loss: 0.01448, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19219 
Train Epoch: 47 [436/1000 13952/32000 (44%)] Loss: 1.95969 (semantic_loss: 0.01321, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18652 
Train Epoch: 47 [441/1000 14112/32000 (44%)] Loss: 1.96159 (semantic_loss: 0.01413, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18828 
Train Epoch: 47 [446/1000 14272/32000 (45%)] Loss: 1.96020 (semantic_loss: 0.01371, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19024 
Train Epoch: 47 [451/1000 14432/32000 (45%)] Loss: 1.96005 (semantic_loss: 0.01356, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.32905 
Train Epoch: 47 [456/1000 14592/32000 (46%)] Loss: 1.96329 (semantic_loss: 0.01582, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20433 
Train Epoch: 47 [461/1000 14752/32000 (46%)] Loss: 1.96118 (semantic_loss: 0.01371, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24135 
Train Epoch: 47 [466/1000 14912/32000 (47%)] Loss: 1.96671 (semantic_loss: 0.01924, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19843 
Train Epoch: 47 [471/1000 15072/32000 (47%)] Loss: 1.96076 (semantic_loss: 0.01329, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18869 
Train Epoch: 47 [476/1000 15232/32000 (48%)] Loss: 1.96244 (semantic_loss: 0.01596, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18773 
Train Epoch: 47 [481/1000 15392/32000 (48%)] Loss: 1.96071 (semantic_loss: 0.01423, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18926 
Train Epoch: 47 [486/1000 15552/32000 (49%)] Loss: 1.96188 (semantic_loss: 0.01539, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18555 
Train Epoch: 47 [491/1000 15712/32000 (49%)] Loss: 1.96065 (semantic_loss: 0.01319, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19892 
Train Epoch: 47 [496/1000 15872/32000 (50%)] Loss: 1.96011 (semantic_loss: 0.01362, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20888 
Train Epoch: 47 [501/1000 16032/32000 (50%)] Loss: 1.96069 (semantic_loss: 0.01322, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18884 
Train Epoch: 47 [506/1000 16192/32000 (51%)] Loss: 1.96109 (semantic_loss: 0.01362, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18528 
Train Epoch: 47 [511/1000 16352/32000 (51%)] Loss: 1.96612 (semantic_loss: 0.01767, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19254 
Train Epoch: 47 [516/1000 16512/32000 (52%)] Loss: 1.95810 (semantic_loss: 0.01064, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18816 
Train Epoch: 47 [521/1000 16672/32000 (52%)] Loss: 1.96235 (semantic_loss: 0.01391, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18835 
Train Epoch: 47 [526/1000 16832/32000 (53%)] Loss: 1.96045 (semantic_loss: 0.01298, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20780 
Train Epoch: 47 [531/1000 16992/32000 (53%)] Loss: 1.96239 (semantic_loss: 0.01395, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18838 
Train Epoch: 47 [536/1000 17152/32000 (54%)] Loss: 1.96418 (semantic_loss: 0.01769, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18973 
Train Epoch: 47 [541/1000 17312/32000 (54%)] Loss: 1.95978 (semantic_loss: 0.01231, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.86127 
Train Epoch: 47 [546/1000 17472/32000 (55%)] Loss: 1.96741 (semantic_loss: 0.01897, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20532 
Train Epoch: 47 [551/1000 17632/32000 (55%)] Loss: 1.96236 (semantic_loss: 0.01392, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22392 
Train Epoch: 47 [556/1000 17792/32000 (56%)] Loss: 1.96684 (semantic_loss: 0.01839, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22043 
Train Epoch: 47 [561/1000 17952/32000 (56%)] Loss: 1.96084 (semantic_loss: 0.01435, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21594 
Train Epoch: 47 [566/1000 18112/32000 (57%)] Loss: 1.96144 (semantic_loss: 0.01397, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19481 
Train Epoch: 47 [571/1000 18272/32000 (57%)] Loss: 1.96301 (semantic_loss: 0.01554, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20103 
Train Epoch: 47 [576/1000 18432/32000 (58%)] Loss: 1.96261 (semantic_loss: 0.01514, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19749 
Train Epoch: 47 [581/1000 18592/32000 (58%)] Loss: 1.96401 (semantic_loss: 0.01655, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20102 
Train Epoch: 47 [586/1000 18752/32000 (59%)] Loss: 1.95790 (semantic_loss: 0.01141, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19175 
Train Epoch: 47 [591/1000 18912/32000 (59%)] Loss: 1.96352 (semantic_loss: 0.01605, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18921 
Train Epoch: 47 [596/1000 19072/32000 (60%)] Loss: 1.95914 (semantic_loss: 0.01167, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18899 
Train Epoch: 47 [601/1000 19232/32000 (60%)] Loss: 1.96144 (semantic_loss: 0.01398, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19389 
Train Epoch: 47 [606/1000 19392/32000 (61%)] Loss: 1.96242 (semantic_loss: 0.01593, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18874 
Train Epoch: 47 [611/1000 19552/32000 (61%)] Loss: 1.96437 (semantic_loss: 0.01593, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20296 
Train Epoch: 47 [616/1000 19712/32000 (62%)] Loss: 1.95937 (semantic_loss: 0.01288, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20261 
Train Epoch: 47 [621/1000 19872/32000 (62%)] Loss: 1.95937 (semantic_loss: 0.01189, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20512 
Train Epoch: 47 [626/1000 20032/32000 (63%)] Loss: 1.96047 (semantic_loss: 0.01300, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19288 
Train Epoch: 47 [631/1000 20192/32000 (63%)] Loss: 1.95793 (semantic_loss: 0.01144, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18950 
Train Epoch: 47 [636/1000 20352/32000 (64%)] Loss: 1.95841 (semantic_loss: 0.01289, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19041 
Train Epoch: 47 [641/1000 20512/32000 (64%)] Loss: 1.96496 (semantic_loss: 0.01652, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19093 
Train Epoch: 47 [646/1000 20672/32000 (65%)] Loss: 1.95739 (semantic_loss: 0.01090, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19071 
Train Epoch: 47 [651/1000 20832/32000 (65%)] Loss: 1.96265 (semantic_loss: 0.01615, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19556 
Train Epoch: 47 [656/1000 20992/32000 (66%)] Loss: 1.95857 (semantic_loss: 0.01208, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.48932 
Train Epoch: 47 [661/1000 21152/32000 (66%)] Loss: 1.95995 (semantic_loss: 0.01346, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18952 
Train Epoch: 47 [666/1000 21312/32000 (67%)] Loss: 1.96165 (semantic_loss: 0.01419, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18980 
Train Epoch: 47 [671/1000 21472/32000 (67%)] Loss: 1.96094 (semantic_loss: 0.01347, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19053 
Train Epoch: 47 [676/1000 21632/32000 (68%)] Loss: 1.96092 (semantic_loss: 0.01443, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18778 
Train Epoch: 47 [681/1000 21792/32000 (68%)] Loss: 1.96362 (semantic_loss: 0.01615, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20676 
Train Epoch: 47 [686/1000 21952/32000 (69%)] Loss: 1.96031 (semantic_loss: 0.01382, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20407 
Train Epoch: 47 [691/1000 22112/32000 (69%)] Loss: 1.96258 (semantic_loss: 0.01609, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19749 
Train Epoch: 47 [696/1000 22272/32000 (70%)] Loss: 1.96061 (semantic_loss: 0.01314, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18665 
Train Epoch: 47 [701/1000 22432/32000 (70%)] Loss: 1.96261 (semantic_loss: 0.01514, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19147 
Train Epoch: 47 [706/1000 22592/32000 (71%)] Loss: 1.96140 (semantic_loss: 0.01296, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.24342 
Train Epoch: 47 [711/1000 22752/32000 (71%)] Loss: 1.96240 (semantic_loss: 0.01591, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22763 
Train Epoch: 47 [716/1000 22912/32000 (72%)] Loss: 1.96297 (semantic_loss: 0.01550, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24576 
Train Epoch: 47 [721/1000 23072/32000 (72%)] Loss: 1.96098 (semantic_loss: 0.01351, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.31308 
Train Epoch: 47 [726/1000 23232/32000 (73%)] Loss: 1.96302 (semantic_loss: 0.01555, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19562 
Train Epoch: 47 [731/1000 23392/32000 (73%)] Loss: 1.96129 (semantic_loss: 0.01480, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22454 
Train Epoch: 47 [736/1000 23552/32000 (74%)] Loss: 1.96056 (semantic_loss: 0.01407, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21346 
Train Epoch: 47 [741/1000 23712/32000 (74%)] Loss: 1.96282 (semantic_loss: 0.01535, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19513 
Train Epoch: 47 [746/1000 23872/32000 (75%)] Loss: 1.96032 (semantic_loss: 0.01383, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18902 
Train Epoch: 47 [751/1000 24032/32000 (75%)] Loss: 1.96378 (semantic_loss: 0.01826, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.18876 
Train Epoch: 47 [756/1000 24192/32000 (76%)] Loss: 1.95984 (semantic_loss: 0.01237, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19246 
Train Epoch: 47 [761/1000 24352/32000 (76%)] Loss: 1.96441 (semantic_loss: 0.01597, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18626 
Train Epoch: 47 [766/1000 24512/32000 (77%)] Loss: 1.96301 (semantic_loss: 0.01555, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19054 
Train Epoch: 47 [771/1000 24672/32000 (77%)] Loss: 1.96415 (semantic_loss: 0.01765, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.33483 
Train Epoch: 47 [776/1000 24832/32000 (78%)] Loss: 1.95935 (semantic_loss: 0.01189, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20080 
Train Epoch: 47 [781/1000 24992/32000 (78%)] Loss: 1.96434 (semantic_loss: 0.01687, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22492 
Train Epoch: 47 [786/1000 25152/32000 (79%)] Loss: 1.96167 (semantic_loss: 0.01518, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18493 
Train Epoch: 47 [791/1000 25312/32000 (79%)] Loss: 1.95952 (semantic_loss: 0.01205, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18673 
Train Epoch: 47 [796/1000 25472/32000 (80%)] Loss: 1.96032 (semantic_loss: 0.01285, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18891 
Train Epoch: 47 [801/1000 25632/32000 (80%)] Loss: 1.96259 (semantic_loss: 0.01512, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21028 
Train Epoch: 47 [806/1000 25792/32000 (81%)] Loss: 1.95897 (semantic_loss: 0.01151, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19331 
Train Epoch: 47 [811/1000 25952/32000 (81%)] Loss: 1.96159 (semantic_loss: 0.01314, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18648 
Train Epoch: 47 [816/1000 26112/32000 (82%)] Loss: 1.96171 (semantic_loss: 0.01522, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20959 
Train Epoch: 47 [821/1000 26272/32000 (82%)] Loss: 1.96050 (semantic_loss: 0.01206, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18605 
Train Epoch: 47 [826/1000 26432/32000 (83%)] Loss: 1.96400 (semantic_loss: 0.01555, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18905 
Train Epoch: 47 [831/1000 26592/32000 (83%)] Loss: 1.96233 (semantic_loss: 0.01584, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19408 
Train Epoch: 47 [836/1000 26752/32000 (84%)] Loss: 1.96199 (semantic_loss: 0.01550, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20813 
Train Epoch: 47 [841/1000 26912/32000 (84%)] Loss: 1.96401 (semantic_loss: 0.01655, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21128 
Train Epoch: 47 [846/1000 27072/32000 (85%)] Loss: 1.95992 (semantic_loss: 0.01343, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20806 
Train Epoch: 47 [851/1000 27232/32000 (85%)] Loss: 1.96202 (semantic_loss: 0.01358, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18844 
Train Epoch: 47 [856/1000 27392/32000 (86%)] Loss: 1.96133 (semantic_loss: 0.01387, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19493 
Train Epoch: 47 [861/1000 27552/32000 (86%)] Loss: 1.96055 (semantic_loss: 0.01308, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.89535 
Train Epoch: 47 [866/1000 27712/32000 (87%)] Loss: 1.95985 (semantic_loss: 0.01238, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20376 
Train Epoch: 47 [871/1000 27872/32000 (87%)] Loss: 1.96565 (semantic_loss: 0.01818, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21176 
Train Epoch: 47 [876/1000 28032/32000 (88%)] Loss: 1.96282 (semantic_loss: 0.01633, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24168 
Train Epoch: 47 [881/1000 28192/32000 (88%)] Loss: 1.96044 (semantic_loss: 0.01297, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.25274 
Train Epoch: 47 [886/1000 28352/32000 (89%)] Loss: 1.96123 (semantic_loss: 0.01377, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19564 
Train Epoch: 47 [891/1000 28512/32000 (89%)] Loss: 1.95893 (semantic_loss: 0.01244, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20100 
Train Epoch: 47 [896/1000 28672/32000 (90%)] Loss: 1.96272 (semantic_loss: 0.01623, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19473 
Train Epoch: 47 [901/1000 28832/32000 (90%)] Loss: 1.96404 (semantic_loss: 0.01657, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19420 
Train Epoch: 47 [906/1000 28992/32000 (91%)] Loss: 1.96399 (semantic_loss: 0.01749, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20510 
Train Epoch: 47 [911/1000 29152/32000 (91%)] Loss: 1.96248 (semantic_loss: 0.01404, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18500 
Train Epoch: 47 [916/1000 29312/32000 (92%)] Loss: 1.96027 (semantic_loss: 0.01280, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18370 
Train Epoch: 47 [921/1000 29472/32000 (92%)] Loss: 1.95999 (semantic_loss: 0.01253, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19419 
Train Epoch: 47 [926/1000 29632/32000 (93%)] Loss: 1.96650 (semantic_loss: 0.01904, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20995 
Train Epoch: 47 [931/1000 29792/32000 (93%)] Loss: 1.96080 (semantic_loss: 0.01236, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19642 
Train Epoch: 47 [936/1000 29952/32000 (94%)] Loss: 1.96124 (semantic_loss: 0.01475, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20135 
Train Epoch: 47 [941/1000 30112/32000 (94%)] Loss: 1.95986 (semantic_loss: 0.01337, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19349 
Train Epoch: 47 [946/1000 30272/32000 (95%)] Loss: 1.96073 (semantic_loss: 0.01327, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18727 
Train Epoch: 47 [951/1000 30432/32000 (95%)] Loss: 1.96327 (semantic_loss: 0.01679, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18762 
Train Epoch: 47 [956/1000 30592/32000 (96%)] Loss: 1.96033 (semantic_loss: 0.01287, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18827 
Train Epoch: 47 [961/1000 30752/32000 (96%)] Loss: 1.96433 (semantic_loss: 0.01686, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18537 
Train Epoch: 47 [966/1000 30912/32000 (97%)] Loss: 1.95926 (semantic_loss: 0.01180, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20106 
Train Epoch: 47 [971/1000 31072/32000 (97%)] Loss: 1.96509 (semantic_loss: 0.01762, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19574 
Train Epoch: 47 [976/1000 31232/32000 (98%)] Loss: 1.95862 (semantic_loss: 0.01212, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.46079 
Train Epoch: 47 [981/1000 31392/32000 (98%)] Loss: 1.95998 (semantic_loss: 0.01252, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18362 
Train Epoch: 47 [986/1000 31552/32000 (99%)] Loss: 1.96039 (semantic_loss: 0.01293, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19271 
Train Epoch: 47 [991/1000 31712/32000 (99%)] Loss: 1.96398 (semantic_loss: 0.01456, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18957 
Train Epoch: 47 [996/1000 31872/32000 (100%)] Loss: 1.96101 (semantic_loss: 0.01256, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18952 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch47.pth ...
Done in 5.055s
removing stale ckpt [epoch 46] [took 0.01s]
 epoch          : 47
 loss           : 1.961661471605301
 learning_rate  : 3.9275836056394745e-07
 n_samples      : 1504000
 n_steps        : 47000
 ActivityNet_val1_test/t2v_metrics/R1: 11.61277201545658
 ActivityNet_val1_test/t2v_metrics/R5: 38.722798454342076
 ActivityNet_val1_test/t2v_metrics/R10: 55.19625788082164
 ActivityNet_val1_test/t2v_metrics/R50: 84.76713443156396
 ActivityNet_val1_test/t2v_metrics/MedR: 9.0
 ActivityNet_val1_test/t2v_metrics/MeanR: 63.25045759609518
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.17006679443412
 ActivityNet_val1_test/v2t_metrics/R1: 12.527964205816556
 ActivityNet_val1_test/v2t_metrics/R5: 38.66178564165141
 ActivityNet_val1_test/v2t_metrics/R10: 55.41997152735408
 ActivityNet_val1_test/v2t_metrics/R50: 84.78747203579418
 ActivityNet_val1_test/v2t_metrics/MedR: 8.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 65.33241814114297
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.941684989508445
 mnt_best       : 29.76339460386141
 not_improved_count: 5
Train Epoch: 48 [1/1000 32/32000 (0%)] Loss: 1.96116 (semantic_loss: 0.01369, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=22.27145 
Train Epoch: 48 [6/1000 192/32000 (1%)] Loss: 1.96377 (semantic_loss: 0.01631, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20705 
Train Epoch: 48 [11/1000 352/32000 (1%)] Loss: 1.96568 (semantic_loss: 0.01821, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19391 
Train Epoch: 48 [16/1000 512/32000 (2%)] Loss: 1.96252 (semantic_loss: 0.01603, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.27281 
Train Epoch: 48 [21/1000 672/32000 (2%)] Loss: 1.96118 (semantic_loss: 0.01273, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18834 
Train Epoch: 48 [26/1000 832/32000 (3%)] Loss: 1.96588 (semantic_loss: 0.01939, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18721 
Train Epoch: 48 [31/1000 992/32000 (3%)] Loss: 1.96170 (semantic_loss: 0.01326, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18645 
Train Epoch: 48 [36/1000 1152/32000 (4%)] Loss: 1.96040 (semantic_loss: 0.01294, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18723 
Train Epoch: 48 [41/1000 1312/32000 (4%)] Loss: 1.96218 (semantic_loss: 0.01374, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.28319 
Train Epoch: 48 [46/1000 1472/32000 (5%)] Loss: 1.96278 (semantic_loss: 0.01336, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.19714 
Train Epoch: 48 [51/1000 1632/32000 (5%)] Loss: 1.96092 (semantic_loss: 0.01442, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19550 
Train Epoch: 48 [56/1000 1792/32000 (6%)] Loss: 1.96330 (semantic_loss: 0.01583, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20301 
Train Epoch: 48 [61/1000 1952/32000 (6%)] Loss: 1.95971 (semantic_loss: 0.01224, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19305 
Train Epoch: 48 [66/1000 2112/32000 (7%)] Loss: 1.96035 (semantic_loss: 0.01386, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18514 
Train Epoch: 48 [71/1000 2272/32000 (7%)] Loss: 1.96100 (semantic_loss: 0.01353, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18409 
Train Epoch: 48 [76/1000 2432/32000 (8%)] Loss: 1.96318 (semantic_loss: 0.01571, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20168 
Train Epoch: 48 [81/1000 2592/32000 (8%)] Loss: 1.96242 (semantic_loss: 0.01496, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24246 
Train Epoch: 48 [86/1000 2752/32000 (9%)] Loss: 1.95842 (semantic_loss: 0.01192, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18852 
Train Epoch: 48 [91/1000 2912/32000 (9%)] Loss: 1.96041 (semantic_loss: 0.01294, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19168 
Train Epoch: 48 [96/1000 3072/32000 (10%)] Loss: 1.95994 (semantic_loss: 0.01247, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18657 
Train Epoch: 48 [101/1000 3232/32000 (10%)] Loss: 1.95822 (semantic_loss: 0.01172, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18731 
Train Epoch: 48 [106/1000 3392/32000 (11%)] Loss: 1.96117 (semantic_loss: 0.01370, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20305 
Train Epoch: 48 [111/1000 3552/32000 (11%)] Loss: 1.97214 (semantic_loss: 0.02370, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22986 
Train Epoch: 48 [116/1000 3712/32000 (12%)] Loss: 1.96156 (semantic_loss: 0.01507, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19476 
Train Epoch: 48 [121/1000 3872/32000 (12%)] Loss: 1.96117 (semantic_loss: 0.01467, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18691 
Train Epoch: 48 [126/1000 4032/32000 (13%)] Loss: 1.96288 (semantic_loss: 0.01541, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19690 
Train Epoch: 48 [131/1000 4192/32000 (13%)] Loss: 1.96350 (semantic_loss: 0.01604, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18795 
Train Epoch: 48 [136/1000 4352/32000 (14%)] Loss: 1.96255 (semantic_loss: 0.01314, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.21614 
Train Epoch: 48 [141/1000 4512/32000 (14%)] Loss: 1.95979 (semantic_loss: 0.01232, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21280 
Train Epoch: 48 [146/1000 4672/32000 (15%)] Loss: 1.95981 (semantic_loss: 0.01332, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20833 
Train Epoch: 48 [151/1000 4832/32000 (15%)] Loss: 1.96037 (semantic_loss: 0.01193, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19188 
Train Epoch: 48 [156/1000 4992/32000 (16%)] Loss: 1.96231 (semantic_loss: 0.01581, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19189 
Train Epoch: 48 [161/1000 5152/32000 (16%)] Loss: 1.96236 (semantic_loss: 0.01391, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19071 
Train Epoch: 48 [166/1000 5312/32000 (17%)] Loss: 1.96343 (semantic_loss: 0.01596, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19452 
Train Epoch: 48 [171/1000 5472/32000 (17%)] Loss: 1.95951 (semantic_loss: 0.01204, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21769 
Train Epoch: 48 [176/1000 5632/32000 (18%)] Loss: 1.96176 (semantic_loss: 0.01429, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.46634 
Train Epoch: 48 [181/1000 5792/32000 (18%)] Loss: 1.96026 (semantic_loss: 0.01279, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18750 
Train Epoch: 48 [186/1000 5952/32000 (19%)] Loss: 1.96051 (semantic_loss: 0.01402, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18805 
Train Epoch: 48 [191/1000 6112/32000 (19%)] Loss: 1.95915 (semantic_loss: 0.01266, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18784 
Train Epoch: 48 [196/1000 6272/32000 (20%)] Loss: 1.96067 (semantic_loss: 0.01321, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24998 
Train Epoch: 48 [201/1000 6432/32000 (20%)] Loss: 1.96121 (semantic_loss: 0.01277, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19417 
Train Epoch: 48 [206/1000 6592/32000 (21%)] Loss: 1.96469 (semantic_loss: 0.01821, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19637 
Train Epoch: 48 [211/1000 6752/32000 (21%)] Loss: 1.96456 (semantic_loss: 0.01709, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.34373 
Train Epoch: 48 [216/1000 6912/32000 (22%)] Loss: 1.96050 (semantic_loss: 0.01401, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.31034 
Train Epoch: 48 [221/1000 7072/32000 (22%)] Loss: 1.95940 (semantic_loss: 0.01292, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18631 
Train Epoch: 48 [226/1000 7232/32000 (23%)] Loss: 1.96032 (semantic_loss: 0.01284, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18560 
Train Epoch: 48 [231/1000 7392/32000 (23%)] Loss: 1.96056 (semantic_loss: 0.01310, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19881 
Train Epoch: 48 [236/1000 7552/32000 (24%)] Loss: 1.96331 (semantic_loss: 0.01486, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18713 
Train Epoch: 48 [241/1000 7712/32000 (24%)] Loss: 1.96081 (semantic_loss: 0.01335, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18540 
Train Epoch: 48 [246/1000 7872/32000 (25%)] Loss: 1.95978 (semantic_loss: 0.01427, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.23654 
Train Epoch: 48 [251/1000 8032/32000 (25%)] Loss: 1.95875 (semantic_loss: 0.01129, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18749 
Train Epoch: 48 [256/1000 8192/32000 (26%)] Loss: 1.96557 (semantic_loss: 0.01810, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.28451 
Train Epoch: 48 [261/1000 8352/32000 (26%)] Loss: 1.96105 (semantic_loss: 0.01358, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18803 
Train Epoch: 48 [266/1000 8512/32000 (27%)] Loss: 1.96117 (semantic_loss: 0.01370, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21352 
Train Epoch: 48 [271/1000 8672/32000 (27%)] Loss: 1.96320 (semantic_loss: 0.01574, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20905 
Train Epoch: 48 [276/1000 8832/32000 (28%)] Loss: 1.96028 (semantic_loss: 0.01476, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18520 
Train Epoch: 48 [281/1000 8992/32000 (28%)] Loss: 1.96139 (semantic_loss: 0.01294, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.24390 
Train Epoch: 48 [286/1000 9152/32000 (29%)] Loss: 1.96147 (semantic_loss: 0.01302, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18838 
Train Epoch: 48 [291/1000 9312/32000 (29%)] Loss: 1.96105 (semantic_loss: 0.01456, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22340 
Train Epoch: 48 [296/1000 9472/32000 (30%)] Loss: 1.95878 (semantic_loss: 0.01229, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24488 
Train Epoch: 48 [301/1000 9632/32000 (30%)] Loss: 1.96298 (semantic_loss: 0.01649, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.28987 
Train Epoch: 48 [306/1000 9792/32000 (31%)] Loss: 1.95887 (semantic_loss: 0.01238, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20334 
Train Epoch: 48 [311/1000 9952/32000 (31%)] Loss: 1.96288 (semantic_loss: 0.01639, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19630 
Train Epoch: 48 [316/1000 10112/32000 (32%)] Loss: 1.96125 (semantic_loss: 0.01378, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19315 
Train Epoch: 48 [321/1000 10272/32000 (32%)] Loss: 1.95997 (semantic_loss: 0.01251, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19276 
Train Epoch: 48 [326/1000 10432/32000 (33%)] Loss: 1.96097 (semantic_loss: 0.01350, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20964 
Train Epoch: 48 [331/1000 10592/32000 (33%)] Loss: 1.95919 (semantic_loss: 0.01172, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18890 
Train Epoch: 48 [336/1000 10752/32000 (34%)] Loss: 1.96082 (semantic_loss: 0.01433, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.25286 
Train Epoch: 48 [341/1000 10912/32000 (34%)] Loss: 1.95985 (semantic_loss: 0.01433, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19108 
Train Epoch: 48 [346/1000 11072/32000 (35%)] Loss: 1.96067 (semantic_loss: 0.01223, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18525 
Train Epoch: 48 [351/1000 11232/32000 (35%)] Loss: 1.95986 (semantic_loss: 0.01337, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19576 
Train Epoch: 48 [356/1000 11392/32000 (36%)] Loss: 1.96193 (semantic_loss: 0.01349, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18892 
Train Epoch: 48 [361/1000 11552/32000 (36%)] Loss: 1.95967 (semantic_loss: 0.01318, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.31018 
Train Epoch: 48 [366/1000 11712/32000 (37%)] Loss: 1.95828 (semantic_loss: 0.01081, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19055 
Train Epoch: 48 [371/1000 11872/32000 (37%)] Loss: 1.95840 (semantic_loss: 0.01192, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19039 
Train Epoch: 48 [376/1000 12032/32000 (38%)] Loss: 1.96011 (semantic_loss: 0.01362, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18844 
Train Epoch: 48 [381/1000 12192/32000 (38%)] Loss: 1.96191 (semantic_loss: 0.01347, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18907 
Train Epoch: 48 [386/1000 12352/32000 (39%)] Loss: 1.96185 (semantic_loss: 0.01439, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18959 
Train Epoch: 48 [391/1000 12512/32000 (39%)] Loss: 1.95934 (semantic_loss: 0.01188, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18658 
Train Epoch: 48 [396/1000 12672/32000 (40%)] Loss: 1.96113 (semantic_loss: 0.01268, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20391 
Train Epoch: 48 [401/1000 12832/32000 (40%)] Loss: 1.95958 (semantic_loss: 0.01309, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24348 
Train Epoch: 48 [406/1000 12992/32000 (41%)] Loss: 1.96188 (semantic_loss: 0.01343, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18768 
Train Epoch: 48 [411/1000 13152/32000 (41%)] Loss: 1.96527 (semantic_loss: 0.01878, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18548 
Train Epoch: 48 [416/1000 13312/32000 (42%)] Loss: 1.96367 (semantic_loss: 0.01523, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18601 
Train Epoch: 48 [421/1000 13472/32000 (42%)] Loss: 1.96189 (semantic_loss: 0.01540, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19460 
Train Epoch: 48 [426/1000 13632/32000 (43%)] Loss: 1.96213 (semantic_loss: 0.01564, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20029 
Train Epoch: 48 [431/1000 13792/32000 (43%)] Loss: 1.95967 (semantic_loss: 0.01318, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22792 
Train Epoch: 48 [436/1000 13952/32000 (44%)] Loss: 1.95963 (semantic_loss: 0.01216, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18660 
Train Epoch: 48 [441/1000 14112/32000 (44%)] Loss: 1.95876 (semantic_loss: 0.01130, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18573 
Train Epoch: 48 [446/1000 14272/32000 (45%)] Loss: 1.96226 (semantic_loss: 0.01577, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22034 
Train Epoch: 48 [451/1000 14432/32000 (45%)] Loss: 1.96224 (semantic_loss: 0.01380, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20396 
Train Epoch: 48 [456/1000 14592/32000 (46%)] Loss: 1.96086 (semantic_loss: 0.01241, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21811 
Train Epoch: 48 [461/1000 14752/32000 (46%)] Loss: 1.95871 (semantic_loss: 0.01222, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21320 
Train Epoch: 48 [466/1000 14912/32000 (47%)] Loss: 1.95906 (semantic_loss: 0.01257, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19232 
Train Epoch: 48 [471/1000 15072/32000 (47%)] Loss: 1.96588 (semantic_loss: 0.01743, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19069 
Train Epoch: 48 [476/1000 15232/32000 (48%)] Loss: 1.96159 (semantic_loss: 0.01510, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21631 
Train Epoch: 48 [481/1000 15392/32000 (48%)] Loss: 1.96552 (semantic_loss: 0.01805, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21255 
Train Epoch: 48 [486/1000 15552/32000 (49%)] Loss: 1.96206 (semantic_loss: 0.01557, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19525 
Train Epoch: 48 [491/1000 15712/32000 (49%)] Loss: 1.96224 (semantic_loss: 0.01477, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20432 
Train Epoch: 48 [496/1000 15872/32000 (50%)] Loss: 1.96187 (semantic_loss: 0.01342, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.45137 
Train Epoch: 48 [501/1000 16032/32000 (50%)] Loss: 1.96032 (semantic_loss: 0.01284, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18855 
Train Epoch: 48 [506/1000 16192/32000 (51%)] Loss: 1.95999 (semantic_loss: 0.01252, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19058 
Train Epoch: 48 [511/1000 16352/32000 (51%)] Loss: 1.96377 (semantic_loss: 0.01436, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18953 
Train Epoch: 48 [516/1000 16512/32000 (52%)] Loss: 1.95966 (semantic_loss: 0.01317, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24598 
Train Epoch: 48 [521/1000 16672/32000 (52%)] Loss: 1.95839 (semantic_loss: 0.01190, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20448 
Train Epoch: 48 [526/1000 16832/32000 (53%)] Loss: 1.96144 (semantic_loss: 0.01398, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19420 
Train Epoch: 48 [531/1000 16992/32000 (53%)] Loss: 1.96454 (semantic_loss: 0.01707, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.33292 
Train Epoch: 48 [536/1000 17152/32000 (54%)] Loss: 1.96011 (semantic_loss: 0.01265, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.29420 
Train Epoch: 48 [541/1000 17312/32000 (54%)] Loss: 1.96585 (semantic_loss: 0.01741, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19714 
Train Epoch: 48 [546/1000 17472/32000 (55%)] Loss: 1.96136 (semantic_loss: 0.01292, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18635 
Train Epoch: 48 [551/1000 17632/32000 (55%)] Loss: 1.96026 (semantic_loss: 0.01280, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20053 
Train Epoch: 48 [556/1000 17792/32000 (56%)] Loss: 1.96161 (semantic_loss: 0.01512, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19467 
Train Epoch: 48 [561/1000 17952/32000 (56%)] Loss: 1.95987 (semantic_loss: 0.01241, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21019 
Train Epoch: 48 [566/1000 18112/32000 (57%)] Loss: 1.95997 (semantic_loss: 0.01251, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23572 
Train Epoch: 48 [571/1000 18272/32000 (57%)] Loss: 1.96437 (semantic_loss: 0.01690, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18766 
Train Epoch: 48 [576/1000 18432/32000 (58%)] Loss: 1.96305 (semantic_loss: 0.01461, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.29361 
Train Epoch: 48 [581/1000 18592/32000 (58%)] Loss: 1.96346 (semantic_loss: 0.01599, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18794 
Train Epoch: 48 [586/1000 18752/32000 (59%)] Loss: 1.95879 (semantic_loss: 0.01132, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21174 
Train Epoch: 48 [591/1000 18912/32000 (59%)] Loss: 1.96071 (semantic_loss: 0.01324, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20349 
Train Epoch: 48 [596/1000 19072/32000 (60%)] Loss: 1.96097 (semantic_loss: 0.01350, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18465 
Train Epoch: 48 [601/1000 19232/32000 (60%)] Loss: 1.95908 (semantic_loss: 0.01161, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.25417 
Train Epoch: 48 [606/1000 19392/32000 (61%)] Loss: 1.96043 (semantic_loss: 0.01297, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18850 
Train Epoch: 48 [611/1000 19552/32000 (61%)] Loss: 1.96090 (semantic_loss: 0.01344, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22437 
Train Epoch: 48 [616/1000 19712/32000 (62%)] Loss: 1.96249 (semantic_loss: 0.01503, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23304 
Train Epoch: 48 [621/1000 19872/32000 (62%)] Loss: 1.96359 (semantic_loss: 0.01710, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.28716 
Train Epoch: 48 [626/1000 20032/32000 (63%)] Loss: 1.96104 (semantic_loss: 0.01358, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19913 
Train Epoch: 48 [631/1000 20192/32000 (63%)] Loss: 1.95866 (semantic_loss: 0.01119, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19777 
Train Epoch: 48 [636/1000 20352/32000 (64%)] Loss: 1.96154 (semantic_loss: 0.01310, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20159 
Train Epoch: 48 [641/1000 20512/32000 (64%)] Loss: 1.96491 (semantic_loss: 0.01647, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22608 
Train Epoch: 48 [646/1000 20672/32000 (65%)] Loss: 1.95824 (semantic_loss: 0.01175, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20846 
Train Epoch: 48 [651/1000 20832/32000 (65%)] Loss: 1.96104 (semantic_loss: 0.01455, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19497 
Train Epoch: 48 [656/1000 20992/32000 (66%)] Loss: 1.96041 (semantic_loss: 0.01294, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.26175 
Train Epoch: 48 [661/1000 21152/32000 (66%)] Loss: 1.95949 (semantic_loss: 0.01202, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18160 
Train Epoch: 48 [666/1000 21312/32000 (67%)] Loss: 1.96206 (semantic_loss: 0.01557, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18932 
Train Epoch: 48 [671/1000 21472/32000 (67%)] Loss: 1.96011 (semantic_loss: 0.01265, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20280 
Train Epoch: 48 [676/1000 21632/32000 (68%)] Loss: 1.96276 (semantic_loss: 0.01627, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19677 
Train Epoch: 48 [681/1000 21792/32000 (68%)] Loss: 1.96122 (semantic_loss: 0.01473, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.29689 
Train Epoch: 48 [686/1000 21952/32000 (69%)] Loss: 1.96598 (semantic_loss: 0.01852, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19293 
Train Epoch: 48 [691/1000 22112/32000 (69%)] Loss: 1.96326 (semantic_loss: 0.01580, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19296 
Train Epoch: 48 [696/1000 22272/32000 (70%)] Loss: 1.96360 (semantic_loss: 0.01516, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18730 
Train Epoch: 48 [701/1000 22432/32000 (70%)] Loss: 1.96239 (semantic_loss: 0.01395, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18742 
Train Epoch: 48 [706/1000 22592/32000 (71%)] Loss: 1.96220 (semantic_loss: 0.01473, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18815 
Train Epoch: 48 [711/1000 22752/32000 (71%)] Loss: 1.96270 (semantic_loss: 0.01523, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18736 
Train Epoch: 48 [716/1000 22912/32000 (72%)] Loss: 1.96324 (semantic_loss: 0.01578, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20463 
Train Epoch: 48 [721/1000 23072/32000 (72%)] Loss: 1.96117 (semantic_loss: 0.01467, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24301 
Train Epoch: 48 [726/1000 23232/32000 (73%)] Loss: 1.96059 (semantic_loss: 0.01410, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18567 
Train Epoch: 48 [731/1000 23392/32000 (73%)] Loss: 1.96086 (semantic_loss: 0.01340, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18697 
Train Epoch: 48 [736/1000 23552/32000 (74%)] Loss: 1.96268 (semantic_loss: 0.01522, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19030 
Train Epoch: 48 [741/1000 23712/32000 (74%)] Loss: 1.96186 (semantic_loss: 0.01341, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18817 
Train Epoch: 48 [746/1000 23872/32000 (75%)] Loss: 1.95815 (semantic_loss: 0.01068, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18598 
Train Epoch: 48 [751/1000 24032/32000 (75%)] Loss: 1.96468 (semantic_loss: 0.01624, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23165 
Train Epoch: 48 [756/1000 24192/32000 (76%)] Loss: 1.96258 (semantic_loss: 0.01609, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18738 
Train Epoch: 48 [761/1000 24352/32000 (76%)] Loss: 1.96036 (semantic_loss: 0.01289, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18670 
Train Epoch: 48 [766/1000 24512/32000 (77%)] Loss: 1.95919 (semantic_loss: 0.01270, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20218 
Train Epoch: 48 [771/1000 24672/32000 (77%)] Loss: 1.96362 (semantic_loss: 0.01811, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.20920 
Train Epoch: 48 [776/1000 24832/32000 (78%)] Loss: 1.96398 (semantic_loss: 0.01749, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20132 
Train Epoch: 48 [781/1000 24992/32000 (78%)] Loss: 1.96261 (semantic_loss: 0.01613, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20160 
Train Epoch: 48 [786/1000 25152/32000 (79%)] Loss: 1.96339 (semantic_loss: 0.01495, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22216 
Train Epoch: 48 [791/1000 25312/32000 (79%)] Loss: 1.96146 (semantic_loss: 0.01399, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19971 
Train Epoch: 48 [796/1000 25472/32000 (80%)] Loss: 1.96106 (semantic_loss: 0.01359, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19538 
Train Epoch: 48 [801/1000 25632/32000 (80%)] Loss: 1.96150 (semantic_loss: 0.01403, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20159 
Train Epoch: 48 [806/1000 25792/32000 (81%)] Loss: 1.95920 (semantic_loss: 0.01174, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19537 
Train Epoch: 48 [811/1000 25952/32000 (81%)] Loss: 1.96679 (semantic_loss: 0.01932, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20702 
Train Epoch: 48 [816/1000 26112/32000 (82%)] Loss: 1.96052 (semantic_loss: 0.01403, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.45360 
Train Epoch: 48 [821/1000 26272/32000 (82%)] Loss: 1.95997 (semantic_loss: 0.01250, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18722 
Train Epoch: 48 [826/1000 26432/32000 (83%)] Loss: 1.96263 (semantic_loss: 0.01614, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18701 
Train Epoch: 48 [831/1000 26592/32000 (83%)] Loss: 1.96162 (semantic_loss: 0.01220, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18938 
Train Epoch: 48 [836/1000 26752/32000 (84%)] Loss: 1.96034 (semantic_loss: 0.01288, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.25249 
Train Epoch: 48 [841/1000 26912/32000 (84%)] Loss: 1.96570 (semantic_loss: 0.01920, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20084 
Train Epoch: 48 [846/1000 27072/32000 (85%)] Loss: 1.96240 (semantic_loss: 0.01493, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20135 
Train Epoch: 48 [851/1000 27232/32000 (85%)] Loss: 1.96006 (semantic_loss: 0.01259, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.32515 
Train Epoch: 48 [856/1000 27392/32000 (86%)] Loss: 1.95836 (semantic_loss: 0.01187, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.28895 
Train Epoch: 48 [861/1000 27552/32000 (86%)] Loss: 1.96102 (semantic_loss: 0.01355, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18483 
Train Epoch: 48 [866/1000 27712/32000 (87%)] Loss: 1.96375 (semantic_loss: 0.01628, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18872 
Train Epoch: 48 [871/1000 27872/32000 (87%)] Loss: 1.96049 (semantic_loss: 0.01401, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22029 
Train Epoch: 48 [876/1000 28032/32000 (88%)] Loss: 1.96283 (semantic_loss: 0.01634, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18977 
Train Epoch: 48 [881/1000 28192/32000 (88%)] Loss: 1.96064 (semantic_loss: 0.01318, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19089 
Train Epoch: 48 [886/1000 28352/32000 (89%)] Loss: 1.96001 (semantic_loss: 0.01255, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22663 
Train Epoch: 48 [891/1000 28512/32000 (89%)] Loss: 1.96846 (semantic_loss: 0.02001, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18534 
Train Epoch: 48 [896/1000 28672/32000 (90%)] Loss: 1.95973 (semantic_loss: 0.01128, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.28225 
Train Epoch: 48 [901/1000 28832/32000 (90%)] Loss: 1.96365 (semantic_loss: 0.01716, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20383 
Train Epoch: 48 [906/1000 28992/32000 (91%)] Loss: 1.96533 (semantic_loss: 0.01689, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20701 
Train Epoch: 48 [911/1000 29152/32000 (91%)] Loss: 1.96175 (semantic_loss: 0.01331, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20874 
Train Epoch: 48 [916/1000 29312/32000 (92%)] Loss: 1.96168 (semantic_loss: 0.01323, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18974 
Train Epoch: 48 [921/1000 29472/32000 (92%)] Loss: 1.96398 (semantic_loss: 0.01651, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.25863 
Train Epoch: 48 [926/1000 29632/32000 (93%)] Loss: 1.95935 (semantic_loss: 0.01189, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18895 
Train Epoch: 48 [931/1000 29792/32000 (93%)] Loss: 1.96175 (semantic_loss: 0.01429, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20299 
Train Epoch: 48 [936/1000 29952/32000 (94%)] Loss: 1.96426 (semantic_loss: 0.01582, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23489 
Train Epoch: 48 [941/1000 30112/32000 (94%)] Loss: 1.96232 (semantic_loss: 0.01388, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.26641 
Train Epoch: 48 [946/1000 30272/32000 (95%)] Loss: 1.96281 (semantic_loss: 0.01437, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23087 
Train Epoch: 48 [951/1000 30432/32000 (95%)] Loss: 1.96004 (semantic_loss: 0.01355, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19909 
Train Epoch: 48 [956/1000 30592/32000 (96%)] Loss: 1.96833 (semantic_loss: 0.01989, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20738 
Train Epoch: 48 [961/1000 30752/32000 (96%)] Loss: 1.96337 (semantic_loss: 0.01591, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19997 
Train Epoch: 48 [966/1000 30912/32000 (97%)] Loss: 1.96349 (semantic_loss: 0.01602, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19892 
Train Epoch: 48 [971/1000 31072/32000 (97%)] Loss: 1.96239 (semantic_loss: 0.01492, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19733 
Train Epoch: 48 [976/1000 31232/32000 (98%)] Loss: 1.96146 (semantic_loss: 0.01301, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.25933 
Train Epoch: 48 [981/1000 31392/32000 (98%)] Loss: 1.96382 (semantic_loss: 0.01537, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18728 
Train Epoch: 48 [986/1000 31552/32000 (99%)] Loss: 1.95927 (semantic_loss: 0.01181, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19064 
Train Epoch: 48 [991/1000 31712/32000 (99%)] Loss: 1.95941 (semantic_loss: 0.01292, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18696 
Train Epoch: 48 [996/1000 31872/32000 (100%)] Loss: 1.95948 (semantic_loss: 0.01299, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19608 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch48.pth ...
Done in 4.660s
removing stale ckpt [epoch 47] [took 0.01s]
 epoch          : 48
 loss           : 1.9616906710863113
 learning_rate  : 3.5348252450755274e-07
 n_samples      : 1536000
 n_steps        : 48000
 ActivityNet_val1_test/t2v_metrics/R1: 11.531421598535692
 ActivityNet_val1_test/t2v_metrics/R5: 38.417734390888754
 ActivityNet_val1_test/t2v_metrics/R10: 55.37929631889364
 ActivityNet_val1_test/t2v_metrics/R50: 84.7061216188733
 ActivityNet_val1_test/t2v_metrics/MedR: 8.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 62.65131177547285
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.05721662319131
 ActivityNet_val1_test/v2t_metrics/R1: 12.487288997356112
 ActivityNet_val1_test/v2t_metrics/R5: 38.49908480780964
 ActivityNet_val1_test/v2t_metrics/R10: 55.78604840349807
 ActivityNet_val1_test/v2t_metrics/R50: 84.86882245271507
 ActivityNet_val1_test/v2t_metrics/MedR: 8.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 64.81960545047794
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 29.932849076805464
 mnt_best       : 29.76339460386141
 not_improved_count: 6
Train Epoch: 49 [1/1000 32/32000 (0%)] Loss: 1.95941 (semantic_loss: 0.01292, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=22.71782 
Train Epoch: 49 [6/1000 192/32000 (1%)] Loss: 1.95975 (semantic_loss: 0.01229, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18545 
Train Epoch: 49 [11/1000 352/32000 (1%)] Loss: 1.96570 (semantic_loss: 0.01726, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19308 
Train Epoch: 49 [16/1000 512/32000 (2%)] Loss: 1.95983 (semantic_loss: 0.01431, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.22003 
Train Epoch: 49 [21/1000 672/32000 (2%)] Loss: 1.95652 (semantic_loss: 0.01101, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18543 
Train Epoch: 49 [26/1000 832/32000 (3%)] Loss: 1.96242 (semantic_loss: 0.01397, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21514 
Train Epoch: 49 [31/1000 992/32000 (3%)] Loss: 1.95887 (semantic_loss: 0.01336, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18638 
Train Epoch: 49 [36/1000 1152/32000 (4%)] Loss: 1.95999 (semantic_loss: 0.01253, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19984 
Train Epoch: 49 [41/1000 1312/32000 (4%)] Loss: 1.96409 (semantic_loss: 0.01760, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18608 
Train Epoch: 49 [46/1000 1472/32000 (5%)] Loss: 1.96358 (semantic_loss: 0.01611, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18911 
Train Epoch: 49 [51/1000 1632/32000 (5%)] Loss: 1.96607 (semantic_loss: 0.01860, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18519 
Train Epoch: 49 [56/1000 1792/32000 (6%)] Loss: 1.96081 (semantic_loss: 0.01334, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18531 
Train Epoch: 49 [61/1000 1952/32000 (6%)] Loss: 1.96491 (semantic_loss: 0.01744, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20887 
Train Epoch: 49 [66/1000 2112/32000 (7%)] Loss: 1.96205 (semantic_loss: 0.01458, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.29490 
Train Epoch: 49 [71/1000 2272/32000 (7%)] Loss: 1.96433 (semantic_loss: 0.01783, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21767 
Train Epoch: 49 [76/1000 2432/32000 (8%)] Loss: 1.95912 (semantic_loss: 0.01165, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20811 
Train Epoch: 49 [81/1000 2592/32000 (8%)] Loss: 1.96226 (semantic_loss: 0.01479, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19063 
Train Epoch: 49 [86/1000 2752/32000 (9%)] Loss: 1.96576 (semantic_loss: 0.01829, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19858 
Train Epoch: 49 [91/1000 2912/32000 (9%)] Loss: 1.95982 (semantic_loss: 0.01236, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20457 
Train Epoch: 49 [96/1000 3072/32000 (10%)] Loss: 1.96117 (semantic_loss: 0.01370, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.70798 
Train Epoch: 49 [101/1000 3232/32000 (10%)] Loss: 1.95792 (semantic_loss: 0.01143, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18383 
Train Epoch: 49 [106/1000 3392/32000 (11%)] Loss: 1.96069 (semantic_loss: 0.01225, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18601 
Train Epoch: 49 [111/1000 3552/32000 (11%)] Loss: 1.96279 (semantic_loss: 0.01434, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18583 
Train Epoch: 49 [116/1000 3712/32000 (12%)] Loss: 1.96332 (semantic_loss: 0.01585, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18432 
Train Epoch: 49 [121/1000 3872/32000 (12%)] Loss: 1.95824 (semantic_loss: 0.01077, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20097 
Train Epoch: 49 [126/1000 4032/32000 (13%)] Loss: 1.96359 (semantic_loss: 0.01710, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19144 
Train Epoch: 49 [131/1000 4192/32000 (13%)] Loss: 1.96094 (semantic_loss: 0.01445, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19887 
Train Epoch: 49 [136/1000 4352/32000 (14%)] Loss: 1.96200 (semantic_loss: 0.01454, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21079 
Train Epoch: 49 [141/1000 4512/32000 (14%)] Loss: 1.95891 (semantic_loss: 0.01241, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21039 
Train Epoch: 49 [146/1000 4672/32000 (15%)] Loss: 1.96771 (semantic_loss: 0.02024, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.58493 
Train Epoch: 49 [151/1000 4832/32000 (15%)] Loss: 1.96122 (semantic_loss: 0.01277, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.29773 
Train Epoch: 49 [156/1000 4992/32000 (16%)] Loss: 1.96257 (semantic_loss: 0.01607, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18513 
Train Epoch: 49 [161/1000 5152/32000 (16%)] Loss: 1.96059 (semantic_loss: 0.01313, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18775 
Train Epoch: 49 [166/1000 5312/32000 (17%)] Loss: 1.95894 (semantic_loss: 0.01245, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18536 
Train Epoch: 49 [171/1000 5472/32000 (17%)] Loss: 1.96353 (semantic_loss: 0.01606, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18588 
Train Epoch: 49 [176/1000 5632/32000 (18%)] Loss: 1.95835 (semantic_loss: 0.01186, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18488 
Train Epoch: 49 [181/1000 5792/32000 (18%)] Loss: 1.96034 (semantic_loss: 0.01287, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18645 
Train Epoch: 49 [186/1000 5952/32000 (19%)] Loss: 1.96323 (semantic_loss: 0.01674, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18491 
Train Epoch: 49 [191/1000 6112/32000 (19%)] Loss: 1.96172 (semantic_loss: 0.01231, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18433 
Train Epoch: 49 [196/1000 6272/32000 (20%)] Loss: 1.96026 (semantic_loss: 0.01280, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20976 
Train Epoch: 49 [201/1000 6432/32000 (20%)] Loss: 1.96073 (semantic_loss: 0.01424, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18731 
Train Epoch: 49 [206/1000 6592/32000 (21%)] Loss: 1.96024 (semantic_loss: 0.01277, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18679 
Train Epoch: 49 [211/1000 6752/32000 (21%)] Loss: 1.96248 (semantic_loss: 0.01403, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22392 
Train Epoch: 49 [216/1000 6912/32000 (22%)] Loss: 1.95890 (semantic_loss: 0.01241, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20327 
Train Epoch: 49 [221/1000 7072/32000 (22%)] Loss: 1.96135 (semantic_loss: 0.01389, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20990 
Train Epoch: 49 [226/1000 7232/32000 (23%)] Loss: 1.96184 (semantic_loss: 0.01339, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.23123 
Train Epoch: 49 [231/1000 7392/32000 (23%)] Loss: 1.96353 (semantic_loss: 0.01607, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21374 
Train Epoch: 49 [236/1000 7552/32000 (24%)] Loss: 1.96054 (semantic_loss: 0.01307, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20177 
Train Epoch: 49 [241/1000 7712/32000 (24%)] Loss: 1.95943 (semantic_loss: 0.01294, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19535 
Train Epoch: 49 [246/1000 7872/32000 (25%)] Loss: 1.96800 (semantic_loss: 0.02053, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19287 
Train Epoch: 49 [251/1000 8032/32000 (25%)] Loss: 1.96265 (semantic_loss: 0.01519, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19820 
Train Epoch: 49 [256/1000 8192/32000 (26%)] Loss: 1.96467 (semantic_loss: 0.01623, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20693 
Train Epoch: 49 [261/1000 8352/32000 (26%)] Loss: 1.96476 (semantic_loss: 0.01827, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18877 
Train Epoch: 49 [266/1000 8512/32000 (27%)] Loss: 1.96340 (semantic_loss: 0.01691, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18742 
Train Epoch: 49 [271/1000 8672/32000 (27%)] Loss: 1.96312 (semantic_loss: 0.01468, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22089 
Train Epoch: 49 [276/1000 8832/32000 (28%)] Loss: 1.95884 (semantic_loss: 0.01235, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.27102 
Train Epoch: 49 [281/1000 8992/32000 (28%)] Loss: 1.96342 (semantic_loss: 0.01596, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18889 
Train Epoch: 49 [286/1000 9152/32000 (29%)] Loss: 1.96785 (semantic_loss: 0.01940, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.20452 
Train Epoch: 49 [291/1000 9312/32000 (29%)] Loss: 1.96294 (semantic_loss: 0.01547, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19132 
Train Epoch: 49 [296/1000 9472/32000 (30%)] Loss: 1.96409 (semantic_loss: 0.01661, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18631 
Train Epoch: 49 [301/1000 9632/32000 (30%)] Loss: 1.96410 (semantic_loss: 0.01664, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18555 
Train Epoch: 49 [306/1000 9792/32000 (31%)] Loss: 1.96004 (semantic_loss: 0.01355, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18945 
Train Epoch: 49 [311/1000 9952/32000 (31%)] Loss: 1.96397 (semantic_loss: 0.01553, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19834 
Train Epoch: 49 [316/1000 10112/32000 (32%)] Loss: 1.96246 (semantic_loss: 0.01499, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18747 
Train Epoch: 49 [321/1000 10272/32000 (32%)] Loss: 1.96362 (semantic_loss: 0.01712, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.21979 
Train Epoch: 49 [326/1000 10432/32000 (33%)] Loss: 1.96143 (semantic_loss: 0.01495, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18949 
Train Epoch: 49 [331/1000 10592/32000 (33%)] Loss: 1.96420 (semantic_loss: 0.01575, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18478 
Train Epoch: 49 [336/1000 10752/32000 (34%)] Loss: 1.96188 (semantic_loss: 0.01441, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22138 
Train Epoch: 49 [341/1000 10912/32000 (34%)] Loss: 1.95982 (semantic_loss: 0.01333, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18621 
Train Epoch: 49 [346/1000 11072/32000 (35%)] Loss: 1.96108 (semantic_loss: 0.01362, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18610 
Train Epoch: 49 [351/1000 11232/32000 (35%)] Loss: 1.96615 (semantic_loss: 0.01771, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18799 
Train Epoch: 49 [356/1000 11392/32000 (36%)] Loss: 1.96235 (semantic_loss: 0.01488, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19031 
Train Epoch: 49 [361/1000 11552/32000 (36%)] Loss: 1.96078 (semantic_loss: 0.01330, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18773 
Train Epoch: 49 [366/1000 11712/32000 (37%)] Loss: 1.96144 (semantic_loss: 0.01397, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20817 
Train Epoch: 49 [371/1000 11872/32000 (37%)] Loss: 1.95987 (semantic_loss: 0.01435, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.21245 
Train Epoch: 49 [376/1000 12032/32000 (38%)] Loss: 1.95868 (semantic_loss: 0.01122, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19481 
Train Epoch: 49 [381/1000 12192/32000 (38%)] Loss: 1.96335 (semantic_loss: 0.01588, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22223 
Train Epoch: 49 [386/1000 12352/32000 (39%)] Loss: 1.96155 (semantic_loss: 0.01506, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.31678 
Train Epoch: 49 [391/1000 12512/32000 (39%)] Loss: 1.96103 (semantic_loss: 0.01356, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19910 
Train Epoch: 49 [396/1000 12672/32000 (40%)] Loss: 1.96235 (semantic_loss: 0.01489, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19366 
Train Epoch: 49 [401/1000 12832/32000 (40%)] Loss: 1.95968 (semantic_loss: 0.01222, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19752 
Train Epoch: 49 [406/1000 12992/32000 (41%)] Loss: 1.96078 (semantic_loss: 0.01331, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.20996 
Train Epoch: 49 [411/1000 13152/32000 (41%)] Loss: 1.95994 (semantic_loss: 0.01247, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20560 
Train Epoch: 49 [416/1000 13312/32000 (42%)] Loss: 1.95961 (semantic_loss: 0.01312, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.65999 
Train Epoch: 49 [421/1000 13472/32000 (42%)] Loss: 1.96092 (semantic_loss: 0.01442, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18837 
Train Epoch: 49 [426/1000 13632/32000 (43%)] Loss: 1.96398 (semantic_loss: 0.01749, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18668 
Train Epoch: 49 [431/1000 13792/32000 (43%)] Loss: 1.95921 (semantic_loss: 0.01174, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21107 
Train Epoch: 49 [436/1000 13952/32000 (44%)] Loss: 1.95982 (semantic_loss: 0.01236, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18756 
Train Epoch: 49 [441/1000 14112/32000 (44%)] Loss: 1.95826 (semantic_loss: 0.01177, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20747 
Train Epoch: 49 [446/1000 14272/32000 (45%)] Loss: 1.96089 (semantic_loss: 0.01245, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19806 
Train Epoch: 49 [451/1000 14432/32000 (45%)] Loss: 1.96006 (semantic_loss: 0.01357, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19112 
Train Epoch: 49 [456/1000 14592/32000 (46%)] Loss: 1.96160 (semantic_loss: 0.01316, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20593 
Train Epoch: 49 [461/1000 14752/32000 (46%)] Loss: 1.96481 (semantic_loss: 0.01735, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20885 
Train Epoch: 49 [466/1000 14912/32000 (47%)] Loss: 1.96274 (semantic_loss: 0.01527, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.58027 
Train Epoch: 49 [471/1000 15072/32000 (47%)] Loss: 1.96627 (semantic_loss: 0.01782, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.30988 
Train Epoch: 49 [476/1000 15232/32000 (48%)] Loss: 1.96125 (semantic_loss: 0.01378, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18810 
Train Epoch: 49 [481/1000 15392/32000 (48%)] Loss: 1.96351 (semantic_loss: 0.01604, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18826 
Train Epoch: 49 [486/1000 15552/32000 (49%)] Loss: 1.95999 (semantic_loss: 0.01252, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18843 
Train Epoch: 49 [491/1000 15712/32000 (49%)] Loss: 1.96016 (semantic_loss: 0.01269, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18657 
Train Epoch: 49 [496/1000 15872/32000 (50%)] Loss: 1.96228 (semantic_loss: 0.01579, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18777 
Train Epoch: 49 [501/1000 16032/32000 (50%)] Loss: 1.96158 (semantic_loss: 0.01411, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18585 
Train Epoch: 49 [506/1000 16192/32000 (51%)] Loss: 1.95975 (semantic_loss: 0.01228, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19600 
Train Epoch: 49 [511/1000 16352/32000 (51%)] Loss: 1.96514 (semantic_loss: 0.01768, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19041 
Train Epoch: 49 [516/1000 16512/32000 (52%)] Loss: 1.96225 (semantic_loss: 0.01479, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19176 
Train Epoch: 49 [521/1000 16672/32000 (52%)] Loss: 1.96353 (semantic_loss: 0.01411, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.18708 
Train Epoch: 49 [526/1000 16832/32000 (53%)] Loss: 1.95969 (semantic_loss: 0.01320, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20717 
Train Epoch: 49 [531/1000 16992/32000 (53%)] Loss: 1.95885 (semantic_loss: 0.01236, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.27386 
Train Epoch: 49 [536/1000 17152/32000 (54%)] Loss: 1.96007 (semantic_loss: 0.01359, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20894 
Train Epoch: 49 [541/1000 17312/32000 (54%)] Loss: 1.95914 (semantic_loss: 0.01167, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19191 
Train Epoch: 49 [546/1000 17472/32000 (55%)] Loss: 1.96033 (semantic_loss: 0.01287, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20190 
Train Epoch: 49 [551/1000 17632/32000 (55%)] Loss: 1.96158 (semantic_loss: 0.01314, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19257 
Train Epoch: 49 [556/1000 17792/32000 (56%)] Loss: 1.96228 (semantic_loss: 0.01481, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20345 
Train Epoch: 49 [561/1000 17952/32000 (56%)] Loss: 1.95998 (semantic_loss: 0.01251, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19247 
Train Epoch: 49 [566/1000 18112/32000 (57%)] Loss: 1.96427 (semantic_loss: 0.01583, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19093 
Train Epoch: 49 [571/1000 18272/32000 (57%)] Loss: 1.96303 (semantic_loss: 0.01556, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19245 
Train Epoch: 49 [576/1000 18432/32000 (58%)] Loss: 1.96403 (semantic_loss: 0.01754, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19987 
Train Epoch: 49 [581/1000 18592/32000 (58%)] Loss: 1.96131 (semantic_loss: 0.01287, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19117 
Train Epoch: 49 [586/1000 18752/32000 (59%)] Loss: 1.96546 (semantic_loss: 0.01800, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19596 
Train Epoch: 49 [591/1000 18912/32000 (59%)] Loss: 1.96086 (semantic_loss: 0.01242, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22698 
Train Epoch: 49 [596/1000 19072/32000 (60%)] Loss: 1.96148 (semantic_loss: 0.01401, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.28318 
Train Epoch: 49 [601/1000 19232/32000 (60%)] Loss: 1.96284 (semantic_loss: 0.01732, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20156 
Train Epoch: 49 [606/1000 19392/32000 (61%)] Loss: 1.95931 (semantic_loss: 0.01282, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18909 
Train Epoch: 49 [611/1000 19552/32000 (61%)] Loss: 1.96224 (semantic_loss: 0.01477, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18769 
Train Epoch: 49 [616/1000 19712/32000 (62%)] Loss: 1.96243 (semantic_loss: 0.01496, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18925 
Train Epoch: 49 [621/1000 19872/32000 (62%)] Loss: 1.96148 (semantic_loss: 0.01499, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18680 
Train Epoch: 49 [626/1000 20032/32000 (63%)] Loss: 1.96232 (semantic_loss: 0.01486, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19154 
Train Epoch: 49 [631/1000 20192/32000 (63%)] Loss: 1.96020 (semantic_loss: 0.01273, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18771 
Train Epoch: 49 [636/1000 20352/32000 (64%)] Loss: 1.96618 (semantic_loss: 0.01871, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21075 
Train Epoch: 49 [641/1000 20512/32000 (64%)] Loss: 1.96528 (semantic_loss: 0.01683, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.21856 
Train Epoch: 49 [646/1000 20672/32000 (65%)] Loss: 1.96031 (semantic_loss: 0.01186, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19308 
Train Epoch: 49 [651/1000 20832/32000 (65%)] Loss: 1.95784 (semantic_loss: 0.01135, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18922 
Train Epoch: 49 [656/1000 20992/32000 (66%)] Loss: 1.96588 (semantic_loss: 0.01841, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21988 
Train Epoch: 49 [661/1000 21152/32000 (66%)] Loss: 1.96198 (semantic_loss: 0.01550, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18451 
Train Epoch: 49 [666/1000 21312/32000 (67%)] Loss: 1.96346 (semantic_loss: 0.01599, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19413 
Train Epoch: 49 [671/1000 21472/32000 (67%)] Loss: 1.96509 (semantic_loss: 0.01763, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18744 
Train Epoch: 49 [676/1000 21632/32000 (68%)] Loss: 1.96239 (semantic_loss: 0.01493, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19140 
Train Epoch: 49 [681/1000 21792/32000 (68%)] Loss: 1.95919 (semantic_loss: 0.01172, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19668 
Train Epoch: 49 [686/1000 21952/32000 (69%)] Loss: 1.96456 (semantic_loss: 0.01612, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22878 
Train Epoch: 49 [691/1000 22112/32000 (69%)] Loss: 1.95892 (semantic_loss: 0.01242, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20956 
Train Epoch: 49 [696/1000 22272/32000 (70%)] Loss: 1.95970 (semantic_loss: 0.01419, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.21151 
Train Epoch: 49 [701/1000 22432/32000 (70%)] Loss: 1.96094 (semantic_loss: 0.01348, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.25883 
Train Epoch: 49 [706/1000 22592/32000 (71%)] Loss: 1.96506 (semantic_loss: 0.01661, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.26650 
Train Epoch: 49 [711/1000 22752/32000 (71%)] Loss: 1.95980 (semantic_loss: 0.01233, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19561 
Train Epoch: 49 [716/1000 22912/32000 (72%)] Loss: 1.96185 (semantic_loss: 0.01438, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19305 
Train Epoch: 49 [721/1000 23072/32000 (72%)] Loss: 1.96177 (semantic_loss: 0.01333, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20157 
Train Epoch: 49 [726/1000 23232/32000 (73%)] Loss: 1.96175 (semantic_loss: 0.01428, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19174 
Train Epoch: 49 [731/1000 23392/32000 (73%)] Loss: 1.95744 (semantic_loss: 0.01192, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.19721 
Train Epoch: 49 [736/1000 23552/32000 (74%)] Loss: 1.95910 (semantic_loss: 0.01164, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.65513 
Train Epoch: 49 [741/1000 23712/32000 (74%)] Loss: 1.95895 (semantic_loss: 0.01148, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19121 
Train Epoch: 49 [746/1000 23872/32000 (75%)] Loss: 1.96191 (semantic_loss: 0.01444, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19887 
Train Epoch: 49 [751/1000 24032/32000 (75%)] Loss: 1.95939 (semantic_loss: 0.01290, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19905 
Train Epoch: 49 [756/1000 24192/32000 (76%)] Loss: 1.96271 (semantic_loss: 0.01427, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18939 
Train Epoch: 49 [761/1000 24352/32000 (76%)] Loss: 1.96547 (semantic_loss: 0.01801, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21227 
Train Epoch: 49 [766/1000 24512/32000 (77%)] Loss: 1.96029 (semantic_loss: 0.01282, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18966 
Train Epoch: 49 [771/1000 24672/32000 (77%)] Loss: 1.96209 (semantic_loss: 0.01365, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20453 
Train Epoch: 49 [776/1000 24832/32000 (78%)] Loss: 1.96556 (semantic_loss: 0.01711, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22369 
Train Epoch: 49 [781/1000 24992/32000 (78%)] Loss: 1.95987 (semantic_loss: 0.01240, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19937 
Train Epoch: 49 [786/1000 25152/32000 (79%)] Loss: 1.96219 (semantic_loss: 0.01375, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.60089 
Train Epoch: 49 [791/1000 25312/32000 (79%)] Loss: 1.96147 (semantic_loss: 0.01401, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.30362 
Train Epoch: 49 [796/1000 25472/32000 (80%)] Loss: 1.95969 (semantic_loss: 0.01223, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20205 
Train Epoch: 49 [801/1000 25632/32000 (80%)] Loss: 1.95998 (semantic_loss: 0.01252, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19610 
Train Epoch: 49 [806/1000 25792/32000 (81%)] Loss: 1.96242 (semantic_loss: 0.01398, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19779 
Train Epoch: 49 [811/1000 25952/32000 (81%)] Loss: 1.96265 (semantic_loss: 0.01518, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18888 
Train Epoch: 49 [816/1000 26112/32000 (82%)] Loss: 1.96117 (semantic_loss: 0.01370, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22894 
Train Epoch: 49 [821/1000 26272/32000 (82%)] Loss: 1.96099 (semantic_loss: 0.01255, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18508 
Train Epoch: 49 [826/1000 26432/32000 (83%)] Loss: 1.96140 (semantic_loss: 0.01393, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18724 
Train Epoch: 49 [831/1000 26592/32000 (83%)] Loss: 1.95987 (semantic_loss: 0.01240, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18770 
Train Epoch: 49 [836/1000 26752/32000 (84%)] Loss: 1.96395 (semantic_loss: 0.01844, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.20766 
Train Epoch: 49 [841/1000 26912/32000 (84%)] Loss: 1.96270 (semantic_loss: 0.01524, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21112 
Train Epoch: 49 [846/1000 27072/32000 (85%)] Loss: 1.96287 (semantic_loss: 0.01541, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22888 
Train Epoch: 49 [851/1000 27232/32000 (85%)] Loss: 1.96530 (semantic_loss: 0.01783, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.29003 
Train Epoch: 49 [856/1000 27392/32000 (86%)] Loss: 1.95928 (semantic_loss: 0.01278, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19810 
Train Epoch: 49 [861/1000 27552/32000 (86%)] Loss: 1.95883 (semantic_loss: 0.01137, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19619 
Train Epoch: 49 [866/1000 27712/32000 (87%)] Loss: 1.95967 (semantic_loss: 0.01319, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.22285 
Train Epoch: 49 [871/1000 27872/32000 (87%)] Loss: 1.95897 (semantic_loss: 0.01151, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.38995 
Train Epoch: 49 [876/1000 28032/32000 (88%)] Loss: 1.96171 (semantic_loss: 0.01326, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19023 
Train Epoch: 49 [881/1000 28192/32000 (88%)] Loss: 1.96479 (semantic_loss: 0.01733, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21382 
Train Epoch: 49 [886/1000 28352/32000 (89%)] Loss: 1.96410 (semantic_loss: 0.01762, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19000 
Train Epoch: 49 [891/1000 28512/32000 (89%)] Loss: 1.96664 (semantic_loss: 0.01820, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18825 
Train Epoch: 49 [896/1000 28672/32000 (90%)] Loss: 1.96259 (semantic_loss: 0.01512, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18609 
Train Epoch: 49 [901/1000 28832/32000 (90%)] Loss: 1.95903 (semantic_loss: 0.01254, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19466 
Train Epoch: 49 [906/1000 28992/32000 (91%)] Loss: 1.96067 (semantic_loss: 0.01418, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19770 
Train Epoch: 49 [911/1000 29152/32000 (91%)] Loss: 1.95960 (semantic_loss: 0.01214, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24195 
Train Epoch: 49 [916/1000 29312/32000 (92%)] Loss: 1.96771 (semantic_loss: 0.02024, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.27457 
Train Epoch: 49 [921/1000 29472/32000 (92%)] Loss: 1.95931 (semantic_loss: 0.01184, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18593 
Train Epoch: 49 [926/1000 29632/32000 (93%)] Loss: 1.96378 (semantic_loss: 0.01631, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18998 
Train Epoch: 49 [931/1000 29792/32000 (93%)] Loss: 1.95920 (semantic_loss: 0.01174, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20520 
Train Epoch: 49 [936/1000 29952/32000 (94%)] Loss: 1.96280 (semantic_loss: 0.01435, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18611 
Train Epoch: 49 [941/1000 30112/32000 (94%)] Loss: 1.96067 (semantic_loss: 0.01321, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18711 
Train Epoch: 49 [946/1000 30272/32000 (95%)] Loss: 1.95983 (semantic_loss: 0.01237, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20840 
Train Epoch: 49 [951/1000 30432/32000 (95%)] Loss: 1.96257 (semantic_loss: 0.01413, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18552 
Train Epoch: 49 [956/1000 30592/32000 (96%)] Loss: 1.96268 (semantic_loss: 0.01619, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18947 
Train Epoch: 49 [961/1000 30752/32000 (96%)] Loss: 1.96040 (semantic_loss: 0.01292, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.21970 
Train Epoch: 49 [966/1000 30912/32000 (97%)] Loss: 1.96238 (semantic_loss: 0.01589, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19795 
Train Epoch: 49 [971/1000 31072/32000 (97%)] Loss: 1.96300 (semantic_loss: 0.01554, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18774 
Train Epoch: 49 [976/1000 31232/32000 (98%)] Loss: 1.95901 (semantic_loss: 0.01155, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22041 
Train Epoch: 49 [981/1000 31392/32000 (98%)] Loss: 1.95946 (semantic_loss: 0.01296, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18898 
Train Epoch: 49 [986/1000 31552/32000 (99%)] Loss: 1.96323 (semantic_loss: 0.01576, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18771 
Train Epoch: 49 [991/1000 31712/32000 (99%)] Loss: 1.96012 (semantic_loss: 0.01168, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21282 
Train Epoch: 49 [996/1000 31872/32000 (100%)] Loss: 1.96001 (semantic_loss: 0.01254, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23077 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch49.pth ...
Done in 4.617s
removing stale ckpt [epoch 48] [took 0.02s]
 epoch          : 49
 loss           : 1.9617607274055482
 learning_rate  : 3.1813427205679747e-07
 n_samples      : 1568000
 n_steps        : 49000
 ActivityNet_val1_test/t2v_metrics/R1: 11.917836078909904
 ActivityNet_val1_test/t2v_metrics/R5: 38.56009762050031
 ActivityNet_val1_test/t2v_metrics/R10: 55.562334756965626
 ActivityNet_val1_test/t2v_metrics/R50: 84.7061216188733
 ActivityNet_val1_test/t2v_metrics/MedR: 8.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 62.960951799877975
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.44683840317328
 ActivityNet_val1_test/v2t_metrics/R1: 12.426276184665447
 ActivityNet_val1_test/v2t_metrics/R5: 39.08887533048607
 ActivityNet_val1_test/v2t_metrics/R10: 55.82672361195851
 ActivityNet_val1_test/v2t_metrics/R50: 84.88916005694529
 ActivityNet_val1_test/v2t_metrics/MedR: 8.5
 ActivityNet_val1_test/v2t_metrics/MeanR: 65.19168191986984
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 30.04314813351128
 mnt_best       : 29.76339460386141
 not_improved_count: 7
Train Epoch: 50 [1/1000 32/32000 (0%)] Loss: 1.96164 (semantic_loss: 0.01418, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=22.97296 
Train Epoch: 50 [6/1000 192/32000 (1%)] Loss: 1.96011 (semantic_loss: 0.01264, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18927 
Train Epoch: 50 [11/1000 352/32000 (1%)] Loss: 1.96006 (semantic_loss: 0.01259, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18529 
Train Epoch: 50 [16/1000 512/32000 (2%)] Loss: 1.95995 (semantic_loss: 0.01346, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.91012 
Train Epoch: 50 [21/1000 672/32000 (2%)] Loss: 1.96011 (semantic_loss: 0.01264, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18778 
Train Epoch: 50 [26/1000 832/32000 (3%)] Loss: 1.96546 (semantic_loss: 0.01897, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20266 
Train Epoch: 50 [31/1000 992/32000 (3%)] Loss: 1.96006 (semantic_loss: 0.01260, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20632 
Train Epoch: 50 [36/1000 1152/32000 (4%)] Loss: 1.95962 (semantic_loss: 0.01313, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20209 
Train Epoch: 50 [41/1000 1312/32000 (4%)] Loss: 1.96060 (semantic_loss: 0.01314, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21054 
Train Epoch: 50 [46/1000 1472/32000 (5%)] Loss: 1.96480 (semantic_loss: 0.01733, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18729 
Train Epoch: 50 [51/1000 1632/32000 (5%)] Loss: 1.96747 (semantic_loss: 0.01804, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.25248 
Train Epoch: 50 [56/1000 1792/32000 (6%)] Loss: 1.95916 (semantic_loss: 0.01170, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19486 
Train Epoch: 50 [61/1000 1952/32000 (6%)] Loss: 1.95956 (semantic_loss: 0.01307, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18635 
Train Epoch: 50 [66/1000 2112/32000 (7%)] Loss: 1.96357 (semantic_loss: 0.01610, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20857 
Train Epoch: 50 [71/1000 2272/32000 (7%)] Loss: 1.96200 (semantic_loss: 0.01453, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19270 
Train Epoch: 50 [76/1000 2432/32000 (8%)] Loss: 1.96090 (semantic_loss: 0.01343, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21678 
Train Epoch: 50 [81/1000 2592/32000 (8%)] Loss: 1.96327 (semantic_loss: 0.01580, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18621 
Train Epoch: 50 [86/1000 2752/32000 (9%)] Loss: 1.96474 (semantic_loss: 0.01629, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18734 
Train Epoch: 50 [91/1000 2912/32000 (9%)] Loss: 1.95881 (semantic_loss: 0.01134, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19082 
Train Epoch: 50 [96/1000 3072/32000 (10%)] Loss: 1.96081 (semantic_loss: 0.01432, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18782 
Train Epoch: 50 [101/1000 3232/32000 (10%)] Loss: 1.96038 (semantic_loss: 0.01291, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18632 
Train Epoch: 50 [106/1000 3392/32000 (11%)] Loss: 1.96515 (semantic_loss: 0.01768, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18778 
Train Epoch: 50 [111/1000 3552/32000 (11%)] Loss: 1.96171 (semantic_loss: 0.01327, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18697 
Train Epoch: 50 [116/1000 3712/32000 (12%)] Loss: 1.96091 (semantic_loss: 0.01442, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.29537 
Train Epoch: 50 [121/1000 3872/32000 (12%)] Loss: 1.96250 (semantic_loss: 0.01601, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19169 
Train Epoch: 50 [126/1000 4032/32000 (13%)] Loss: 1.96471 (semantic_loss: 0.01725, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21833 
Train Epoch: 50 [131/1000 4192/32000 (13%)] Loss: 1.96509 (semantic_loss: 0.01762, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21914 
Train Epoch: 50 [136/1000 4352/32000 (14%)] Loss: 1.96381 (semantic_loss: 0.01439, quant_loss: 1.94922, bit_balance_loss: 0.00020) batch_time=0.22133 
Train Epoch: 50 [141/1000 4512/32000 (14%)] Loss: 1.96018 (semantic_loss: 0.01272, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19941 
Train Epoch: 50 [146/1000 4672/32000 (15%)] Loss: 1.96073 (semantic_loss: 0.01326, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19774 
Train Epoch: 50 [151/1000 4832/32000 (15%)] Loss: 1.96303 (semantic_loss: 0.01654, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19978 
Train Epoch: 50 [156/1000 4992/32000 (16%)] Loss: 1.96865 (semantic_loss: 0.02118, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19258 
Train Epoch: 50 [161/1000 5152/32000 (16%)] Loss: 1.96051 (semantic_loss: 0.01403, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20639 
Train Epoch: 50 [166/1000 5312/32000 (17%)] Loss: 1.96186 (semantic_loss: 0.01440, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18528 
Train Epoch: 50 [171/1000 5472/32000 (17%)] Loss: 1.96080 (semantic_loss: 0.01334, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18853 
Train Epoch: 50 [176/1000 5632/32000 (18%)] Loss: 1.95935 (semantic_loss: 0.01188, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19761 
Train Epoch: 50 [181/1000 5792/32000 (18%)] Loss: 1.95798 (semantic_loss: 0.01148, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24157 
Train Epoch: 50 [186/1000 5952/32000 (19%)] Loss: 1.96209 (semantic_loss: 0.01365, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21391 
Train Epoch: 50 [191/1000 6112/32000 (19%)] Loss: 1.96232 (semantic_loss: 0.01582, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19995 
Train Epoch: 50 [196/1000 6272/32000 (20%)] Loss: 1.95736 (semantic_loss: 0.01185, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.23157 
Train Epoch: 50 [201/1000 6432/32000 (20%)] Loss: 1.96419 (semantic_loss: 0.01672, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20288 
Train Epoch: 50 [206/1000 6592/32000 (21%)] Loss: 1.95961 (semantic_loss: 0.01312, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.39712 
Train Epoch: 50 [211/1000 6752/32000 (21%)] Loss: 1.96281 (semantic_loss: 0.01436, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18844 
Train Epoch: 50 [216/1000 6912/32000 (22%)] Loss: 1.96016 (semantic_loss: 0.01171, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19105 
Train Epoch: 50 [221/1000 7072/32000 (22%)] Loss: 1.96680 (semantic_loss: 0.01835, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.19569 
Train Epoch: 50 [226/1000 7232/32000 (23%)] Loss: 1.96284 (semantic_loss: 0.01440, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18617 
Train Epoch: 50 [231/1000 7392/32000 (23%)] Loss: 1.96368 (semantic_loss: 0.01523, quant_loss: 1.94824, bit_balance_loss: 0.00021) batch_time=0.18805 
Train Epoch: 50 [236/1000 7552/32000 (24%)] Loss: 1.96127 (semantic_loss: 0.01381, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18663 
Train Epoch: 50 [241/1000 7712/32000 (24%)] Loss: 1.96193 (semantic_loss: 0.01446, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18802 
Train Epoch: 50 [246/1000 7872/32000 (25%)] Loss: 1.96195 (semantic_loss: 0.01448, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18700 
Train Epoch: 50 [251/1000 8032/32000 (25%)] Loss: 1.96134 (semantic_loss: 0.01387, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.19712 
Train Epoch: 50 [256/1000 8192/32000 (26%)] Loss: 1.96501 (semantic_loss: 0.01754, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20107 
Train Epoch: 50 [261/1000 8352/32000 (26%)] Loss: 1.96136 (semantic_loss: 0.01487, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18833 
Train Epoch: 50 [266/1000 8512/32000 (27%)] Loss: 1.96315 (semantic_loss: 0.01471, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18779 
Train Epoch: 50 [271/1000 8672/32000 (27%)] Loss: 1.96063 (semantic_loss: 0.01414, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19367 
Train Epoch: 50 [276/1000 8832/32000 (28%)] Loss: 1.96358 (semantic_loss: 0.01514, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21491 
Train Epoch: 50 [281/1000 8992/32000 (28%)] Loss: 1.96140 (semantic_loss: 0.01392, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23336 
Train Epoch: 50 [286/1000 9152/32000 (29%)] Loss: 1.95923 (semantic_loss: 0.01274, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23427 
Train Epoch: 50 [291/1000 9312/32000 (29%)] Loss: 1.96116 (semantic_loss: 0.01369, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22481 
Train Epoch: 50 [296/1000 9472/32000 (30%)] Loss: 1.95905 (semantic_loss: 0.01159, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19713 
Train Epoch: 50 [301/1000 9632/32000 (30%)] Loss: 1.96080 (semantic_loss: 0.01236, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21304 
Train Epoch: 50 [306/1000 9792/32000 (31%)] Loss: 1.96303 (semantic_loss: 0.01654, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19509 
Train Epoch: 50 [311/1000 9952/32000 (31%)] Loss: 1.96057 (semantic_loss: 0.01505, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.21177 
Train Epoch: 50 [316/1000 10112/32000 (32%)] Loss: 1.96426 (semantic_loss: 0.01679, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24895 
Train Epoch: 50 [321/1000 10272/32000 (32%)] Loss: 1.96148 (semantic_loss: 0.01500, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19096 
Train Epoch: 50 [326/1000 10432/32000 (33%)] Loss: 1.95822 (semantic_loss: 0.01172, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19067 
Train Epoch: 50 [331/1000 10592/32000 (33%)] Loss: 1.96022 (semantic_loss: 0.01373, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18928 
Train Epoch: 50 [336/1000 10752/32000 (34%)] Loss: 1.96268 (semantic_loss: 0.01522, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.76928 
Train Epoch: 50 [341/1000 10912/32000 (34%)] Loss: 1.96314 (semantic_loss: 0.01567, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19725 
Train Epoch: 50 [346/1000 11072/32000 (35%)] Loss: 1.96846 (semantic_loss: 0.02001, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20826 
Train Epoch: 50 [351/1000 11232/32000 (35%)] Loss: 1.96005 (semantic_loss: 0.01259, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19110 
Train Epoch: 50 [356/1000 11392/32000 (36%)] Loss: 1.95810 (semantic_loss: 0.01258, quant_loss: 1.94531, bit_balance_loss: 0.00021) batch_time=0.20577 
Train Epoch: 50 [361/1000 11552/32000 (36%)] Loss: 1.96345 (semantic_loss: 0.01501, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18560 
Train Epoch: 50 [366/1000 11712/32000 (37%)] Loss: 1.96409 (semantic_loss: 0.01662, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18884 
Train Epoch: 50 [371/1000 11872/32000 (37%)] Loss: 1.96184 (semantic_loss: 0.01535, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24971 
Train Epoch: 50 [376/1000 12032/32000 (38%)] Loss: 1.95882 (semantic_loss: 0.01135, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18846 
Train Epoch: 50 [381/1000 12192/32000 (38%)] Loss: 1.96080 (semantic_loss: 0.01334, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18735 
Train Epoch: 50 [386/1000 12352/32000 (39%)] Loss: 1.95867 (semantic_loss: 0.01219, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19636 
Train Epoch: 50 [391/1000 12512/32000 (39%)] Loss: 1.96499 (semantic_loss: 0.01849, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19278 
Train Epoch: 50 [396/1000 12672/32000 (40%)] Loss: 1.96251 (semantic_loss: 0.01603, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21907 
Train Epoch: 50 [401/1000 12832/32000 (40%)] Loss: 1.95984 (semantic_loss: 0.01237, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18549 
Train Epoch: 50 [406/1000 12992/32000 (41%)] Loss: 1.96204 (semantic_loss: 0.01652, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18798 
Train Epoch: 50 [411/1000 13152/32000 (41%)] Loss: 1.96216 (semantic_loss: 0.01469, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18729 
Train Epoch: 50 [416/1000 13312/32000 (42%)] Loss: 1.95983 (semantic_loss: 0.01334, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19170 
Train Epoch: 50 [421/1000 13472/32000 (42%)] Loss: 1.96340 (semantic_loss: 0.01594, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18855 
Train Epoch: 50 [426/1000 13632/32000 (43%)] Loss: 1.96011 (semantic_loss: 0.01362, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18671 
Train Epoch: 50 [431/1000 13792/32000 (43%)] Loss: 1.96760 (semantic_loss: 0.01916, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18807 
Train Epoch: 50 [436/1000 13952/32000 (44%)] Loss: 1.95947 (semantic_loss: 0.01297, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.30444 
Train Epoch: 50 [441/1000 14112/32000 (44%)] Loss: 1.96377 (semantic_loss: 0.01533, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18762 
Train Epoch: 50 [446/1000 14272/32000 (45%)] Loss: 1.96151 (semantic_loss: 0.01404, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.26102 
Train Epoch: 50 [451/1000 14432/32000 (45%)] Loss: 1.96542 (semantic_loss: 0.01698, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.24109 
Train Epoch: 50 [456/1000 14592/32000 (46%)] Loss: 1.95799 (semantic_loss: 0.01247, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.22726 
Train Epoch: 50 [461/1000 14752/32000 (46%)] Loss: 1.95890 (semantic_loss: 0.01241, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20239 
Train Epoch: 50 [466/1000 14912/32000 (47%)] Loss: 1.95950 (semantic_loss: 0.01301, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23704 
Train Epoch: 50 [471/1000 15072/32000 (47%)] Loss: 1.96010 (semantic_loss: 0.01166, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21018 
Train Epoch: 50 [476/1000 15232/32000 (48%)] Loss: 1.96263 (semantic_loss: 0.01419, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20460 
Train Epoch: 50 [481/1000 15392/32000 (48%)] Loss: 1.96211 (semantic_loss: 0.01465, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19299 
Train Epoch: 50 [486/1000 15552/32000 (49%)] Loss: 1.96091 (semantic_loss: 0.01441, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.18941 
Train Epoch: 50 [491/1000 15712/32000 (49%)] Loss: 1.96099 (semantic_loss: 0.01352, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18949 
Train Epoch: 50 [496/1000 15872/32000 (50%)] Loss: 1.96207 (semantic_loss: 0.01460, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21292 
Train Epoch: 50 [501/1000 16032/32000 (50%)] Loss: 1.96097 (semantic_loss: 0.01447, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.26112 
Train Epoch: 50 [506/1000 16192/32000 (51%)] Loss: 1.96305 (semantic_loss: 0.01559, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21489 
Train Epoch: 50 [511/1000 16352/32000 (51%)] Loss: 1.96110 (semantic_loss: 0.01363, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20127 
Train Epoch: 50 [516/1000 16512/32000 (52%)] Loss: 1.96317 (semantic_loss: 0.01668, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24727 
Train Epoch: 50 [521/1000 16672/32000 (52%)] Loss: 1.96026 (semantic_loss: 0.01280, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19309 
Train Epoch: 50 [526/1000 16832/32000 (53%)] Loss: 1.96074 (semantic_loss: 0.01425, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.34745 
Train Epoch: 50 [531/1000 16992/32000 (53%)] Loss: 1.96224 (semantic_loss: 0.01575, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18606 
Train Epoch: 50 [536/1000 17152/32000 (54%)] Loss: 1.96119 (semantic_loss: 0.01275, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18670 
Train Epoch: 50 [541/1000 17312/32000 (54%)] Loss: 1.96200 (semantic_loss: 0.01454, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18779 
Train Epoch: 50 [546/1000 17472/32000 (55%)] Loss: 1.96711 (semantic_loss: 0.01965, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18559 
Train Epoch: 50 [551/1000 17632/32000 (55%)] Loss: 1.96343 (semantic_loss: 0.01498, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18683 
Train Epoch: 50 [556/1000 17792/32000 (56%)] Loss: 1.96075 (semantic_loss: 0.01231, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18625 
Train Epoch: 50 [561/1000 17952/32000 (56%)] Loss: 1.95966 (semantic_loss: 0.01317, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18514 
Train Epoch: 50 [566/1000 18112/32000 (57%)] Loss: 1.96386 (semantic_loss: 0.01542, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18654 
Train Epoch: 50 [571/1000 18272/32000 (57%)] Loss: 1.96115 (semantic_loss: 0.01368, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18751 
Train Epoch: 50 [576/1000 18432/32000 (58%)] Loss: 1.96174 (semantic_loss: 0.01427, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19044 
Train Epoch: 50 [581/1000 18592/32000 (58%)] Loss: 1.96037 (semantic_loss: 0.01291, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19625 
Train Epoch: 50 [586/1000 18752/32000 (59%)] Loss: 1.95986 (semantic_loss: 0.01435, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.18596 
Train Epoch: 50 [591/1000 18912/32000 (59%)] Loss: 1.96251 (semantic_loss: 0.01504, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19536 
Train Epoch: 50 [596/1000 19072/32000 (60%)] Loss: 1.96134 (semantic_loss: 0.01388, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19124 
Train Epoch: 50 [601/1000 19232/32000 (60%)] Loss: 1.96352 (semantic_loss: 0.01605, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22610 
Train Epoch: 50 [606/1000 19392/32000 (61%)] Loss: 1.96047 (semantic_loss: 0.01300, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.23050 
Train Epoch: 50 [611/1000 19552/32000 (61%)] Loss: 1.97119 (semantic_loss: 0.02275, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.22128 
Train Epoch: 50 [616/1000 19712/32000 (62%)] Loss: 1.96000 (semantic_loss: 0.01156, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.21857 
Train Epoch: 50 [621/1000 19872/32000 (62%)] Loss: 1.96211 (semantic_loss: 0.01561, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21587 
Train Epoch: 50 [626/1000 20032/32000 (63%)] Loss: 1.96157 (semantic_loss: 0.01313, quant_loss: 1.94824, bit_balance_loss: 0.00019) batch_time=0.20846 
Train Epoch: 50 [631/1000 20192/32000 (63%)] Loss: 1.96130 (semantic_loss: 0.01384, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22695 
Train Epoch: 50 [636/1000 20352/32000 (64%)] Loss: 1.96212 (semantic_loss: 0.01464, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.31015 
Train Epoch: 50 [641/1000 20512/32000 (64%)] Loss: 1.95915 (semantic_loss: 0.01266, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19067 
Train Epoch: 50 [646/1000 20672/32000 (65%)] Loss: 1.96315 (semantic_loss: 0.01569, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21891 
Train Epoch: 50 [651/1000 20832/32000 (65%)] Loss: 1.95965 (semantic_loss: 0.01317, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18855 
Train Epoch: 50 [656/1000 20992/32000 (66%)] Loss: 1.95998 (semantic_loss: 0.01348, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.72864 
Train Epoch: 50 [661/1000 21152/32000 (66%)] Loss: 1.96119 (semantic_loss: 0.01373, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18825 
Train Epoch: 50 [666/1000 21312/32000 (67%)] Loss: 1.95970 (semantic_loss: 0.01223, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19575 
Train Epoch: 50 [671/1000 21472/32000 (67%)] Loss: 1.95937 (semantic_loss: 0.01191, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19287 
Train Epoch: 50 [676/1000 21632/32000 (68%)] Loss: 1.96033 (semantic_loss: 0.01384, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20716 
Train Epoch: 50 [681/1000 21792/32000 (68%)] Loss: 1.95966 (semantic_loss: 0.01415, quant_loss: 1.94531, bit_balance_loss: 0.00020) batch_time=0.20544 
Train Epoch: 50 [686/1000 21952/32000 (69%)] Loss: 1.95977 (semantic_loss: 0.01230, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18525 
Train Epoch: 50 [691/1000 22112/32000 (69%)] Loss: 1.96443 (semantic_loss: 0.01794, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.24960 
Train Epoch: 50 [696/1000 22272/32000 (70%)] Loss: 1.96109 (semantic_loss: 0.01362, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18373 
Train Epoch: 50 [701/1000 22432/32000 (70%)] Loss: 1.95836 (semantic_loss: 0.01187, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18386 
Train Epoch: 50 [706/1000 22592/32000 (71%)] Loss: 1.96356 (semantic_loss: 0.01608, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18794 
Train Epoch: 50 [711/1000 22752/32000 (71%)] Loss: 1.96269 (semantic_loss: 0.01522, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18820 
Train Epoch: 50 [716/1000 22912/32000 (72%)] Loss: 1.96243 (semantic_loss: 0.01496, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21617 
Train Epoch: 50 [721/1000 23072/32000 (72%)] Loss: 1.96294 (semantic_loss: 0.01548, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18621 
Train Epoch: 50 [726/1000 23232/32000 (73%)] Loss: 1.96042 (semantic_loss: 0.01295, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19607 
Train Epoch: 50 [731/1000 23392/32000 (73%)] Loss: 1.96591 (semantic_loss: 0.01746, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18751 
Train Epoch: 50 [736/1000 23552/32000 (74%)] Loss: 1.96232 (semantic_loss: 0.01584, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18750 
Train Epoch: 50 [741/1000 23712/32000 (74%)] Loss: 1.96012 (semantic_loss: 0.01266, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19802 
Train Epoch: 50 [746/1000 23872/32000 (75%)] Loss: 1.96101 (semantic_loss: 0.01355, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18888 
Train Epoch: 50 [751/1000 24032/32000 (75%)] Loss: 1.96391 (semantic_loss: 0.01644, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18763 
Train Epoch: 50 [756/1000 24192/32000 (76%)] Loss: 1.95973 (semantic_loss: 0.01129, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.29763 
Train Epoch: 50 [761/1000 24352/32000 (76%)] Loss: 1.96070 (semantic_loss: 0.01323, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18508 
Train Epoch: 50 [766/1000 24512/32000 (77%)] Loss: 1.96048 (semantic_loss: 0.01399, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21690 
Train Epoch: 50 [771/1000 24672/32000 (77%)] Loss: 1.96118 (semantic_loss: 0.01274, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.20559 
Train Epoch: 50 [776/1000 24832/32000 (78%)] Loss: 1.95954 (semantic_loss: 0.01208, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21775 
Train Epoch: 50 [781/1000 24992/32000 (78%)] Loss: 1.96024 (semantic_loss: 0.01278, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.24907 
Train Epoch: 50 [786/1000 25152/32000 (79%)] Loss: 1.95981 (semantic_loss: 0.01234, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21166 
Train Epoch: 50 [791/1000 25312/32000 (79%)] Loss: 1.96427 (semantic_loss: 0.01583, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19195 
Train Epoch: 50 [796/1000 25472/32000 (80%)] Loss: 1.95730 (semantic_loss: 0.01081, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19124 
Train Epoch: 50 [801/1000 25632/32000 (80%)] Loss: 1.96280 (semantic_loss: 0.01631, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.19813 
Train Epoch: 50 [806/1000 25792/32000 (81%)] Loss: 1.96341 (semantic_loss: 0.01497, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19237 
Train Epoch: 50 [811/1000 25952/32000 (81%)] Loss: 1.96150 (semantic_loss: 0.01404, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18823 
Train Epoch: 50 [816/1000 26112/32000 (82%)] Loss: 1.95725 (semantic_loss: 0.01076, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18853 
Train Epoch: 50 [821/1000 26272/32000 (82%)] Loss: 1.96099 (semantic_loss: 0.01449, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.24558 
Train Epoch: 50 [826/1000 26432/32000 (83%)] Loss: 1.96202 (semantic_loss: 0.01553, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.20385 
Train Epoch: 50 [831/1000 26592/32000 (83%)] Loss: 1.96059 (semantic_loss: 0.01313, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19357 
Train Epoch: 50 [836/1000 26752/32000 (84%)] Loss: 1.95867 (semantic_loss: 0.01218, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.27593 
Train Epoch: 50 [841/1000 26912/32000 (84%)] Loss: 1.95853 (semantic_loss: 0.01106, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19936 
Train Epoch: 50 [846/1000 27072/32000 (85%)] Loss: 1.96852 (semantic_loss: 0.02008, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.36036 
Train Epoch: 50 [851/1000 27232/32000 (85%)] Loss: 1.96030 (semantic_loss: 0.01283, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18934 
Train Epoch: 50 [856/1000 27392/32000 (86%)] Loss: 1.96155 (semantic_loss: 0.01408, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18941 
Train Epoch: 50 [861/1000 27552/32000 (86%)] Loss: 1.96492 (semantic_loss: 0.01648, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18541 
Train Epoch: 50 [866/1000 27712/32000 (87%)] Loss: 1.95885 (semantic_loss: 0.01236, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18857 
Train Epoch: 50 [871/1000 27872/32000 (87%)] Loss: 1.96046 (semantic_loss: 0.01397, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18755 
Train Epoch: 50 [876/1000 28032/32000 (88%)] Loss: 1.96387 (semantic_loss: 0.01641, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19732 
Train Epoch: 50 [881/1000 28192/32000 (88%)] Loss: 1.96207 (semantic_loss: 0.01362, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18848 
Train Epoch: 50 [886/1000 28352/32000 (89%)] Loss: 1.96289 (semantic_loss: 0.01542, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18452 
Train Epoch: 50 [891/1000 28512/32000 (89%)] Loss: 1.95898 (semantic_loss: 0.01151, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20507 
Train Epoch: 50 [896/1000 28672/32000 (90%)] Loss: 1.96125 (semantic_loss: 0.01377, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18711 
Train Epoch: 50 [901/1000 28832/32000 (90%)] Loss: 1.96200 (semantic_loss: 0.01551, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.19496 
Train Epoch: 50 [906/1000 28992/32000 (91%)] Loss: 1.95815 (semantic_loss: 0.01166, quant_loss: 1.94629, bit_balance_loss: 0.00021) batch_time=0.20985 
Train Epoch: 50 [911/1000 29152/32000 (91%)] Loss: 1.95916 (semantic_loss: 0.01170, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18905 
Train Epoch: 50 [916/1000 29312/32000 (92%)] Loss: 1.96049 (semantic_loss: 0.01204, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.18922 
Train Epoch: 50 [921/1000 29472/32000 (92%)] Loss: 1.95851 (semantic_loss: 0.01104, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.18890 
Train Epoch: 50 [926/1000 29632/32000 (93%)] Loss: 1.95870 (semantic_loss: 0.01123, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20588 
Train Epoch: 50 [931/1000 29792/32000 (93%)] Loss: 1.96128 (semantic_loss: 0.01382, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.21857 
Train Epoch: 50 [936/1000 29952/32000 (94%)] Loss: 1.96220 (semantic_loss: 0.01473, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.23264 
Train Epoch: 50 [941/1000 30112/32000 (94%)] Loss: 1.96322 (semantic_loss: 0.01576, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.20974 
Train Epoch: 50 [946/1000 30272/32000 (95%)] Loss: 1.96176 (semantic_loss: 0.01430, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19472 
Train Epoch: 50 [951/1000 30432/32000 (95%)] Loss: 1.96210 (semantic_loss: 0.01561, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.23232 
Train Epoch: 50 [956/1000 30592/32000 (96%)] Loss: 1.95972 (semantic_loss: 0.01225, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.26454 
Train Epoch: 50 [961/1000 30752/32000 (96%)] Loss: 1.96235 (semantic_loss: 0.01586, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.21016 
Train Epoch: 50 [966/1000 30912/32000 (97%)] Loss: 1.96540 (semantic_loss: 0.01793, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.22581 
Train Epoch: 50 [971/1000 31072/32000 (97%)] Loss: 1.96073 (semantic_loss: 0.01228, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19597 
Train Epoch: 50 [976/1000 31232/32000 (98%)] Loss: 1.96834 (semantic_loss: 0.02087, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.84507 
Train Epoch: 50 [981/1000 31392/32000 (98%)] Loss: 1.96726 (semantic_loss: 0.01979, quant_loss: 1.94727, bit_balance_loss: 0.00021) batch_time=0.18547 
Train Epoch: 50 [986/1000 31552/32000 (99%)] Loss: 1.96190 (semantic_loss: 0.01541, quant_loss: 1.94629, bit_balance_loss: 0.00020) batch_time=0.18739 
Train Epoch: 50 [991/1000 31712/32000 (99%)] Loss: 1.95883 (semantic_loss: 0.01136, quant_loss: 1.94727, bit_balance_loss: 0.00020) batch_time=0.19525 
Train Epoch: 50 [996/1000 31872/32000 (100%)] Loss: 1.96211 (semantic_loss: 0.01367, quant_loss: 1.94824, bit_balance_loss: 0.00020) batch_time=0.19648 
Saving checkpoint: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/checkpoint-epoch50.pth ...
Done in 5.143s
removing stale ckpt [epoch 49] [took 0.01s]
 epoch          : 50
 loss           : 1.961635125875473
 learning_rate  : 2.8632084485111774e-07
 n_samples      : 1600000
 n_steps        : 50000
 ActivityNet_val1_test/t2v_metrics/R1: 12.121212121212121
 ActivityNet_val1_test/t2v_metrics/R5: 38.783811267032746
 ActivityNet_val1_test/t2v_metrics/R10: 55.541997152735405
 ActivityNet_val1_test/t2v_metrics/R50: 84.74679682733374
 ActivityNet_val1_test/t2v_metrics/MedR: 8.5
 ActivityNet_val1_test/t2v_metrics/MeanR: 62.92332723205207
 ActivityNet_val1_test/t2v_metrics/geometric_mean_R1-R5-R10: 29.666934722783196
 ActivityNet_val1_test/v2t_metrics/R1: 12.304250559284116
 ActivityNet_val1_test/v2t_metrics/R5: 39.43461460239984
 ActivityNet_val1_test/v2t_metrics/R10: 55.847061216188735
 ActivityNet_val1_test/v2t_metrics/R50: 84.92983526540573
 ActivityNet_val1_test/v2t_metrics/MedR: 8.0
 ActivityNet_val1_test/v2t_metrics/MeanR: 64.89526133821437
 ActivityNet_val1_test/v2t_metrics/geometric_mean_R1-R5-R10: 30.036156874602593
 mnt_best       : 29.76339460386141
 not_improved_count: 8
Final evaluation ...
Loading checkpoint from: /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/trained_model.pth ...
Ckpt loaded at epoch 42.
Saved v2t similarity matrix to /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/ActivityNet-test-sims.npy
ActivityNet_val1_test:
 t2v_metrics/R1/final_eval: 12.34492576774456
 t2v_metrics/R5/final_eval: 38.39739678665853
 t2v_metrics/R10/final_eval: 55.623347569656296
 t2v_metrics/R50/final_eval: 84.62477120195241
 t2v_metrics/MedR/final_eval: 8.5
 t2v_metrics/MeanR/final_eval: 63.4082774049217
 t2v_metrics/geometric_mean_R1-R5-R10/final_eval: 29.76339460386141
 v2t_metrics/R1/final_eval: 12.446613788895668
 v2t_metrics/R5/final_eval: 39.190563351637174
 v2t_metrics/R10/final_eval: 55.521659548505184
 v2t_metrics/R50/final_eval: 84.58409599349197
 v2t_metrics/MedR/final_eval: 8.5
 v2t_metrics/MeanR/final_eval: 65.42942851332113
 v2t_metrics/geometric_mean_R1-R5-R10/final_eval: 30.030672167495368
Best epoch for the monitored metric: 42
Script took 04h21m37s
The best performing ckpt can be found at /apdcephfs/share_47076/gimwang/HCQ/exps/DCMH_ActivityNet/trained_model.pth
